import os
os.environ['FLAGS_cinn_new_group_scheduler'] = '1'
os.environ['FLAGS_group_schedule_tiling_first'] = '1'
os.environ['FLAGS_enable_pir_api'] = '1'
os.environ['FLAGS_cinn_bucket_compile'] = '1'
import sys
import unittest
import numpy as np
from dataclasses import dataclass
import typing as t
import itertools

@dataclass
class Stage:
    name: str
    env_vars: t.Dict[str, str]

cinn_stages = [
    Stage(
        name="dynamic_to_static",
        env_vars=dict(
            PADDLE_DEBUG_ENABLE_CINN=False,
            FLAGS_prim_all=False,
            FLAGS_prim_enable_dynamic=False,
        ),
    ),
    Stage(
        name="prim",
        env_vars=dict(
            PADDLE_DEBUG_ENABLE_CINN=False,
            FLAGS_prim_all=True,
            FLAGS_prim_enable_dynamic=True,
        ),
    ),
    Stage(
        name="infer_symbolic",
        env_vars=dict(
            PADDLE_DEBUG_ENABLE_CINN=False,
            FLAGS_prim_all=True,
            FLAGS_prim_enable_dynamic=True,
            FLAGS_use_cinn=False,
            FLAGS_check_infer_symbolic=True,
        ),
    ),
	Stage(
        name="frontend",
        env_vars=dict(
            PADDLE_DEBUG_ENABLE_CINN=True,
            FLAGS_prim_all=True,
            FLAGS_prim_enable_dynamic=True,
            FLAGS_use_cinn=True,
            FLAGS_check_infer_symbolic=False,
            FLAGS_enable_fusion_fallback=True,
        ), 
    ),
    Stage(
        name="backend",
        env_vars=dict(
            PADDLE_DEBUG_ENABLE_CINN=True,
            FLAGS_prim_all=True,
            FLAGS_prim_enable_dynamic=True,
            FLAGS_use_cinn=True,
            FLAGS_check_infer_symbolic=False,
            FLAGS_enable_fusion_fallback=False,
        ), 
    ),
]

def GetCinnStageByName(name):
    for stage in cinn_stages:
        if stage.name == name:
            return stage
    return None

def GetCurrentCinnStage():
    name = os.getenv('PADDLE_DEBUG_CINN_STAGE_NAME')
    if name is None:
        return None
    stage_names = [stage.name for stage in cinn_stages]
    assert name in stage_names, (
        f"PADDLE_DEBUG_CINN_STAGE_NAME should be in {stage_names}"
    )
    return GetCinnStageByName(name)

def GetPrevCinnStage(stage):
    for i in range(1, len(cinn_stages)):
        if stage is cinn_stages[i]:
            return cinn_stages[i - 1]
    return None

def IsCinnStageEnableDiff():
    value = os.getenv('PADDLE_DEBUG_CINN_STAGE_ENABLE_DIFF')
    enabled = value in {
        '1',
        'true',
        'True',
    }
    if enabled:
        assert GetCurrentCinnStage() is not None
    return enabled

def GetExitCodeAndStdErr(cmd, env):
    env = {
        k:v
        for k, v in env.items()
        if v is not None
    }
    import subprocess
    result = subprocess.run(
        cmd,
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        text=True,
        env=env,
    )
    return result.returncode, result.stderr

def GetStageExitCodeAndStdErr(stage):
    return GetExitCodeAndStdErr(
        [sys.executable, __file__],
        env=dict(
            PADDLE_DEBUG_CINN_STAGE_NAME=stage.name,
            PADDLE_DEBUG_CINN_STAGE_ENABLE_DIFF='0',
            PYTHONPATH=os.getenv('PYTHONPATH'),
            ATHENA_ENABLE_TRY_RUN="False",
        ),
    )

def AthenaTryRunEnabled():
    return os.getenv('ATHENA_ENABLE_TRY_RUN') not in {
        "0",
        "False",
        "false",
        "OFF"
    }

def GetNeedSkipAndSkipMessage():
    current_stage = GetCurrentCinnStage()
    assert current_stage is not None
    if not IsCinnStageEnableDiff():
        return False, ""
    last_stage = GetPrevCinnStage(current_stage)
    if last_stage is None:
        return False, ""
    exitcode, stderr = GetStageExitCodeAndStdErr(last_stage)
    if exitcode != 0:
        return True, "last stage failed."
    return False, ""

def GetCurrentStageTryRunExitCodeAndStdErr():
    if not AthenaTryRunEnabled():
        return False, ""
    current_stage = GetCurrentCinnStage()
    assert current_stage is not None
    return GetStageExitCodeAndStdErr(current_stage)

def SetDefaultEnv(**env_var2value):
    for env_var, value in env_var2value.items():
        if os.getenv(env_var) is None:
            os.environ[env_var] = str(value)

SetDefaultEnv(
    PADDLE_DEBUG_CINN_STAGE_NAME="backend",
    PADDLE_DEBUG_CINN_STAGE_ENABLE_DIFF=False,
    PADDLE_DEBUG_ENABLE_CINN=True,
    FLAGS_enable_pir_api=True,
    FLAGS_prim_all=True,
    FLAGS_prim_enable_dynamic=True,
    FLAGS_use_cinn=False,
    FLAGS_check_infer_symbolic=False,
    FLAGS_enable_fusion_fallback=False,
)

import paddle

def SetEnvVar(env_var2value):
    for env_var, value in env_var2value.items():
        os.environ[env_var] = str(value)
    paddle.set_flags({
        env_var:value
        for env_var, value in env_var2value.items()
        if env_var.startswith('FLAGS_')
    })

if GetCurrentCinnStage() is not None:
    SetEnvVar(GetCurrentCinnStage().env_vars)

def GetEnvVarEnableJit():
    enable_jit = os.getenv('PADDLE_DEBUG_ENABLE_JIT')
    return enable_jit not in {
        "0",
        "False",
        "false",
        "OFF",
    }

def GetEnvVarEnableCinn():
    enable_cinn = os.getenv('PADDLE_DEBUG_ENABLE_CINN')
    if enable_cinn is None:
        return True
    return enable_cinn not in {
        "0",
        "False",
        "false",
        "OFF",
    }


def GetTolerance(dtype):
    if dtype == np.float16:
        return GetFloat16Tolerance()
    if dtype == np.float32:
        return GetFloat32Tolerance()
    return 1e-6

def GetFloat16Tolerance():
    try:
        return float(os.getenv('PADDLE_DEBUG_FLOAT16_TOL'))
    except:
        return 1e-3

def GetFloat32Tolerance():
    try:
        return float(os.getenv('PADDLE_DEBUG_FLOAT32_TOL'))
    except:
        return 1e-6

def IsInteger(dtype):
    return np.dtype(dtype).char in np.typecodes['AllInteger']

def ApplyToStatic(net, use_cinn):
    build_strategy = paddle.static.BuildStrategy()
    build_strategy.build_cinn_pass = use_cinn
    return paddle.jit.to_static(
        net,
        input_spec=net.get_input_spec(),
        build_strategy=build_strategy,
        full_graph=True,
    )

class InstanceTrait:

    @classmethod
    def instance(cls):
        if cls.instance_ is None:
            cls.instance_ = cls()
        return cls.instance_

    @classmethod
    def static_instance_with_cinn(cls):
        if cls.static_instance_with_cinn_ is None:
            cls.static_instance_with_cinn_ = ApplyToStatic(
                cls.instance(),
                use_cinn=True
            )
        return cls.static_instance_with_cinn_

    @classmethod
    def static_instance_without_cinn(cls):
        if cls.static_instance_without_cinn_ is None:
            cls.static_instance_without_cinn_ = ApplyToStatic(
                cls.instance(),
                use_cinn=False
            )
        return cls.static_instance_without_cinn_


class CinnTestBase:

    def setUp(self):
        paddle.seed(2024)
        self.prepare_data()

    def _test_entry(self):
        dy_outs = self.train(use_cinn=False)
        cinn_outs = self.train(use_cinn=GetEnvVarEnableCinn())

        for cinn_out, dy_out in zip(cinn_outs, dy_outs):
          if type(cinn_out) is list and type(dy_out) is list:
            for x, y in zip(cinn_out, dy_out):
              self.assert_all_close(x, y)
          else:
            self.assert_all_close(cinn_out, dy_out)

    def train(self, use_cinn):
        if GetEnvVarEnableJit():
            net = self.prepare_static_net(use_cinn)
        else:
            net = self.prepare_net()
        paddle.seed(2024)
        out = net(*self.inputs)
        return out
    
    def prepare_data(self):
        self.inputs = self.get_inputs()
        for input in self.inputs:
            input.stop_gradient = True

    def prepare_net(self):
        return self.get_test_class().instance()

    def prepare_static_net(self, use_cinn):
        if use_cinn:
            return self.get_test_class().static_instance_with_cinn()
        else:
            return self.get_test_class().static_instance_without_cinn()

    def assert_all_close(self, x, y):
        if (hasattr(x, "numpy") and hasattr(y, "numpy")):
            x_numpy = x.numpy()
            y_numpy = y.numpy()
            assert x_numpy.dtype == y_numpy.dtype
            if IsInteger(x_numpy.dtype):
                np.testing.assert_equal(x_numpy, y_numpy)
            else:
                tol = GetTolerance(x_numpy.dtype)
                np.testing.assert_allclose(x_numpy, y_numpy, atol=tol, rtol=tol)
        else:
            assert x == y





need_skip, skip_message = GetNeedSkipAndSkipMessage()
try_run_exit_code, try_run_stderr = GetCurrentStageTryRunExitCodeAndStdErr()
class TestTryRun(unittest.TestCase):
    def test_panic(self):
        if not AthenaTryRunEnabled():
            return
        if try_run_exit_code == 0:
            # All unittest cases passed.
            return
        if try_run_exit_code > 0:
            # program failed but not panic.
            return
        # program panicked.
        kOutputLimit = 65536
        message = try_run_stderr[-kOutputLimit:]
        raise RuntimeError(f"panicked. last {kOutputLimit} characters of stderr: \n{message}")
class PrimitiveOp_1a2b114b88ba56b95370f75ce469d256(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c0876997be363f1cdc0d5dbb3d88d4c2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_4497fcdfb9639144ea558daa18214543(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ca41427217a7d7bf58dbe08cc973d267(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 28, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.2539108991622925, 0.04361545667052269, 0.263587087392807, 0.2069195657968521, 0.2471093386411667, 0.4766791760921478, 0.41965770721435547, 0.21034061908721924, 0.292338490486145, 0.42772144079208374, 0.21356350183486938, 0.08689208328723907, 0.08150310069322586, 0.23691166937351227, 0.22830720245838165, 0.4269714951515198, 0.29193150997161865, 0.49656811356544495, 0.43665221333503723, 0.031288646161556244, 0.41861292719841003, 0.06521023064851761, 0.46340489387512207, 0.45454490184783936, 0.12563985586166382, 0.010602814145386219, 0.10884451866149902, 0.4942816197872162], dtype='float32').reshape([28]),
            paddle.to_tensor([0.15618936717510223, 0.11180835962295532, 0.3263687789440155, 0.038229674100875854, 0.348064661026001, 0.16788890957832336, 0.09844309091567993, 0.429990291595459, 0.39049410820007324, 0.18933860957622528, 0.25609806180000305, 0.39475056529045105, 0.28761354088783264, 0.3684166967868805, 0.46172982454299927, 0.2266560196876526, 0.08382529020309448, 0.34136855602264404, 0.39962807297706604, 0.3485899567604065, 0.15875869989395142, 0.37738972902297974, 0.09109848737716675, 0.2764112651348114, 0.28949910402297974, 0.19043467938899994, 0.2698177397251129, 0.25872018933296204], dtype='float32').reshape([28]),
            paddle.to_tensor([0.3143777847290039, 0.19119176268577576, 0.019883105531334877, 0.20340706408023834, 0.1131647378206253, 0.34421291947364807, 0.44806522130966187, 0.21163536608219147, 0.3398559093475342, 0.33656126260757446, 0.22847384214401245, 0.40726128220558167, 0.2516961097717285, 0.08685428649187088, 0.17534829676151276, 0.13225336372852325, 0.17841807007789612, 0.01027805358171463, 0.21219880878925323, 0.24117104709148407, 0.30806660652160645, 0.07812125235795975, 0.47406262159347534, 0.21750123798847198, 0.3979859948158264, 0.009707232937216759, 0.14154750108718872, 0.47721415758132935], dtype='float32').reshape([28]),
            paddle.to_tensor([0.36690863966941833, 0.2631989121437073, 0.14965470135211945, 0.1476171761751175, 0.4324318766593933, 0.4872628450393677, 0.28673428297042847, 0.011428373865783215, 0.17574751377105713, 0.05396158620715141, 0.18491263687610626, 0.28355300426483154, 0.4076497554779053, 0.49591001868247986, 0.011547297239303589, 0.1470034271478653, 0.49354875087738037, 0.4496054947376251, 0.17693929374217987, 0.46430402994155884, 0.0805819183588028, 0.1655082255601883, 0.39510393142700195, 0.46550917625427246, 0.452041357755661, 0.07492803782224655, 0.24619591236114502, 0.4667913317680359], dtype='float32').reshape([28]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_da4d85bc213f70819a72e68f21214cdb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9a5e4fd533f3ef908a0a0957f57690ca(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_da4d85bc213f70819a72e68f21214cdb
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 32, 8, 8], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0366cd303febed6bbb667de44d70348e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fad3d9727d031a0118c16d8c6f47380a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 512, 1024], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.41202685236930847, 0.04122728109359741, 0.2807959318161011, 0.2580726146697998, 0.33695849776268005, 0.03564431518316269, 0.10749317705631256, 0.28247952461242676, 0.12237633019685745, 0.2887946665287018, 0.3454401195049286, 0.3363429009914398, 0.012799903750419617, 0.04540253430604935, 0.47678259015083313, 0.30467742681503296], dtype='float32').reshape([16]),
            paddle.to_tensor([0.05292407423257828, 0.41896724700927734, 0.14862217009067535, 0.26402634382247925, 0.06765971332788467, 0.3420906960964203, 0.16873277723789215, 0.25719472765922546, 0.17976972460746765, 0.31599414348602295, 0.008386271074414253, 0.22695578634738922, 0.2068493813276291, 0.14713212847709656, 0.33895343542099, 0.10526449233293533], dtype='float32').reshape([16]),
            paddle.to_tensor([0.05421803146600723, 0.38380667567253113, 0.016555720940232277, 0.2654115855693817, 0.28236544132232666, 0.2863174378871918, 0.054602835327386856, 0.002855246886610985, 0.1182645782828331, 0.24404476583003998, 0.2766186594963074, 0.33402806520462036, 0.2979277968406677, 0.19067910313606262, 0.4909893572330475, 0.29654330015182495], dtype='float32').reshape([16]),
            paddle.to_tensor([0.258965402841568, 0.4612809419631958, 0.3600257635116577, 0.11828668415546417, 0.014000327326357365, 0.48399946093559265, 0.024162016808986664, 0.13872437179088593, 0.16768857836723328, 0.024028977379202843, 0.18975214660167694, 0.35615089535713196, 0.11261200904846191, 0.3402208387851715, 0.3608933985233307, 0.3291870057582855], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_1a8301b810bccba10e4dfdee0df73334(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f5c2b63aa04256c2e96bbc7aefcc7de3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a8301b810bccba10e4dfdee0df73334
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cfca0abcb28d56c468721ebd0aeedf64(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_85b3ea24693c6539d08bc70f949fb8d1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 80, 20, 20], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6145d805b9d2025bec8ae85e89b783ed(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 16, 12], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f69a2e73735b68ca9a3ae9e3efe0d08a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c99fde2121ca9becd4ba587e9aaf98c4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8a6a060c8f346231c0eb898a1e937ab3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 120, 120], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_63f9501466b901100aa4faa14f405b10(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 28, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.16296973824501038, 0.22734597325325012, 0.40963172912597656, 0.03495505079627037, 0.07976435124874115, 0.4725581407546997, 0.3516472578048706, 0.13801158964633942, 0.3728197515010834, 0.18825800716876984, 0.06629636138677597, 0.4982999563217163, 0.3733232319355011, 0.35072827339172363, 0.2614023983478546, 0.31265270709991455, 0.16617098450660706, 0.04870958253741264, 0.07954652607440948, 0.04833688586950302, 0.07400215417146683, 0.25001052021980286, 0.2967578172683716, 0.14989252388477325, 0.29912659525871277, 0.12440451234579086, 0.4957898259162903, 0.4655647277832031], dtype='float32').reshape([28]),
            paddle.to_tensor([0.14807374775409698, 0.41570812463760376, 0.2520654499530792, 0.4062891900539398, 0.11600346863269806, 0.0027054608799517155, 0.2514190971851349, 0.48674821853637695, 0.13023371994495392, 0.34379690885543823, 0.41301071643829346, 0.43783271312713623, 0.31503745913505554, 0.20311328768730164, 0.10106604546308517, 0.44623956084251404, 0.056292757391929626, 0.36461448669433594, 0.21093985438346863, 0.4388721287250519, 0.08345503360033035, 0.35863444209098816, 0.3750538229942322, 0.36475566029548645, 0.3526310622692108, 0.03406931832432747, 0.40011778473854065, 0.4767630398273468], dtype='float32').reshape([28]),
            paddle.to_tensor([0.37057986855506897, 0.4355347752571106, 0.40480437874794006, 0.21327053010463715, 0.1557532697916031, 0.21225082874298096, 0.030170049518346786, 0.48916861414909363, 0.30457746982574463, 0.4732431173324585, 0.41410118341445923, 0.2133956402540207, 0.006241437513381243, 0.13760297000408173, 0.239761084318161, 0.1794075071811676, 0.4512205421924591, 0.15737284719944, 0.016684407368302345, 0.030260467901825905, 0.0746133103966713, 0.2454521805047989, 0.34709298610687256, 0.08024758100509644, 0.16353031992912292, 0.20990529656410217, 0.2570462226867676, 0.07247856259346008], dtype='float32').reshape([28]),
            paddle.to_tensor([0.15841226279735565, 0.03130137920379639, 0.06843864917755127, 0.3981837332248688, 0.1831233650445938, 0.49987122416496277, 0.15454469621181488, 0.010170729830861092, 0.01116741169244051, 0.3825562596321106, 0.07342255115509033, 0.2936808168888092, 0.1533186435699463, 0.026778284460306168, 0.3856906294822693, 0.3646647334098816, 0.25454002618789673, 0.0416545607149601, 0.3896181285381317, 0.044322557747364044, 0.03439031541347504, 0.40118640661239624, 0.20644840598106384, 0.071788489818573, 0.2064240425825119, 0.22150081396102905, 0.2877882421016693, 0.4419819116592407], dtype='float32').reshape([28]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_94507acd165b07b6baa4d8d0246e1900(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 4, 26], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e7a9764e0434639b5aa0462bb4e10c00(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 1, 1], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a56f45f32136d8931f4bd1978a289cf6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 40, 40], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1ad9879d9fcee3a1929dbc8acc726c49(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 1776, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([1776], dtype='float32', min=0, max=0.5),
            paddle.uniform([1776], dtype='float32', min=0, max=0.5),
            paddle.uniform([1776], dtype='float32', min=0, max=0.5),
            paddle.uniform([1776], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cd9aae4fc180c83520e004d951968db9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b85e75b2975728d83795b5ff050672a7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 8, 8], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_84d939733796fc236cb10d79b8053a56(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 384, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8475c464a7ea42b8ab132bd49c791dea(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 20, 20], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_43b7fef28f60b921a302b4d5bd0c9f06(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c0eeefefc0b8b59802798f7eda3591e5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 8, 6], dtype='float16', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_af91295b59c5fca17f890a802deb001f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f15d84b32f3d2376412361a552a03bbf(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_af91295b59c5fca17f890a802deb001f
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 32, 32, 32], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.11314581334590912, 0.3839322626590729, 0.23581358790397644, 0.21412231028079987, 0.48091524839401245, 0.4215567708015442, 0.2383139729499817, 0.02207397110760212, 0.2752912938594818, 0.028455177322030067, 0.1340927928686142, 0.4080776274204254, 0.4055410623550415, 0.4834873676300049, 0.1921909749507904, 0.07804224640130997], dtype='float32').reshape([16]),
            paddle.to_tensor([0.29647600650787354, 0.19420650601387024, 0.34600716829299927, 0.19377531111240387, 0.36046484112739563, 0.008294548839330673, 0.021269522607326508, 0.33865734934806824, 0.36485376954078674, 0.19875971972942352, 0.17328959703445435, 0.2222781926393509, 0.24363501369953156, 0.4580070376396179, 0.01575845666229725, 0.49582937359809875], dtype='float32').reshape([16]),
            paddle.to_tensor([0.23530039191246033, 0.2920255661010742, 0.26411062479019165, 0.20199835300445557, 0.4208296239376068, 0.46495285630226135, 0.05554424226284027, 0.19006969034671783, 0.48733246326446533, 0.007315059192478657, 0.11730415374040604, 0.34508639574050903, 0.47651487588882446, 0.05506262183189392, 0.18452338874340057, 0.2454373985528946], dtype='float32').reshape([16]),
            paddle.to_tensor([0.42911335825920105, 0.31127414107322693, 0.24580705165863037, 0.4342988431453705, 0.11772502958774567, 0.4779110252857208, 0.172725647687912, 0.22616980969905853, 0.2999953031539917, 0.3780989646911621, 0.28443434834480286, 0.15984678268432617, 0.05706149712204933, 0.11988896876573563, 0.093915656208992, 0.4460354447364807], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fde2b4dd08efa7bcbdf3d088fe09e9e3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 144, 16, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2ed4c31fcd8c6db6a7350e137957e302(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 640, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_851cb4eebe53841cc73526feded6db07(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 60, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1607281dae9b06cea99104cef7fb27c9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 160, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_be54e93bdd5284ccf261683f4fa893c4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f27b73d4f01bbe03786b1a2c1cd9ca5b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 2048, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f2d9bdf1d2964eadc3bf99e0b1aa186a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 80, 80], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1ccb5464a2502be094df9404274e1449(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 104, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_68f2cd7a65d59005d894f319802fd6ab(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_58c3adacf0d2f4b7df3d8db3096515c4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7cf0e49397bc65bd9b722f9b03a00623(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 36, 64, 128], dtype='float16', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_41b6de854fd474036a513ea9a448d340(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 61, 61], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b9f2630dedb605cb6cc4d1e88a6fabd0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d023fc383f4c2160e6d1bd7a3050ed22(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e54a6a35d70cac51a3d14aa2063b9f61(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 50, 84], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1b1c7e4c8d991282aa0d0aa711fd3448(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 175, 25], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_176ec3a270238334358328d07a041566(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 240, 240], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fe1797688acc54ad18730fa2adcedda9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 120, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f4ff2de90f1e2053179dce16bb07149a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.3881002962589264, 0.4339942932128906, 0.41947129368782043, 0.4979628324508667, 0.4682668149471283, 0.46449539065361023, 0.4334024488925934, 0.26876235008239746, 0.029167573899030685, 0.4194330871105194, 0.22840583324432373, 0.41935184597969055, 0.06664084643125534, 0.12117214500904083, 0.29442116618156433, 0.25516945123672485, 0.16272327303886414, 0.06830023229122162, 0.3079259395599365, 0.22719846665859222, 0.24224857985973358, 0.012456453405320644, 0.47259610891342163, 0.07342389225959778], dtype='float32').reshape([24]),
            paddle.to_tensor([0.44337302446365356, 0.3464604914188385, 0.3220490515232086, 0.06321143358945847, 0.03147834911942482, 0.3523229658603668, 0.015028126537799835, 0.22444666922092438, 0.47689494490623474, 0.23279748857021332, 0.10935217887163162, 0.3091333210468292, 0.16892796754837036, 0.1398472785949707, 0.46642202138900757, 0.2597234547138214, 0.13829253613948822, 0.009073890745639801, 0.2780194580554962, 0.08570413291454315, 0.40685150027275085, 0.36270469427108765, 0.46108049154281616, 0.2897336483001709], dtype='float32').reshape([24]),
            paddle.to_tensor([0.42953595519065857, 0.477120578289032, 0.37403860688209534, 0.492804616689682, 0.0949540063738823, 0.494461327791214, 0.13508673012256622, 0.3597694933414459, 0.1339888721704483, 0.44430264830589294, 0.39732953906059265, 0.46136999130249023, 0.3830154836177826, 0.03682051971554756, 0.2845030426979065, 0.016863832250237465, 0.20402471721172333, 0.12512005865573883, 0.39072689414024353, 0.19613586366176605, 0.31791090965270996, 0.01050667092204094, 0.39505666494369507, 0.30272525548934937], dtype='float32').reshape([24]),
            paddle.to_tensor([0.3697007894515991, 0.0897369384765625, 0.24825139343738556, 0.011305879801511765, 0.4679570496082306, 0.0734853595495224, 0.17682163417339325, 0.4782702326774597, 0.39913037419319153, 0.2018677443265915, 0.18380768597126007, 0.11227983236312866, 0.03645961731672287, 0.2997353971004486, 0.04136256128549576, 0.21772873401641846, 0.16799432039260864, 0.14410792291164398, 0.4610165059566498, 0.49297529458999634, 0.3258855938911438, 0.30773696303367615, 0.4827496409416199, 0.11592836678028107], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_dd1244b85e930cef3cc03a36ad1ea9c0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.04214863479137421, 0.09239821881055832, 0.19570910930633545, 0.059039901942014694, 0.34352442622184753, 0.36716532707214355, 0.05737737566232681, 0.12633562088012695, 0.2183976024389267, 0.2020915150642395, 0.21674102544784546, 0.20296567678451538, 0.48366838693618774, 0.10681094229221344, 0.3360271155834198, 0.073822021484375], dtype='float32').reshape([16]),
            paddle.to_tensor([0.2639583945274353, 0.11293789744377136, 0.1447414755821228, 0.4226173460483551, 0.24303914606571198, 0.182643860578537, 0.48686861991882324, 0.10150088369846344, 0.022000880911946297, 0.011041998863220215, 0.2883318066596985, 0.1718328893184662, 0.4318225681781769, 0.06490106880664825, 0.48817890882492065, 0.40253734588623047], dtype='float32').reshape([16]),
            paddle.to_tensor([0.31320512294769287, 0.284698486328125, 0.20571434497833252, 0.1621962934732437, 0.41225144267082214, 0.49206799268722534, 0.04537893831729889, 0.06792540103197098, 0.14123602211475372, 0.04053544998168945, 0.41655081510543823, 0.13089485466480255, 0.18555094301700592, 0.27138206362724304, 0.04806821793317795, 0.44794291257858276], dtype='float32').reshape([16]),
            paddle.to_tensor([0.38196709752082825, 0.31397882103919983, 0.18531452119350433, 0.18711405992507935, 0.4886676073074341, 0.03327777609229088, 0.2767944037914276, 0.20528848469257355, 0.0728265568614006, 0.2761286795139313, 0.2824760973453522, 0.11939632147550583, 0.4898504614830017, 0.27547943592071533, 0.32088524103164673, 0.2414202243089676], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b9d591383faf61ac24d4df5776fecde7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 160, 160], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.39653828740119934, 0.3458191752433777, 0.0635392963886261, 0.25347813963890076, 0.12727485597133636, 0.4413096010684967, 0.480685830116272, 0.12832070887088776, 0.45488041639328003, 0.05135645344853401, 0.368666410446167, 0.15674902498722076, 0.011353891342878342, 0.11266504228115082, 0.18960703909397125, 0.008468402549624443, 0.0356077142059803, 0.2611873745918274, 0.4846304953098297, 0.44514209032058716, 0.42553409934043884, 0.04915836825966835, 0.3910132944583893, 0.1899794489145279], dtype='float32').reshape([24]),
            paddle.to_tensor([0.32716119289398193, 0.0892118588089943, 0.1881685107946396, 0.024641137570142746, 0.4622013568878174, 0.3396914005279541, 0.2939470708370209, 0.12648648023605347, 0.40180516242980957, 0.10930144041776657, 0.19085586071014404, 0.2710116505622864, 0.3589196503162384, 0.14030994474887848, 0.4855857789516449, 0.05569930374622345, 0.014910153113305569, 0.1924879401922226, 0.2652747333049774, 0.4678700566291809, 0.24537841975688934, 0.16213566064834595, 0.2444005161523819, 0.045825373381376266], dtype='float32').reshape([24]),
            paddle.to_tensor([0.06994731724262238, 0.22143100202083588, 0.4053206443786621, 0.3226464092731476, 0.32994019985198975, 0.46025481820106506, 0.30281463265419006, 0.3397427201271057, 0.39272773265838623, 0.4741949737071991, 0.28176334500312805, 0.4534468352794647, 0.4218008518218994, 0.025356266647577286, 0.19733591377735138, 0.0544961616396904, 0.009716004133224487, 0.22589370608329773, 0.29700571298599243, 0.1321849673986435, 0.060378316789865494, 0.2598434090614319, 0.19229908287525177, 0.1808718889951706], dtype='float32').reshape([24]),
            paddle.to_tensor([0.13169682025909424, 0.21312907338142395, 0.2143261581659317, 0.2777712047100067, 0.29374635219573975, 0.0001998160150833428, 0.4967028498649597, 0.3710384964942932, 0.18551947176456451, 0.04667498543858528, 0.18792811036109924, 0.13009995222091675, 0.4416634440422058, 0.18231071531772614, 0.3643339276313782, 0.47368383407592773, 0.38378748297691345, 0.3842170238494873, 0.4183301627635956, 0.37990134954452515, 0.11229131370782852, 0.2583393454551697, 0.18542124330997467, 0.43617719411849976], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_775d0d09940f6f652b7193572281ccaf(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 1044, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([1044], dtype='float32', min=0, max=0.5),
            paddle.uniform([1044], dtype='float32', min=0, max=0.5),
            paddle.uniform([1044], dtype='float32', min=0, max=0.5),
            paddle.uniform([1044], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b2df2ce156ba2a71e0dc28492905ca9d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8834139b6e81cbef21eb6c154545d04e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_58368c6b0d068b6517111e20c141e2fe(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 140, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([140], dtype='float32', min=0, max=0.5),
            paddle.uniform([140], dtype='float32', min=0, max=0.5),
            paddle.uniform([140], dtype='float32', min=0, max=0.5),
            paddle.uniform([140], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_55f6309c904b67ae9325a31d81b30395(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 36, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8c6dccf1c702ebe105911afd50e22556(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 160, 4, 3], dtype='float16', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6a9b6334f75cb517fd3b5aff823dace1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e2e55b2214eeff4b7df8046b485348f7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_39f301f0b7726800c3f64144f3a058ec(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 4, 26], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5f414939bf280b87e1fa443c0c619365(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d093bb5b3176553b1ae4908ad48037c3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 40, 40], dtype='float16', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_de12a6be1265860c792439aef6cf9f78(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 38, 38], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_03058d14e912fd6847449ad535bc356d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 88, 25], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fad187d2b4a6b4e3200b8fe4b32f501c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.2640567123889923, 0.29053789377212524, 0.49932315945625305, 0.2199472337961197, 0.22679460048675537, 0.011289956048130989, 0.3886520564556122, 0.07244399935007095, 0.34247735142707825, 0.3548758924007416, 0.3517374098300934, 0.10409773141145706, 0.246302992105484, 0.2618072032928467, 0.17200326919555664, 0.41097718477249146, 0.4935944378376007, 0.32830026745796204, 0.1153322383761406, 0.051362499594688416, 0.18498006463050842, 0.21523703634738922, 0.10655170679092407, 0.48470014333724976], dtype='float32').reshape([24]),
            paddle.to_tensor([0.29897335171699524, 0.2613067030906677, 0.46163874864578247, 0.28281497955322266, 0.10690446943044662, 0.26428720355033875, 0.21619142591953278, 0.375697523355484, 0.2530949115753174, 0.1546916365623474, 0.43127068877220154, 0.10679683834314346, 0.1817714124917984, 0.3962947726249695, 0.47220951318740845, 0.39021798968315125, 0.42799681425094604, 0.09597914665937424, 0.07485152781009674, 0.05327381566166878, 0.008990185335278511, 0.29790422320365906, 0.1550256460905075, 0.3581671118736267], dtype='float32').reshape([24]),
            paddle.to_tensor([0.17011405527591705, 0.4290009140968323, 0.22519947588443756, 0.3036573827266693, 0.07801619917154312, 0.17620812356472015, 0.08427853882312775, 0.29197612404823303, 0.2620430290699005, 0.09870428591966629, 0.1993367224931717, 0.29082536697387695, 0.38908156752586365, 0.18876288831233978, 0.22323456406593323, 0.3813883662223816, 0.36014553904533386, 0.4913616478443146, 0.21824198961257935, 0.25816550850868225, 0.36091306805610657, 0.11233670264482498, 0.2803184390068054, 0.25860992074012756], dtype='float32').reshape([24]),
            paddle.to_tensor([0.29743462800979614, 0.23290297389030457, 0.23795552551746368, 0.24296419322490692, 0.24531568586826324, 0.08221139758825302, 0.4119639992713928, 0.4480319917201996, 0.41032320261001587, 0.2276763767004013, 0.062226008623838425, 0.41975411772727966, 0.3625178635120392, 0.21983850002288818, 0.012403998523950577, 0.29308921098709106, 0.3955455720424652, 0.05467832088470459, 0.28478631377220154, 0.33946487307548523, 0.12938039004802704, 0.24380142986774445, 0.19764740765094757, 0.2844630181789398], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7e8a295152984ff9fd4da842361e1aa1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 80, 80], dtype='float16', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_430952fe988d651b72c9e9c7ec89096b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([8, 256, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c8255f67f9cf5c116c8d83bbab8c58c7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 240, 40, 40], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1012792595c8e203b63c6e4e0f975c52(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a8301b810bccba10e4dfdee0df73334
    def get_inputs(self):
        return [
            paddle.uniform([1, 728, 19, 19], dtype='float32', min=0, max=0.5),
            paddle.uniform([728], dtype='float32', min=0, max=0.5),
            paddle.uniform([728], dtype='float32', min=0, max=0.5),
            paddle.uniform([728], dtype='float32', min=0, max=0.5),
            paddle.uniform([728], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cbc7dc883b1282f2e30c2f1460ba5a74(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 112, 112], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.1805398315191269, 0.1355758011341095, 0.4327388405799866, 0.15527381002902985, 0.0656619668006897, 0.057090822607278824, 0.47098514437675476, 0.06358544528484344, 0.10845436900854111, 0.06430976837873459, 0.38456225395202637, 0.2942063808441162, 0.481552392244339, 0.47892338037490845, 0.45709115266799927, 0.20311294496059418, 0.06945190578699112, 0.023538876324892044, 0.49513834714889526, 0.4322347342967987, 0.18788158893585205, 0.2834188640117645, 0.40845122933387756, 0.1711844503879547], dtype='float32').reshape([24]),
            paddle.to_tensor([0.49837934970855713, 0.30111631751060486, 0.2580633759498596, 0.04595502093434334, 0.3610028326511383, 0.3714756667613983, 0.0723111554980278, 0.20924405753612518, 0.10181053727865219, 0.397128701210022, 0.48529571294784546, 0.22043035924434662, 0.030053069815039635, 0.22945287823677063, 0.41119712591171265, 0.40401890873908997, 0.24143193662166595, 0.3218253254890442, 0.18020999431610107, 0.14675119519233704, 0.2703549265861511, 0.3643915057182312, 0.20614276826381683, 0.31583547592163086], dtype='float32').reshape([24]),
            paddle.to_tensor([0.09221801161766052, 0.12888818979263306, 0.1181754469871521, 0.24742618203163147, 0.10183320194482803, 0.07928521186113358, 0.043888773769140244, 0.3189423680305481, 0.30326157808303833, 0.0861406996846199, 0.4206722676753998, 0.18298529088497162, 0.24330013990402222, 0.328510046005249, 0.47092270851135254, 0.3609640300273895, 0.2653268873691559, 0.3640953302383423, 0.38356369733810425, 0.24652400612831116, 0.14186161756515503, 0.4740319550037384, 0.35616832971572876, 0.11643358319997787], dtype='float32').reshape([24]),
            paddle.to_tensor([0.3506646454334259, 0.4272277057170868, 0.22635705769062042, 0.49798494577407837, 0.25648441910743713, 0.01161427702754736, 0.04961639642715454, 0.14762988686561584, 0.4709770977497101, 0.13444669544696808, 0.48835131525993347, 0.4055814743041992, 0.12294571101665497, 0.3467899560928345, 0.38907113671302795, 0.32657086849212646, 0.3852590024471283, 0.33701807260513306, 0.49673759937286377, 0.22130699455738068, 0.20378707349300385, 0.20751617848873138, 0.38950052857398987, 0.1420305371284485], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d1900f2e229a8c61388c6b41417f6e76(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 416, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([416], dtype='float32', min=0, max=0.5),
            paddle.uniform([416], dtype='float32', min=0, max=0.5),
            paddle.uniform([416], dtype='float32', min=0, max=0.5),
            paddle.uniform([416], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_eaa680a4ae5cdb85f42a36941b55ea3b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_31d0c137872e62333693de7cab0e3ead(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_92c4fddad78dc30469318b4cc27a1620(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 256, 512], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.11180118471384048, 0.287222295999527, 0.03903674706816673, 0.016071807593107224, 0.10668119043111801, 0.1082477942109108, 0.33808445930480957, 0.04443060979247093, 0.3748982548713684, 0.21568575501441956, 0.25619664788246155, 0.07254427671432495, 0.2424718290567398, 0.3175053298473358, 0.3143509328365326, 0.15841324627399445], dtype='float32').reshape([16]),
            paddle.to_tensor([0.06552276015281677, 0.43976035714149475, 0.3883051574230194, 0.3981759250164032, 0.03940734267234802, 0.1460251361131668, 0.2932281494140625, 0.23603318631649017, 0.3565349876880646, 0.07890135794878006, 0.383136510848999, 0.07663893699645996, 0.3263368010520935, 0.060194022953510284, 0.1904008984565735, 0.023295152932405472], dtype='float32').reshape([16]),
            paddle.to_tensor([0.19361399114131927, 0.012438703328371048, 0.3794361352920532, 0.38915789127349854, 0.491327166557312, 0.3575866222381592, 0.2013712227344513, 0.29317307472229004, 0.26827266812324524, 0.032261256128549576, 0.38166773319244385, 0.1961381882429123, 0.28914371132850647, 0.11615215986967087, 0.2655949890613556, 0.2694162130355835], dtype='float32').reshape([16]),
            paddle.to_tensor([0.1939808428287506, 0.043205827474594116, 0.15068213641643524, 0.49464502930641174, 0.19403107464313507, 0.20718419551849365, 0.4062656760215759, 0.024000810459256172, 0.33725860714912415, 0.15535975992679596, 0.4478515088558197, 0.34026309847831726, 0.37473854422569275, 0.36206161975860596, 0.4934665858745575, 0.17563295364379883], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e7c005e538a8f13f23d6853b6847aa84(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.27026990056037903, 0.47161316871643066, 0.3564910590648651, 0.2474830001592636, 0.13820333778858185, 0.16459348797798157, 0.1879001408815384, 0.12181191146373749, 0.08883839100599289, 0.22833290696144104, 0.08673200756311417, 0.4591776132583618, 0.05521776154637337, 0.34492623805999756, 0.2938109338283539, 0.4824359118938446, 0.10803557932376862, 0.35451436042785645], dtype='float32').reshape([18]),
            paddle.to_tensor([0.10537147521972656, 0.4541442096233368, 0.09571423381567001, 0.29679837822914124, 0.47176268696784973, 0.011959806084632874, 0.29442456364631653, 0.4856460988521576, 0.16681279242038727, 0.32360100746154785, 0.33377939462661743, 0.27385371923446655, 0.29340407252311707, 0.057526905089616776, 0.4166506230831146, 0.11459902673959732, 0.19108796119689941, 0.43720775842666626], dtype='float32').reshape([18]),
            paddle.to_tensor([0.4047824740409851, 0.3387684226036072, 0.4903446435928345, 0.36090975999832153, 0.1128251850605011, 0.40508636832237244, 0.22111663222312927, 0.4471150040626526, 0.36779284477233887, 0.32125338912010193, 0.3655508756637573, 0.16932645440101624, 0.49818283319473267, 0.4940667450428009, 0.10881803929805756, 0.014242487028241158, 0.4750387370586395, 0.11551257967948914], dtype='float32').reshape([18]),
            paddle.to_tensor([0.37738409638404846, 0.3690652847290039, 0.4022524356842041, 0.408299058675766, 0.3768075406551361, 0.03739962726831436, 0.19709512591362, 0.39661404490470886, 0.4008389711380005, 0.10294771939516068, 0.3978530764579773, 0.3228157162666321, 0.0384577140212059, 0.11943517625331879, 0.48449063301086426, 0.03174559026956558, 0.21989351511001587, 0.13694359362125397], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d97e1437c3d4251789e3959e264d122b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 240, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_06f5fa28038d4fd23982c16fe85519d2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 32, 128], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cb581f74c4127889a3da3402b1dd7167(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_da4d85bc213f70819a72e68f21214cdb
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 4, 64, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_be0e8241d3ddbbbcafb19b5ce765b079(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 160, 160], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d76dc775e3dff6308121c6b6063db1f8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d85cef4f6d25e07559a3ca5135d5fd04(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 138, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([138], dtype='float32', min=0, max=0.5),
            paddle.uniform([138], dtype='float32', min=0, max=0.5),
            paddle.uniform([138], dtype='float32', min=0, max=0.5),
            paddle.uniform([138], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_000345d3fef3fda8308d6553c21137f6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d3bb317e2490c6a31555be1ecc45448d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 72, 32, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_01a63ca6997edcb13c2d46ab5e49decf(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 61, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([61], dtype='float32', min=0, max=0.5),
            paddle.uniform([61], dtype='float32', min=0, max=0.5),
            paddle.uniform([61], dtype='float32', min=0, max=0.5),
            paddle.uniform([61], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_22e16a2388ae9fc6ae9cde5950dab568(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 17, 17], dtype='float16', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_09270ad82cffaa34fe6ede5846bd8fbb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bbe9af8f06b58db52b978ea1089b990d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 64, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ded7a00ee14fde3b549baaf66f48dcc2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 88, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([88], dtype='float32', min=0, max=0.5),
            paddle.uniform([88], dtype='float32', min=0, max=0.5),
            paddle.uniform([88], dtype='float32', min=0, max=0.5),
            paddle.uniform([88], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_932bdc22f3c5cf405ec0d4df0befe3f3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([8, 1024, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f1c904092ea8394ce9f25737cf8479ed(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 104, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8392266e22057d57d07b65bb0b93c6f8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_08ecfbca71b5233f6357b289e88ae3f0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_fc6b6917e04b7b7cf9fdb39dde97ec40(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_218d9f0dcbfe880db124463f56d7db14(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_fc6b6917e04b7b7cf9fdb39dde97ec40
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 80, 80], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b6579daf6a7ea8852537f1ebafd127f1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 17, 17], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_506f37d223f243152f90bc806a124c3a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_026827c7cb44a2e3bd54ae7dd55d25d6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.42139554023742676, 0.03406580910086632, 0.29064589738845825, 0.42377182841300964, 0.20318743586540222, 0.25893622636795044, 0.46767139434814453, 0.1544031798839569, 0.023761151358485222, 0.23299869894981384, 0.2758779227733612, 0.15604814887046814, 0.18409335613250732, 0.3333298861980438, 0.3872057795524597, 0.4325207471847534, 0.06871270388364792, 0.1624070703983307, 0.2555927634239197, 0.2064737230539322, 0.1187577098608017, 0.3634510040283203, 0.028394754976034164, 0.2317550629377365, 0.4131660461425781, 0.0340641550719738, 0.19831086695194244, 0.005306191276758909, 0.11549708247184753, 0.011231404729187489], dtype='float32').reshape([30]),
            paddle.to_tensor([0.09577331691980362, 0.14098161458969116, 0.08698363602161407, 0.14832696318626404, 0.018585393205285072, 0.33530399203300476, 0.1346024125814438, 0.1574869453907013, 0.07640081644058228, 0.061992041766643524, 0.27968430519104004, 0.05690942332148552, 0.38352182507514954, 0.47968119382858276, 0.2482633888721466, 0.3581748306751251, 0.08602815866470337, 0.23597432672977448, 0.36781564354896545, 0.18204258382320404, 0.10211252421140671, 0.11323078721761703, 0.014976433478295803, 0.23481866717338562, 0.03504130244255066, 0.4210485816001892, 0.008641010150313377, 0.3097226917743683, 0.14017453789710999, 0.16007274389266968], dtype='float32').reshape([30]),
            paddle.to_tensor([0.05936908349394798, 0.30910879373550415, 0.1687820851802826, 0.38220569491386414, 0.04345165565609932, 0.028151294216513634, 0.471249520778656, 0.2810927629470825, 0.23509979248046875, 0.1602163165807724, 0.19392673671245575, 0.09802171587944031, 0.07190649211406708, 0.44294825196266174, 0.2767643332481384, 0.11985249072313309, 0.38695210218429565, 0.1480143666267395, 0.21607452630996704, 0.08825335651636124, 0.4723502993583679, 0.07719173282384872, 0.14354366064071655, 0.4456029236316681, 0.19725002348423004, 0.3178512752056122, 0.3377052843570709, 0.08097867667675018, 0.22688911855220795, 0.17170929908752441], dtype='float32').reshape([30]),
            paddle.to_tensor([0.48261186480522156, 0.48969370126724243, 0.24401499330997467, 0.09035861492156982, 0.15517397224903107, 0.08402670919895172, 0.38020166754722595, 0.10059567540884018, 0.48069506883621216, 0.1423831582069397, 0.22338275611400604, 0.46233251690864563, 0.4501512944698334, 0.1529349833726883, 0.38308486342430115, 0.15959127247333527, 0.20519377291202545, 0.42249202728271484, 0.15894609689712524, 0.35849785804748535, 0.17688171565532684, 0.12668104469776154, 0.18161633610725403, 0.4456281363964081, 0.006195192690938711, 0.2014325112104416, 0.35472339391708374, 0.32631808519363403, 0.2429128736257553, 0.11801916360855103], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a9d995cd107bf2c85f74ba83b519d991(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 40, 40], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0cb619b3c87fd20e766b170517f46a22(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 40, 40], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5f9b44244e4a66db980429f6c981bb22(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 128, 128], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2bcefde44639e3e32ddeb3fb6c2dff1b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c50c1c92563e15641752ced1e35ed47b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1d1109c16c316b9a965a4a94622f0c0c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 120, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_62e5a96212b0fcc6919dad2ef21cf47e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 4, 3], dtype='float16', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3650ad2b4f4d06890e2b8beb13f20640(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 20, 20], dtype='float16', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_41c4069d4ffdc77ecbca165e7912fd61(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4a81b38ecb5cf95d75bb8e36ec109d70(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 36, 64, 128], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2e511898d097c8793526f12b7410bf9a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 256, 512], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.14784036576747894, 0.1923452615737915, 0.12710678577423096, 0.4680420458316803, 0.36875972151756287, 0.07686439901590347, 0.017774486914277077, 0.12801901996135712, 0.046546127647161484, 0.42976391315460205, 0.13871562480926514, 0.2576156258583069, 0.12607739865779877, 0.2286723554134369, 0.0042807250283658504, 0.059594880789518356], dtype='float32').reshape([16]),
            paddle.to_tensor([0.38740384578704834, 0.20269565284252167, 0.01634291000664234, 0.4818756580352783, 0.08383969962596893, 0.4622616469860077, 0.02669709548354149, 0.40213871002197266, 0.42550361156463623, 0.10058329999446869, 0.36128440499305725, 0.2279319167137146, 0.05697845295071602, 0.06060688570141792, 0.2716887593269348, 0.37189924716949463], dtype='float32').reshape([16]),
            paddle.to_tensor([0.39901265501976013, 0.29885175824165344, 0.19643324613571167, 0.2044934183359146, 0.46467867493629456, 0.44087257981300354, 0.28486934304237366, 0.13060708343982697, 0.34471938014030457, 0.16926303505897522, 0.07973099499940872, 0.08224784582853317, 0.01650579459965229, 0.23487278819084167, 0.0021077049896121025, 0.21514134109020233], dtype='float32').reshape([16]),
            paddle.to_tensor([0.48764896392822266, 0.37929537892341614, 0.3971806466579437, 0.1728736013174057, 0.14754930138587952, 0.06554108113050461, 0.4444113075733185, 0.13199904561042786, 0.17298656702041626, 0.4397566318511963, 0.27833378314971924, 0.21309737861156464, 0.1287590116262436, 0.3315589129924774, 0.08764394372701645, 0.23431311547756195], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7ba53cdbed8af5d97053baf1faa5d1c7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 31, 31], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2f7708197cbf2a878490f5319b673eaa(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 144, 16, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_88f594b94ed79c90a3762907063b8ca6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 50, 84], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f536b4656537b8c9fafa34f5be9cc133(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 112, 112], dtype='float16', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c2ed835ccb7cea31f6d8f17e874f319e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 2, 50], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f7cde1e3dc2d36571261679838498427(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 80, 80], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_590b938bdacfcee33d0a4259d23a37a6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e188bcc482351a5581983ba472c5bce5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 31, 31], dtype='float16', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_07e65756e32cdacabbf7fdc73c01ecee(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_585f319f0a336dc373218775aa1e9029(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 72, 14, 25], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1c89209b4004f62773bea4ea22bea530(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 240, 240], dtype='float16', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b296843461f4801635debe6dd56e3452(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 906, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([906], dtype='float32', min=0, max=0.5),
            paddle.uniform([906], dtype='float32', min=0, max=0.5),
            paddle.uniform([906], dtype='float32', min=0, max=0.5),
            paddle.uniform([906], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_33db085ef535fddb00f4f4ba88538109(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.06446705013513565, 0.4187636971473694, 0.3838133215904236, 0.2907026708126068, 0.07582702487707138, 0.32670629024505615, 0.03263097628951073, 0.31616243720054626, 0.029614035040140152, 0.4452679455280304, 0.20363053679466248, 0.4890032112598419, 0.09040836244821548, 0.10802531987428665, 0.05587846785783768, 0.42138516902923584, 0.23497922718524933, 0.1400766521692276, 0.07919582724571228, 0.27989673614501953, 0.1810300201177597, 0.289666086435318, 0.27869725227355957, 0.2766357958316803, 0.3504430055618286, 0.01981385052204132, 0.42241114377975464, 0.2146010398864746, 0.3082158863544464, 0.15323184430599213], dtype='float32').reshape([30]),
            paddle.to_tensor([0.24289682507514954, 0.4737260341644287, 0.41912442445755005, 0.35030704736709595, 0.16148900985717773, 0.057163774967193604, 0.304231196641922, 0.12856964766979218, 0.0946350172162056, 0.4481244385242462, 0.10468745231628418, 0.3522816300392151, 0.40326887369155884, 0.36348000168800354, 0.2674607038497925, 0.49196383357048035, 0.03858977183699608, 0.47191181778907776, 0.45470955967903137, 0.42049741744995117, 0.26488909125328064, 0.3688337504863739, 0.2467658668756485, 0.31768858432769775, 0.25896668434143066, 0.31198862195014954, 0.4842366874217987, 0.3150946795940399, 0.48546352982521057, 0.3045285642147064], dtype='float32').reshape([30]),
            paddle.to_tensor([0.0944107323884964, 0.03768293932080269, 0.4893677532672882, 0.3455919623374939, 0.31524309515953064, 0.07330771535634995, 0.1535312533378601, 0.11688341945409775, 0.005497246980667114, 0.2555634677410126, 0.3699539005756378, 0.37811926007270813, 0.22934122383594513, 0.015064393170177937, 0.48378026485443115, 0.06391118466854095, 0.4773608148097992, 0.2043689489364624, 0.22390757501125336, 0.040564920753240585, 0.30103111267089844, 0.19423925876617432, 0.0585394985973835, 0.21288785338401794, 0.14143426716327667, 0.2731720209121704, 0.028742285445332527, 0.3718603551387787, 0.35961923003196716, 0.32183709740638733], dtype='float32').reshape([30]),
            paddle.to_tensor([0.3185417056083679, 0.10066632181406021, 0.4247049391269684, 0.052710242569446564, 0.1588510423898697, 0.31484729051589966, 0.3904756009578705, 0.40589818358421326, 0.11506588757038116, 0.45059385895729065, 0.498904824256897, 0.16131046414375305, 0.05728277936577797, 0.4264236092567444, 0.05523869767785072, 0.28073471784591675, 0.4421308934688568, 0.4810326099395752, 0.16980569064617157, 0.14503897726535797, 0.14335457980632782, 0.2988302707672119, 0.49867182970046997, 0.3389888405799866, 0.11322347819805145, 0.371569961309433, 0.014341589994728565, 0.006274365819990635, 0.37951424717903137, 0.4890817403793335], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_68c885aa9fd50f14fa1e73c99efaa7b6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 768, 32, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b340f80c367196e43f70603799bfa159(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 336, 30, 30], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_3a47a120d62a3f3a422a747f511df8d9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6159b4ea3940844fcc323f50c946d485(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_3a47a120d62a3f3a422a747f511df8d9
    def get_inputs(self):
        return [
            paddle.uniform([1, 728, 19, 19], dtype='float16', min=0, max=0.5),
            paddle.uniform([728], dtype='float32', min=0, max=0.5),
            paddle.uniform([728], dtype='float32', min=0, max=0.5),
            paddle.uniform([728], dtype='float32', min=0, max=0.5),
            paddle.uniform([728], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_df250feca2d1420b00e1bbd3a374d953(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 128, 128], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e4bfe680fe98b4715b413f0a652d4cc3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e05b80f7b3a899460748f64ebe53c56d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5cbd2a5def56dcd3286531931b581b79(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 31, 31], dtype='float16', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bb182dec18071514978b6fbe32bd98fa(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 40, 40], dtype='float16', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b11522da1514d39c31ec9e60418e62d8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 60, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2f2ec5544e17c5110d7a56a24e7f5d9f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 640, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e3ea92bb071d55144e266a6c8f999e16(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 32, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5608b3b7b344235002e92ff2b7205172(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([8, 2048, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1a053da07c1a1887a7d1207207dbc5b2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 16, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_5e7cb23119f65b33bdbad71952f6dd12(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_381f0b5a6d42f83baba97e9057d16213(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_5e7cb23119f65b33bdbad71952f6dd12
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 32, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7416acc162940ae2b54e25746a3360fe(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.22900614142417908, 0.10477956384420395, 0.05085553973913193, 0.45558494329452515, 0.30686676502227783, 0.1702760010957718, 0.4796625077724457, 0.2831428349018097, 0.4719119668006897, 0.26697900891304016, 0.030331240966916084, 0.48327457904815674, 0.08763570338487625, 0.3408660292625427, 0.4683696925640106, 0.1259494423866272, 0.20128950476646423, 0.3144550025463104, 0.24421626329421997, 0.1457386165857315], dtype='float32').reshape([20]),
            paddle.to_tensor([0.3414515554904938, 0.40826621651649475, 0.0859764814376831, 0.4087790846824646, 0.3871367275714874, 0.22375550866127014, 0.14911895990371704, 0.18701763451099396, 0.31195488572120667, 0.36369484663009644, 0.2649211883544922, 0.45253729820251465, 0.08992528170347214, 0.4603237509727478, 0.21755509078502655, 0.4106748402118683, 0.46627941727638245, 0.12611310184001923, 0.21678322553634644, 0.37716370820999146], dtype='float32').reshape([20]),
            paddle.to_tensor([0.4480248987674713, 0.32285627722740173, 0.36476439237594604, 0.48310956358909607, 0.42530685663223267, 0.13492050766944885, 0.0038876920007169247, 0.0366390161216259, 0.44237229228019714, 0.009667741134762764, 0.05290374904870987, 0.34727200865745544, 0.3561830222606659, 0.41582179069519043, 0.38627395033836365, 0.07099136710166931, 0.42601269483566284, 0.11465474963188171, 0.12802566587924957, 0.11659770458936691], dtype='float32').reshape([20]),
            paddle.to_tensor([0.49268919229507446, 0.1622532308101654, 0.28352636098861694, 0.1772642880678177, 0.03140748664736748, 0.36221110820770264, 0.014599324204027653, 0.12174674868583679, 0.1938592940568924, 0.30620673298835754, 0.32271188497543335, 0.264396607875824, 0.20788271725177765, 0.2881271243095398, 0.15984658896923065, 0.06708266586065292, 0.13812701404094696, 0.06720351427793503, 0.13749468326568604, 0.2584025263786316], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8b9b4a3cc08e30f84f507863d51d9db3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 60, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_148beb9566058f0eefb7d5b86e4d322e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.1320543736219406, 0.46202534437179565, 0.40016502141952515, 0.1278034895658493, 0.3244100511074066, 0.07672934979200363, 0.2299579530954361, 0.3743538558483124, 0.16434821486473083, 0.07094600051641464, 0.005597150884568691, 0.18460537493228912, 0.20499302446842194, 0.2169252187013626, 0.3916391432285309, 0.4612959027290344], dtype='float32').reshape([16]),
            paddle.to_tensor([0.15315309166908264, 0.3214118480682373, 0.49825164675712585, 0.19410307705402374, 0.1730237603187561, 0.38127994537353516, 0.3622138202190399, 0.33612510561943054, 0.10058286041021347, 0.1368720382452011, 0.40691837668418884, 0.1623140126466751, 0.39070698618888855, 0.3403816223144531, 0.05442698672413826, 0.06855489313602448], dtype='float32').reshape([16]),
            paddle.to_tensor([0.4702501893043518, 0.1738605946302414, 0.3063492476940155, 0.4298981726169586, 0.05151166766881943, 0.2788911461830139, 0.38130447268486023, 0.08216642588376999, 0.22018180787563324, 0.45688149333000183, 0.48227542638778687, 0.008939667604863644, 0.06101108714938164, 0.44473642110824585, 0.23865310847759247, 0.11273075640201569], dtype='float32').reshape([16]),
            paddle.to_tensor([0.4281567931175232, 0.06626744568347931, 0.14404982328414917, 0.37684565782546997, 0.049313921481370926, 0.12736545503139496, 0.14409415423870087, 0.2786664068698883, 0.24014724791049957, 0.22324879467487335, 0.3302842080593109, 0.36616888642311096, 0.22350601851940155, 0.14384277164936066, 0.25477638840675354, 0.21971018612384796], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_145ad20e7a3b3c8aa31f6b40d4fe3d45(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e5eeb78d95074cfe82c876ece07775ff(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_145ad20e7a3b3c8aa31f6b40d4fe3d45
    def get_inputs(self):
        return [
            paddle.uniform([16, 512], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4895b06e4cbd5b787836470329a05327(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 80, 12, 12], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_334fec48584078251260f1d954509b8f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 128, 128], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.4268942177295685, 0.37912222743034363, 0.41616442799568176, 0.08647879958152771, 0.2527906596660614, 0.34847673773765564, 0.479097455739975, 0.13525280356407166, 0.25531333684921265, 0.0974419116973877, 0.1356259137392044, 0.1460873782634735, 0.44107910990715027, 0.37887588143348694, 0.014621196314692497, 0.20184190571308136], dtype='float32').reshape([16]),
            paddle.to_tensor([0.19490143656730652, 0.2276311218738556, 0.04740109294652939, 0.31638088822364807, 0.19974160194396973, 0.40288981795310974, 0.09352532774209976, 0.43295302987098694, 0.13745887577533722, 0.1357840895652771, 0.30002284049987793, 0.36498475074768066, 0.25857973098754883, 0.3074428141117096, 0.42219406366348267, 0.04950583353638649], dtype='float32').reshape([16]),
            paddle.to_tensor([0.12299484759569168, 0.465108186006546, 0.02761617675423622, 0.26587486267089844, 0.3233770430088043, 0.2969285845756531, 0.43385788798332214, 0.010500192642211914, 0.3471463918685913, 0.16937385499477386, 0.22577232122421265, 0.25731736421585083, 0.2678251564502716, 0.23959290981292725, 0.17908795177936554, 0.3890547454357147], dtype='float32').reshape([16]),
            paddle.to_tensor([0.2992279529571533, 0.1447499543428421, 0.43881285190582275, 0.27598997950553894, 0.01654181256890297, 0.301865816116333, 0.17722225189208984, 0.35884594917297363, 0.15199914574623108, 0.4871465861797333, 0.22579525411128998, 0.20141592621803284, 0.4613049328327179, 0.45278894901275635, 0.37076011300086975, 0.4706365466117859], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c3d92c075c1aae352ac998a2003ff5dd(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.4540644884109497, 0.3213058114051819, 0.016172848641872406, 0.1523425281047821, 0.22663067281246185, 0.0003146343515254557, 0.35738569498062134, 0.37154340744018555, 0.0011627280618995428, 0.06388891488313675, 0.1969618797302246, 0.13172172009944916, 0.07161728292703629, 0.22017793357372284, 0.3574819266796112, 0.41591084003448486, 0.4771420359611511, 0.4956549108028412, 0.18347372114658356, 0.07957465946674347, 0.49105578660964966, 0.4378519058227539, 0.14741012454032898, 0.1744200885295868, 0.15746629238128662, 0.4221331477165222, 0.06768572330474854, 0.3172026574611664, 0.38630566000938416, 0.2942250072956085], dtype='float32').reshape([30]),
            paddle.to_tensor([0.12003037333488464, 0.1279965043067932, 0.38452786207199097, 0.452238529920578, 0.19776545464992523, 0.2950602173805237, 0.2913699746131897, 0.2982294261455536, 0.4270259141921997, 0.04670891910791397, 0.31082895398139954, 0.14519061148166656, 0.4255756139755249, 0.49804890155792236, 0.2030331939458847, 0.20680448412895203, 0.29187440872192383, 0.4297126233577728, 0.45476454496383667, 0.09095704555511475, 0.13794618844985962, 0.06800694018602371, 0.052644550800323486, 0.46847811341285706, 0.3350464701652527, 0.46288126707077026, 0.48687446117401123, 0.22383810579776764, 0.002176701556891203, 0.2426706999540329], dtype='float32').reshape([30]),
            paddle.to_tensor([0.1421251893043518, 0.3738352060317993, 0.17047886550426483, 0.320513516664505, 0.32608848810195923, 0.13312049210071564, 0.3540729284286499, 0.05709785223007202, 0.26974454522132874, 0.36904096603393555, 0.3593938648700714, 0.2801894545555115, 0.08106273412704468, 0.26297637820243835, 0.3980870544910431, 0.27645808458328247, 0.4229929745197296, 0.35446715354919434, 0.4566989243030548, 0.2863933742046356, 0.18459118902683258, 0.3772584795951843, 0.07735778391361237, 0.18355238437652588, 0.36598604917526245, 0.3367646038532257, 0.3956860899925232, 0.1203523576259613, 0.06901302188634872, 0.4554324746131897], dtype='float32').reshape([30]),
            paddle.to_tensor([0.19751617312431335, 0.3446904420852661, 0.40537428855895996, 0.36017513275146484, 0.1813289076089859, 0.15848232805728912, 0.39108219742774963, 0.4731391966342926, 0.1845121532678604, 0.31658050417900085, 0.11729755252599716, 0.29027387499809265, 0.4786806106567383, 0.023992976173758507, 0.3228473961353302, 0.04168974980711937, 0.3384770452976227, 0.05241619050502777, 0.2801324129104614, 0.28354746103286743, 0.4564981162548065, 0.31336936354637146, 0.21584424376487732, 0.40187543630599976, 0.07795196026563644, 0.11938147991895676, 0.44140735268592834, 0.03728961572051048, 0.30159929394721985, 0.22320504486560822], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_03c8ae4fce64e43cf7a8152d977d6096(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 96, 96], dtype='float16', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_284ed47c16345de7f833e928ca40178c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.4453331232070923, 0.2664141356945038, 0.45651674270629883, 0.41413140296936035, 0.19447466731071472, 0.45867738127708435, 0.4771433174610138, 0.28373172879219055, 0.12037995457649231, 0.3344631791114807, 0.1511785089969635, 0.3102761507034302, 0.09329323470592499, 0.022926120087504387, 0.4489542543888092, 0.30689147114753723, 0.4227290749549866, 0.035837434232234955, 0.47220173478126526, 0.3215627074241638], dtype='float32').reshape([20]),
            paddle.to_tensor([0.42187297344207764, 0.42175400257110596, 0.18918010592460632, 0.38132593035697937, 0.024579515680670738, 0.39425379037857056, 0.1658829301595688, 0.18257692456245422, 0.053949806839227676, 0.2219388484954834, 0.06699283421039581, 0.3113606870174408, 0.3117460310459137, 0.2620885372161865, 0.44250932335853577, 0.03743353113532066, 0.018793320283293724, 0.21549928188323975, 0.02545967511832714, 0.2433217614889145], dtype='float32').reshape([20]),
            paddle.to_tensor([0.15620985627174377, 0.19339390099048615, 0.20242583751678467, 0.3399026095867157, 0.14530900120735168, 0.2669180929660797, 0.28534579277038574, 0.47654077410697937, 0.07448619604110718, 0.44795361161231995, 0.33080142736434937, 0.4415205717086792, 0.0938543975353241, 0.10849729925394058, 0.22064606845378876, 0.1428821086883545, 0.22607405483722687, 0.018928484991192818, 0.46473371982574463, 0.218929260969162], dtype='float32').reshape([20]),
            paddle.to_tensor([0.3617132008075714, 0.04790745675563812, 0.41473516821861267, 0.17052912712097168, 0.4536922574043274, 0.09573332965373993, 0.1666279286146164, 0.4018276035785675, 0.48448705673217773, 0.2329740673303604, 0.2413635402917862, 0.19296686351299286, 0.03929385915398598, 0.01298047136515379, 0.2057037055492401, 0.2854672968387604, 0.2970333397388458, 0.041212115436792374, 0.43023866415023804, 0.08973350375890732], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a33430326591022362acec9607a09fce(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 80, 8, 6], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_229c342c4d2a42c93b5574cac4614fe2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 60, 60], dtype='float16', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_60d97c6ba1391dc0c0dc72235997d16e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6122b1a0441921508d39c58afd6eb10e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_204e71a57a4ae8f77d070c90e0476623(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.03701528534293175, 0.13552413880825043, 0.36369407176971436, 0.3051898181438446, 0.4166172742843628, 0.35933518409729004, 0.41694125533103943, 0.3598581850528717, 0.07525251060724258, 0.35772615671157837, 0.3795485496520996, 0.16370441019535065, 0.2714255750179291, 0.3359270393848419, 0.33418008685112, 0.2950645983219147, 0.4246159791946411, 0.19787676632404327, 0.39024844765663147, 0.49368172883987427], dtype='float32').reshape([20]),
            paddle.to_tensor([0.18175779283046722, 0.15969230234622955, 0.4062221348285675, 0.19537724554538727, 0.451751172542572, 0.2629377245903015, 0.3336249887943268, 0.13863998651504517, 0.13604623079299927, 0.20055435597896576, 0.37493056058883667, 0.0853797197341919, 0.03280937671661377, 0.3193157911300659, 0.3701341450214386, 0.3634513020515442, 0.09948164969682693, 0.37603530287742615, 0.04758481681346893, 0.09828560799360275], dtype='float32').reshape([20]),
            paddle.to_tensor([0.46763095259666443, 0.30252763628959656, 0.38012272119522095, 0.0010471732821315527, 0.1711113601922989, 0.25283515453338623, 0.3369410037994385, 0.34993183612823486, 0.44147056341171265, 0.4019591808319092, 0.04701656848192215, 0.32133644819259644, 0.17570656538009644, 0.13255275785923004, 0.13029523193836212, 0.36551880836486816, 0.059565428644418716, 0.4065360128879547, 0.33459964394569397, 0.162232905626297], dtype='float32').reshape([20]),
            paddle.to_tensor([0.21407364308834076, 0.044441428035497665, 0.3966917395591736, 0.4347981810569763, 0.059058189392089844, 0.06831947714090347, 0.13047346472740173, 0.021013202145695686, 0.348993718624115, 0.15266859531402588, 0.011737208813428879, 0.14322450757026672, 0.08501530438661575, 0.44114601612091064, 0.1174784004688263, 0.036639466881752014, 0.31331777572631836, 0.1086176261305809, 0.2784106433391571, 0.30219796299934387], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8121ff122a866ad7f46e7f3d482e3e91(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6f1bc158519854c96ce4c18119f95ff1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_623d84ccd2ed545d0bf6955d58c7d3e7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 1104, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([1104], dtype='float32', min=0, max=0.5),
            paddle.uniform([1104], dtype='float32', min=0, max=0.5),
            paddle.uniform([1104], dtype='float32', min=0, max=0.5),
            paddle.uniform([1104], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bc70696477abe5858d665b60f3258817(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 2048, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8907ebe5b54236f34c2c366f97ba615f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 40, 40], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5b87fd4da397f07b5966f2923c3da6dc(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.42698538303375244, 0.01027851551771164, 0.36898669600486755, 0.48723646998405457, 0.45757636427879333, 0.13891032338142395, 0.21012334525585175, 0.4238586127758026, 0.2984996736049652, 0.2596283555030823, 0.1683272272348404, 0.1742369830608368, 0.1710386574268341, 0.33586353063583374, 0.1874329149723053, 0.4753015339374542], dtype='float32').reshape([16]),
            paddle.to_tensor([0.23305505514144897, 0.4285769760608673, 0.3383118808269501, 0.4062415659427643, 0.41671764850616455, 0.19374629855155945, 0.37637826800346375, 0.24503253400325775, 0.4414917826652527, 0.08088277280330658, 0.11541610211133957, 0.06672962009906769, 0.49023571610450745, 0.07014976441860199, 0.4867230951786041, 0.12612076103687286], dtype='float32').reshape([16]),
            paddle.to_tensor([0.4976913034915924, 0.19199883937835693, 0.0007141417008824646, 0.4022161066532135, 0.36167773604393005, 0.18671512603759766, 0.21353119611740112, 0.12060721963644028, 0.4841749370098114, 0.3622707724571228, 0.066469706594944, 0.3608860671520233, 0.434561163187027, 0.30693331360816956, 0.36281418800354004, 0.3433946967124939], dtype='float32').reshape([16]),
            paddle.to_tensor([0.30920878052711487, 0.28134068846702576, 0.3952723443508148, 0.2030743807554245, 0.31684160232543945, 0.2966546416282654, 0.1881922036409378, 0.09617839753627777, 0.12445132434368134, 0.3893156945705414, 0.4915817081928253, 0.2420330047607422, 0.4802953898906708, 0.4450894296169281, 0.07402262836694717, 0.3293006122112274], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_50d223a0100c42923eb0df076c8c3721(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 100, 168], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ea9f98a236307734504362036f512508(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 1200, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([1200], dtype='float32', min=0, max=0.5),
            paddle.uniform([1200], dtype='float32', min=0, max=0.5),
            paddle.uniform([1200], dtype='float32', min=0, max=0.5),
            paddle.uniform([1200], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c3119127655093771f0117dddb4208ac(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.09031738340854645, 0.17440883815288544, 0.04279743507504463, 0.40296441316604614, 0.23795054852962494, 0.12811516225337982, 0.0001078246277756989, 0.11659228801727295, 0.47218525409698486, 0.41116833686828613, 0.13128750026226044, 0.18910425901412964, 0.012069552205502987, 0.4505127966403961, 0.1423146277666092, 0.41904863715171814], dtype='float32').reshape([16]),
            paddle.to_tensor([0.42944902181625366, 0.2721019387245178, 0.09156108647584915, 0.1450803279876709, 0.3530854880809784, 0.3048689365386963, 0.15648230910301208, 0.30870333313941956, 0.46537521481513977, 0.1393839418888092, 0.04752950742840767, 0.0033020873088389635, 0.45425868034362793, 0.09217797219753265, 0.3406468629837036, 0.47103479504585266], dtype='float32').reshape([16]),
            paddle.to_tensor([0.3231598138809204, 0.3863285481929779, 0.3334227502346039, 0.06559441238641739, 0.46898603439331055, 0.21538883447647095, 0.2943769693374634, 0.05692563205957413, 0.2740805149078369, 0.33003315329551697, 0.22779391705989838, 0.41004475951194763, 0.27126502990722656, 0.37071123719215393, 0.1665962189435959, 0.24811811745166779], dtype='float32').reshape([16]),
            paddle.to_tensor([0.4457961916923523, 0.35115545988082886, 0.18114399909973145, 0.23636232316493988, 0.1378270834684372, 0.05114223062992096, 0.4071735739707947, 0.30732157826423645, 0.21397638320922852, 0.473155677318573, 0.29961729049682617, 0.08632678538560867, 0.16009584069252014, 0.20644532144069672, 0.3045211732387543, 0.2129514217376709], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b22e9848e7dc653c9b6541e201d805a4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 1824, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([1824], dtype='float32', min=0, max=0.5),
            paddle.uniform([1824], dtype='float32', min=0, max=0.5),
            paddle.uniform([1824], dtype='float32', min=0, max=0.5),
            paddle.uniform([1824], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2afed2188fc5658e23815e9de49cc7fd(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_73bc17bd75d5f77e970b0b6758de3277(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 350, 25], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_79ce24998f3e759d86ab3b960574ab50(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 20, 20], dtype='float16', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f03a2c54d78746e32418082eb7ba541e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 17, 17], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d0ee06d1d50e024c538f90647fdd85eb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d6e688879d583f7e5a7f0bd5d9d746df(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 672, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a81f8e974259482675dca744840ce209(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 62, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([62], dtype='float32', min=0, max=0.5),
            paddle.uniform([62], dtype='float32', min=0, max=0.5),
            paddle.uniform([62], dtype='float32', min=0, max=0.5),
            paddle.uniform([62], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3ed55101396cef1aebedb1b2f7de279b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 80, 80], dtype='float16', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8e0409ce6f1d287aed1b82add7f827e6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b2286ecc58aae8c27c5dc5ab2b12f31a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 36, 28, 50], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fce65fb236704f8df35b21778860d531(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 160, 17, 17], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_955e1571a68c2039cd04a0e3f11eb212(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c149bad194d58a7cf519e4772571a1c2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 240, 12, 12], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5ed6de9e531cc6cc951a718a979a5d3c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 480, 20, 20], dtype='float16', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ea00d92eb13b92b6930b53cc3ccc47a0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4f0fd0c676bc139bcca0f149bb46bcf2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.2599691152572632, 0.18844521045684814, 0.2920176386833191, 0.43205541372299194, 0.252051442861557, 0.3996197581291199, 0.2179143875837326, 0.12297244369983673, 0.34301817417144775, 0.10269919782876968, 0.18104054033756256, 0.30696502327919006, 0.15704679489135742, 0.28662109375, 0.41251227259635925, 0.22152675688266754, 0.3121697008609772, 0.10792829841375351], dtype='float32').reshape([18]),
            paddle.to_tensor([0.007445076480507851, 0.038229554891586304, 0.32417774200439453, 0.36292240023612976, 0.3067094683647156, 0.22008922696113586, 0.032827068120241165, 0.4915253520011902, 0.1554366648197174, 0.16347558796405792, 0.1436070203781128, 0.2366529256105423, 0.029968485236167908, 0.4616928994655609, 0.4727180600166321, 0.17756760120391846, 0.05492089316248894, 0.25044241547584534], dtype='float32').reshape([18]),
            paddle.to_tensor([0.10169678926467896, 0.18867194652557373, 0.4156879782676697, 0.2792581021785736, 0.08890403062105179, 0.44633251428604126, 0.2514943480491638, 0.3614618480205536, 0.29861781001091003, 0.029609786346554756, 0.043633099645376205, 0.007825125008821487, 0.4396437108516693, 0.07884520292282104, 0.3695366680622101, 0.03784683346748352, 0.20211702585220337, 0.46350952982902527], dtype='float32').reshape([18]),
            paddle.to_tensor([0.1473998874425888, 0.04307445511221886, 0.49573981761932373, 0.12878228724002838, 0.479661226272583, 0.03741150721907616, 0.38331425189971924, 0.10835999250411987, 0.37497642636299133, 0.23661227524280548, 0.49974721670150757, 0.2315317541360855, 0.30959850549697876, 0.4067213535308838, 0.133538618683815, 0.4623015820980072, 0.1741451919078827, 0.40392327308654785], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c8018c476c49557c79391b85ee8a14f4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 64, 48], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_80618ec09d85a360ca4bb9a6cdd5ddd5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_3a47a120d62a3f3a422a747f511df8d9
    def get_inputs(self):
        return [
            paddle.uniform([1, 1536, 10, 10], dtype='float16', min=0, max=0.5),
            paddle.uniform([1536], dtype='float32', min=0, max=0.5),
            paddle.uniform([1536], dtype='float32', min=0, max=0.5),
            paddle.uniform([1536], dtype='float32', min=0, max=0.5),
            paddle.uniform([1536], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9a514b6704e0bb9be7d5318fdd779588(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.15739358961582184, 0.4364131987094879, 0.2903406620025635, 0.18242813646793365, 0.09650538116693497, 0.16300012171268463, 0.1816529631614685, 0.07451768219470978], dtype='float32').reshape([8]),
            paddle.to_tensor([0.363371878862381, 0.2434200644493103, 0.11767067015171051, 0.34985285997390747, 0.02456619404256344, 0.0026882486417889595, 0.047441910952329636, 0.24458029866218567], dtype='float32').reshape([8]),
            paddle.to_tensor([0.3221726715564728, 0.48362967371940613, 0.0948689877986908, 0.4931567311286926, 0.09816846996545792, 0.012949289754033089, 0.2674148678779602, 0.4672984182834625], dtype='float32').reshape([8]),
            paddle.to_tensor([0.03172881901264191, 0.3688262104988098, 0.4013787806034088, 0.39432403445243835, 0.07124710828065872, 0.2600475549697876, 0.4418739974498749, 0.23126369714736938], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d67ebb77c8963001d70e9bedeb759bd8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_af91295b59c5fca17f890a802deb001f
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 32, 16, 16], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b1f8c57a96ac87455883a1c03cde7874(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 20, 20], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_20a4151a7d8d2f84eda8efbfc3dd3382(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 80, 80], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b9701a2fcddbf1a292d4499882b279be(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 38, 38], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9c681ed43e80a75a4ce3eac352fc99b4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 256, 512], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1bd65f8b66a9a27a5a6e440cb030323f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 16, 12], dtype='float16', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2afe8847fe333d0d2c78ba47be862446(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 35, 35], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5f5281b792ccc920bc3c90cf7d0ea893(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 160, 160], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f45a9f65212f712d627b53a023240e28(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 16, 16], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fe357bd2132e30fdfeb88040c3888d08(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 128, 128], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_01c289ac9d05072efc778c1a98e677cd(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fe687a83c7395703e953e11f1301849f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_aa92acc5daa3be34542850d1371b9f27(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5d83adfc976e364b2188f88785a89c56(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 80, 73, 73], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1f6a740214568bfffc91c14bcd016a71(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 6, 40], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a7d621546fe8aeb87ee36491b37c8ee4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.03854002058506012, 0.33410516381263733, 0.23224598169326782, 0.4604056477546692, 0.44468268752098083, 0.4801070988178253, 0.3489528298377991, 0.46098434925079346, 0.35148999094963074, 0.2937585413455963, 0.4493763744831085, 0.4553394019603729, 0.356566458940506, 0.35647040605545044, 0.19161748886108398, 0.31569117307662964, 0.3274744749069214, 0.11810709536075592, 0.004275303799659014, 0.27372801303863525, 0.3346424698829651, 0.27778640389442444, 0.2277463674545288, 0.07122800499200821, 0.2155870497226715, 0.3085203766822815, 0.06618434190750122, 0.01128560584038496, 0.2721944749355316, 0.03451402112841606], dtype='float32').reshape([30]),
            paddle.to_tensor([0.44058698415756226, 0.36066821217536926, 0.36918729543685913, 0.21019165217876434, 0.47329458594322205, 0.425272136926651, 0.005196261219680309, 0.03027796559035778, 0.06712544709444046, 0.02865474671125412, 0.10408521443605423, 0.4240351617336273, 0.05414697527885437, 0.18295077979564667, 0.06632418185472488, 0.4297891855239868, 0.2553059458732605, 0.10751649737358093, 0.05292714014649391, 0.48918256163597107, 0.4991578757762909, 0.19652362167835236, 0.06914449483156204, 0.491687148809433, 0.25373777747154236, 0.2093939185142517, 0.12459281086921692, 0.16079843044281006, 0.03252832591533661, 0.3358388841152191], dtype='float32').reshape([30]),
            paddle.to_tensor([0.23855586349964142, 0.3821546137332916, 0.4124200940132141, 0.09898544102907181, 0.24659885466098785, 0.05742821842432022, 0.020755523815751076, 0.38360336422920227, 0.03470608964562416, 0.23848596215248108, 0.23767104744911194, 0.17973414063453674, 0.07129083573818207, 0.20643410086631775, 0.1874871701002121, 0.45205554366111755, 0.4049524962902069, 0.3354417383670807, 0.3166499137878418, 0.20479567348957062, 0.010119233280420303, 0.07157769054174423, 0.4781997501850128, 0.1900302618741989, 0.0464925542473793, 0.2533140480518341, 0.05914850905537605, 0.20443053543567657, 0.17966406047344208, 0.10934717953205109], dtype='float32').reshape([30]),
            paddle.to_tensor([0.3234504759311676, 0.2885006368160248, 0.4568757116794586, 0.13631625473499298, 0.40481555461883545, 0.11760009825229645, 0.18809157609939575, 0.4023899734020233, 0.38684481382369995, 0.15183691680431366, 0.2103983610868454, 0.18298636376857758, 0.42842745780944824, 0.05933663249015808, 0.29093194007873535, 0.3973644971847534, 0.1231275126338005, 0.15710271894931793, 0.2686699330806732, 0.37308600544929504, 0.4179992377758026, 0.013282299973070621, 0.36778801679611206, 0.3355053961277008, 0.4390391707420349, 0.11476448178291321, 0.07228008657693863, 0.040548164397478104, 0.09864286333322525, 0.01679510809481144], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_268ed7fe830c2a9b0807c0851cd9624d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_75ac840bb5a2c5077b68bddcab1f1008(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 576, 32, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([576], dtype='float32', min=0, max=0.5),
            paddle.uniform([576], dtype='float32', min=0, max=0.5),
            paddle.uniform([576], dtype='float32', min=0, max=0.5),
            paddle.uniform([576], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9822818a17920b1c2cfeeaad3bc862a1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 32, 128], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_11e8a113028b0b90c736dffe07c4e3be(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_62b1c7b03d597e3952ac5a7571654bb1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 208, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([208], dtype='float32', min=0, max=0.5),
            paddle.uniform([208], dtype='float32', min=0, max=0.5),
            paddle.uniform([208], dtype='float32', min=0, max=0.5),
            paddle.uniform([208], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d8d6f16cdbf1f1096df1810058621e0f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8d2a39122b7a526c9e2e4525cdb487bb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 102, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([102], dtype='float32', min=0, max=0.5),
            paddle.uniform([102], dtype='float32', min=0, max=0.5),
            paddle.uniform([102], dtype='float32', min=0, max=0.5),
            paddle.uniform([102], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6d2663cf504fd5ddc1e076a5382cdf53(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 2432, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([2432], dtype='float32', min=0, max=0.5),
            paddle.uniform([2432], dtype='float32', min=0, max=0.5),
            paddle.uniform([2432], dtype='float32', min=0, max=0.5),
            paddle.uniform([2432], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e0f15764a58e7ecbce9119f3eafcd630(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 320, 4, 3], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3a2f02479652f72aa6fd2920bc5b11df(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 17, 17], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a35b056ff6b22e643a44e99fbe23ecbf(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 64, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_35252d7289e9a4700b3ee6ee08beb8f2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 840, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([840], dtype='float32', min=0, max=0.5),
            paddle.uniform([840], dtype='float32', min=0, max=0.5),
            paddle.uniform([840], dtype='float32', min=0, max=0.5),
            paddle.uniform([840], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_72cb8a36e3cd1ba6503da1b3e3cb740d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.2075052112340927, 0.18772132694721222, 0.4868921935558319, 0.16453373432159424, 0.2129572182893753, 0.4805675446987152, 0.20072528719902039, 0.4558633863925934, 0.3406354784965515, 0.28522080183029175, 0.06849133223295212, 0.34575164318084717, 0.4773293435573578, 0.00454490352421999, 0.19724971055984497, 0.3763692378997803, 0.32398521900177, 0.3486805558204651, 0.1691322773694992, 0.15208061039447784, 0.4385707974433899, 0.35875749588012695, 0.00855453871190548, 0.2135036736726761, 0.37770023941993713, 0.2467956840991974, 0.2909843623638153, 0.46007049083709717, 0.15563051402568817, 0.26000353693962097], dtype='float32').reshape([30]),
            paddle.to_tensor([0.055800557136535645, 0.39768245816230774, 0.4156719744205475, 0.24206724762916565, 0.4551424980163574, 0.11802821606397629, 0.3056368827819824, 0.2576403319835663, 0.004167511127889156, 0.14304612576961517, 0.44466474652290344, 0.31459248065948486, 0.17024849355220795, 0.4108673930168152, 0.3429698348045349, 0.05689722299575806, 0.27861523628234863, 0.076084204018116, 0.1982099860906601, 0.37670519948005676, 0.10706327110528946, 0.31387585401535034, 0.2896594703197479, 0.21702058613300323, 0.49650129675865173, 0.1255219429731369, 0.3864128291606903, 0.17800995707511902, 0.4746052324771881, 0.201201394200325], dtype='float32').reshape([30]),
            paddle.to_tensor([0.15598592162132263, 0.43563511967658997, 0.3149092197418213, 0.1427057981491089, 0.24323050677776337, 0.20121333003044128, 0.21206840872764587, 0.3695825934410095, 0.029451441019773483, 0.31913119554519653, 0.3703898787498474, 0.05309058353304863, 0.015544023364782333, 0.3036327064037323, 0.3162838816642761, 0.2525847852230072, 0.4456600844860077, 0.1703375279903412, 0.4382953941822052, 0.307805597782135, 0.07880613952875137, 0.35737305879592896, 0.05623743310570717, 0.3452281057834625, 0.28809842467308044, 0.18809348344802856, 0.051646195352077484, 0.1250266432762146, 0.17307446897029877, 0.006441348697990179], dtype='float32').reshape([30]),
            paddle.to_tensor([0.15465512871742249, 0.46554046869277954, 0.31007012724876404, 0.08057954907417297, 0.26091665029525757, 0.046277474611997604, 0.1541651338338852, 0.3507945239543915, 0.19077078998088837, 0.4451403319835663, 0.0010903936345130205, 0.3821336328983307, 0.1351621150970459, 0.41971781849861145, 0.42123621702194214, 0.38816848397254944, 0.291262686252594, 0.49469855427742004, 0.028240058571100235, 0.18375444412231445, 0.18425005674362183, 0.01955215446650982, 0.14383955299854279, 0.4255685806274414, 0.19019408524036407, 0.2529469132423401, 0.43165168166160583, 0.42014384269714355, 0.11915966123342514, 0.2500513195991516], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c0a42c4d0cd0acc18c313f59deac9fcf(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 336, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c41c9e48dda54688f69a7eae23f48105(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.19620569050312042, 0.031492941081523895, 0.2106497883796692, 0.1424427032470703, 0.32582414150238037, 0.08721789717674255, 0.02585563063621521, 0.14958356320858002, 0.4654078781604767, 0.43395012617111206, 0.3635713756084442, 0.07266109436750412, 0.1341409683227539, 0.37401527166366577, 0.34796470403671265, 0.23951496183872223, 0.4384024143218994, 0.45379483699798584, 0.2872329652309418, 0.06730920076370239], dtype='float32').reshape([20]),
            paddle.to_tensor([0.36349770426750183, 0.19930313527584076, 0.0373176708817482, 0.2981964647769928, 0.14159829914569855, 0.3462960422039032, 0.25911515951156616, 0.260983943939209, 0.17249420285224915, 0.28726598620414734, 0.23418566584587097, 0.31731143593788147, 0.48754993081092834, 0.414186954498291, 0.30464881658554077, 0.36652031540870667, 0.09613595902919769, 0.33505672216415405, 0.11249623447656631, 0.38995474576950073], dtype='float32').reshape([20]),
            paddle.to_tensor([0.3727289140224457, 0.4239748418331146, 0.294953316450119, 0.36847662925720215, 0.3115859031677246, 0.01444285549223423, 0.4754159450531006, 0.14525781571865082, 0.2113707959651947, 0.4893692433834076, 0.20734094083309174, 0.45894381403923035, 0.28134989738464355, 0.48293864727020264, 0.19296126067638397, 0.22873075306415558, 0.28802159428596497, 0.13374784588813782, 0.2712099254131317, 0.21553577482700348], dtype='float32').reshape([20]),
            paddle.to_tensor([0.3054523169994354, 0.05085914209485054, 0.4459637999534607, 0.3835670053958893, 0.42204299569129944, 0.1266128569841385, 0.35028666257858276, 0.04984287917613983, 0.36014148592948914, 0.06270264089107513, 0.2037433683872223, 0.3994835615158081, 0.31325557827949524, 0.14609454572200775, 0.175234854221344, 0.22230878472328186, 0.44134464859962463, 0.3271143436431885, 0.23150944709777832, 0.3158828020095825], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_518c63d08a0a618b675e0240f4a2b37a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 240, 240], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e9e03ce9eb9db3725cf5453210511178(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 2048, 64, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b897f709e1d59bc561a93b8325ce60ee(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.2807348072528839, 0.17519018054008484, 0.46051865816116333, 0.33139556646347046, 0.17400404810905457, 0.3336958885192871, 0.4005514979362488, 0.1828192174434662, 0.3977139890193939, 0.0407843217253685, 0.47792837023735046, 0.05004124715924263, 0.31266888976097107, 0.07556207478046417, 0.32104018330574036, 0.3560953438282013, 0.26072466373443604, 0.20191723108291626, 0.4459124505519867, 0.06484969705343246, 0.21714545786380768, 0.07941640913486481, 0.31165704131126404, 0.20094892382621765, 0.13529478013515472, 0.03306750953197479, 0.10462642461061478, 0.25746145844459534, 0.07237224280834198, 0.012029342353343964], dtype='float32').reshape([30]),
            paddle.to_tensor([0.02098606526851654, 0.3861754238605499, 0.14546604454517365, 0.25701284408569336, 0.4384409487247467, 0.15156586468219757, 0.11545055359601974, 0.3594439923763275, 0.15305571258068085, 0.13564755022525787, 0.35108914971351624, 0.36091718077659607, 0.42335453629493713, 0.4285984933376312, 0.31779298186302185, 0.020333576947450638, 0.47042617201805115, 0.39312806725502014, 0.40487903356552124, 0.10820832848548889, 0.21925422549247742, 0.3363596796989441, 0.3951508700847626, 0.0009742670808918774, 0.4593029320240021, 0.03434298187494278, 0.23519748449325562, 0.3298169672489166, 0.3810805082321167, 0.493894100189209], dtype='float32').reshape([30]),
            paddle.to_tensor([0.3936716914176941, 0.1868979036808014, 0.4343361556529999, 0.17651689052581787, 0.25284910202026367, 0.4182015061378479, 0.3273410499095917, 0.05400574207305908, 0.3134927749633789, 0.36546969413757324, 0.29816538095474243, 0.0012102744076400995, 0.12380670011043549, 0.2390277236700058, 0.16621087491512299, 0.019901346415281296, 0.18703928589820862, 0.033815253525972366, 0.34364864230155945, 0.3084176480770111, 0.4053995609283447, 0.22159157693386078, 0.07916533946990967, 0.11100977659225464, 0.017910592257976532, 0.28758472204208374, 0.43971124291419983, 0.35102224349975586, 0.2733428180217743, 0.22280468046665192], dtype='float32').reshape([30]),
            paddle.to_tensor([0.13021613657474518, 0.27198830246925354, 0.2716083824634552, 0.1448950171470642, 0.45746946334838867, 0.19811519980430603, 0.16429711878299713, 0.39784109592437744, 0.41883957386016846, 0.08993741869926453, 0.08966459333896637, 0.4835937023162842, 0.14938999712467194, 0.43088585138320923, 0.2679612636566162, 0.4171455502510071, 0.37590280175209045, 0.07574981451034546, 0.1595076024532318, 0.2771385610103607, 0.049204614013433456, 0.19116492569446564, 0.08053045719861984, 0.25107505917549133, 0.15441691875457764, 0.07375484704971313, 0.33499768376350403, 0.11138544976711273, 0.054707325994968414, 0.33172014355659485], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cc1faf6f796f1bb90da08d6fb8b18275(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9e0f0195b29b2cb2b6eedbca9410741b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.42771992087364197, 0.2705816626548767, 0.3120092451572418, 0.12464036792516708, 0.16572101414203644, 0.09446485340595245, 0.17453716695308685, 0.2142968475818634, 0.2794326841831207, 0.3338963985443115, 0.014511581510305405, 0.4311477839946747, 0.45057177543640137, 0.20996792614459991, 0.2902737855911255, 0.44895535707473755, 0.48028889298439026, 0.010303592309355736], dtype='float32').reshape([18]),
            paddle.to_tensor([0.1965292990207672, 0.08421900868415833, 0.09084449708461761, 0.09205681830644608, 0.41377925872802734, 0.24575936794281006, 0.12763336300849915, 0.1739816814661026, 0.15445730090141296, 0.3110993802547455, 0.12313903123140335, 0.03161938115954399, 0.4920307397842407, 0.20659111440181732, 0.47049471735954285, 0.29516100883483887, 0.15071044862270355, 0.4938000738620758], dtype='float32').reshape([18]),
            paddle.to_tensor([0.4868205487728119, 0.06334175169467926, 0.3488852083683014, 0.47457635402679443, 0.23269778490066528, 0.4668607711791992, 0.18645137548446655, 0.32632842659950256, 0.0236972589045763, 0.3794105350971222, 0.4426124095916748, 0.25269410014152527, 0.046995747834444046, 0.019507424905896187, 0.26903992891311646, 0.3430950939655304, 0.4194348156452179, 0.49133408069610596], dtype='float32').reshape([18]),
            paddle.to_tensor([0.011835764162242413, 0.015244237147271633, 0.0166766420006752, 0.08947564661502838, 0.012623508460819721, 0.24461306631565094, 0.38933971524238586, 0.1588827520608902, 0.32143285870552063, 0.1636383980512619, 0.33900874853134155, 0.3416382670402527, 0.30049562454223633, 0.19985619187355042, 0.4916781187057495, 0.3442310392856598, 0.4705667197704315, 0.4074234366416931], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9cde48c19eaae5df03ffe522c53c9c38(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 5, 5], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3a661a55a7fad362bb165f61941e5011(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 144, 31, 31], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cb46dd70d7b5e55c4a33f4fad36ab71d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 480, 480], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.10708365589380264, 0.4907958209514618, 0.1264253407716751, 0.033839210867881775, 0.21913166344165802, 0.20543362200260162, 0.1397368609905243, 0.24599944055080414, 0.19865354895591736, 0.4553896188735962, 0.25497308373451233, 0.1997000128030777, 0.06011129915714264, 0.17504402995109558, 0.3987213671207428, 0.16457942128181458, 0.2612628638744354, 0.36759549379348755, 0.4283401370048523, 0.17485608160495758, 0.37052273750305176, 0.3466479480266571, 0.3141423761844635, 0.19439290463924408], dtype='float32').reshape([24]),
            paddle.to_tensor([0.385394811630249, 0.23090143501758575, 0.06340251863002777, 0.40439870953559875, 0.48088735342025757, 0.4659495949745178, 0.11752723157405853, 0.17483951151371002, 0.2015780806541443, 0.35354816913604736, 0.3408747613430023, 0.31361129879951477, 0.004391449503600597, 0.41978219151496887, 0.2683563232421875, 0.009044642560184002, 0.1499761939048767, 0.0795159637928009, 0.4598643183708191, 0.46451976895332336, 0.4352932572364807, 0.200784832239151, 0.2986951470375061, 0.08159729093313217], dtype='float32').reshape([24]),
            paddle.to_tensor([0.30373135209083557, 0.23554372787475586, 0.11411876976490021, 0.007188595365732908, 0.30577921867370605, 0.4928114712238312, 0.4879496693611145, 0.4292300343513489, 0.3993423879146576, 0.4184325933456421, 0.06358431279659271, 0.2275601476430893, 0.26678866147994995, 0.1451624184846878, 0.3610542416572571, 0.4932154417037964, 0.3619392514228821, 0.12942680716514587, 0.21253293752670288, 0.17643049359321594, 0.369096964597702, 0.4344049394130707, 0.3183712959289551, 0.3936246633529663], dtype='float32').reshape([24]),
            paddle.to_tensor([0.03583214432001114, 0.4135962128639221, 0.38425421714782715, 0.45321494340896606, 0.1443857103586197, 0.3588201105594635, 0.4781494438648224, 0.4932284355163574, 0.34462618827819824, 0.10204680263996124, 0.3564858138561249, 0.457585871219635, 0.029533028602600098, 0.2780682444572449, 0.08922702819108963, 0.1252545416355133, 0.2379799783229828, 0.38973546028137207, 0.28186917304992676, 0.09574630856513977, 0.0640195980668068, 0.27598029375076294, 0.14748385548591614, 0.294552743434906], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fe2d6412a4949ed7695fbb7d210274d3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 128, 128], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_12de28554061a33e3c070c052e56e32b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3b3851018604ccb275c28a8a2b15424a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_af91295b59c5fca17f890a802deb001f
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 4, 8, 8], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_39c44eae2c4401a94b8973a0dde89986(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.3279154598712921, 0.008977380581200123, 0.4403607249259949, 0.3251815438270569, 0.2693515717983246, 0.19038520753383636, 0.40957221388816833, 0.4930720031261444, 0.10828432440757751, 0.2739822566509247, 0.08498004078865051, 0.41744768619537354, 0.3498801589012146, 0.1710241734981537, 0.33348220586776733, 0.3174493610858917, 0.24928836524486542, 0.039812203496694565, 0.32162928581237793, 0.0006970174727030098, 0.049946434795856476, 0.032991793006658554, 0.47429946064949036, 0.0034879869781434536, 0.4172254204750061, 0.33174675703048706, 0.32598739862442017, 0.3212662935256958, 0.4257323741912842, 0.3836548626422882], dtype='float32').reshape([30]),
            paddle.to_tensor([0.33722028136253357, 0.41733765602111816, 0.026929186657071114, 0.27183735370635986, 0.42409858107566833, 0.017606498673558235, 0.03160328418016434, 0.10645551979541779, 0.403899222612381, 0.43151289224624634, 0.1311485916376114, 0.424635648727417, 0.01462620124220848, 0.004279719665646553, 0.3082200586795807, 0.1772070676088333, 0.07122085243463516, 0.4416133761405945, 0.34284138679504395, 0.45016419887542725, 0.009817924350500107, 0.45327067375183105, 0.47264227271080017, 0.20561785995960236, 0.48514991998672485, 0.07553762197494507, 0.32653507590293884, 0.09611537307500839, 0.20177386701107025, 0.45897966623306274], dtype='float32').reshape([30]),
            paddle.to_tensor([0.3373372256755829, 0.30489858984947205, 0.2370608001947403, 0.024108698591589928, 0.021733444184064865, 0.06666823476552963, 0.47646665573120117, 0.47139257192611694, 0.2873219847679138, 0.3200356960296631, 0.022316494956612587, 0.46719038486480713, 0.2197241187095642, 0.23918330669403076, 0.4218429625034332, 0.1296909898519516, 0.22596223652362823, 0.3217429518699646, 0.47816571593284607, 0.29742178320884705, 0.12377076596021652, 0.35540494322776794, 0.4000778794288635, 0.09981158375740051, 0.3018980026245117, 0.4894793629646301, 0.35160449147224426, 0.45314621925354004, 0.3966536819934845, 0.18012377619743347], dtype='float32').reshape([30]),
            paddle.to_tensor([0.013622485101222992, 0.4764893054962158, 0.12385441362857819, 0.12742598354816437, 0.20127716660499573, 0.3852529525756836, 0.06194481626152992, 0.10330528765916824, 0.4357061982154846, 0.2655073404312134, 0.27962204813957214, 0.3587197959423065, 0.06547462195158005, 0.09360902011394501, 0.4616270065307617, 0.01104559563100338, 0.44989603757858276, 0.42468753457069397, 0.4306887090206146, 0.23537543416023254, 0.23945021629333496, 0.4083080291748047, 0.271007239818573, 0.2958475351333618, 0.08192956447601318, 0.2041454166173935, 0.297640323638916, 0.4903000295162201, 0.027582287788391113, 0.4266517758369446], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_548e5b257d3e6edd65480a65972b2be6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c3f352fca7c4a9234b3a30630a15e146(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 20, 20], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7334d82914828d2e07ceb816ab037200(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 320, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2d732a450914bb13d7cb81f12a9ed0de(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 32, 64], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.2970661520957947, 0.47778454422950745, 0.29755905270576477, 0.3202434778213501, 0.11913265287876129, 0.04810132458806038, 0.27793487906455994, 0.4848255217075348, 0.13478921353816986, 0.4190378487110138, 0.2051885426044464, 0.010443661361932755, 0.40233227610588074, 0.2493269294500351, 0.3682028651237488, 0.4194626808166504, 0.282941609621048, 0.25733381509780884], dtype='float32').reshape([18]),
            paddle.to_tensor([0.19413572549819946, 0.07172232121229172, 0.1233024075627327, 0.45966410636901855, 0.25606104731559753, 0.35481417179107666, 0.01930871792137623, 0.03739221394062042, 0.166746586561203, 0.4862764775753021, 0.2654626965522766, 0.3418598175048828, 0.42796117067337036, 0.4177042245864868, 0.15560731291770935, 0.2067674994468689, 0.20754195749759674, 0.3476110100746155], dtype='float32').reshape([18]),
            paddle.to_tensor([0.41484713554382324, 0.4177927076816559, 0.2435210645198822, 0.3053989112377167, 0.07422871142625809, 0.02318744733929634, 0.455969899892807, 0.4496648609638214, 0.018972402438521385, 0.17645740509033203, 0.2482890784740448, 0.05471453070640564, 0.4989457428455353, 0.17246513068675995, 0.2713801860809326, 0.2153564989566803, 0.3421648144721985, 0.013392528519034386], dtype='float32').reshape([18]),
            paddle.to_tensor([0.415118932723999, 0.2757967412471771, 0.07190759479999542, 0.39650294184684753, 0.14244157075881958, 0.4827318489551544, 0.21396705508232117, 0.24089626967906952, 0.4032130539417267, 0.45408710837364197, 0.12424368411302567, 0.21412554383277893, 0.46601757407188416, 0.35666343569755554, 0.037064943462610245, 0.3030051290988922, 0.19878119230270386, 0.4387251138687134], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fc69e580e1f5bf769ce9df1dbdadaf51(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 72, 32, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9d8bd1b2c7f45dc387a114a33b4648ef(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 12, 640, 640], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.21621985733509064, 0.01984327845275402, 0.3708823621273041, 0.04956528916954994, 0.209158793091774, 0.49928420782089233, 0.3802245557308197, 0.45398837327957153, 0.49882635474205017, 0.12584152817726135, 0.26952987909317017, 0.02119634486734867], dtype='float32').reshape([12]),
            paddle.to_tensor([0.05505439639091492, 0.40198978781700134, 0.19388161599636078, 0.485247403383255, 0.4500722587108612, 0.3015396296977997, 0.04115107282996178, 0.19783607125282288, 0.1525815725326538, 0.3035392463207245, 0.10010365396738052, 0.3729844093322754], dtype='float32').reshape([12]),
            paddle.to_tensor([0.08246023207902908, 0.07334030419588089, 0.49089229106903076, 0.1506827026605606, 0.3095208406448364, 0.456937700510025, 0.36099350452423096, 0.12137600779533386, 0.2977505326271057, 0.1561228334903717, 0.34629714488983154, 0.4617937505245209], dtype='float32').reshape([12]),
            paddle.to_tensor([0.15187968313694, 0.15579260885715485, 0.3416655957698822, 0.42544111609458923, 0.15964028239250183, 0.483530730009079, 0.42174026370048523, 0.42741453647613525, 0.0975966677069664, 0.27943313121795654, 0.30923691391944885, 0.3455451726913452], dtype='float32').reshape([12]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_35491c961526e879d50d197c2e6548e3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 1888, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([1888], dtype='float32', min=0, max=0.5),
            paddle.uniform([1888], dtype='float32', min=0, max=0.5),
            paddle.uniform([1888], dtype='float32', min=0, max=0.5),
            paddle.uniform([1888], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_b4f4da3a8d44d1b5b3a2903a097df186(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3876249f6b63dac34902e628ca392b86(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_b4f4da3a8d44d1b5b3a2903a097df186
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 256, 256], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_934ad465a492909700d4b4e94475bb82(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 320, 320], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_98ad11f12769c1d4440740148e4e95fe(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 12, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.07673622667789459, 0.009042399004101753, 0.028564313426613808, 0.4471937119960785, 0.2697567343711853, 0.2745305895805359, 0.3640361726284027, 0.3598288595676422, 0.277924120426178, 0.4821460247039795, 0.4663904309272766, 0.017126988619565964], dtype='float32').reshape([12]),
            paddle.to_tensor([0.05275062099099159, 0.46252959966659546, 0.3910539150238037, 0.05401350185275078, 0.38913413882255554, 0.15031574666500092, 0.377143919467926, 0.2397817224264145, 0.39673978090286255, 0.3878614008426666, 0.2997305691242218, 0.3329770565032959], dtype='float32').reshape([12]),
            paddle.to_tensor([0.2548538148403168, 0.26927489042282104, 0.07289686053991318, 0.030649442225694656, 0.19465291500091553, 0.33331015706062317, 0.21019065380096436, 0.3731761872768402, 0.17826154828071594, 0.27383244037628174, 0.4794338047504425, 0.47804346680641174], dtype='float32').reshape([12]),
            paddle.to_tensor([0.4956066310405731, 0.03799009323120117, 0.15195097029209137, 0.24556611478328705, 0.3213540315628052, 0.14325706660747528, 0.30590441823005676, 0.13372406363487244, 0.02502761222422123, 0.10303597152233124, 0.18738970160484314, 0.009186864830553532], dtype='float32').reshape([12]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0553766af004ab2053e7e77bda0f041d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 120, 120], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6e1cb41af5e8a27ab3f42479816490bb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 32, 64], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.2709074020385742, 0.42205631732940674, 0.44701823592185974, 0.1631680279970169, 0.07306227087974548, 0.39965060353279114, 0.012542388401925564, 0.41177985072135925, 0.24388183653354645, 0.24367913603782654, 0.15132740139961243, 0.0827106386423111, 0.1666010022163391, 0.18860551714897156, 0.14731548726558685, 0.279814213514328, 0.27248719334602356, 0.07942063361406326], dtype='float32').reshape([18]),
            paddle.to_tensor([0.15371504426002502, 0.3033253252506256, 0.10522458702325821, 0.43852856755256653, 0.0509333573281765, 0.48600244522094727, 0.2953796684741974, 0.2849532961845398, 0.26585653424263, 0.49973252415657043, 0.34123119711875916, 0.32162243127822876, 0.2796144485473633, 0.40184950828552246, 0.08808359503746033, 0.30017340183258057, 0.3698680102825165, 0.08756634593009949], dtype='float32').reshape([18]),
            paddle.to_tensor([0.14900074899196625, 0.03842144459486008, 0.10108675062656403, 0.318503201007843, 0.1537388116121292, 0.1344524770975113, 0.255362331867218, 0.10281158983707428, 0.26870670914649963, 0.009301804006099701, 0.31615352630615234, 0.06507626175880432, 0.3184570372104645, 0.23850521445274353, 0.2509510815143585, 0.06670954078435898, 0.4307129681110382, 0.0872827097773552], dtype='float32').reshape([18]),
            paddle.to_tensor([0.037301067262887955, 0.10604267567396164, 0.21586838364601135, 0.038026317954063416, 0.2790902256965637, 0.37214913964271545, 0.12975507974624634, 0.20655612647533417, 0.28779980540275574, 0.03523006662726402, 0.3094521760940552, 0.09933008253574371, 0.3952299654483795, 0.3206803500652313, 0.38077282905578613, 0.045323342084884644, 0.22660154104232788, 0.38328301906585693], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5e5eb34c544f0d8be9c54bffb2b67cd4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.36678382754325867, 0.08921121060848236, 0.05469502881169319, 0.44107893109321594, 0.05502580478787422, 0.2086738795042038, 0.05600479990243912, 0.24380981922149658, 0.19339600205421448, 0.032193779945373535, 0.36157211661338806, 0.35781171917915344, 0.36164215207099915, 0.1193060651421547, 0.12210065871477127, 0.3079715073108673, 0.060892194509506226, 0.163628950715065, 0.273701936006546, 0.2685222029685974, 0.46704810857772827, 0.06800318509340286, 0.0673803985118866, 0.4212232232093811, 0.3114197850227356, 0.46931153535842896, 0.19282127916812897, 0.39925774931907654, 0.4662657678127289, 0.1415771096944809], dtype='float32').reshape([30]),
            paddle.to_tensor([0.11752407252788544, 0.24518638849258423, 0.472673237323761, 0.31787171959877014, 0.23007424175739288, 0.09134338796138763, 0.05701577290892601, 0.3725297152996063, 0.06286876648664474, 0.46123725175857544, 0.19830629229545593, 0.30392932891845703, 0.22256091237068176, 0.422883540391922, 0.27593323588371277, 0.35679692029953003, 0.49989554286003113, 0.284397691488266, 0.05431719496846199, 0.20677711069583893, 0.41504257917404175, 0.4421946704387665, 0.30231553316116333, 0.23651668429374695, 0.06864248216152191, 0.42370468378067017, 0.42838528752326965, 0.47834286093711853, 0.05282103270292282, 0.43135178089141846], dtype='float32').reshape([30]),
            paddle.to_tensor([0.24039505422115326, 0.08874695003032684, 0.3595595955848694, 0.4483637809753418, 0.39246681332588196, 0.3371594548225403, 0.45759159326553345, 0.4397708475589752, 0.34978577494621277, 0.4849361479282379, 0.39191895723342896, 0.24784624576568604, 0.3443470299243927, 0.34322360157966614, 0.47247856855392456, 0.2789148688316345, 0.3664310574531555, 0.1784016191959381, 0.4931754469871521, 0.24791012704372406, 0.23328521847724915, 0.4319220185279846, 0.050636935979127884, 0.0757635161280632, 0.029203757643699646, 0.1566595584154129, 0.37202188372612, 0.22498056292533875, 0.42879533767700195, 0.15186730027198792], dtype='float32').reshape([30]),
            paddle.to_tensor([0.18514180183410645, 0.48129531741142273, 0.15956756472587585, 0.32815873622894287, 0.36798083782196045, 0.22358275949954987, 0.37781620025634766, 0.49222853779792786, 0.11265450716018677, 0.3218716084957123, 0.2516116499900818, 0.4470297694206238, 0.32016611099243164, 0.07866444438695908, 0.14789974689483643, 0.044436365365982056, 0.3439222574234009, 0.39214929938316345, 0.018317606300115585, 0.08086169511079788, 0.26429232954978943, 0.1144075095653534, 0.08605247735977173, 0.0075826700776815414, 0.2605845332145691, 0.18768592178821564, 0.17462120950222015, 0.4665164649486542, 0.3826886713504791, 0.3156473636627197], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0725043ab09ccf9e1775d3b0afe76c08(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 80, 80], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c9e24031fa87c6968e4c41876914940e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 8, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5bbe19c3db75d844af2e676a3bd663d1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 232, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([232], dtype='float32', min=0, max=0.5),
            paddle.uniform([232], dtype='float32', min=0, max=0.5),
            paddle.uniform([232], dtype='float32', min=0, max=0.5),
            paddle.uniform([232], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ac449147d45b699904bc3c9fbe4095d0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 80, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7f87325796c8b773ed8597e60ca11d5f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_78a3a656a23a3289999b8cd36546af40(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 82, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([82], dtype='float32', min=0, max=0.5),
            paddle.uniform([82], dtype='float32', min=0, max=0.5),
            paddle.uniform([82], dtype='float32', min=0, max=0.5),
            paddle.uniform([82], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fee8e06f7df46e36bc40d7292b833836(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 24, 80], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8c0997081eae239d4682a43fd717b251(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 28, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.37345656752586365, 0.25256699323654175, 0.3671415448188782, 0.39048030972480774, 0.49772629141807556, 0.11574875563383102, 0.4867752194404602, 0.1527005285024643, 0.47481754422187805, 0.3743579387664795, 0.12302875518798828, 0.10189222544431686, 0.19650177657604218, 0.03857386112213135, 0.190353661775589, 0.31140127778053284, 0.1037745252251625, 0.3511044979095459, 0.17083610594272614, 0.0728844478726387, 0.46096375584602356, 0.3726295828819275, 0.13984303176403046, 0.41242876648902893, 0.2993604242801666, 0.39541104435920715, 0.3128872811794281, 0.023385267704725266], dtype='float32').reshape([28]),
            paddle.to_tensor([0.38243064284324646, 0.003977995365858078, 0.012468980625271797, 0.26128658652305603, 0.23763497173786163, 0.19427792727947235, 0.15854571759700775, 0.35191646218299866, 0.07300739735364914, 0.16151484847068787, 0.27771925926208496, 0.29990944266319275, 0.45108187198638916, 0.12086348980665207, 0.10619661211967468, 0.34629401564598083, 0.32697972655296326, 0.46476078033447266, 0.26064532995224, 0.41819092631340027, 0.362784206867218, 0.4894511103630066, 0.3488961160182953, 0.27399688959121704, 0.47973936796188354, 0.05687658488750458, 0.2656439542770386, 0.26139694452285767], dtype='float32').reshape([28]),
            paddle.to_tensor([0.44708630442619324, 0.18846967816352844, 0.44440957903862, 0.09992444515228271, 0.46516260504722595, 0.10645285993814468, 0.32838860154151917, 0.15077322721481323, 0.3430837094783783, 0.20660196244716644, 0.16399040818214417, 0.42745813727378845, 0.3244926333427429, 0.22230133414268494, 0.17103798687458038, 0.06453568488359451, 0.25228777527809143, 0.4677923917770386, 0.21074660122394562, 0.033797550946474075, 0.4823913276195526, 0.486564964056015, 0.2720353603363037, 0.3342011570930481, 0.22677502036094666, 0.14609791338443756, 0.23198148608207703, 0.3526589870452881], dtype='float32').reshape([28]),
            paddle.to_tensor([0.17568305134773254, 0.1029667779803276, 0.19730879366397858, 0.14984773099422455, 0.38185814023017883, 0.3220290243625641, 0.1455545276403427, 0.26985877752304077, 0.4845399260520935, 0.2868942618370056, 0.2484518587589264, 0.45832112431526184, 0.36610808968544006, 0.2050199806690216, 0.0550202950835228, 0.3183938264846802, 0.41417378187179565, 0.03662916645407677, 0.185120090842247, 0.11488594859838486, 0.07329389452934265, 0.08344912528991699, 0.4525180459022522, 0.3469184935092926, 0.4111692011356354, 0.0305937547236681, 0.21864226460456848, 0.17034180462360382], dtype='float32').reshape([28]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c36ba18e77462c0a827bc6c454f266dc(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 8, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_58bb9af4fbe307924ddbad611e35bedd(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 270, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.uniform([270], dtype='float32', min=0, max=0.5),
            paddle.uniform([270], dtype='float32', min=0, max=0.5),
            paddle.uniform([270], dtype='float32', min=0, max=0.5),
            paddle.uniform([270], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e15276b31c10e67b61121ff3648d8c8e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 384, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3e0d312b5bd4371fbc3343dada545a3a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 128, 128], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9b4ddf112b4c78e73e1831e99073c066(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_aae891d65f73839d8b677464a24d7a8e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 160, 32, 16], dtype='float16', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fbdb8ac77e97c0db51c073d4b42eb789(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 40, 40], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d937ecd7c74bfacbba5e6a85e5c37917(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 240, 60, 60], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d785fee8ea8409b14dae99604fef72ae(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 80, 80], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0ff408318a8f614c4a83c5b1470205da(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 200, 336], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b20fd070ff37444adfd28a0c1a24d2d3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 4, 50], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.13739603757858276, 0.033197008073329926, 0.03853892162442207, 0.334256112575531, 0.16599616408348083, 0.33016231656074524, 0.1681966781616211, 0.18172214925289154, 0.2654659152030945, 0.4809147119522095, 0.04376143962144852, 0.28023087978363037, 0.2351592779159546, 0.05882564187049866, 0.45851394534111023, 0.10317941009998322, 0.4177335500717163, 0.23572784662246704, 0.018298985436558723, 0.08844555169343948, 0.4439311623573303, 0.12281380593776703, 0.41446611285209656, 0.41986337304115295], dtype='float32').reshape([24]),
            paddle.to_tensor([0.008083721622824669, 0.05903022736310959, 0.1698506474494934, 0.13920240104198456, 0.19233456254005432, 0.037660714238882065, 0.1764935702085495, 0.10865375399589539, 0.029867764562368393, 0.42915603518486023, 0.3713925778865814, 0.04827234148979187, 0.01061667874455452, 0.3779906928539276, 0.2764877676963806, 0.03170441836118698, 0.1608649492263794, 0.06519557535648346, 0.1230631023645401, 0.09204841405153275, 0.07462767511606216, 0.24357183277606964, 0.4291093945503235, 0.13787484169006348], dtype='float32').reshape([24]),
            paddle.to_tensor([0.03509197011590004, 0.32098060846328735, 0.392987459897995, 0.07692673802375793, 0.1274106353521347, 0.4546140432357788, 0.47965189814567566, 0.4778810143470764, 0.30766335129737854, 0.390593945980072, 0.3371466398239136, 0.36205658316612244, 0.014429046772420406, 0.2065550684928894, 0.0781111791729927, 0.03731222078204155, 0.4327831268310547, 0.29481077194213867, 0.35594266653060913, 0.01990640163421631, 0.02590268850326538, 0.2752056121826172, 0.04681776463985443, 0.031923431903123856], dtype='float32').reshape([24]),
            paddle.to_tensor([0.49392247200012207, 0.431044340133667, 0.4588772654533386, 0.37282103300094604, 0.17314140498638153, 0.44976645708084106, 0.061029884964227676, 0.3653206527233124, 0.3093317449092865, 0.4838373064994812, 0.2245188057422638, 0.30948877334594727, 0.12085355073213577, 0.49968311190605164, 0.1958925873041153, 0.27378493547439575, 0.0697527602314949, 0.3670697808265686, 0.13582471013069153, 0.3511029779911041, 0.08927200734615326, 0.4118863344192505, 0.03672526404261589, 0.23648160696029663], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_16b3f84f42f4a8263accbda1d3cf5793(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 40, 40], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_be2bd168300abbc99cf65a0d6a8fa214(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.04685244336724281, 0.3093228340148926, 0.36770594120025635, 0.17252343893051147, 0.33093124628067017, 0.015977825969457626, 0.2753158211708069, 0.2246832698583603, 0.16900692880153656, 0.0017309708055108786, 0.034319255501031876, 0.41730204224586487, 0.4468107223510742, 0.013324498198926449, 0.36022767424583435, 0.40707024931907654, 0.29014670848846436, 0.3004536032676697, 0.23011980950832367, 0.1576232612133026], dtype='float32').reshape([20]),
            paddle.to_tensor([0.1062256321310997, 0.1148587018251419, 0.18916869163513184, 0.4703218638896942, 0.48661181330680847, 0.30520597100257874, 0.13876374065876007, 0.3619084060192108, 0.061883438378572464, 0.10684598237276077, 0.3401193916797638, 0.2161787897348404, 0.2449004054069519, 0.3043358325958252, 0.09425003826618195, 0.48232460021972656, 0.49982574582099915, 0.15566514432430267, 0.00026426889235153794, 0.06456863135099411], dtype='float32').reshape([20]),
            paddle.to_tensor([0.24047806859016418, 0.09933757036924362, 0.06101643294095993, 0.32136884331703186, 0.3933609426021576, 0.1864699274301529, 0.044852711260318756, 0.16755685210227966, 0.26206573843955994, 0.3444456458091736, 0.11867577582597733, 0.26178988814353943, 0.22821243107318878, 0.3022872805595398, 0.3944462239742279, 0.08929740637540817, 0.3173890709877014, 0.18734271824359894, 0.4555233120918274, 0.4769124686717987], dtype='float32').reshape([20]),
            paddle.to_tensor([0.3163192868232727, 0.4540666341781616, 0.3656933903694153, 0.2646288275718689, 0.49498602747917175, 0.28455325961112976, 0.36217010021209717, 0.03402811288833618, 0.24313701689243317, 0.07636838406324387, 0.08155561983585358, 0.35549795627593994, 0.13203783333301544, 0.1732044667005539, 0.09377063065767288, 0.29291263222694397, 0.1320180594921112, 0.05405321344733238, 0.46990010142326355, 0.33283111453056335], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_416f514070c8425962b5cfd514483b60(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 704, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([704], dtype='float32', min=0, max=0.5),
            paddle.uniform([704], dtype='float32', min=0, max=0.5),
            paddle.uniform([704], dtype='float32', min=0, max=0.5),
            paddle.uniform([704], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8164af0929bcfe07c3ae25ad3472902f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 104, 40, 40], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3bb059612ece0d7fb3eafe9c4788596f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 240, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_14cd2430129ad120f4d011c17b1b697b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 320, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b6c33a7e1e8c7a776a21c959e0bce6bf(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b082f346cd2b4ebc03ecb8cc0c7a5419(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e5ad618e2d8c7fef5b7de9edc2d635b4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 160, 160], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.45249345898628235, 0.33413296937942505, 0.46688148379325867, 0.1925317794084549, 0.32019755244255066, 0.41779735684394836, 0.2933103144168854, 0.09720519930124283, 0.32718899846076965, 0.0868145152926445, 0.2313394993543625, 0.08573737740516663, 0.2769187390804291, 0.2540980577468872, 0.0055901771411299706, 0.146719828248024, 0.12428150326013565, 0.09086263179779053, 0.2975827753543854, 0.3963586091995239, 0.10955742746591568, 0.34973692893981934, 0.2829199433326721, 0.2005540281534195], dtype='float32').reshape([24]),
            paddle.to_tensor([0.4637608528137207, 0.03398546949028969, 0.11254473775625229, 0.3715132176876068, 0.4010230302810669, 0.06929106265306473, 0.45989665389060974, 0.11153563857078552, 0.2125764638185501, 0.005247629247605801, 0.26572349667549133, 0.11996643245220184, 0.07852931320667267, 0.48174816370010376, 0.06188427284359932, 0.4651535749435425, 0.06831511855125427, 0.2962885797023773, 0.31968045234680176, 0.10458572953939438, 0.15310291945934296, 0.34088942408561707, 0.47610676288604736, 0.1213035061955452], dtype='float32').reshape([24]),
            paddle.to_tensor([0.3274703919887543, 0.12614110112190247, 0.15358130633831024, 0.11138550192117691, 0.2346358448266983, 0.2136138677597046, 0.4527222514152527, 0.2818503975868225, 0.2357102930545807, 0.29780247807502747, 0.0366593673825264, 0.428903192281723, 0.1734529286623001, 0.26022791862487793, 0.3226539194583893, 0.09758049994707108, 0.3936993479728699, 0.4872325360774994, 0.037627752870321274, 0.3187394440174103, 0.06375403702259064, 0.09893371909856796, 0.10196090489625931, 0.0760486051440239], dtype='float32').reshape([24]),
            paddle.to_tensor([0.04284684360027313, 0.016475902870297432, 0.24881668388843536, 0.13651899993419647, 0.31485122442245483, 0.02345062606036663, 0.09047160297632217, 0.431111216545105, 0.21479086577892303, 0.28672632575035095, 0.2683257758617401, 0.17574314773082733, 0.1770186573266983, 0.2986696660518646, 0.3184625506401062, 0.120432049036026, 0.2213318645954132, 0.31529831886291504, 0.2886525094509125, 0.46037063002586365, 0.14866472780704498, 0.31515267491340637, 0.004467298276722431, 0.3045109808444977], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_41dde5624aaa94a6e01c001574e93b08(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 240, 240], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8ce4157efefa11a214a1659c7289cbae(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 256, 256], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.26609402894973755, 0.013103630393743515, 0.4964413046836853, 0.07851982861757278, 0.34741640090942383, 0.4244922697544098, 0.37921226024627686, 0.20184887945652008, 0.40021637082099915, 0.07972446084022522, 0.06499157845973969, 0.4418913424015045, 0.46009814739227295, 0.1704578399658203, 0.441921204328537, 0.004614321980625391], dtype='float32').reshape([16]),
            paddle.to_tensor([0.12780243158340454, 0.3817335069179535, 0.40116414427757263, 0.27426469326019287, 0.01715013198554516, 0.18787378072738647, 0.46759000420570374, 0.3731157183647156, 0.18045338988304138, 0.14029094576835632, 0.07981012761592865, 0.37376096844673157, 0.17614226043224335, 0.014500362798571587, 0.16087616980075836, 0.1445060521364212], dtype='float32').reshape([16]),
            paddle.to_tensor([0.2645479142665863, 0.3706802725791931, 0.4130106270313263, 0.4327252209186554, 0.009713815525174141, 0.1083693727850914, 0.09692635387182236, 0.15132063627243042, 0.2313731610774994, 0.25210997462272644, 0.1625049114227295, 0.35695624351501465, 0.0032677820418030024, 0.10996934771537781, 0.19176343083381653, 0.24163061380386353], dtype='float32').reshape([16]),
            paddle.to_tensor([0.10649067908525467, 0.03637013956904411, 0.21787972748279572, 0.03364372253417969, 0.38529106974601746, 0.03729195147752762, 0.012892039492726326, 0.19542741775512695, 0.3438766300678253, 0.02687099389731884, 0.08986514061689377, 0.12413500994443893, 0.2735840678215027, 0.4513896405696869, 0.3595633804798126, 0.41518670320510864], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_eeeeca2dda4d38b2f54c2bbdefdbb1f2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_aa1ff7f928644f9ecfdc3dda1831901f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_165ed706feb1399b6287dc7e160f3839(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 64, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_dfa7293be3e85a800fec9ef01c5f2284(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 120, 60, 60], dtype='float16', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a9efed97d2578016508328964aec588c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 672, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bf87ae2f597cb157cf84607637714ea8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7270d05b9878aa316cc03e10379371e4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 32, 24], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e1e6b68cf9da2bb15321dca6561c2a24(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 366, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([366], dtype='float32', min=0, max=0.5),
            paddle.uniform([366], dtype='float32', min=0, max=0.5),
            paddle.uniform([366], dtype='float32', min=0, max=0.5),
            paddle.uniform([366], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7814c2b162eff6e291888776d6bdee7a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.3168472647666931, 0.48765790462493896, 0.46201714873313904, 0.22650375962257385, 0.46845656633377075, 0.07696995139122009, 0.1305823177099228, 0.231262668967247, 0.0396474152803421, 0.024273304268717766, 0.2838580906391144, 0.02943810075521469, 0.4779292345046997, 0.2782706320285797, 0.16269393265247345, 0.4504636824131012], dtype='float32').reshape([16]),
            paddle.to_tensor([0.26254892349243164, 0.36345434188842773, 0.24876902997493744, 0.4784666895866394, 0.3241066634654999, 0.28904321789741516, 0.21343666315078735, 0.19572637975215912, 0.4410054087638855, 0.3021022379398346, 0.07575997710227966, 0.0703192874789238, 0.29949653148651123, 0.17575141787528992, 0.4734772741794586, 0.061076775193214417], dtype='float32').reshape([16]),
            paddle.to_tensor([0.09009586274623871, 0.025514613837003708, 0.48124268651008606, 0.2801782786846161, 0.1348685622215271, 0.45121991634368896, 0.1881459355354309, 0.05297800898551941, 0.2968094050884247, 0.34004995226860046, 0.3395969569683075, 0.18269944190979004, 0.32165372371673584, 0.0706472098827362, 0.2851420044898987, 0.1408892422914505], dtype='float32').reshape([16]),
            paddle.to_tensor([0.20140816271305084, 0.41017723083496094, 0.38864725828170776, 0.04466252028942108, 0.1531991809606552, 0.2250441312789917, 0.35569265484809875, 0.044503457844257355, 0.05350276082754135, 0.07328455150127411, 0.14648589491844177, 0.04980458319187164, 0.2959851026535034, 0.22486212849617004, 0.4628830850124359, 0.1244727224111557], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d9c34e6cdfef867934789908592b675a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 480, 2, 50], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c1f33cf53455cb0ef3e1a8824fe888d0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1fa8320b5e3c67f9c47e5a1586a1dc75(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_013ddcec6017c89b64f9987936c1a751(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 256, 512], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bb66abaf33964acf8d2499cd219c5e82(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 19, 19], dtype='float16', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a4f28d1c9faba36fdf23c515ebcba825(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 2, 2], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d7a8d6faa735c9663483a6e184463e36(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 60, 60], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_554f5a573087e07cce72d818e0ec3493(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2ff46c1d2dcda6e5af0e9fe360aa25d3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 28, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.41946688294410706, 0.3361547887325287, 0.4688112437725067, 0.2786893844604492, 0.33528777956962585, 0.19525612890720367, 0.12111034244298935, 0.3599858283996582, 0.24037209153175354, 0.14306463301181793, 0.28199848532676697, 0.14203284680843353, 0.4632878005504608, 0.03338401019573212, 0.02112618461251259, 0.21447068452835083, 0.12168469280004501, 0.26860880851745605, 0.16962185502052307, 0.21674863994121552, 0.17165806889533997, 0.1150907650589943, 0.28876617550849915, 0.3562379479408264, 0.12506283819675446, 0.15037016570568085, 0.4068959653377533, 0.013868696056306362], dtype='float32').reshape([28]),
            paddle.to_tensor([0.2158176749944687, 0.2859192490577698, 0.3622589707374573, 0.22926707565784454, 0.15564045310020447, 0.22482311725616455, 0.3631042540073395, 0.2891428470611572, 0.3503173589706421, 0.006019681226462126, 0.37075015902519226, 0.028699498623609543, 0.47460678219795227, 0.20466367900371552, 0.13733413815498352, 0.20172683894634247, 0.45451346039772034, 0.03993440419435501, 0.19399690628051758, 0.3615516722202301, 0.3551173210144043, 0.186377614736557, 0.054460324347019196, 0.022525178268551826, 0.02033284492790699, 0.20175042748451233, 0.35213330388069153, 0.09622474759817123], dtype='float32').reshape([28]),
            paddle.to_tensor([0.046012360602617264, 0.4796544909477234, 0.05390653759241104, 0.05439649149775505, 0.3791806697845459, 0.1117752194404602, 0.20065253973007202, 0.058326128870248795, 0.1215675100684166, 0.12590603530406952, 0.41276299953460693, 0.3251582086086273, 0.1225380077958107, 0.027558956295251846, 0.4204666018486023, 0.2523263990879059, 0.3741101026535034, 0.334878146648407, 0.3379003405570984, 0.2881808280944824, 0.4495636820793152, 0.09470414370298386, 0.2668669819831848, 0.18036635220050812, 0.3574223518371582, 0.36100703477859497, 0.47615015506744385, 0.22640536725521088], dtype='float32').reshape([28]),
            paddle.to_tensor([0.3664398193359375, 0.306694895029068, 0.20128224790096283, 0.0163426473736763, 0.47662657499313354, 0.3182404339313507, 0.247713103890419, 0.20345276594161987, 0.45246994495391846, 0.42991113662719727, 0.22879649698734283, 0.02752559632062912, 0.2361682951450348, 0.16607843339443207, 0.06549181044101715, 0.030673298984766006, 0.4968421161174774, 0.027029793709516525, 0.21428504586219788, 0.3961416482925415, 0.2940267324447632, 0.06616942584514618, 0.1897585690021515, 0.008576656691730022, 0.2114289104938507, 0.0903482586145401, 0.22087498009204865, 0.19704337418079376], dtype='float32').reshape([28]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bde1c16c29a25806cede94c737d59361(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 1, 1], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_610ce5b56b1d3f7c5bdbb6dd3f2b9c52(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 336, 40, 40], dtype='float16', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_413e9bf57a4f044069c86c92c72e60b6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 160, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3da388fec608d0b53db3f2482c4c6e3f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 1344, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([1344], dtype='float32', min=0, max=0.5),
            paddle.uniform([1344], dtype='float32', min=0, max=0.5),
            paddle.uniform([1344], dtype='float32', min=0, max=0.5),
            paddle.uniform([1344], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e117494db308a5339041ca22fb156a22(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 40, 40], dtype='float16', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1c1809567be7789c657be7168a9fe5fc(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 480, 30, 30], dtype='float16', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_cdc3b0a148b551f86897e4668e0bdb6b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, True), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_272123507c34111166d6f9d764fcb4b8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cdc3b0a148b551f86897e4668e0bdb6b
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 4, 4], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0322554593724cd06c8ec715201197ff(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.2718631625175476, 0.3028188943862915, 0.35885053873062134, 0.3669615685939789, 0.20672476291656494, 0.039182912558317184, 0.06224341690540314, 0.12820939719676971, 0.2506963610649109, 0.49076583981513977, 0.1262200027704239, 0.04746013134717941, 0.4776974320411682, 0.024131176993250847, 0.3934729993343353, 0.17939439415931702, 0.12312759459018707, 0.039109013974666595, 0.10598908364772797, 0.2610031068325043], dtype='float32').reshape([20]),
            paddle.to_tensor([0.1303635686635971, 0.132266104221344, 0.0747661143541336, 0.14026468992233276, 0.24706706404685974, 0.1061314195394516, 0.08214586228132248, 0.38265708088874817, 0.060947056859731674, 0.37010717391967773, 0.14729352295398712, 0.4304203689098358, 0.07769089192152023, 0.4697968661785126, 0.34891435503959656, 0.4065530002117157, 0.29887938499450684, 0.16971391439437866, 0.4943836033344269, 0.08910509198904037], dtype='float32').reshape([20]),
            paddle.to_tensor([0.30454131960868835, 0.2398962676525116, 0.11422380059957504, 0.08777851611375809, 0.46666377782821655, 0.32722681760787964, 0.31714510917663574, 0.3563857078552246, 0.41788434982299805, 0.03651867061853409, 0.39050838351249695, 0.05576813966035843, 0.14622245728969574, 0.35459622740745544, 0.28612780570983887, 0.19878153502941132, 0.001392529346048832, 0.4325908422470093, 0.05466654151678085, 0.10567115247249603], dtype='float32').reshape([20]),
            paddle.to_tensor([0.10855758190155029, 0.314665824174881, 0.4358908534049988, 0.060155563056468964, 0.17513243854045868, 0.18167896568775177, 0.3663840889930725, 0.41784319281578064, 0.10412304103374481, 0.26354122161865234, 0.0829155370593071, 0.022817298769950867, 0.37115103006362915, 0.22605551779270172, 0.35517364740371704, 0.06679157167673111, 0.20404064655303955, 0.49339863657951355, 0.3079445958137512, 0.14675453305244446], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e5991472f2896dda80674a6de20b6ead(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 512, 256], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_eb777c0c87b0930ad8ac08a1fcc8297f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_79c48ed5d4755be6fba4d3c3b7d19d6a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 1184, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([1184], dtype='float32', min=0, max=0.5),
            paddle.uniform([1184], dtype='float32', min=0, max=0.5),
            paddle.uniform([1184], dtype='float32', min=0, max=0.5),
            paddle.uniform([1184], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d80ad80c6536ec5615ec97e69d0daee3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 112, 112], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4fe2f7e1f9e5f7542105523a4dc622be(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8469db2003f3c47ca9124c89d2abd9d9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 480, 12, 12], dtype='float16', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e4b96402f35818f2b36e367e2de4eef7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d01d12106e4628ede8a538dd21906a14(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 40, 40], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6390db9e941d4b2588b0f93d072cdc44(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 128, 128], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_95b5016f483150de6dd04d96375a0416(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 40, 40], dtype='float16', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8f24ec13192d9569554bfd7d1f1f6c5b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 80, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7da1b8c2655200382914833b7ee25736(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 120, 40, 40], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cdcd10a9a13f61fccab1d926f6ceacdc(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 240, 240], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bd6c171b8d142780d5a9c90478a450a2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.4839033782482147, 0.4111121594905853, 0.22595356404781342, 0.36388325691223145, 0.25453126430511475, 0.3286971151828766, 0.24886159598827362, 0.2647128999233246, 0.3118627667427063, 0.07385294139385223, 0.22218585014343262, 0.4390058219432831, 0.06769140809774399, 0.31468722224235535, 0.34403473138809204, 0.3497615158557892, 0.45877403020858765, 0.2688249945640564, 0.29762688279151917, 0.2716567814350128, 0.49367305636405945, 0.210170716047287, 0.4252818822860718, 0.49016156792640686], dtype='float32').reshape([24]),
            paddle.to_tensor([0.29018634557724, 0.3435429334640503, 0.12292099744081497, 0.26441317796707153, 0.19925092160701752, 0.11823657900094986, 0.03811374306678772, 0.1562013179063797, 0.28366324305534363, 0.22333186864852905, 0.38535141944885254, 0.10135413706302643, 0.43994084000587463, 0.4317675530910492, 0.0414310097694397, 0.03738746419548988, 0.49448272585868835, 0.32856911420822144, 0.24726854264736176, 0.30187147855758667, 0.33814555406570435, 0.45698320865631104, 0.04715649411082268, 0.3189610242843628], dtype='float32').reshape([24]),
            paddle.to_tensor([0.07958714663982391, 0.1589643806219101, 0.09500666707754135, 0.298367977142334, 0.42151710391044617, 0.048834942281246185, 0.30315083265304565, 0.4610145092010498, 0.14037178456783295, 0.1163535788655281, 0.2160450965166092, 0.34261757135391235, 0.07502871006727219, 0.01952354423701763, 0.4557066559791565, 0.32854509353637695, 0.15463043749332428, 0.30956342816352844, 0.22954581677913666, 0.17601194977760315, 0.33941370248794556, 0.31513190269470215, 0.039125651121139526, 0.20770898461341858], dtype='float32').reshape([24]),
            paddle.to_tensor([0.31894880533218384, 0.3748323619365692, 0.31179431080818176, 0.38407284021377563, 0.08675000816583633, 0.29914817214012146, 0.2653128504753113, 0.19680573046207428, 0.4583238959312439, 0.41616955399513245, 0.32157108187675476, 0.17704585194587708, 0.15372775495052338, 0.4615698456764221, 0.32349008321762085, 0.26135557889938354, 0.2383929044008255, 0.447828084230423, 0.4715869128704071, 0.3706735670566559, 0.14004230499267578, 0.10324162244796753, 0.39355936646461487, 0.4456244707107544], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_524ed3569788c4f7144cecd30f44c32d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 40, 40], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3c3b069d19f6e31898c5fce36ce7b253(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 120, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_faece81c88b8cb5cfee46d23390eb268(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 80, 80], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ad62b448b74fca5df53ef3b866d62c87(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 120, 4, 50], dtype='float16', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5f77dcaca1bb4bea7546b97b1b4acd25(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 12, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.045997634530067444, 0.48849761486053467, 0.2592219412326813, 0.41832226514816284, 0.042282864451408386, 0.3671905994415283, 0.11883682757616043, 0.15563346445560455, 0.042454153299331665, 0.499411404132843, 0.2762339115142822, 0.4589558243751526], dtype='float32').reshape([12]),
            paddle.to_tensor([0.3973246216773987, 0.21736258268356323, 0.47167444229125977, 0.17986877262592316, 0.28241926431655884, 0.4193577766418457, 0.2793360948562622, 0.41948190331459045, 0.01685592718422413, 0.1826329380273819, 0.17300911247730255, 0.1367681324481964], dtype='float32').reshape([12]),
            paddle.to_tensor([0.2446409910917282, 0.22634877264499664, 0.30767104029655457, 0.19604003429412842, 0.36052849888801575, 0.4388844072818756, 0.06892304867506027, 0.23948299884796143, 0.2861461043357849, 0.36615052819252014, 0.1052720695734024, 0.2170807123184204], dtype='float32').reshape([12]),
            paddle.to_tensor([0.1788458526134491, 0.3790024518966675, 0.14781641960144043, 0.4943396747112274, 0.36343374848365784, 0.3846815526485443, 0.3158087432384491, 0.3886532783508301, 0.1116994246840477, 0.2785809338092804, 0.0009906752966344357, 0.1039150208234787], dtype='float32').reshape([12]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1cca9b05a96a5910e0f54cac541a1bcb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_af91295b59c5fca17f890a802deb001f
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 4, 64, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_baa2acc5ac4f28070257c4799a8e0fbc(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cd7eb1b0c25429421d48c29b50497aa9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 160, 160], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6698206ccd58209134b1df29b48d85d0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_fc6b6917e04b7b7cf9fdb39dde97ec40
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 40, 40], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_996546047e2484ad1442c09db5fc56b3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.19325636327266693, 0.3926459550857544, 0.11874979734420776, 0.05125683546066284, 0.4160827100276947, 0.21566267311573029, 0.10763023793697357, 0.41681283712387085, 0.07977594435214996, 0.35308486223220825, 0.47896531224250793, 0.19774475693702698, 0.19984503090381622, 0.03245553374290466, 0.48982247710227966, 0.11008544266223907, 0.192241832613945, 0.3621985614299774, 0.19914288818836212, 0.01300221960991621, 0.09268096834421158, 0.19764676690101624, 0.3742494583129883, 0.20374035835266113, 0.1188291609287262, 0.4909675121307373, 0.2220553308725357, 0.14660575985908508, 0.0600159727036953, 0.16593095660209656], dtype='float32').reshape([30]),
            paddle.to_tensor([0.09510930627584457, 0.41978272795677185, 0.41591185331344604, 0.2982069253921509, 0.032716602087020874, 0.17197389900684357, 0.3847985565662384, 0.45283961296081543, 0.3102932274341583, 0.48951154947280884, 0.4898993968963623, 0.20039516687393188, 0.4587724506855011, 0.10681052505970001, 0.4513099193572998, 0.07371653616428375, 0.014914335682988167, 0.14989477396011353, 0.4301513433456421, 0.44280800223350525, 0.09917933493852615, 0.05135650187730789, 0.4632033407688141, 0.4691407084465027, 0.4692176282405853, 0.07650397717952728, 0.3060097098350525, 0.25503331422805786, 0.16898559033870697, 0.4444735646247864], dtype='float32').reshape([30]),
            paddle.to_tensor([0.4877139627933502, 0.41190871596336365, 0.3984876275062561, 0.17692433297634125, 0.4657532274723053, 0.47249338030815125, 0.016565997153520584, 0.4155704379081726, 0.12274550646543503, 0.42636948823928833, 0.3356080651283264, 0.40217721462249756, 0.294249564409256, 0.23841634392738342, 0.4196287989616394, 0.3103575110435486, 0.3675002455711365, 0.20485222339630127, 0.05067801848053932, 0.17014507949352264, 0.45067042112350464, 0.4492523670196533, 0.24288511276245117, 0.40069109201431274, 0.4674110412597656, 0.47089794278144836, 0.15663617849349976, 0.15425950288772583, 0.34327444434165955, 0.1638733446598053], dtype='float32').reshape([30]),
            paddle.to_tensor([0.32022517919540405, 0.4274601638317108, 0.03774171322584152, 0.1544676423072815, 0.42650118470191956, 0.22770412266254425, 0.2721680700778961, 0.026505185291171074, 0.41557368636131287, 0.3422762155532837, 0.32823076844215393, 0.331349641084671, 0.1554054468870163, 0.19886671006679535, 0.4529935121536255, 0.36087217926979065, 0.4870256185531616, 0.28056952357292175, 0.27084359526634216, 0.004004301968961954, 0.4763231575489044, 0.4156702160835266, 0.2923671305179596, 0.02222973294556141, 0.25401005148887634, 0.4494975805282593, 0.28950345516204834, 0.30373451113700867, 0.46619588136672974, 0.16064605116844177], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_565a3604b7f82964097be11ffb38d124(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 56, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_752a1e247095788c510518931c242ac8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 1, 1], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b273708324a2715050b6fdb156600308(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 70, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([70], dtype='float32', min=0, max=0.5),
            paddle.uniform([70], dtype='float32', min=0, max=0.5),
            paddle.uniform([70], dtype='float32', min=0, max=0.5),
            paddle.uniform([70], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0293525ad84f3cbc92442f582349c7f5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 1, 1], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_69f0b23b35e04b4f40ff0526837c3d25(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 28, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.333364337682724, 0.2159520983695984, 0.17494265735149384, 0.40788257122039795, 0.2243531495332718, 0.37377047538757324, 0.3944597840309143, 0.2694341242313385, 0.06329768151044846, 0.4454807639122009, 0.046959660947322845, 0.4467284679412842, 0.3031803369522095, 0.17971760034561157, 0.24594642221927643, 0.00487078120931983, 0.0070469132624566555, 0.03126440942287445, 0.24644853174686432, 0.24649786949157715, 0.31577253341674805, 0.3205529451370239, 0.34278199076652527, 0.27701687812805176, 0.12978234887123108, 0.15102910995483398, 0.06845252960920334, 0.027870502322912216], dtype='float32').reshape([28]),
            paddle.to_tensor([0.06390221416950226, 0.15296784043312073, 0.24767398834228516, 0.426383376121521, 0.4947238266468048, 0.35893675684928894, 0.11530018597841263, 0.22375814616680145, 0.1257595717906952, 0.20194897055625916, 0.4709601402282715, 0.4586358368396759, 0.11729194968938828, 0.31713682413101196, 0.30156776309013367, 0.18038810789585114, 0.2538309693336487, 0.2710227966308594, 0.45436981320381165, 0.2058371752500534, 0.4737558364868164, 0.1173899695277214, 0.1540643870830536, 0.46788084506988525, 0.2887316346168518, 0.300033301115036, 0.4779641032218933, 0.24581046402454376], dtype='float32').reshape([28]),
            paddle.to_tensor([0.22296561300754547, 0.47979575395584106, 0.20828373730182648, 0.22353704273700714, 0.3783847987651825, 0.4452672302722931, 0.07713977992534637, 0.044580623507499695, 0.34803837537765503, 0.22175101935863495, 0.03834592550992966, 0.19630272686481476, 0.3341790735721588, 0.12431080639362335, 0.12609745562076569, 0.3980044424533844, 0.2165522426366806, 0.34283530712127686, 0.08913780748844147, 0.3386155068874359, 0.34977346658706665, 0.2704484760761261, 0.26246723532676697, 0.49167877435684204, 0.34205174446105957, 0.4069794714450836, 0.4978112280368805, 0.4661887586116791], dtype='float32').reshape([28]),
            paddle.to_tensor([0.3285004496574402, 0.44496601819992065, 0.05722302943468094, 0.36024215817451477, 0.03839051350951195, 0.1754942536354065, 0.3899228274822235, 0.22189028561115265, 0.4307344853878021, 0.24156081676483154, 0.13219371438026428, 0.2704716622829437, 0.31925591826438904, 0.2123875916004181, 0.36919429898262024, 0.0234156996011734, 0.09003116935491562, 0.3601732552051544, 0.06687633693218231, 0.0027061221189796925, 0.3743339776992798, 0.00973787996917963, 0.12197462469339371, 0.4694306552410126, 0.31990766525268555, 0.046128060668706894, 0.49628594517707825, 0.4262232184410095], dtype='float32').reshape([28]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_788967407a7266cb0bdd1b0135763419(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 112, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b46042fd4384181bc348ef81c010d449(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 80, 8, 6], dtype='float16', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8b60bbfee1fd574c6ea0d4d654a72ce3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 160, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7188b4d418a0ff2d78e985bba505af69(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.4469989538192749, 0.24687515199184418, 0.25893735885620117, 0.3313044607639313, 0.2853347063064575, 0.27882298827171326, 0.26967546343803406, 0.1905471682548523, 0.04812788590788841, 0.05506705492734909, 0.026194747537374496, 0.026250334456562996, 0.4681045114994049, 0.27432024478912354, 0.0361064188182354, 0.42747077345848083, 0.12132188677787781, 0.2937608063220978, 0.3115946650505066, 0.1945771723985672, 0.2878098487854004, 0.385874480009079, 0.29753002524375916, 0.22481560707092285, 0.11554230749607086, 0.19491273164749146, 0.3254792392253876, 0.1371997594833374, 0.43329429626464844, 0.1755305379629135], dtype='float32').reshape([30]),
            paddle.to_tensor([0.3766676187515259, 0.14681926369667053, 0.13151463866233826, 0.10304354131221771, 0.31664741039276123, 0.4564912021160126, 0.33597061038017273, 0.14332541823387146, 0.035058408975601196, 0.1631912887096405, 0.3383186459541321, 0.09634876251220703, 0.39452168345451355, 0.37056252360343933, 0.08622542768716812, 0.30711328983306885, 0.271180659532547, 0.3223361074924469, 0.06780322641134262, 0.44236916303634644, 0.4001665413379669, 0.44647061824798584, 0.027129296213388443, 0.2784769833087921, 0.1980661153793335, 0.22146785259246826, 0.43184274435043335, 0.18853974342346191, 0.1732061207294464, 0.2833608090877533], dtype='float32').reshape([30]),
            paddle.to_tensor([0.31992605328559875, 0.16653196513652802, 0.3548533320426941, 0.4171786904335022, 0.010846171528100967, 0.4741794466972351, 0.3817969858646393, 0.3665744960308075, 0.226876363158226, 0.2558627724647522, 0.004156392998993397, 0.0682825967669487, 0.3368077278137207, 0.21410009264945984, 0.37911394238471985, 0.3754926919937134, 0.11254820972681046, 0.0371791310608387, 0.4256959557533264, 0.4904738962650299, 0.44469723105430603, 0.3303888142108917, 0.383961021900177, 0.07077431678771973, 0.4541729688644409, 0.07805471122264862, 0.110019750893116, 0.042009130120277405, 0.1592273712158203, 0.4067445993423462], dtype='float32').reshape([30]),
            paddle.to_tensor([0.32751744985580444, 0.11213618516921997, 0.349239319562912, 0.3785525858402252, 0.19266176223754883, 0.3096669316291809, 0.4488271176815033, 0.0961790382862091, 0.17515908181667328, 0.3732757866382599, 0.44142934679985046, 0.4649617671966553, 0.3541452884674072, 0.4063686728477478, 0.4919717609882355, 0.03384333476424217, 0.103077732026577, 0.1387556940317154, 0.15814179182052612, 0.3874886929988861, 0.300480455160141, 0.3340306580066681, 0.27175694704055786, 0.0344465896487236, 0.4051870107650757, 0.3014696538448334, 0.046143606305122375, 0.14966727793216705, 0.14650391042232513, 0.05731165409088135], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8171d7164b4887cfd5df2e4ad7e30cc4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 16, 50], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4bdb90595ddf49643317659b0909588a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 52, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([52], dtype='float32', min=0, max=0.5),
            paddle.uniform([52], dtype='float32', min=0, max=0.5),
            paddle.uniform([52], dtype='float32', min=0, max=0.5),
            paddle.uniform([52], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9d30bb3f33c55292521493f2b45da89e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 2048, 30, 30], dtype='float16', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a4c2cd58bd8d455823bed9cb9544eee5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_5e7cb23119f65b33bdbad71952f6dd12
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 128, 128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_64aec9621ba7b2e43e0f3a0bd25a6207(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 160, 160], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c72ca6b7bf6dfe647f72f954b138fe92(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 175, 25], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_df8e1f9ea8d21ab9f2cccdbe698593e8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 64, 48], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d3f51daf20078d925a2b616f5f78870f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 320, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0e8e13213e1803188b870ccd5e98c352(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 704, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([704], dtype='float32', min=0, max=0.5),
            paddle.uniform([704], dtype='float32', min=0, max=0.5),
            paddle.uniform([704], dtype='float32', min=0, max=0.5),
            paddle.uniform([704], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f67aaf10828d2d53d7e48405f98d5357(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 104, 40, 40], dtype='float16', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_59550b940c85dd549d5da0830a0f50b8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 64, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e8fecbdcda28c829aa8e0167cc20dc5a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 32, 100], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f14a1f7fccd4a55476e2f3701b907f2a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 1, 1], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_adbf939a1b5e3c3f5a447ec9ee78813f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 80, 80], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6762addcc2a51213c9eb938135577f06(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.39228442311286926, 0.3214145302772522, 0.2949114441871643, 0.3993779122829437, 0.40112823247909546, 0.22653141617774963, 0.3090272545814514, 0.4193761646747589, 0.011407580226659775, 0.25751811265945435, 0.2248191088438034, 0.23940323293209076, 0.00038919009966775775, 0.0590180903673172, 0.14612923562526703, 0.37141284346580505, 0.18449866771697998, 0.46579745411872864], dtype='float32').reshape([18]),
            paddle.to_tensor([0.014057128690183163, 0.15288415551185608, 0.16348892450332642, 0.12243063002824783, 0.4853353798389435, 0.1004033088684082, 0.1745927333831787, 0.4448540210723877, 0.3771180808544159, 0.4105077385902405, 0.27126920223236084, 0.1927284300327301, 0.43325909972190857, 0.27606549859046936, 0.09103255718946457, 0.09212065488100052, 0.43040215969085693, 0.3872537910938263], dtype='float32').reshape([18]),
            paddle.to_tensor([0.025425942614674568, 0.1089494451880455, 0.14824752509593964, 0.49508845806121826, 0.31695419549942017, 0.42849069833755493, 0.4245118796825409, 0.27684372663497925, 0.41719290614128113, 0.3578595221042633, 0.4368308186531067, 0.05113746598362923, 0.266706258058548, 0.14860443770885468, 0.13244861364364624, 0.20669633150100708, 0.4686465263366699, 0.16759651899337769], dtype='float32').reshape([18]),
            paddle.to_tensor([0.05460132285952568, 0.34848514199256897, 0.2789207696914673, 0.19653668999671936, 0.06287539750337601, 0.012256035581231117, 0.44386816024780273, 0.37647223472595215, 0.02328399196267128, 0.3097885847091675, 0.30434152483940125, 0.26680633425712585, 0.4055097997188568, 0.3486359119415283, 0.3973008990287781, 0.19043228030204773, 0.16744934022426605, 0.24335837364196777], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_304d885984149afbda27f1e5e49a3b9d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 160, 160], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_29068d7ee42eef6ad05384db1d2fb165(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.1995806097984314, 0.11976069957017899, 0.4609081447124481, 0.4148085415363312, 0.45491093397140503, 0.2923564016819, 0.24707065522670746, 0.37754544615745544, 0.3070431351661682, 0.07715801894664764, 0.07743800431489944, 0.33551833033561707, 0.20808251202106476, 0.2485881745815277, 0.4309247136116028, 0.2958220839500427, 0.3901631534099579, 0.4952074885368347, 0.17916037142276764, 0.47752419114112854, 0.17320963740348816, 0.45185208320617676, 0.3837270140647888, 0.40322789549827576, 0.0012816875241696835, 0.3293449282646179, 0.34097781777381897, 0.2195647954940796, 0.34776416420936584, 0.41803738474845886], dtype='float32').reshape([30]),
            paddle.to_tensor([0.03560034558176994, 0.06164412945508957, 0.13448338210582733, 0.16745853424072266, 0.1135261282324791, 0.21572324633598328, 0.4907197952270508, 0.45797207951545715, 0.010037521831691265, 0.29615914821624756, 0.4352744519710541, 0.22765356302261353, 0.12362802028656006, 0.08457361906766891, 0.38490405678749084, 0.05195458233356476, 0.3097982704639435, 0.36489662528038025, 0.1262192577123642, 0.2839500606060028, 0.38994312286376953, 0.2793070673942566, 0.07153835147619247, 0.18053406476974487, 0.1415157914161682, 0.4412364065647125, 0.2747727632522583, 0.3399392068386078, 0.3389422297477722, 0.2862347364425659], dtype='float32').reshape([30]),
            paddle.to_tensor([0.36306852102279663, 0.08984466642141342, 0.2397182583808899, 0.37796077132225037, 0.18423211574554443, 0.4880099296569824, 0.26370149850845337, 0.43405309319496155, 0.14067614078521729, 0.07478246837854385, 0.1612793505191803, 0.4140230119228363, 0.18775556981563568, 0.43873330950737, 0.01936943642795086, 0.41292691230773926, 0.20710977911949158, 0.2662194073200226, 0.05413239821791649, 0.43560126423835754, 0.4360077381134033, 0.44916069507598877, 0.1941184103488922, 0.38302648067474365, 0.45635756850242615, 0.2651159167289734, 0.35467347502708435, 0.33650535345077515, 0.37182021141052246, 0.16246162354946136], dtype='float32').reshape([30]),
            paddle.to_tensor([0.28513234853744507, 0.02412351220846176, 0.4979182481765747, 0.21409599483013153, 0.27320578694343567, 0.014373593032360077, 0.010905551724135876, 0.11516567319631577, 0.4410302937030792, 0.17027795314788818, 0.07508585602045059, 0.2553577423095703, 0.4064788520336151, 0.23133346438407898, 0.2059163600206375, 0.12840576469898224, 0.28645533323287964, 0.27568820118904114, 0.41745612025260925, 0.13462547957897186, 0.219129279255867, 0.40573516488075256, 0.3862202763557434, 0.10449779778718948, 0.011329186148941517, 0.055672913789749146, 0.43261051177978516, 0.4416069984436035, 0.14236660301685333, 0.47419092059135437], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7a51618b2bb8f8ad94ac0a39323d25d5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 160, 8, 6], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bace87724506ed61238291d590778c8a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 128, 128], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a83c093fc4925508d4cbebbbe2a3e146(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_3a47a120d62a3f3a422a747f511df8d9
    def get_inputs(self):
        return [
            paddle.uniform([1, 480, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_451bd7d7a9ac5cca9e5ab0a249c7b660(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.457028865814209, 0.09404709190130234, 0.4877640902996063, 0.108727365732193, 0.36923694610595703, 0.20557190477848053, 0.2076130509376526, 0.22134770452976227, 0.0034945630468428135, 0.1825053095817566, 0.2518385648727417, 0.043649956583976746, 0.05753278732299805, 0.18971017003059387, 0.48889556527137756, 0.10881134867668152, 0.22353246808052063, 0.30436649918556213], dtype='float32').reshape([18]),
            paddle.to_tensor([0.08140034973621368, 0.4260844588279724, 0.39329561591148376, 0.1539067178964615, 0.49211061000823975, 0.1103019043803215, 0.25482437014579773, 0.17098015546798706, 0.14442230761051178, 0.19891932606697083, 0.4885496497154236, 0.10552544891834259, 0.26413169503211975, 0.3295343816280365, 0.367299884557724, 0.2627020478248596, 0.07778928428888321, 0.09933969378471375], dtype='float32').reshape([18]),
            paddle.to_tensor([0.47763583064079285, 0.21929064393043518, 0.45200419425964355, 0.49979841709136963, 0.4282993972301483, 0.16166365146636963, 0.043822456151247025, 0.005358824506402016, 0.32348567247390747, 0.07703441381454468, 0.20122601091861725, 0.43307721614837646, 0.2571342885494232, 0.2796490490436554, 0.037495099008083344, 0.4036506414413452, 0.322507381439209, 0.19513174891471863], dtype='float32').reshape([18]),
            paddle.to_tensor([0.2843902111053467, 0.25691741704940796, 0.17617978155612946, 0.3229634463787079, 0.024612752720713615, 0.12464214861392975, 0.2823694050312042, 0.08617749810218811, 0.3099333643913269, 0.2024693638086319, 0.28225967288017273, 0.45898905396461487, 0.031202154234051704, 0.29620999097824097, 0.13870936632156372, 0.07197839021682739, 0.38684627413749695, 0.4715593159198761], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_d589af777f5a8ae699611515c41499fb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_54f5a721ce9366e2ba070e62b0a00190(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_d589af777f5a8ae699611515c41499fb
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 40, 40], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a4325fa88056847cad207cd9ee22f2a4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1bf66c7105f3099c0b458ae455682bf3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 208, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([208], dtype='float32', min=0, max=0.5),
            paddle.uniform([208], dtype='float32', min=0, max=0.5),
            paddle.uniform([208], dtype='float32', min=0, max=0.5),
            paddle.uniform([208], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9fd003444bad580c146954150c333c1d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 112, 112], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5cd3ff384bbbf84383f00198f4a1d399(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_23eb6c11ae7ffebf5efbb1b8545f47ef(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_88a8ee9d072122fda9e0bf496da8d48b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 64, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4ed36c561db9348c57239e2504b42148(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 32, 100], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6d7dc90d004482c6e0d52fd4a82a4da1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.44476601481437683, 0.013840210624039173, 0.2705836892127991, 0.37646427750587463, 0.3095870316028595, 0.46259716153144836, 0.33884134888648987, 0.36549830436706543, 0.36749956011772156, 0.47011080384254456, 0.08846642076969147, 0.39459648728370667, 0.4559953808784485, 0.3951869308948517, 0.4396918714046478, 0.09601029753684998, 0.24243101477622986, 0.10982146114110947], dtype='float32').reshape([18]),
            paddle.to_tensor([0.2907159924507141, 0.271906316280365, 0.34518906474113464, 0.04838027432560921, 0.49478986859321594, 0.11142337322235107, 0.10711246728897095, 0.04712541401386261, 0.030643992125988007, 0.18889927864074707, 0.3417026102542877, 0.3104361295700073, 0.03262826427817345, 0.1553909182548523, 0.09703680872917175, 0.21306800842285156, 0.14508360624313354, 0.2787085473537445], dtype='float32').reshape([18]),
            paddle.to_tensor([0.2485441118478775, 0.3611298203468323, 0.41547465324401855, 0.2905125916004181, 0.2993954122066498, 0.4151739180088043, 0.007944345474243164, 0.046639274805784225, 0.22003725171089172, 0.029501257464289665, 0.19823592901229858, 0.17003194987773895, 0.36503928899765015, 0.22525683045387268, 0.1490555852651596, 0.1155935600399971, 0.31746357679367065, 0.34214311838150024], dtype='float32').reshape([18]),
            paddle.to_tensor([0.38551566004753113, 0.34581756591796875, 0.22182384133338928, 0.25732672214508057, 0.16642428934574127, 0.27431926131248474, 0.23442524671554565, 0.4287928640842438, 0.018595676869153976, 0.2840532958507538, 0.39847296476364136, 0.23907829821109772, 0.0021196280140429735, 0.2035450041294098, 0.3012566864490509, 0.166718527674675, 0.48797696828842163, 0.037238866090774536], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a6ed405cf659c4787d83ff95f1485357(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 32, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9225e615465c1c888698e73a2fe2d888(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_3a47a120d62a3f3a422a747f511df8d9
    def get_inputs(self):
        return [
            paddle.uniform([1, 80, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c750cbc8bf5dc7e47fa2bd9214ec567a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 40, 40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e1b75669a79ba755159a560e8fa1e491(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 61, 61], dtype='float16', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2b206aec1f440ef43cd77a185160fdc2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 31, 31], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c3a01b52fc46ee276640f536fea5cf89(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 640, 640], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4b1d4fc444ff2756b134e8c0fd4d9311(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 4, 240, 240], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.2456567883491516, 0.030721016228199005, 0.3633972704410553, 0.3371869623661041], dtype='float32').reshape([4]),
            paddle.to_tensor([0.49918267130851746, 0.45804455876350403, 0.03302489593625069, 0.18485420942306519], dtype='float32').reshape([4]),
            paddle.to_tensor([0.08373728394508362, 0.22450247406959534, 0.46804532408714294, 0.36650288105010986], dtype='float32').reshape([4]),
            paddle.to_tensor([0.237947016954422, 0.19353654980659485, 0.09845919907093048, 0.05164699628949165], dtype='float32').reshape([4]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b9994796a07e6c2001401a07f7f6f92b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.3115256130695343, 0.0871197059750557, 0.14915956556797028, 0.24824567139148712, 0.44902247190475464, 0.03704828396439552, 0.3990876078605652, 0.007574455812573433], dtype='float32').reshape([8]),
            paddle.to_tensor([0.04399670660495758, 0.46081143617630005, 0.19664083421230316, 0.11913448572158813, 0.24465805292129517, 0.3753165602684021, 0.08093001693487167, 0.40778103470802307], dtype='float32').reshape([8]),
            paddle.to_tensor([0.2138134241104126, 0.06660030037164688, 0.09723786264657974, 0.3005061447620392, 0.054074838757514954, 0.2480507791042328, 0.3140846788883209, 0.03157396242022514], dtype='float32').reshape([8]),
            paddle.to_tensor([0.21344530582427979, 0.04048975929617882, 0.25624486804008484, 0.2542869746685028, 0.06257706880569458, 0.4852598309516907, 0.02291746251285076, 0.0796491950750351], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fb440921a2081dfef2c49c166e5ec6c8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 672, 12, 12], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e6b553aee25a14095fcaea1a114717a9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 60, 60], dtype='float16', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e54d5841d053102fe6c9b571c065550d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 4, 26], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8cffce9648203988a5f986e3ac4c4f15(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 320, 320], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_42fc67e405a3a1aee052ba4751d3651a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 10, 10], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_337e8a6127afd839fe64de0ec65c3a9d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 448, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([448], dtype='float32', min=0, max=0.5),
            paddle.uniform([448], dtype='float32', min=0, max=0.5),
            paddle.uniform([448], dtype='float32', min=0, max=0.5),
            paddle.uniform([448], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_340a03d94ac70a4d4b84aad747aa5fcf(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 2, 160, 160], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.3480111360549927, 0.3792327642440796], dtype='float32').reshape([2]),
            paddle.to_tensor([0.4062037467956543, 0.13423192501068115], dtype='float32').reshape([2]),
            paddle.to_tensor([0.12254276871681213, 0.24158044159412384], dtype='float32').reshape([2]),
            paddle.to_tensor([0.07038971036672592, 0.09008625894784927], dtype='float32').reshape([2]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2db40cbb9a109edde20787e94ce6c742(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.2646864652633667, 0.3758615553379059, 0.14933520555496216, 0.2588030993938446, 0.3952791392803192, 0.12732914090156555, 0.01071555819362402, 0.11171919852495193, 0.424360066652298, 0.018869109451770782, 0.29966622591018677, 0.3513237237930298, 0.46602627635002136, 0.20331522822380066, 0.49844682216644287, 0.18826079368591309, 0.1658194214105606, 0.21485789120197296, 0.03332427144050598, 0.4090564548969269, 0.2693527638912201, 0.16263090074062347, 0.3122347891330719, 0.23409131169319153], dtype='float32').reshape([24]),
            paddle.to_tensor([0.0571223646402359, 0.48089155554771423, 0.3111899495124817, 0.40327611565589905, 0.34522297978401184, 0.157146155834198, 0.1577237993478775, 0.26311764121055603, 0.1707497090101242, 0.45443546772003174, 0.07827804982662201, 0.2264438420534134, 0.3972422480583191, 0.484622061252594, 0.34660473465919495, 0.4690428674221039, 0.007523312233388424, 0.46992364525794983, 0.03143308311700821, 0.05761559307575226, 0.27000752091407776, 0.2327539473772049, 0.40702947974205017, 0.2344636470079422], dtype='float32').reshape([24]),
            paddle.to_tensor([0.3343167006969452, 0.4677945077419281, 0.19246292114257812, 0.32544273138046265, 0.4179996848106384, 0.21076011657714844, 0.49766549468040466, 0.46633926033973694, 0.11813115328550339, 0.3886611759662628, 0.0791759267449379, 0.20966582000255585, 0.43198785185813904, 0.42871445417404175, 0.40058213472366333, 0.20618176460266113, 0.04812713339924812, 0.14387482404708862, 0.015233674086630344, 0.08636433631181717, 0.2911958396434784, 0.1766142100095749, 0.13881678879261017, 0.3793391287326813], dtype='float32').reshape([24]),
            paddle.to_tensor([0.21592411398887634, 0.2580965757369995, 0.3067775070667267, 0.037184324115514755, 0.21920348703861237, 0.13198880851268768, 0.07771241664886475, 0.41358697414398193, 0.13666699826717377, 0.02542458102107048, 0.21517011523246765, 0.3955917954444885, 0.24526701867580414, 0.22853244841098785, 0.4896193742752075, 0.22225023806095123, 0.14254559576511383, 0.2338661551475525, 0.46991443634033203, 0.08507991582155228, 0.1767270863056183, 0.15400440990924835, 0.19429127871990204, 0.03126867860555649], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_78d05869059c0de0a3565dd91bb748aa(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([8, 256, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6d06781799d928555bb293864ac60264(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 26, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.13362184166908264, 0.08644498139619827, 0.35316944122314453, 0.011716609820723534, 0.4433189928531647, 0.16172614693641663, 0.04472769424319267, 0.25369593501091003, 0.3178180158138275, 0.47248977422714233, 0.4406867027282715, 0.20898430049419403, 0.2467992901802063, 0.1404273509979248, 0.19482064247131348, 0.05123397335410118, 0.37221020460128784, 0.37780001759529114, 0.08705125004053116, 0.32206711173057556, 0.16277769207954407, 0.32577234506607056, 0.45613375306129456, 0.42331135272979736, 0.1001259982585907, 0.4010266661643982], dtype='float32').reshape([26]),
            paddle.to_tensor([0.39159759879112244, 9.046419290825725e-05, 0.2698068618774414, 0.014597597531974316, 0.43518173694610596, 0.4396857023239136, 0.06469188630580902, 0.13179990649223328, 0.3097805976867676, 0.3080715835094452, 0.1299182027578354, 0.173394113779068, 0.38945549726486206, 0.026839954778552055, 0.10878867655992508, 0.09630477428436279, 0.3590161204338074, 0.48406386375427246, 0.07991952449083328, 0.14824964106082916, 0.2574440836906433, 0.06992713361978531, 0.3918524384498596, 0.24067455530166626, 0.41780340671539307, 0.21294088661670685], dtype='float32').reshape([26]),
            paddle.to_tensor([0.28760862350463867, 0.4775669276714325, 0.4608195424079895, 0.1952761709690094, 0.0073211779817938805, 0.4842904210090637, 0.17310163378715515, 0.11757823079824448, 0.32725533843040466, 0.022009452804923058, 0.40838056802749634, 0.04090537503361702, 0.0935482457280159, 0.43098166584968567, 0.19098374247550964, 0.1260119527578354, 0.4921557605266571, 0.04061269387602806, 0.37044233083724976, 0.08445505052804947, 0.06012513116002083, 0.42894628643989563, 0.16256031394004822, 0.40212181210517883, 0.4293535053730011, 0.1897999495267868], dtype='float32').reshape([26]),
            paddle.to_tensor([0.009416951797902584, 0.05928392708301544, 0.1937384307384491, 0.051102638244628906, 0.27732938528060913, 0.0816815048456192, 0.4726470112800598, 0.10449469834566116, 0.13742578029632568, 0.06766409426927567, 0.4338512122631073, 0.1917175054550171, 0.30066123604774475, 0.40410810708999634, 0.05962343513965607, 0.07792449742555618, 0.3896878659725189, 0.29342901706695557, 0.09235341846942902, 0.2762380540370941, 0.14928632974624634, 0.07870099693536758, 0.29497194290161133, 0.08538266271352768, 0.18210670351982117, 0.2217852622270584], dtype='float32').reshape([26]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9a5c8fd16e5e495fe7f909f999eb01a9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 4, 50], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6311937a32c65a39e7ecdb865d1effb3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 2048, 20, 20], dtype='float16', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b5f1b1300277d91b7787acd36c82b254(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 15, 15], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_92f31a4adb928839bfa85e646d6a25bb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_b4f4da3a8d44d1b5b3a2903a097df186
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 32, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d2e1ceffe158abf6aae47dfc53c8eda3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.15298399329185486, 0.08096863329410553, 0.24680361151695251, 0.09885484725236893, 0.4444945454597473, 0.35980531573295593, 0.2042008936405182, 0.25699225068092346, 0.3562373220920563, 0.23234125971794128, 0.30830642580986023, 0.22492897510528564, 0.33005303144454956, 0.061066459864377975, 0.36119648814201355, 0.10573030263185501], dtype='float32').reshape([16]),
            paddle.to_tensor([0.19992795586585999, 0.4440745413303375, 0.3486956059932709, 0.012053818441927433, 0.36667904257774353, 0.32958972454071045, 0.17639300227165222, 0.02898305281996727, 0.144718199968338, 0.3079683482646942, 0.25840649008750916, 0.42417412996292114, 0.3990858495235443, 0.14439982175827026, 0.06168901547789574, 0.2398735135793686], dtype='float32').reshape([16]),
            paddle.to_tensor([0.37907522916793823, 0.22384515404701233, 0.05598185956478119, 0.31866559386253357, 0.4295628070831299, 0.4316358268260956, 0.0024304473772644997, 0.08043786138296127, 0.46857237815856934, 0.15632642805576324, 0.11299902200698853, 0.10459499061107635, 0.4883902668952942, 0.4573144018650055, 0.3478161692619324, 0.4345683157444], dtype='float32').reshape([16]),
            paddle.to_tensor([0.24560126662254333, 0.4615214467048645, 0.012245755642652512, 0.10251102596521378, 0.390876829624176, 0.1339259296655655, 0.03384515643119812, 0.28383520245552063, 0.2049572914838791, 0.3137418031692505, 0.3658590018749237, 0.12013649195432663, 0.3810690641403198, 0.364793062210083, 0.4814942181110382, 0.20832255482673645], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_34200e6f0ee69419882d7a6ed01bee0f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 40, 40], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d3659f024956290ab910c58f1526246e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8f167c7c1c6ab5acba6d5d85f78ab826(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 28, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.21216981112957, 0.4330849051475525, 0.020308395847678185, 0.12212400883436203, 0.3719989061355591, 0.4941164255142212, 0.4853965640068054, 0.4009983539581299, 0.041712742298841476, 0.3934299647808075, 0.11124300956726074, 0.15102533996105194, 0.2749781906604767, 0.1456809639930725, 0.2521314322948456, 0.25446438789367676, 0.2898798882961273, 0.021709462627768517, 0.22070421278476715, 0.04515393450856209, 0.4091031551361084, 0.38341304659843445, 0.44427475333213806, 0.4766429662704468, 0.016927137970924377, 0.16879166662693024, 0.29076629877090454, 0.4291241466999054], dtype='float32').reshape([28]),
            paddle.to_tensor([0.4522489607334137, 0.3546319007873535, 0.3456157147884369, 0.47984176874160767, 0.29546135663986206, 0.3278300166130066, 0.2985308766365051, 0.4178597629070282, 0.4185325503349304, 0.1259133368730545, 0.34352052211761475, 0.41812121868133545, 0.09949671477079391, 0.19518104195594788, 0.4086201786994934, 0.35826167464256287, 0.011119009926915169, 0.13513724505901337, 0.11687900125980377, 0.4210837781429291, 0.4740508496761322, 0.4467361867427826, 0.4872353971004486, 0.18403978645801544, 0.18395735323429108, 0.005534939002245665, 0.24438981711864471, 0.3759188652038574], dtype='float32').reshape([28]),
            paddle.to_tensor([0.2381284534931183, 0.09116090834140778, 0.2712145149707794, 0.4283827543258667, 0.1957254409790039, 0.30989062786102295, 0.3860909640789032, 0.3725881278514862, 0.2187640368938446, 0.04962040111422539, 0.02220780961215496, 0.14135853946208954, 0.4144347906112671, 0.03236725926399231, 0.2753443717956543, 0.48951905965805054, 0.18077366054058075, 0.0038959707599133253, 0.23957450687885284, 0.4472057819366455, 0.17930911481380463, 0.19211150705814362, 0.08230036497116089, 0.2963375747203827, 0.10970927029848099, 0.49527809023857117, 0.40321001410484314, 0.04679102823138237], dtype='float32').reshape([28]),
            paddle.to_tensor([0.11339333653450012, 0.1249048039317131, 0.4681335985660553, 0.4601726531982422, 0.3248181939125061, 0.13240909576416016, 0.4953373372554779, 0.2928011119365692, 0.2108980268239975, 0.4283047318458557, 0.09759681671857834, 0.4590604901313782, 0.20135323703289032, 0.13832296431064606, 0.37597814202308655, 0.14408135414123535, 0.07454779744148254, 0.0459417887032032, 0.2604491114616394, 0.4938179552555084, 0.14243407547473907, 0.06114508956670761, 0.1012953594326973, 0.38413694500923157, 0.37257787585258484, 0.2585948705673218, 0.10642878711223602, 0.03204071894288063], dtype='float32').reshape([28]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5cf8893654aa6ed0d25983cfd73a1818(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([8, 512, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_54ea3d5f6e734140fce1bb7c816486ef(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 240, 40, 40], dtype='float16', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0dbd676d82985f3bf53c8d27fa4f8652(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 144, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9a1d1cb8bcbefc02a04c913a31b69141(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 112, 112], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_79fd552d1a7561c7bde5bb8ae4d3b774(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 128, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_413cd0c7f6a7232225e7c39f7af0f02d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 232, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([232], dtype='float32', min=0, max=0.5),
            paddle.uniform([232], dtype='float32', min=0, max=0.5),
            paddle.uniform([232], dtype='float32', min=0, max=0.5),
            paddle.uniform([232], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5421538e781890ee61cf6601fa2192e9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 384, 64, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d119b6afa91ce6c31ac0b78d8b3ca506(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 120, 120], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_471eb9c9b00a8986ee134cfdb7992098(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 112, 112], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.4061950743198395, 0.11750754714012146, 0.012729568406939507, 0.4930550754070282, 0.46644797921180725, 0.48076331615448, 0.3936442732810974, 0.33193814754486084, 0.04627956822514534, 0.4295077323913574, 0.24402424693107605, 0.049003321677446365, 0.10740125924348831, 0.09554415196180344, 0.050186946988105774, 0.1851687878370285, 0.4118499159812927, 0.396608829498291, 0.1449621468782425, 0.47193753719329834, 0.2675723135471344, 0.1591114103794098, 0.12301694601774216, 0.35156241059303284], dtype='float32').reshape([24]),
            paddle.to_tensor([0.06445363163948059, 0.4489726722240448, 0.27190014719963074, 0.10924581438302994, 0.29583653807640076, 0.4198392629623413, 0.2086368054151535, 0.36241966485977173, 0.3350387215614319, 0.01743871159851551, 0.4398629665374756, 0.49124306440353394, 0.2551235258579254, 0.3353339433670044, 0.26395583152770996, 0.1091332733631134, 0.4628694951534271, 0.16050495207309723, 0.22300490736961365, 0.4919290840625763, 0.08571722358465195, 0.30728545784950256, 0.4413754642009735, 0.42135778069496155], dtype='float32').reshape([24]),
            paddle.to_tensor([0.22507870197296143, 0.3175134062767029, 0.06601528078317642, 0.4923296570777893, 0.4004162549972534, 0.250356525182724, 0.08367025852203369, 0.05996917188167572, 0.4044913351535797, 0.3658193051815033, 0.4149310290813446, 0.26642513275146484, 0.4725506901741028, 0.20087432861328125, 0.20959515869617462, 0.47460782527923584, 0.38839104771614075, 0.07649388164281845, 0.3461657464504242, 0.3253861665725708, 0.058670494705438614, 0.34618067741394043, 0.4229469299316406, 0.25850605964660645], dtype='float32').reshape([24]),
            paddle.to_tensor([0.08181720972061157, 0.33894312381744385, 0.026617441326379776, 0.005772807635366917, 0.20994114875793457, 0.1416090577840805, 0.057172179222106934, 0.285382479429245, 0.31606805324554443, 0.37141111493110657, 0.10144257545471191, 0.27042022347450256, 0.2427993267774582, 0.17843718826770782, 0.3318916857242584, 0.29992789030075073, 0.01749594695866108, 0.48199015855789185, 0.07322333008050919, 0.34825360774993896, 0.35717862844467163, 0.26997536420822144, 0.46449732780456543, 0.4189714789390564], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b3c793509459427262008e3c390a1f8c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.3499051332473755, 0.34862831234931946, 0.22910338640213013, 0.1758856177330017, 0.22654584050178528, 0.15793108940124512, 0.22883428633213043, 0.4610584080219269, 0.2237134873867035, 0.07546702027320862, 0.3334086537361145, 0.4078945815563202, 0.4966931641101837, 0.4697027802467346, 0.15613853931427002, 0.08833184838294983, 0.32452118396759033, 0.1398150473833084, 0.3131750226020813, 0.39353886246681213, 0.3195120692253113, 0.07378200441598892, 0.2884904146194458, 0.12152272462844849, 0.2541649639606476, 0.20946788787841797, 0.1099916398525238, 0.28013312816619873, 0.247820645570755, 0.375274121761322], dtype='float32').reshape([30]),
            paddle.to_tensor([0.40563493967056274, 0.30499425530433655, 0.06951157003641129, 0.3481267988681793, 0.10442767292261124, 0.17408572137355804, 0.2067558616399765, 0.3445921242237091, 0.10128677636384964, 0.12836022675037384, 0.016248686239123344, 0.16891326010227203, 0.09008655697107315, 0.006843226030468941, 0.12349728494882584, 0.3043554425239563, 0.21024534106254578, 0.16841861605644226, 0.29653534293174744, 0.2418091595172882, 0.36220550537109375, 0.12096687406301498, 0.039103563874959946, 0.25498494505882263, 0.30595412850379944, 0.327477365732193, 0.22068174183368683, 0.22003984451293945, 0.3365687131881714, 0.1464061737060547], dtype='float32').reshape([30]),
            paddle.to_tensor([0.49249565601348877, 0.12599606812000275, 0.3223794996738434, 0.36427122354507446, 0.2984289526939392, 0.46196743845939636, 0.12041380256414413, 0.3697333335876465, 0.45932480692863464, 0.03580249100923538, 0.45803216099739075, 0.0481279119849205, 0.35648003220558167, 0.42846691608428955, 0.38636624813079834, 0.3080996870994568, 0.05573175847530365, 0.030249159783124924, 0.22524294257164001, 0.4372034966945648, 0.45879632234573364, 0.3639897108078003, 0.2719370424747467, 0.1834651678800583, 0.2665315866470337, 0.3910914361476898, 0.34645769000053406, 0.21704824268817902, 0.007710921578109264, 0.07053503394126892], dtype='float32').reshape([30]),
            paddle.to_tensor([0.02402021735906601, 0.47197967767715454, 0.08556383848190308, 0.2496054470539093, 0.018777702003717422, 0.4718736410140991, 0.4942914545536041, 0.36228373646736145, 0.4767894148826599, 0.3247860074043274, 0.24195198714733124, 0.4546321928501129, 0.4077587425708771, 0.06627073884010315, 0.09078926593065262, 0.28544753789901733, 0.21504686772823334, 0.36224043369293213, 0.4663243293762207, 0.4580325186252594, 0.17901335656642914, 0.357177197933197, 0.32962653040885925, 0.2690579891204834, 0.3985554873943329, 0.02008671872317791, 0.43910345435142517, 0.22242893278598785, 0.2376399040222168, 0.15899905562400818], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_93525d94ea551542a1ce1b3867134a97(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_da4d85bc213f70819a72e68f21214cdb
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 32, 16, 16], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_13f40dc2c75f555d40eaccee03db5ecb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 1200, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([1200], dtype='float32', min=0, max=0.5),
            paddle.uniform([1200], dtype='float32', min=0, max=0.5),
            paddle.uniform([1200], dtype='float32', min=0, max=0.5),
            paddle.uniform([1200], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_13f5931cc81d4ed288e8d97b01b9297f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 1296, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([1296], dtype='float32', min=0, max=0.5),
            paddle.uniform([1296], dtype='float32', min=0, max=0.5),
            paddle.uniform([1296], dtype='float32', min=0, max=0.5),
            paddle.uniform([1296], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a189650ad2cf49138ddade1316ee5cb6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_da4d85bc213f70819a72e68f21214cdb
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 4, 16, 16], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8c076af1fe575b77f8aae34dc3668591(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 32, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b47afa29699235ebf29b5683bb51b6d3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.354026734828949, 0.3163531720638275, 0.24389338493347168, 0.2670633792877197, 0.46131476759910583, 0.22827208042144775, 0.010381306521594524, 0.357184499502182, 0.38125962018966675, 0.27947428822517395, 0.1939694881439209, 0.24194298684597015, 0.4595930576324463, 0.2171449512243271, 0.42630767822265625, 0.09206748008728027, 0.06510616093873978, 0.47374463081359863, 0.47219741344451904, 0.3279024064540863], dtype='float32').reshape([20]),
            paddle.to_tensor([0.4434535801410675, 0.039132487028837204, 0.06844056397676468, 0.3143620491027832, 0.46785420179367065, 0.20822851359844208, 0.2757760286331177, 0.48821312189102173, 0.4584648013114929, 0.17607656121253967, 0.09324365854263306, 0.4809601604938507, 0.42846864461898804, 0.47138968110084534, 0.3101239502429962, 0.13746532797813416, 0.07506956160068512, 0.10064785182476044, 0.23284606635570526, 0.33919772505760193], dtype='float32').reshape([20]),
            paddle.to_tensor([0.27499210834503174, 0.1415020078420639, 0.3579586446285248, 0.23779964447021484, 0.10100872814655304, 0.202396959066391, 0.3291808068752289, 0.0505029633641243, 0.1242477297782898, 0.048094674944877625, 0.17019957304000854, 0.30259886384010315, 0.05797269567847252, 0.48218268156051636, 0.3009636104106903, 0.4878941774368286, 0.10540518164634705, 0.1732378900051117, 0.28559163212776184, 0.20670360326766968], dtype='float32').reshape([20]),
            paddle.to_tensor([0.4108128249645233, 0.4445648193359375, 0.07814893126487732, 0.311717689037323, 0.12076590955257416, 0.21065948903560638, 0.0014412438031286001, 0.2967817187309265, 0.11633873730897903, 0.1310754269361496, 0.28274646401405334, 0.20482657849788666, 0.4047060012817383, 0.077527716755867, 0.1337793469429016, 0.3516327142715454, 0.34788262844085693, 0.23417958617210388, 0.4070581793785095, 0.23466359078884125], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e546b708569e8d38741e5ecf599d46fc(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 61, 61], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b9f6a474f5cb148d14fbd14dbc7d9622(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 160, 4, 3], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d4d162861c6a48e95e14b0fc8a4c99c8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 4, 4], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6fc3d4cad8473c3e61d331713dd0fb7d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 1280, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([1280], dtype='float32', min=0, max=0.5),
            paddle.uniform([1280], dtype='float32', min=0, max=0.5),
            paddle.uniform([1280], dtype='float32', min=0, max=0.5),
            paddle.uniform([1280], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9653a8c9658758f169e155e1ff65ac71(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 128, 128], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d6ac714905b2ac63cbb4a0802ccd3de4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 20, 20], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b59f47c4d53477462af5d042f93103b4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 76, 76], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_889f7e03798cab8767f6e2c1a0829b84(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 35, 35], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2b0ea1323cdd00efc1f1b8507c8bfa56(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 160, 160], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_650cb3d5dac21c3a05ed2dba9346ea24(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 8, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_db302fc68674ced0050fd6a551693744(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_465c0017c1a52c557472087970776538(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e019fb13c2774a1ba9c3252289915b90(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 256, 512], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_924480489ef6cd24267b8fa2bd147c72(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 80, 80], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_0edd578f48a07aa5d1d79f9f678b472e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f04931d0e94d92ec95a8440ae91fe6f0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_0edd578f48a07aa5d1d79f9f678b472e
    def get_inputs(self):
        return [
            paddle.uniform([196, 128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c4bee5819c3d00a59574ced23ae964ae(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 80, 12, 12], dtype='float16', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5f35f5ad20a4138972922db14005ece2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_da4d85bc213f70819a72e68f21214cdb
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 4, 16, 16], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_04c66ca1bad5da9dfa7ef06a38b23927(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 152, 152], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9d3d9307638e54f3a3be3459544e804d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_af91295b59c5fca17f890a802deb001f
    def get_inputs(self):
        return [
            paddle.uniform([1, 2048, 4, 8, 8], dtype='float16', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f579fc1fbf49acb98d6c0801fd66939a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 28, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.05944199115037918, 0.35123211145401, 0.49072903394699097, 0.13821575045585632, 0.13060511648654938, 0.44130831956863403, 0.4680791199207306, 0.3186846673488617, 0.3058294653892517, 0.07339099794626236, 0.004788680467754602, 0.31017056107521057, 0.37697169184684753, 0.06671490520238876, 0.43838030099868774, 0.186931312084198, 0.11523933708667755, 0.37931931018829346, 0.27772173285484314, 0.006929927971214056, 0.3645560145378113, 0.29431840777397156, 0.06403247267007828, 0.0197756364941597, 0.3597615957260132, 0.10932484269142151, 0.2676604688167572, 0.23298698663711548], dtype='float32').reshape([28]),
            paddle.to_tensor([0.28597357869148254, 0.3393113315105438, 0.1896585375070572, 0.294087678194046, 0.02064935490489006, 0.14394217729568481, 0.051652777940034866, 0.13945935666561127, 0.3988516926765442, 0.4485222101211548, 0.2542550563812256, 0.057243816554546356, 0.23189222812652588, 0.024396371096372604, 0.4575980305671692, 0.3780623972415924, 0.09298385679721832, 0.06171059235930443, 0.4510325491428375, 0.41978517174720764, 0.12848423421382904, 0.40275809168815613, 0.10212043672800064, 0.2741091847419739, 0.42250779271125793, 0.16019389033317566, 0.11519371718168259, 0.19933250546455383], dtype='float32').reshape([28]),
            paddle.to_tensor([0.04254550859332085, 0.07374733686447144, 0.4100591540336609, 0.3375592827796936, 0.42172425985336304, 0.14268870651721954, 0.04147534817457199, 0.03361701965332031, 0.3997102677822113, 0.4799502193927765, 0.12093976140022278, 0.15875723958015442, 0.3308292329311371, 0.12479313462972641, 0.39871498942375183, 0.43707627058029175, 0.2134622037410736, 0.34393835067749023, 0.450997531414032, 0.03640260919928551, 0.20072758197784424, 0.24283859133720398, 0.37715205550193787, 0.3565651774406433, 0.3151634931564331, 0.12063230574131012, 0.4949004650115967, 0.23461538553237915], dtype='float32').reshape([28]),
            paddle.to_tensor([0.1653456836938858, 0.45285022258758545, 0.11350599676370621, 0.4182470440864563, 0.013141810894012451, 0.1019178107380867, 0.07445584982633591, 0.06559346616268158, 0.4376641511917114, 0.09412199258804321, 0.002827852265909314, 0.4683606028556824, 0.43223467469215393, 0.4926180839538574, 0.4657607674598694, 0.18278951942920685, 0.3804432153701782, 0.3693312704563141, 0.39166852831840515, 0.26644667983055115, 0.2059866338968277, 0.3715713620185852, 0.18388879299163818, 0.035249557346105576, 0.040337029844522476, 0.3079814016819, 0.46284711360931396, 0.025437550619244576], dtype='float32').reshape([28]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3b52e88539213ff090c9fb0e83268d2c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 64, 128], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.36947453022003174, 0.4653927683830261, 0.3364753723144531, 0.08446670323610306, 0.42676278948783875, 0.49526655673980713, 0.28679558634757996, 0.46271803975105286, 0.49395960569381714, 0.3818988800048828, 0.34047645330429077, 0.05071524158120155, 0.2621227204799652, 0.26931577920913696, 0.08329887688159943, 0.3898497521877289, 0.07359861582517624, 0.3456763029098511], dtype='float32').reshape([18]),
            paddle.to_tensor([0.1072506532073021, 0.33497679233551025, 0.14592726528644562, 0.40993717312812805, 0.2531600594520569, 0.0760916993021965, 0.19865185022354126, 0.021472597494721413, 0.06468791514635086, 0.3297443091869354, 0.44817888736724854, 0.18704666197299957, 0.34221163392066956, 0.408935010433197, 0.07151342183351517, 0.415073961019516, 0.03132734075188637, 0.38165438175201416], dtype='float32').reshape([18]),
            paddle.to_tensor([0.01888526976108551, 0.10447443276643753, 0.1986367404460907, 0.22773712873458862, 0.1694660186767578, 0.3032558858394623, 0.3385086953639984, 0.3797469437122345, 0.4201335906982422, 0.3598604202270508, 0.3775714635848999, 0.05781876668334007, 0.37688419222831726, 0.11326251178979874, 0.00730713177472353, 0.19643300771713257, 0.06755910813808441, 0.12463970482349396], dtype='float32').reshape([18]),
            paddle.to_tensor([0.012846882455050945, 0.12108869105577469, 0.023545047268271446, 0.4638887345790863, 0.2423262894153595, 0.2793028652667999, 0.0808129534125328, 0.27165958285331726, 0.2483328878879547, 0.2570308744907379, 0.35958316922187805, 0.13037815690040588, 0.09440557658672333, 0.17594684660434723, 0.47905194759368896, 0.2852191627025604, 0.24692949652671814, 0.2369743138551712], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_71298adb35dbb06a6d3ad8a560f5cedd(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 60, 60], dtype='float16', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_973010d737b2fea524b0d0b791e42cfd(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 1152, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_80bf6d2d6ec5bc20c39fb90b0a13552b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 256, 512], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.10449041426181793, 0.497561514377594, 0.319103479385376, 0.14913128316402435, 0.00984807126224041, 0.32820582389831543, 0.3511260151863098, 0.23558956384658813, 0.4684748947620392, 0.20714248716831207, 0.16117814183235168, 0.34598323702812195, 0.3050348162651062, 0.049111928790807724, 0.3192081153392792, 0.2599124312400818], dtype='float32').reshape([16]),
            paddle.to_tensor([0.40372225642204285, 0.23130692541599274, 0.07815855741500854, 0.4391046166419983, 0.4477454125881195, 0.347368061542511, 0.060947395861148834, 0.47573742270469666, 0.08553646504878998, 0.28111356496810913, 0.19461867213249207, 0.3322475552558899, 0.42320722341537476, 0.457349956035614, 0.15841352939605713, 0.37884652614593506], dtype='float32').reshape([16]),
            paddle.to_tensor([0.18774370849132538, 0.08536597341299057, 0.11592325568199158, 0.3207171559333801, 0.15122030675411224, 0.01759614795446396, 0.3580661714076996, 0.23537208139896393, 0.22339752316474915, 0.39719516038894653, 0.06435372680425644, 0.277029424905777, 0.2971102297306061, 0.41823121905326843, 0.44657859206199646, 0.2966095209121704], dtype='float32').reshape([16]),
            paddle.to_tensor([0.3573598265647888, 0.41792383790016174, 0.2963006794452667, 0.06144152581691742, 0.07942333817481995, 0.2570774257183075, 0.14158028364181519, 0.3657761812210083, 0.4694879651069641, 0.482454389333725, 0.29606297612190247, 0.019553931429982185, 0.20205442607402802, 0.09073746204376221, 0.037556298077106476, 0.4437851011753082], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d386eec99f100409834d17530b9de638(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_3a47a120d62a3f3a422a747f511df8d9
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 112, 112], dtype='float16', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_577e7fdafa0ede6e8b29ab6af134cb42(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 4, 50], dtype='float16', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e47bd296a1aa7690394e4a17e574a3b8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 38, 38], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b595bf894a8205c40f28878331e17792(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 60, 60], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_af9607409fab9687ad7a79b08f452b04(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 80, 4, 3], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6d2554ee8ed65d66dea15c3b791c97a1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_0edd578f48a07aa5d1d79f9f678b472e
    def get_inputs(self):
        return [
            paddle.uniform([16, 768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8dd4c317cf5758b55a9053f37b9833c1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4185e54fa5d6a852cbb95af1bfccc3f2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.3164184093475342, 0.1344955414533615, 0.27453935146331787, 0.46899256110191345, 0.4514597952365875, 0.30526670813560486, 0.13384214043617249, 0.08518178015947342, 0.14607250690460205, 0.11594691872596741, 0.030026117339730263, 0.4134596288204193, 0.48937976360321045, 0.00601918064057827, 0.06142594292759895, 0.38473182916641235, 0.2828564941883087, 0.26724886894226074], dtype='float32').reshape([18]),
            paddle.to_tensor([0.07006416469812393, 0.3819999694824219, 0.352314293384552, 0.3969886004924774, 0.15654002130031586, 0.4695013463497162, 0.15344169735908508, 0.14526736736297607, 0.42839470505714417, 0.1520577222108841, 0.06907616555690765, 0.1452263593673706, 0.015271458774805069, 0.4237792193889618, 0.3198639154434204, 0.479566752910614, 0.32803478837013245, 0.40901392698287964], dtype='float32').reshape([18]),
            paddle.to_tensor([0.05454368516802788, 0.3821921944618225, 0.22987140715122223, 0.24323813617229462, 0.3163555860519409, 0.25834450125694275, 0.2894149124622345, 0.39286863803863525, 0.34746289253234863, 0.05606052279472351, 0.10050859302282333, 0.2779247462749481, 0.3290826380252838, 0.25652170181274414, 0.022010695189237595, 0.25935494899749756, 0.02294307015836239, 0.08902852982282639], dtype='float32').reshape([18]),
            paddle.to_tensor([0.06930756568908691, 0.1074361503124237, 0.15423056483268738, 0.29814422130584717, 0.38431790471076965, 0.46112948656082153, 0.037390947341918945, 0.1291922777891159, 0.17662788927555084, 0.24620971083641052, 0.30508723855018616, 0.3607925772666931, 0.2986094653606415, 0.08928439766168594, 0.13282495737075806, 0.4627944231033325, 0.19629479944705963, 0.4924582242965698], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4c92e138001460c71fe891d9b2838dd0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 320, 320], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.48780563473701477, 0.21818004548549652, 0.22375355660915375, 0.336033433675766, 0.16786839067935944, 0.49946263432502747, 0.4104824662208557, 0.056426193565130234], dtype='float32').reshape([8]),
            paddle.to_tensor([0.168185755610466, 0.09187471866607666, 0.1962246596813202, 0.19685502350330353, 0.3838537633419037, 0.23259620368480682, 0.21108809113502502, 0.22572164237499237], dtype='float32').reshape([8]),
            paddle.to_tensor([0.10808418691158295, 0.15588077902793884, 0.3020839989185333, 0.24888014793395996, 0.23386980593204498, 0.464437872171402, 0.45970332622528076, 0.21504941582679749], dtype='float32').reshape([8]),
            paddle.to_tensor([0.225224107503891, 0.24888823926448822, 0.04628878831863403, 0.20130307972431183, 0.21284855902194977, 0.4188290536403656, 0.4471122622489929, 0.28677430748939514], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b6c67ebce72c9370858852a910970c0c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 42, 1, 1], dtype='float32', min=0, max=0.5),
            paddle.uniform([42], dtype='float32', min=0, max=0.5),
            paddle.uniform([42], dtype='float32', min=0, max=0.5),
            paddle.uniform([42], dtype='float32', min=0, max=0.5),
            paddle.uniform([42], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c36a42869d60f34992b5389045efa76d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 2048, 64, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e6184524e9dcb00cbd72629db575d52e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_3a47a120d62a3f3a422a747f511df8d9
    def get_inputs(self):
        return [
            paddle.uniform([1, 672, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_81c17e36b9c6eefa66b6ef59a7b7bebc(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 10, 10], dtype='float16', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_08db05122c1b633a0530508462126b52(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 384, 8, 8], dtype='float16', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1a1f2849bb8b420b4f1f901c960beb59(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 72, 14, 25], dtype='float16', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_12d34811fb0f715838c846227b41d3fb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_fc6b6917e04b7b7cf9fdb39dde97ec40
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 160, 160], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4342d2501d2cd0876870d79d2ab66e74(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 120, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_52f20426ca3b3d3801b70239fe780c57(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 72, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_41f0670a340cf77e1a0b1483f57420e0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 1152, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_148e5e94a11833f351678c562ddc2b60(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 60, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f6a45a570cc406a7f756c7a24e5cb430(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 256, 512], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.07337762415409088, 0.15994463860988617, 0.4299646317958832, 0.43273890018463135, 0.1883554607629776, 0.0883696898818016, 0.09381209313869476, 0.29965052008628845, 0.10459855943918228, 0.11294645071029663, 0.20138368010520935, 0.3571130037307739, 0.15682004392147064, 0.010192643851041794, 0.4798964560031891, 0.21387000381946564], dtype='float32').reshape([16]),
            paddle.to_tensor([0.043620765209198, 0.020972568541765213, 0.32924196124076843, 0.16746966540813446, 0.2928487956523895, 0.10271436721086502, 0.1569707840681076, 0.14298135042190552, 0.11449875682592392, 0.42709681391716003, 0.1364525407552719, 0.2539854049682617, 0.27736222743988037, 0.20308810472488403, 0.10104628652334213, 0.20851454138755798], dtype='float32').reshape([16]),
            paddle.to_tensor([0.055736150592565536, 0.05113145336508751, 0.3002559244632721, 0.24080480635166168, 0.12633557617664337, 0.3336648643016815, 0.16665779054164886, 0.11698494106531143, 0.03953254222869873, 0.15196777880191803, 0.18010789155960083, 0.4773995876312256, 0.1185096949338913, 0.1088225468993187, 0.4753013551235199, 0.23152536153793335], dtype='float32').reshape([16]),
            paddle.to_tensor([0.36517325043678284, 0.1495986133813858, 0.0546971932053566, 0.29641786217689514, 0.1374678760766983, 0.1863337606191635, 0.12743255496025085, 0.14388005435466766, 0.21166644990444183, 0.06908862292766571, 0.3859065771102905, 0.11522889137268066, 0.265472948551178, 0.31015539169311523, 0.4542032480239868, 0.3424663245677948], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3b096f7743dfa54831b5c4d0127ec9f2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 16, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7c4eefeee543a858ed28d58eee0d7a2e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_3a47a120d62a3f3a422a747f511df8d9
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 150, 150], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_59fa8dc4cf6b29afe9a267dcd92a3136(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 1776, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([1776], dtype='float32', min=0, max=0.5),
            paddle.uniform([1776], dtype='float32', min=0, max=0.5),
            paddle.uniform([1776], dtype='float32', min=0, max=0.5),
            paddle.uniform([1776], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9a35ee208a6d0d28e7e95bea9ba909d4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_da4d85bc213f70819a72e68f21214cdb
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 32, 64, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_df060ffc9d12c083d3e8d5d17c354ccd(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 4, 256, 512], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.1834540069103241, 0.024977602064609528, 0.25229915976524353, 0.46150681376457214], dtype='float32').reshape([4]),
            paddle.to_tensor([0.15628549456596375, 0.06035503372550011, 0.3724788725376129, 0.10237032175064087], dtype='float32').reshape([4]),
            paddle.to_tensor([0.0688350573182106, 0.2827354073524475, 0.47977685928344727, 0.19574464857578278], dtype='float32').reshape([4]),
            paddle.to_tensor([0.24857880175113678, 0.14123764634132385, 0.09871376305818558, 0.084771528840065], dtype='float32').reshape([4]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d2e0fc47bf68fd24771b08758fb50861(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 4, 50], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_70c3a826014ac36cb0a6b384a831cb90(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.22188176214694977, 0.27527105808258057, 0.48016148805618286, 0.4519614577293396, 0.28660744428634644, 0.12899886071681976, 0.4457583427429199, 0.09343080222606659, 0.3077554404735565, 0.4112585484981537, 0.04200639948248863, 0.3055749833583832, 0.2944807708263397, 0.48049473762512207, 0.13493387401103973, 0.057773735374212265, 0.03037061169743538, 0.30390334129333496, 0.1540125608444214, 0.49971067905426025, 0.4298586845397949, 0.2710988223552704, 0.3875858783721924, 0.18389573693275452, 0.10106754302978516, 0.4808553159236908, 0.0938626304268837, 0.22310522198677063, 0.443391352891922, 0.3016033172607422], dtype='float32').reshape([30]),
            paddle.to_tensor([0.3936898410320282, 0.2589089572429657, 0.49089255928993225, 0.048769399523735046, 0.38508689403533936, 0.39834997057914734, 0.34384143352508545, 0.12245293706655502, 0.46204903721809387, 0.06581876426935196, 0.2591749131679535, 0.37674978375434875, 0.487114280462265, 0.4071900248527527, 0.17410027980804443, 0.25334763526916504, 0.4809947609901428, 0.22333626449108124, 0.00898281205445528, 0.1354670524597168, 0.3505248427391052, 0.4460490942001343, 0.49559396505355835, 0.38529491424560547, 0.027942856773734093, 0.36651328206062317, 0.25845006108283997, 0.3760848641395569, 0.2797064483165741, 0.3816532492637634], dtype='float32').reshape([30]),
            paddle.to_tensor([0.004687925800681114, 0.19992028176784515, 0.47134047746658325, 0.34435948729515076, 0.37619471549987793, 0.3924158215522766, 0.04310878366231918, 0.01209506019949913, 0.19369634985923767, 0.2709463834762573, 0.3865619897842407, 0.17728397250175476, 0.17984721064567566, 0.2955915629863739, 0.14103791117668152, 0.1243007555603981, 0.3515753746032715, 0.19895368814468384, 0.17003245651721954, 0.21099843084812164, 0.2833872437477112, 0.4761359393596649, 0.4452819228172302, 0.47538164258003235, 0.15436653792858124, 0.04908698797225952, 0.3959868848323822, 0.2517269551753998, 0.07515529543161392, 0.40945151448249817], dtype='float32').reshape([30]),
            paddle.to_tensor([0.26733824610710144, 0.21849043667316437, 0.3795580565929413, 0.37640616297721863, 0.16930560767650604, 0.16246725618839264, 0.33081820607185364, 0.4622054994106293, 0.39055609703063965, 0.4127846658229828, 0.15945899486541748, 0.2938125431537628, 0.346657395362854, 0.3812171220779419, 0.4985109865665436, 0.13728874921798706, 0.49749788641929626, 0.06275469809770584, 0.47212162613868713, 0.45479440689086914, 0.44187524914741516, 0.49025410413742065, 0.29370975494384766, 0.30325815081596375, 0.12362253665924072, 0.13702933490276337, 0.29272571206092834, 0.47111672163009644, 0.44622644782066345, 0.2979416847229004], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_91a9e2d82dac75a22a91fd1a7718312b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d1a836f6b4f6bdd2f959c62ba0ca3fde(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_da4d85bc213f70819a72e68f21214cdb
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 4, 32, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4d18c1cfcc58c5093d1779f277da386e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 16, 16], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_00fe16de063510ec07aad52a136c8fdf(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 122, 122], dtype='float16', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d9169045ab642af435891b0879aadaa9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 480, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_abe07b9c8f267539da34390f8ef009f1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 512, 512], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_97c4070257cbb4c2a6ef119fbc133e23(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 416, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([416], dtype='float32', min=0, max=0.5),
            paddle.uniform([416], dtype='float32', min=0, max=0.5),
            paddle.uniform([416], dtype='float32', min=0, max=0.5),
            paddle.uniform([416], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b6fabc629ca05279349e33d357ccfb69(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_af91295b59c5fca17f890a802deb001f
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 4, 64, 64], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.47248053550720215, 0.2418496012687683, 0.06178392469882965, 0.3380126953125, 0.35868844389915466, 0.11716675758361816, 0.0175583828240633, 0.09165141731500626, 0.4522545337677002, 0.3690929412841797, 0.4831046164035797, 0.12079614400863647, 0.38682878017425537, 0.43733322620391846, 0.3259173333644867, 0.4035533666610718], dtype='float32').reshape([16]),
            paddle.to_tensor([0.2791754901409149, 0.46715664863586426, 0.05026601254940033, 0.11146917939186096, 0.041651222854852676, 0.04854968562722206, 0.35457688570022583, 0.4359801709651947, 0.12126056849956512, 0.3054613769054413, 0.1562226116657257, 0.06858933717012405, 0.43420127034187317, 0.48884621262550354, 0.14055275917053223, 0.11362308263778687], dtype='float32').reshape([16]),
            paddle.to_tensor([0.4664209187030792, 0.2642359435558319, 0.2876673638820648, 0.38789278268814087, 0.412943035364151, 0.0960719883441925, 0.31697216629981995, 0.47439825534820557, 0.399742066860199, 0.4223059117794037, 0.20642562210559845, 0.43806561827659607, 0.3279005289077759, 0.18470771610736847, 0.11121220141649246, 0.3890312910079956], dtype='float32').reshape([16]),
            paddle.to_tensor([0.40225520730018616, 0.4594588875770569, 0.4043586254119873, 0.0673547089099884, 0.24943238496780396, 0.08482033759355545, 0.24227102100849152, 0.24861082434654236, 0.21368764340877533, 0.04803862050175667, 0.07275900989770889, 0.3535121977329254, 0.043432947248220444, 0.2904672920703888, 0.3583833873271942, 0.12440325319766998], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_94889e251c81646ff70adc771777b2cc(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 30, 30], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_58108d30d0de6cb7e7d13696ae52d2ba(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 2048, 30, 30], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_def578a427cb3c6a5d6ba0b59092d5c9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 36, 32, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_76a6b0b7ce29c2d0b5a7c2c27507c271(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 972, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([972], dtype='float32', min=0, max=0.5),
            paddle.uniform([972], dtype='float32', min=0, max=0.5),
            paddle.uniform([972], dtype='float32', min=0, max=0.5),
            paddle.uniform([972], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8d524bc3a72041d016abab1faac5dd33(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 31, 31], dtype='float16', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6af922ab14acc3c11d8287c3c65e3cce(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 102, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([102], dtype='float32', min=0, max=0.5),
            paddle.uniform([102], dtype='float32', min=0, max=0.5),
            paddle.uniform([102], dtype='float32', min=0, max=0.5),
            paddle.uniform([102], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2428fe41e069348e823b33e06ac6ea70(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1e9e2d8c307da4aed98572186b931b75(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 80, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_19fb0b419aecf532784d39a5e2755480(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.21939079463481903, 0.47282668948173523, 0.14497613906860352, 0.41229453682899475, 0.23927903175354004, 0.27792006731033325, 0.28879863023757935, 0.08072736859321594, 0.0023098071105778217, 0.1881597936153412, 0.43866387009620667, 0.2523106038570404, 0.19720378518104553, 0.40437012910842896, 0.17443089187145233, 0.2755351960659027, 0.1734960973262787, 0.49441927671432495, 0.1510746330022812, 0.23325714468955994], dtype='float32').reshape([20]),
            paddle.to_tensor([0.42454472184181213, 0.10287997126579285, 0.18433932960033417, 0.1948651224374771, 0.3274363577365875, 0.4686301052570343, 0.33766037225723267, 0.4410809874534607, 0.05932484567165375, 0.3430213928222656, 0.034562353044748306, 0.2212858945131302, 0.2646419107913971, 0.4370178282260895, 0.19886066019535065, 0.18852798640727997, 0.4985419511795044, 0.28172412514686584, 0.33588361740112305, 0.032513514161109924], dtype='float32').reshape([20]),
            paddle.to_tensor([0.20452335476875305, 0.06340149790048599, 0.277252733707428, 0.3491639792919159, 0.13112276792526245, 0.2782546579837799, 0.43466636538505554, 0.053726907819509506, 0.36018654704093933, 0.35475513339042664, 0.28974592685699463, 0.08789742738008499, 0.22496196627616882, 0.39018163084983826, 0.0007205732981674373, 0.47020548582077026, 0.07889816910028458, 0.11316506564617157, 0.12922731041908264, 0.2922172248363495], dtype='float32').reshape([20]),
            paddle.to_tensor([0.06778303533792496, 0.39616167545318604, 0.2852085530757904, 0.3791981041431427, 0.189536914229393, 0.14049175381660461, 0.30209386348724365, 0.4645363986492157, 0.08813571184873581, 0.48166558146476746, 0.06455102562904358, 0.01838277466595173, 0.32092610001564026, 0.2606780529022217, 0.03465015068650246, 0.05261794850230217, 0.42999958992004395, 0.3975370228290558, 0.34270179271698, 0.4765963852405548], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f2b24eb78fdb0b1da8118e8b9bbb2534(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 112, 64, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_580a8dc76c1852c40f837519f3979f03(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.28214266896247864, 0.4414592683315277, 0.40478751063346863, 0.4360293745994568, 0.38896554708480835, 0.0755959004163742, 0.39199817180633545, 0.17133945226669312, 0.1802103966474533, 0.361041396856308, 0.4219282567501068, 0.1419575810432434, 0.1300729513168335, 0.24648435413837433, 0.4278000593185425, 0.18479475378990173, 0.16799527406692505, 0.4000335931777954, 0.1561812162399292, 0.048324476927518845, 0.3942118287086487, 0.3341057002544403, 0.29864364862442017, 0.08421564102172852, 0.46680840849876404, 0.0759006217122078, 0.4003296494483948, 0.024611810222268105, 0.3522781729698181, 0.28125], dtype='float32').reshape([30]),
            paddle.to_tensor([0.14991934597492218, 0.04812665283679962, 0.28570306301116943, 0.4625515639781952, 0.0028081992641091347, 0.012740205973386765, 0.22910037636756897, 0.03129538521170616, 0.3926410675048828, 0.17607329785823822, 0.1453154981136322, 0.20011205971240997, 0.012202784419059753, 0.14466975629329681, 0.48798611760139465, 0.45501402020454407, 0.3155939280986786, 0.35982468724250793, 0.298391729593277, 0.37483489513397217, 0.3258581757545471, 0.1726827770471573, 0.011926865205168724, 0.0638902336359024, 0.03038901649415493, 0.15484897792339325, 0.2325654774904251, 0.3019730746746063, 0.21725374460220337, 0.28581446409225464], dtype='float32').reshape([30]),
            paddle.to_tensor([0.09939208626747131, 0.3059856593608856, 0.32882213592529297, 0.15494704246520996, 0.025079283863306046, 0.29626792669296265, 0.47709906101226807, 0.15479236841201782, 0.15132971107959747, 0.04039795696735382, 0.3744923770427704, 0.32943233847618103, 0.4945344030857086, 0.22420455515384674, 0.16989892721176147, 0.19749286770820618, 0.28585976362228394, 0.16067789494991302, 0.46037617325782776, 0.18289059400558472, 0.3312242031097412, 0.18003346025943756, 0.30978134274482727, 0.4748592674732208, 0.041089851409196854, 0.09924131631851196, 0.2345481663942337, 0.2772766947746277, 0.4094058871269226, 0.00921030156314373], dtype='float32').reshape([30]),
            paddle.to_tensor([0.34353673458099365, 0.07499151676893234, 0.10038996487855911, 0.03820442408323288, 0.46728837490081787, 0.2861264944076538, 0.34131792187690735, 0.07634922862052917, 0.02756626158952713, 0.3454841673374176, 0.1748451590538025, 0.07025626301765442, 0.2947758436203003, 0.24538318812847137, 0.4776652753353119, 0.4243912994861603, 0.45004802942276, 0.23269374668598175, 0.07399490475654602, 0.4390909969806671, 0.03968114033341408, 0.016569694504141808, 0.32933124899864197, 0.268912672996521, 0.2603618800640106, 0.16679313778877258, 0.2580789029598236, 0.15783850848674774, 0.15183334052562714, 0.3539022207260132], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0c845e0918402437383bcfa5d465828e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([8, 64, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_92307ecbed030511174339039665bb95(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 40, 40], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_44ae85a5e2502f2b75db0b7d2421bbcd(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 7, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a8ff5193cd45fc1bc8ca75ed4bb74723(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_0edd578f48a07aa5d1d79f9f678b472e
    def get_inputs(self):
        return [
            paddle.uniform([1, 1000], dtype='float32', min=0, max=0.5),
            paddle.uniform([1000], dtype='float32', min=0, max=0.5),
            paddle.uniform([1000], dtype='float32', min=0, max=0.5),
            paddle.uniform([1000], dtype='float32', min=0, max=0.5),
            paddle.uniform([1000], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e88afeaa1e24bc02d3f4c6f52061312d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 128, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_c8eca113b3029d88b25158f95c03e4fe(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-06'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6075657ddf3635d7d429d63e40580396(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_c8eca113b3029d88b25158f95c03e4fe
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 128, 128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a18e3bc9ba2a91006de7fd3445b8225e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 112, 112], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_857952382a6bd6dc65a94eaae970e439(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.39736270904541016, 0.2842990458011627, 0.24005724489688873, 0.1642422080039978, 0.3094308078289032, 0.341694176197052, 0.18646956980228424, 0.12229958921670914, 0.34714949131011963, 0.366415411233902, 0.41498056054115295, 0.21894162893295288, 0.0717228576540947, 0.2454272359609604, 0.11864685267210007, 0.1497606486082077, 0.22142206132411957, 0.3911477327346802, 0.4084354043006897, 0.3631216287612915], dtype='float32').reshape([20]),
            paddle.to_tensor([0.38281965255737305, 0.4328556954860687, 0.05554606020450592, 0.4826534390449524, 0.3050723969936371, 0.4162196218967438, 0.04206015169620514, 0.20176498591899872, 0.3625951409339905, 0.19183947145938873, 0.40649446845054626, 0.46109962463378906, 0.37921929359436035, 0.3939478099346161, 0.27516213059425354, 0.12050943076610565, 0.12779439985752106, 0.17607149481773376, 0.017396701499819756, 0.306589812040329], dtype='float32').reshape([20]),
            paddle.to_tensor([0.22456170618534088, 0.17109517753124237, 0.4525911509990692, 0.38168707489967346, 0.0055717951618134975, 0.3629259467124939, 0.06732954829931259, 0.48030778765678406, 0.06023390591144562, 0.15106475353240967, 0.42415985465049744, 0.40761858224868774, 0.2047908753156662, 0.22014424204826355, 0.15395395457744598, 0.41083216667175293, 0.3878801167011261, 0.10839607566595078, 0.07349513471126556, 0.08102130889892578], dtype='float32').reshape([20]),
            paddle.to_tensor([0.2434055507183075, 0.3615807592868805, 0.14263901114463806, 0.12512274086475372, 0.43595805764198303, 0.04951736330986023, 0.09731659293174744, 0.012132550589740276, 0.253807008266449, 0.1512337177991867, 0.29296475648880005, 0.10788334161043167, 0.47575923800468445, 0.1659500002861023, 0.4083041846752167, 0.36743777990341187, 0.16120101511478424, 0.11347363144159317, 0.3485504984855652, 0.34046316146850586], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d3e2359ca4e3523a60d293f7adbf1408(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 20, 20], dtype='float16', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_38ee47d84c502f0ea2de34186f9e0654(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 16, 16], dtype='float16', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8727c76d7ba0d77d282537f30b3cf317(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 16, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_51d4383fbf12e184cee6696049d34f6c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 35, 35], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5d6aa4c14b1d96fa21098cc02ff27c28(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 480, 480], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.07619008421897888, 0.36971789598464966, 0.35390138626098633, 0.45277807116508484, 0.1918385773897171, 0.22456477582454681, 0.10429902374744415, 0.44089674949645996], dtype='float32').reshape([8]),
            paddle.to_tensor([0.06101341173052788, 0.08476851880550385, 0.11403797566890717, 0.3273636996746063, 0.40948668122291565, 0.006884709931910038, 0.12376286089420319, 0.07895774394273758], dtype='float32').reshape([8]),
            paddle.to_tensor([0.15499602258205414, 0.0667988508939743, 0.30418118834495544, 0.2632904648780823, 0.23432454466819763, 0.36753013730049133, 0.48688626289367676, 0.26116594672203064], dtype='float32').reshape([8]),
            paddle.to_tensor([0.2577393352985382, 0.4997815489768982, 0.2440880686044693, 0.37314215302467346, 0.4640924632549286, 0.395018070936203, 0.06752178072929382, 0.22948254644870758], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2bba1baf959b6e4ac8a282c1b3494e9c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 122, 122], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c68773823fceb62fe030cd4123bea310(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 112, 112], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.10093266516923904, 0.16785761713981628, 0.3448741137981415, 0.3916095495223999, 0.48447367548942566, 0.4738534986972809, 0.13723185658454895, 0.20796458423137665, 0.036908108741045, 0.24303820729255676, 0.32264965772628784, 0.13645994663238525, 0.3899182379245758, 0.0217878520488739, 0.44092538952827454, 0.07019198685884476, 0.15832072496414185, 0.3021415174007416, 0.052834928035736084, 0.2574065327644348, 0.4160279631614685, 0.2508334815502167, 0.2018354833126068, 0.051106713712215424], dtype='float32').reshape([24]),
            paddle.to_tensor([0.2790616750717163, 0.37009525299072266, 0.09592127799987793, 0.059630803763866425, 0.4315752685070038, 0.27407944202423096, 0.24173958599567413, 0.4528663754463196, 0.4854796230792999, 0.361354798078537, 0.2177329808473587, 0.08839305490255356, 0.13297992944717407, 0.3763863742351532, 0.16118460893630981, 0.030721737071871758, 0.3851029574871063, 0.3459407687187195, 0.31747961044311523, 0.16480869054794312, 0.3795739710330963, 0.3923220634460449, 0.10166608542203903, 0.06252492964267731], dtype='float32').reshape([24]),
            paddle.to_tensor([0.18894757330417633, 0.07060802727937698, 0.2879359722137451, 0.4173484742641449, 0.2884323298931122, 0.2274346798658371, 0.1725401133298874, 0.022047294303774834, 0.4781331419944763, 0.2159591019153595, 0.290278822183609, 0.35984325408935547, 0.3111349940299988, 0.4991357624530792, 0.17173072695732117, 0.06660547852516174, 0.03852279111742973, 0.31142956018447876, 0.30312585830688477, 0.4361552894115448, 0.05373896658420563, 0.017690666019916534, 0.3780713379383087, 0.3649957776069641], dtype='float32').reshape([24]),
            paddle.to_tensor([0.06357967108488083, 0.02138957753777504, 0.014226985163986683, 0.23544807732105255, 0.431492418050766, 0.21344907581806183, 0.05564015358686447, 0.1194567009806633, 0.2183074951171875, 0.21306869387626648, 0.47261548042297363, 0.3732546269893646, 0.08713620901107788, 0.34340494871139526, 0.49457481503486633, 0.21474623680114746, 0.2968224287033081, 0.33035680651664734, 0.24601033329963684, 0.12570959329605103, 0.031093928962945938, 0.31993550062179565, 0.11741511523723602, 0.29336243867874146], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2a274714bbe1f0c866ce7d05a20668d4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 31, 31], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_50e089ec7e36a9a08abcad02cfd2ddad(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.4161897301673889, 0.4581455886363983, 0.49985823035240173, 0.45580440759658813, 0.08983302116394043, 0.2257898896932602, 0.47258782386779785, 0.3163037896156311, 0.43560555577278137, 0.1128963753581047, 0.07099355012178421, 0.133536696434021, 0.12016390264034271, 0.4518204927444458, 0.07082964479923248, 0.33980289101600647, 0.41863247752189636, 0.2126091718673706, 0.3769449293613434, 0.063261479139328, 0.08082061260938644, 0.44998326897621155, 0.06847380101680756, 0.33580270409584045, 0.056762438267469406, 0.4455101788043976, 0.030637316405773163, 0.12378593534231186, 0.24776774644851685, 0.3737153112888336], dtype='float32').reshape([30]),
            paddle.to_tensor([0.187461718916893, 0.49539878964424133, 0.4170413017272949, 0.4482189416885376, 0.3200903832912445, 0.3125174343585968, 0.15812869369983673, 0.29179495573043823, 0.28177961707115173, 0.04021526873111725, 0.0065844119526445866, 0.2680743634700775, 0.37897101044654846, 0.31914910674095154, 0.20097316801548004, 0.12325012683868408, 0.295788049697876, 0.053377166390419006, 0.16616228222846985, 0.43690231442451477, 0.3489595949649811, 0.17378979921340942, 0.09262686967849731, 0.36362236738204956, 0.4582725763320923, 0.06252854317426682, 0.3981611132621765, 0.2315385788679123, 0.4548799395561218, 0.09131026268005371], dtype='float32').reshape([30]),
            paddle.to_tensor([0.30015829205513, 0.0914941355586052, 0.47504013776779175, 0.1993573158979416, 0.061633411794900894, 0.48408186435699463, 0.4900102913379669, 0.03156933933496475, 0.37213125824928284, 0.137290358543396, 0.03807634115219116, 0.3014657497406006, 0.3454728424549103, 0.3253841698169708, 0.4901028275489807, 0.3353317379951477, 0.39134690165519714, 0.14847882091999054, 0.22297018766403198, 0.4075224697589874, 0.1987200230360031, 0.22562965750694275, 0.11423839628696442, 0.4519384503364563, 0.3906901478767395, 0.2686620354652405, 0.23429065942764282, 0.1540524959564209, 0.00877504050731659, 0.47907552123069763], dtype='float32').reshape([30]),
            paddle.to_tensor([0.3526945412158966, 0.13169702887535095, 0.3362560570240021, 0.08158215880393982, 0.38536107540130615, 0.06967871636152267, 0.44681116938591003, 0.29363617300987244, 0.265194296836853, 0.25262507796287537, 0.21717506647109985, 0.0705542117357254, 0.21310673654079437, 0.3377002775669098, 0.22741971909999847, 0.14819593727588654, 0.42263853549957275, 0.17053140699863434, 0.4035547077655792, 0.27792420983314514, 0.22025810182094574, 0.3744974136352539, 0.21551170945167542, 0.33607739210128784, 0.34172698855400085, 0.40226665139198303, 0.39394694566726685, 0.008462672121822834, 0.062991201877594, 0.18340325355529785], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_24e18b7ca275c84a958f348eb920fd8f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.4182151257991791, 0.36908087134361267, 0.01616903394460678, 0.33762821555137634, 0.09732374548912048, 0.10251322388648987, 0.2608344256877899, 0.3746277093887329, 0.32791945338249207, 0.36176490783691406, 0.47462958097457886, 0.1675952672958374, 0.42941516637802124, 0.3940978944301605, 0.3749150037765503, 0.4202093482017517], dtype='float32').reshape([16]),
            paddle.to_tensor([0.3024759888648987, 0.0531383641064167, 0.14253102242946625, 0.41195932030677795, 0.2143782675266266, 0.4820512533187866, 0.06116582825779915, 0.08360236138105392, 0.4615672826766968, 0.4194404184818268, 0.2535478174686432, 0.12321323156356812, 0.1022445484995842, 0.16725505888462067, 0.20683793723583221, 0.1237628161907196], dtype='float32').reshape([16]),
            paddle.to_tensor([0.18311840295791626, 0.45104631781578064, 0.3332286775112152, 0.23513266444206238, 0.13379767537117004, 0.45039159059524536, 0.32996904850006104, 0.494804322719574, 0.08283743262290955, 0.2617364823818207, 0.10638684779405594, 0.020009446889162064, 0.15907534956932068, 0.10949181765317917, 0.26518020033836365, 0.11097093671560287], dtype='float32').reshape([16]),
            paddle.to_tensor([0.15608592331409454, 0.32466745376586914, 0.42772021889686584, 0.19251713156700134, 0.23263700306415558, 0.10808173567056656, 0.3008233606815338, 0.04200559854507446, 0.03568432107567787, 0.11430776119232178, 0.006900020409375429, 0.39164164662361145, 0.4842908978462219, 0.1648252010345459, 0.48978158831596375, 0.34285110235214233], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_322ae7522cdcbab0dcd690fa4f6763d8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 2, 112, 199], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.27627888321876526, 0.31018728017807007], dtype='float32').reshape([2]),
            paddle.to_tensor([0.26228249073028564, 0.20877118408679962], dtype='float32').reshape([2]),
            paddle.to_tensor([0.001778187695890665, 0.09051848948001862], dtype='float32').reshape([2]),
            paddle.to_tensor([0.3249369263648987, 0.11385150998830795], dtype='float32').reshape([2]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5d4358ed2144d225cd8ffe58ee0be497(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 1280, 1, 1], dtype='float32', min=0, max=0.5),
            paddle.uniform([1280], dtype='float32', min=0, max=0.5),
            paddle.uniform([1280], dtype='float32', min=0, max=0.5),
            paddle.uniform([1280], dtype='float32', min=0, max=0.5),
            paddle.uniform([1280], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_85b798c303135d19ff37608537e732da(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 144, 31, 31], dtype='float16', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a55e3b5f07afb933a82493b7f65d9409(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_3a47a120d62a3f3a422a747f511df8d9
    def get_inputs(self):
        return [
            paddle.uniform([1, 1152, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a3ccb678001541d2502b2c9400458688(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 32, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0a001b2820be98ff44a03464682375ed(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2e571bf784119ecab8a740a3e5d09c95(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 512, 512], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d5248ea0432185b0509d3a6cd0a4da00(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 36, 28, 50], dtype='float16', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_13fd79f59d5a3ccdad9d00348f898296(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 112, 112], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.2993742823600769, 0.006020344793796539, 0.3439114987850189, 0.20772963762283325, 0.3892611861228943, 0.1660027652978897, 0.12075941264629364, 0.21968446671962738], dtype='float32').reshape([8]),
            paddle.to_tensor([0.16834506392478943, 0.2898019850254059, 0.22407113015651703, 0.345230370759964, 0.24775779247283936, 0.46874138712882996, 0.2009953111410141, 0.26191937923431396], dtype='float32').reshape([8]),
            paddle.to_tensor([0.19860754907131195, 0.07524009048938751, 0.2973400354385376, 0.12497266381978989, 0.23399744927883148, 0.4618533253669739, 0.29392173886299133, 0.32604503631591797], dtype='float32').reshape([8]),
            paddle.to_tensor([0.13735663890838623, 0.2532894015312195, 0.2924516797065735, 0.32805386185646057, 0.23860891163349152, 0.4896753132343292, 0.10591721534729004, 0.07611251622438431], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7c06599f6493bcbbe7e79a56051bc716(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 32, 128], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_516912a9a3c4d9429ae562b758632c72(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([8, 128, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c6817780e887321a945ce91793f698a7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 1504, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([1504], dtype='float32', min=0, max=0.5),
            paddle.uniform([1504], dtype='float32', min=0, max=0.5),
            paddle.uniform([1504], dtype='float32', min=0, max=0.5),
            paddle.uniform([1504], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_871c8f8b395a78d62545aaa0b818e530(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_3a47a120d62a3f3a422a747f511df8d9
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.45895591378211975, 0.4809182286262512, 0.19900022447109222, 0.14572057127952576, 0.1266602873802185, 0.14127714931964874, 0.07283439487218857, 0.029135674238204956, 0.014444810338318348, 0.2934321463108063, 0.032538630068302155, 0.016688838601112366, 0.09310463070869446, 0.4246157705783844, 0.052618127316236496, 0.4366127848625183, 0.2551218271255493, 0.23156821727752686, 0.3898021876811981, 0.025043966248631477, 0.346122145652771, 0.25668588280677795, 0.1988580822944641, 0.22366680204868317], dtype='float32').reshape([24]),
            paddle.to_tensor([0.23990538716316223, 0.3453725576400757, 0.33136019110679626, 0.29176729917526245, 0.471189945936203, 0.3672442138195038, 0.29195642471313477, 0.1042650118470192, 0.4913553297519684, 0.018963083624839783, 0.0968228355050087, 0.003603067249059677, 0.10940787941217422, 0.44482848048210144, 0.399116575717926, 0.15419328212738037, 0.4366633892059326, 0.37127190828323364, 0.1702059954404831, 0.1501254141330719, 0.4953753352165222, 0.4214014708995819, 0.18868154287338257, 0.07434126734733582], dtype='float32').reshape([24]),
            paddle.to_tensor([0.0661148801445961, 0.043444592505693436, 0.08336388319730759, 0.019493799656629562, 0.23019583523273468, 0.39346200227737427, 0.1427532285451889, 0.016499489545822144, 0.054753441363573074, 0.14908359944820404, 0.02322731539607048, 0.05557297542691231, 0.17113414406776428, 0.04172268509864807, 0.1411195993423462, 0.3521522879600525, 0.3101535439491272, 0.4220423400402069, 0.25791135430336, 0.15473131835460663, 0.47069960832595825, 0.3279353082180023, 0.25989288091659546, 0.10344236344099045], dtype='float32').reshape([24]),
            paddle.to_tensor([0.1335436850786209, 0.3893641531467438, 0.39312440156936646, 0.0035286133643239737, 0.2965396046638489, 0.26276469230651855, 0.01299726590514183, 0.09275940805673599, 0.41923588514328003, 0.06918800622224808, 0.430117666721344, 0.35826587677001953, 0.14524322748184204, 0.15976302325725555, 0.2746351659297943, 0.2949661910533905, 0.15894360840320587, 0.44280675053596497, 0.2989408075809479, 0.3466419577598572, 0.4830595850944519, 0.30320343375205994, 0.47053369879722595, 0.3433488607406616], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b86fb3bdbbf5fa75cfcdee76ebe19d2d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 48, 48], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.39361053705215454, 0.134023517370224, 0.2622164189815521, 0.3846375346183777, 0.2844935655593872, 0.3688172399997711, 0.0503397136926651, 0.21728748083114624, 0.3787168562412262, 0.11099380999803543, 0.2782638967037201, 0.28633010387420654, 0.06060674041509628, 0.2805957794189453, 0.31298476457595825, 0.47732776403427124, 0.4409725069999695, 0.4973612129688263, 0.12022106349468231, 0.17156502604484558, 0.03703835979104042, 0.43224334716796875, 0.15406876802444458, 0.36500319838523865], dtype='float32').reshape([24]),
            paddle.to_tensor([0.1355714499950409, 0.4822692275047302, 0.26027512550354004, 0.03763040155172348, 0.3746222257614136, 0.055448658764362335, 0.4116643965244293, 0.41828104853630066, 0.11506907641887665, 0.4825190305709839, 0.31221020221710205, 0.23949681222438812, 0.34436121582984924, 0.3910922408103943, 0.052689798176288605, 0.4392838478088379, 0.41515490412712097, 0.14111274480819702, 0.06385385245084763, 0.23394420742988586, 0.27227601408958435, 0.41599225997924805, 0.35248875617980957, 0.3237469494342804], dtype='float32').reshape([24]),
            paddle.to_tensor([0.41199958324432373, 0.22139203548431396, 0.45860496163368225, 0.1271437406539917, 0.26397258043289185, 0.4309247136116028, 0.4755273163318634, 0.3720921576023102, 0.41039571166038513, 0.49845805764198303, 0.33189669251441956, 0.39895129203796387, 0.18219630420207977, 0.10689672082662582, 0.26486340165138245, 0.17063647508621216, 0.1230873167514801, 0.14056317508220673, 0.021308735013008118, 0.1274334341287613, 0.07290223240852356, 0.2915308475494385, 0.2664106786251068, 0.26217684149742126], dtype='float32').reshape([24]),
            paddle.to_tensor([0.3621622622013092, 0.17887601256370544, 0.24548378586769104, 0.4803389310836792, 0.45368385314941406, 0.47035712003707886, 0.40434959530830383, 0.41505947709083557, 0.039787426590919495, 0.018178053200244904, 0.488770455121994, 0.06710328161716461, 0.1593426764011383, 0.00020208250498399138, 0.18391141295433044, 0.20229004323482513, 0.10117495059967041, 0.1030249148607254, 0.06435691565275192, 0.2277289628982544, 0.4701109230518341, 0.14108119904994965, 0.21095098555088043, 0.44330453872680664], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9169a6687477bfda78bb7973d02820e1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 6, 40], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c45ff138eb9b9ae2bc6e7c9ac3976d0c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 720, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([720], dtype='float32', min=0, max=0.5),
            paddle.uniform([720], dtype='float32', min=0, max=0.5),
            paddle.uniform([720], dtype='float32', min=0, max=0.5),
            paddle.uniform([720], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1391b067f1bcc6085c8cae6fb9938993(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 120, 120], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_dbc805aaf9a3720fa287c2e5d6468b97(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 32, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8e29e5274dd02c9c2b872fef41b776a8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 40, 40], dtype='float16', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bff8055309ffd2a139290228ee38ec13(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.05476243048906326, 0.031132135540246964, 0.21733883023262024, 0.4824252128601074, 0.37196263670921326, 0.16675370931625366, 0.4821854829788208, 0.3294406235218048, 0.13649998605251312, 0.07118811458349228, 0.15812963247299194, 0.18909429013729095, 0.43360665440559387, 0.3662275969982147, 0.2571758031845093, 0.4190937578678131], dtype='float32').reshape([16]),
            paddle.to_tensor([0.020145609974861145, 0.18574810028076172, 0.32532772421836853, 0.39220568537712097, 0.23778143525123596, 0.13193708658218384, 0.32683122158050537, 0.1383955031633377, 0.2659066617488861, 0.09501278400421143, 0.14214040338993073, 0.2202286720275879, 0.05650433897972107, 0.411277711391449, 0.39112389087677, 0.012080411426723003], dtype='float32').reshape([16]),
            paddle.to_tensor([0.3275521397590637, 0.4696199595928192, 0.27926963567733765, 0.11578366905450821, 0.28060394525527954, 0.321449875831604, 0.320290744304657, 0.48989638686180115, 0.22013691067695618, 0.27035704255104065, 0.1848922222852707, 0.28758832812309265, 0.42254915833473206, 0.3878282904624939, 0.10472358763217926, 0.4082224369049072], dtype='float32').reshape([16]),
            paddle.to_tensor([0.4914848804473877, 0.05440812557935715, 0.11634869873523712, 0.3749680817127228, 0.03831939026713371, 0.0030441889539361, 0.14702117443084717, 0.2545292377471924, 0.08659592270851135, 0.08994518220424652, 0.010712970048189163, 0.09617029875516891, 0.4144960343837738, 0.26673054695129395, 0.2704484760761261, 0.11999944597482681], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a142c909e153d8fb714a0d19c8065431(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 80, 80], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f92742e473482f0ba92e3ff5a5d57a50(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.3490796983242035, 0.20369818806648254, 0.26708680391311646, 0.32195648550987244, 0.2707160413265228, 0.06922909617424011, 0.41228851675987244, 0.4266969561576843, 0.14343340694904327, 0.4278888702392578, 0.11546751856803894, 0.2512328624725342, 0.17665599286556244, 0.08571804314851761, 0.044187031686306, 0.030558260157704353, 0.025022685527801514, 0.35176563262939453, 0.1607881337404251, 0.4686286449432373, 0.4324701130390167, 0.43941718339920044, 0.05725434049963951, 0.42476195096969604], dtype='float32').reshape([24]),
            paddle.to_tensor([0.4981057941913605, 0.38907840847969055, 0.2108382135629654, 0.4473453164100647, 0.08368197083473206, 0.3096000552177429, 0.48115500807762146, 0.1344289779663086, 0.44515255093574524, 0.27246588468551636, 0.229776993393898, 0.35761386156082153, 0.2246069312095642, 0.34810128808021545, 0.24920441210269928, 0.046930886805057526, 0.11772239208221436, 0.32018348574638367, 0.03518088907003403, 0.25417983531951904, 0.3813740611076355, 0.3371792435646057, 0.16939248144626617, 0.10600794851779938], dtype='float32').reshape([24]),
            paddle.to_tensor([0.450481653213501, 0.43282586336135864, 0.4234842360019684, 0.3931634724140167, 0.009458361193537712, 0.11415516585111618, 0.4665096700191498, 0.1268969625234604, 0.41778337955474854, 0.03411022201180458, 0.007804439403116703, 0.27033689618110657, 0.2762267291545868, 0.05918183922767639, 0.33457306027412415, 0.15192803740501404, 0.12356880307197571, 0.02655230648815632, 0.04939551278948784, 0.25452449917793274, 0.36713075637817383, 0.19669866561889648, 0.35063692927360535, 0.18268130719661713], dtype='float32').reshape([24]),
            paddle.to_tensor([0.28455647826194763, 0.31423071026802063, 0.35617372393608093, 0.17515365779399872, 0.2831558585166931, 0.3636843264102936, 0.07355567812919617, 0.2086913138628006, 0.4231206476688385, 0.3956359326839447, 0.22123853862285614, 0.13323655724525452, 0.47136643528938293, 0.012964216060936451, 0.1539522409439087, 0.4543011486530304, 0.48848554491996765, 0.0007356296409852803, 0.443244069814682, 0.43398168683052063, 0.47797220945358276, 0.3151792883872986, 0.05048764869570732, 0.41402021050453186], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_18c85722b7f424e734508dd0a966147e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_975b258f7fdc5512a8c22ad1eebf2c8a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 20, 20], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1aca5b2f7d5193b8c956405eaa029246(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 31, 31], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f1b679c1fd068ef82390ae378e3dbed9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 28, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.06441764533519745, 0.2304152101278305, 0.4157482981681824, 0.2719128727912903, 0.09137620776891708, 0.286630243062973, 0.06489058583974838, 0.08987928181886673, 0.21871186792850494, 0.4587583839893341, 0.0008618526044301689, 0.10747085511684418, 0.34791818261146545, 0.4600071609020233, 0.12366785109043121, 0.09834463149309158, 0.08584631979465485, 0.3522235155105591, 0.09672210365533829, 0.2177271991968155, 0.1538894772529602, 0.3275521695613861, 0.47998473048210144, 0.283644437789917, 0.4683052897453308, 0.1511155664920807, 0.4035280644893646, 0.39137154817581177], dtype='float32').reshape([28]),
            paddle.to_tensor([0.22578740119934082, 0.27686184644699097, 0.07451307773590088, 0.0783328264951706, 0.28793594241142273, 0.15706157684326172, 0.12533079087734222, 0.42949986457824707, 0.16353324055671692, 0.46144285798072815, 0.4508685767650604, 0.1330408900976181, 0.14831678569316864, 0.46395912766456604, 0.377766877412796, 0.3413240909576416, 0.39148980379104614, 0.21563415229320526, 0.06550642102956772, 0.3344312608242035, 0.013927011750638485, 0.04057754576206207, 0.10231064260005951, 0.4291962683200836, 0.09344988316297531, 0.47187015414237976, 0.24685736000537872, 0.4775168001651764], dtype='float32').reshape([28]),
            paddle.to_tensor([0.3085116147994995, 0.156784325838089, 0.1172885149717331, 0.2578338384628296, 0.04014236107468605, 0.0264304056763649, 0.2213437706232071, 0.27572327852249146, 0.3326105773448944, 0.42163825035095215, 0.4899872839450836, 0.1079445630311966, 0.07959122210741043, 0.25059837102890015, 0.009160046465694904, 0.12090487778186798, 0.45636245608329773, 0.20039992034435272, 0.28353020548820496, 0.38274508714675903, 0.22920285165309906, 0.24366584420204163, 0.13030873239040375, 0.22126713395118713, 0.39496058225631714, 0.3563506305217743, 0.49378731846809387, 0.2660640478134155], dtype='float32').reshape([28]),
            paddle.to_tensor([0.21587669849395752, 0.250935822725296, 0.12359276413917542, 0.4097929000854492, 0.28839555382728577, 0.41205599904060364, 0.4266126751899719, 0.419580340385437, 0.14379118382930756, 0.4305410087108612, 0.11618582904338837, 0.38223862648010254, 0.062983937561512, 0.13337525725364685, 0.025400010868906975, 0.2894292175769806, 0.19923977553844452, 0.1677175909280777, 0.49120455980300903, 0.18235597014427185, 0.017794352024793625, 0.3606554865837097, 0.21354392170906067, 0.027506893500685692, 0.0950692743062973, 0.03385895490646362, 0.4062891900539398, 0.00033258291659876704], dtype='float32').reshape([28]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a7e0d149c519d3a422d50aa390ec5832(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 112, 112], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3c560d72834438ed3c0feda28291a02d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_da4d85bc213f70819a72e68f21214cdb
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 4, 64, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e59e5e3b4e3af3a1769b356c33d1f663(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_77c96a32af44d736ea42aaa65d0ef19d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_876d3d31e6507da55f65d8f3e0226555(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 80, 16, 12], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1557ec0894a2f8cfb08dfd5c77714928(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 4, 26], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7e746e3013ad885c814ef8db594b1a65(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 80, 16, 12], dtype='float16', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_942b8df679ebb3cfd16faf7acebaa474(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_0edd578f48a07aa5d1d79f9f678b472e
    def get_inputs(self):
        return [
            paddle.uniform([16, 256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9507d2c9bd78a8c106f98786da18b846(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 32, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_99fe90696970908a799e2b36ff07c60f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c190a69dc63f6b6c849ebc8759236ce5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 2064, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([2064], dtype='float32', min=0, max=0.5),
            paddle.uniform([2064], dtype='float32', min=0, max=0.5),
            paddle.uniform([2064], dtype='float32', min=0, max=0.5),
            paddle.uniform([2064], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_174ba8a686b31a139884b683a29846ee(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 12, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.07908699661493301, 0.01234672125428915, 0.426288902759552, 0.2885499894618988, 0.03375423327088356, 0.48904410004615784, 0.37968504428863525, 0.3936954438686371, 0.31037524342536926, 0.29972079396247864, 0.0630788505077362, 0.16419313848018646], dtype='float32').reshape([12]),
            paddle.to_tensor([0.013319904915988445, 0.015714531764388084, 0.16176889836788177, 0.028957905247807503, 0.1895226389169693, 0.1957654356956482, 0.4474030137062073, 0.1749127209186554, 0.2899458110332489, 0.37201541662216187, 0.21256449818611145, 0.14186403155326843], dtype='float32').reshape([12]),
            paddle.to_tensor([0.031181827187538147, 0.2626030743122101, 0.40386638045310974, 0.3911549746990204, 0.4931536614894867, 0.19721008837223053, 0.45522767305374146, 0.4623068571090698, 0.35493016242980957, 0.39994800090789795, 0.1203455924987793, 0.27546414732933044], dtype='float32').reshape([12]),
            paddle.to_tensor([0.3821395933628082, 0.00518621364608407, 0.22438864409923553, 0.19868318736553192, 0.15697520971298218, 0.3597995936870575, 0.04232305660843849, 0.19674929976463318, 0.4581795334815979, 0.26992088556289673, 0.3231078088283539, 0.28229445219039917], dtype='float32').reshape([12]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fe21f721291651268fdc797fca08e5b2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.21882425248622894, 0.1389249563217163, 0.11055532097816467, 0.19748970866203308, 0.31171929836273193, 0.10536255687475204, 0.3804101347923279, 0.21683256328105927, 0.463186115026474, 0.10410898923873901, 0.18352706730365753, 0.16158200800418854, 0.027541836723685265, 0.420299232006073, 0.07018871605396271, 0.12526987493038177, 0.31647050380706787, 0.19583748281002045, 0.2606566250324249, 0.21271361410617828, 0.49550390243530273, 0.08833598345518112, 0.4820340871810913, 0.060625869780778885, 0.35511514544487, 0.38792720437049866, 0.38192829489707947, 0.23532681167125702, 0.4807276725769043, 0.07593624293804169], dtype='float32').reshape([30]),
            paddle.to_tensor([0.2629927098751068, 0.37363824248313904, 0.07460198551416397, 0.16298089921474457, 0.38314399123191833, 0.42593684792518616, 0.14389733970165253, 0.3547164499759674, 0.16930215060710907, 0.4986172914505005, 0.2973324954509735, 0.2229013741016388, 0.40708011388778687, 0.30319443345069885, 0.4321850538253784, 0.443448930978775, 0.126448392868042, 0.226407989859581, 0.347215473651886, 0.11899113655090332, 0.28345465660095215, 0.40986865758895874, 0.377270370721817, 0.4167531430721283, 0.39582422375679016, 0.4935895800590515, 0.031746022403240204, 0.2519785761833191, 0.24844227731227875, 0.018941164016723633], dtype='float32').reshape([30]),
            paddle.to_tensor([0.3377603590488434, 0.11708610504865646, 0.4850616455078125, 0.12164530158042908, 0.46841099858283997, 0.3550862967967987, 0.15745685994625092, 0.4668615162372589, 0.48220813274383545, 0.01779552735388279, 0.4689796268939972, 0.13126078248023987, 0.23080715537071228, 0.1390509009361267, 0.015172149986028671, 0.4095502495765686, 0.10779666900634766, 0.3065028786659241, 0.19750331342220306, 0.3409886658191681, 0.40624046325683594, 0.43145379424095154, 0.24353866279125214, 0.10416627675294876, 0.39030084013938904, 0.04762044921517372, 0.1899394989013672, 0.4940953254699707, 0.3347261846065521, 0.39431220293045044], dtype='float32').reshape([30]),
            paddle.to_tensor([0.04041926562786102, 0.18863622844219208, 0.0235665962100029, 0.4337448179721832, 0.4520430266857147, 0.3664209842681885, 0.17490866780281067, 0.40346458554267883, 0.288501501083374, 0.123879075050354, 0.42042407393455505, 0.3235291838645935, 0.030786912888288498, 0.14593394100666046, 0.24341152608394623, 0.32862135767936707, 0.42937958240509033, 0.015980590134859085, 0.09851162880659103, 0.2632277309894562, 0.3417971730232239, 0.14960366487503052, 0.36421236395835876, 0.14549992978572845, 0.2726368010044098, 0.25340989232063293, 0.17022544145584106, 0.44374406337738037, 0.3245584964752197, 0.46969324350357056], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9bcf356110d246c8d7fb3a578628f16f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 112, 199], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b114937a1009f2830939d865db725b1d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 384, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5b369a643ee5402490d0e12a3e9e33ec(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 27, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.44166287779808044, 0.1330879032611847, 0.18037988245487213, 0.09050434082746506, 0.45848673582077026, 0.489990234375, 0.4750936031341553, 0.3495168387889862, 0.00936137605458498, 0.2923896312713623, 0.21646088361740112, 0.3352120518684387, 0.34278225898742676, 0.4321507513523102, 0.4073769748210907, 0.37848103046417236, 0.48093220591545105, 0.11118166148662567, 0.04530083015561104, 0.07231748104095459, 0.04652688279747963, 0.27386119961738586, 0.2684244215488434, 0.44479238986968994, 0.2920147180557251, 0.23917116224765778, 0.06078669801354408], dtype='float32').reshape([27]),
            paddle.to_tensor([0.3176652193069458, 0.3666018843650818, 0.26225149631500244, 0.4674242436885834, 0.199700266122818, 0.37939849495887756, 0.026977676898241043, 0.33891117572784424, 0.09939531981945038, 0.07575340569019318, 0.4737181067466736, 0.36729514598846436, 0.2180616408586502, 0.4011196494102478, 0.14695467054843903, 0.4040534496307373, 0.4443719685077667, 0.41377314925193787, 0.26044246554374695, 0.2438589334487915, 0.20871663093566895, 0.4328630864620209, 0.3294018805027008, 0.27408719062805176, 0.11585770547389984, 0.17034392058849335, 0.01440022885799408], dtype='float32').reshape([27]),
            paddle.to_tensor([0.11121731996536255, 0.08292090892791748, 0.10803841054439545, 0.06716562807559967, 0.05563822761178017, 0.4472294747829437, 0.21336692571640015, 0.06324691325426102, 0.3842516243457794, 0.13870851695537567, 0.38576021790504456, 0.3676798641681671, 0.2859888970851898, 0.1987449675798416, 0.4585554003715515, 0.3605009913444519, 0.33273985981941223, 0.03473570942878723, 0.4140288233757019, 0.20646889507770538, 0.46667802333831787, 0.44022631645202637, 0.4720797538757324, 0.12635155022144318, 0.16144613921642303, 0.038617029786109924, 0.2297850102186203], dtype='float32').reshape([27]),
            paddle.to_tensor([0.24465475976467133, 0.4337771236896515, 0.36338576674461365, 0.3652947247028351, 0.4799644649028778, 0.44801342487335205, 0.45450788736343384, 0.4300173819065094, 0.3528826832771301, 0.09832039475440979, 0.1704844981431961, 0.13690446317195892, 0.14191573858261108, 0.06353600323200226, 0.372922420501709, 0.43405401706695557, 0.0672444999217987, 0.01839822717010975, 0.09240802377462387, 0.14125238358974457, 0.06252219527959824, 0.32839086651802063, 0.4390040934085846, 0.17754434049129486, 0.155321404337883, 0.3389991223812103, 0.1985456645488739], dtype='float32').reshape([27]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a70e31195ec33b775153f12e5b850cf7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 256, 512], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.012770693749189377, 0.09224754571914673, 0.3678065836429596, 0.4061473309993744, 0.14780879020690918, 0.34046292304992676, 0.4786231219768524, 0.13666632771492004, 0.1794370412826538, 0.286527156829834, 0.2063024342060089, 0.10985323786735535, 0.17989949882030487, 0.18859632313251495, 0.4605320692062378, 0.21722233295440674], dtype='float32').reshape([16]),
            paddle.to_tensor([0.18354877829551697, 0.36450567841529846, 0.03963572531938553, 0.0026643252931535244, 0.1688520312309265, 0.2120785415172577, 0.37377452850341797, 0.0012719288934022188, 0.04746628180146217, 0.26544323563575745, 0.035620465874671936, 0.4924551546573639, 0.17943495512008667, 0.044965051114559174, 0.47010326385498047, 0.20682501792907715], dtype='float32').reshape([16]),
            paddle.to_tensor([0.06382617354393005, 0.3970174789428711, 0.2032020092010498, 0.17404282093048096, 0.4267050325870514, 0.17876331508159637, 0.037738509476184845, 0.4599243402481079, 0.3959178030490875, 0.40681591629981995, 0.14215081930160522, 0.1527138650417328, 0.09625540673732758, 0.14778593182563782, 0.23384763300418854, 0.3487522006034851], dtype='float32').reshape([16]),
            paddle.to_tensor([0.1404770463705063, 0.433492511510849, 0.10368143767118454, 0.10243093967437744, 0.3447910249233246, 0.3538876175880432, 0.23101527988910675, 0.2529882490634918, 0.26348811388015747, 0.21064163744449615, 0.014008983038365841, 0.32239311933517456, 0.48614177107810974, 0.17458920180797577, 0.25977823138237, 0.46023544669151306], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_95582b097ca80839366adf3c675e47e2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 40, 40], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.012887639924883842, 0.4489157199859619, 0.4636407196521759, 0.4838355481624603, 0.2075710892677307, 0.23732531070709229, 0.4991108775138855, 0.3790690004825592, 0.23169957101345062, 0.4052397310733795, 0.26903703808784485, 0.2425842136144638, 0.12527672946453094, 0.3338884711265564, 0.4717269539833069, 0.40285438299179077, 0.4903101623058319, 0.38511329889297485, 0.020858172327280045, 0.42116400599479675, 0.4074484407901764, 0.22573095560073853, 0.48660197854042053, 0.1916864812374115], dtype='float32').reshape([24]),
            paddle.to_tensor([0.4125228226184845, 0.016909513622522354, 0.3842860460281372, 0.3822731673717499, 0.09070749580860138, 0.42317408323287964, 0.01697777956724167, 0.054754361510276794, 0.28475186228752136, 0.4438849985599518, 0.2679439187049866, 0.17220355570316315, 0.20603306591510773, 0.46622881293296814, 0.12177223712205887, 0.28028082847595215, 0.2306985706090927, 0.008041562512516975, 0.2746507525444031, 0.017329081892967224, 0.13859112560749054, 0.1598585993051529, 0.35863929986953735, 0.32986027002334595], dtype='float32').reshape([24]),
            paddle.to_tensor([0.30507513880729675, 0.018139267340302467, 0.10108724981546402, 0.3298293948173523, 0.21893949806690216, 0.0026439614593982697, 0.0820407122373581, 0.3054462969303131, 0.36685213446617126, 0.25195109844207764, 0.4921607971191406, 0.03882063552737236, 0.1568545252084732, 0.47957250475883484, 0.14829112589359283, 0.40838614106178284, 0.22429314255714417, 0.15480953454971313, 0.47560542821884155, 0.48397839069366455, 0.31633251905441284, 0.187654048204422, 0.15826520323753357, 0.1727776974439621], dtype='float32').reshape([24]),
            paddle.to_tensor([0.39834854006767273, 0.23301126062870026, 0.3411708474159241, 0.0038473643362522125, 0.1125253438949585, 0.47141849994659424, 0.19384966790676117, 0.1922927349805832, 0.48637717962265015, 0.32931017875671387, 0.31227561831474304, 0.11508820205926895, 0.0846017450094223, 0.26296454668045044, 0.35406437516212463, 0.22141540050506592, 0.1973852962255478, 0.292296826839447, 0.32257285714149475, 0.34279701113700867, 0.40202534198760986, 0.016572127118706703, 0.4003368020057678, 0.04379677399992943], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9dcfdb1b8f17407555be9c9f68ac9e3d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a8301b810bccba10e4dfdee0df73334
    def get_inputs(self):
        return [
            paddle.uniform([1, 144, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_153987f87f39829ab2ab71aa1fa90fce(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.09546855092048645, 0.03944920748472214, 0.05156305059790611, 0.230562224984169, 0.2634161114692688, 0.48774346709251404, 0.005816769320517778, 0.3076387643814087, 0.34806036949157715, 0.21400196850299835, 0.08115522563457489, 0.3759099841117859, 0.09638291597366333, 0.28216204047203064, 0.2494611293077469, 0.38682425022125244, 0.3409022092819214, 0.303475558757782, 0.14889749884605408, 0.35565489530563354, 0.4401341378688812, 0.01809273473918438, 0.24365317821502686, 0.3076900839805603], dtype='float32').reshape([24]),
            paddle.to_tensor([0.12183129787445068, 0.12617652118206024, 0.2487345039844513, 0.18369647860527039, 0.3215607702732086, 0.2867809534072876, 0.3353050947189331, 0.20389904081821442, 0.1239004135131836, 0.030063625425100327, 0.22321271896362305, 0.19201724231243134, 0.1803426891565323, 0.35234132409095764, 0.04004502296447754, 0.12936995923519135, 0.15788765251636505, 0.3374701738357544, 0.4754006266593933, 0.13815434277057648, 0.3792963922023773, 0.3195859491825104, 0.0519886277616024, 0.017012575641274452], dtype='float32').reshape([24]),
            paddle.to_tensor([0.18367156386375427, 0.19222846627235413, 0.22569087147712708, 0.21283996105194092, 0.16062875092029572, 0.4712291359901428, 0.015231386758387089, 0.42574697732925415, 0.18300561606884003, 0.4405146837234497, 0.19476421177387238, 0.1994704306125641, 0.10823730379343033, 0.37129876017570496, 0.0072487471625208855, 0.11415109038352966, 0.40484189987182617, 0.33951976895332336, 0.4205578565597534, 0.11292073875665665, 0.14128340780735016, 0.012215822003781796, 0.300429105758667, 0.22920803725719452], dtype='float32').reshape([24]),
            paddle.to_tensor([0.3426760733127594, 0.4249121844768524, 0.22363102436065674, 0.36151209473609924, 0.3143764138221741, 0.0438092015683651, 0.0069496966898441315, 0.35751762986183167, 0.3594997823238373, 0.12435184419155121, 0.4021099805831909, 0.12587524950504303, 0.453100323677063, 0.29476019740104675, 0.405007928609848, 0.21888794004917145, 0.4695861339569092, 0.2276100218296051, 0.2278870791196823, 0.25688934326171875, 0.09115229547023773, 0.34184834361076355, 0.15261059999465942, 0.01245134137570858], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8031e37c59c50d1b3be9ff9a8d19a38d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.35773351788520813, 0.4572857916355133, 0.4503660500049591, 0.19712962210178375, 0.28879091143608093, 0.024229779839515686, 0.33698129653930664, 0.38784259557724, 0.28231263160705566, 0.21481984853744507, 0.25383612513542175, 0.21564273536205292, 0.39244556427001953, 0.3562232553958893, 0.35054197907447815, 0.32513749599456787, 0.03210439905524254, 0.07689138501882553], dtype='float32').reshape([18]),
            paddle.to_tensor([0.32512056827545166, 0.3831402063369751, 0.07478807866573334, 0.1583891659975052, 0.11362077295780182, 0.1819906085729599, 0.17914101481437683, 0.11469250172376633, 0.16257072985172272, 0.09613838791847229, 0.3149000108242035, 0.06151200458407402, 0.29445725679397583, 0.25560182332992554, 0.18758684396743774, 0.2993334233760834, 0.1611635535955429, 0.12405566871166229], dtype='float32').reshape([18]),
            paddle.to_tensor([0.16070713102817535, 0.4873862862586975, 0.4447582960128784, 0.11541181057691574, 0.37703099846839905, 0.1679052859544754, 0.08068699389696121, 0.273801326751709, 0.025649618357419968, 0.40505802631378174, 0.25264957547187805, 0.01653287559747696, 0.1805710345506668, 0.3989243805408478, 0.19580301642417908, 0.32637783885002136, 0.49309661984443665, 0.1390964239835739], dtype='float32').reshape([18]),
            paddle.to_tensor([0.3208882212638855, 0.32833385467529297, 0.33408114314079285, 0.12850315868854523, 0.007116648834198713, 0.054144587367773056, 0.1728941649198532, 0.07681450992822647, 0.1653323769569397, 0.24591410160064697, 0.4819999933242798, 0.39368337392807007, 0.023421764373779297, 0.44069963693618774, 0.003568941494449973, 0.056415457278490067, 0.15624172985553741, 0.10929324477910995], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d5d0b565d241d22a5412d5817c0a2a2f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.03679162263870239, 0.3424282371997833, 0.43884703516960144, 0.38671940565109253, 0.4322510063648224, 0.27687883377075195, 0.17855525016784668, 0.11353275179862976, 0.29559126496315, 0.05265628919005394, 0.18604858219623566, 0.3212428689002991, 0.46757641434669495, 0.19359086453914642, 0.2616829574108124, 0.07686886191368103, 0.3164363503456116, 0.28642961382865906, 0.44718316197395325, 0.018572529777884483], dtype='float32').reshape([20]),
            paddle.to_tensor([0.3367310166358948, 0.4502514600753784, 0.20318080484867096, 0.1471771001815796, 0.17331646382808685, 0.43909499049186707, 0.47624126076698303, 0.2839256823062897, 0.10846929252147675, 0.28470155596733093, 0.31388840079307556, 0.4961531162261963, 0.11194483935832977, 0.4899488687515259, 0.08901375532150269, 0.3235733211040497, 0.2978293001651764, 0.3897939622402191, 0.08802304416894913, 0.40811601281166077], dtype='float32').reshape([20]),
            paddle.to_tensor([0.17864391207695007, 0.19283507764339447, 0.27851831912994385, 0.14895017445087433, 0.05033017694950104, 0.06457918137311935, 0.05498339608311653, 0.2250431925058365, 0.014168785884976387, 0.20430408418178558, 0.4256736636161804, 0.47923341393470764, 0.19944562017917633, 0.11368519067764282, 0.2910887897014618, 0.1007356271147728, 0.03374515846371651, 0.2503122389316559, 0.3099338114261627, 0.4036765396595001], dtype='float32').reshape([20]),
            paddle.to_tensor([0.4015093445777893, 0.06742144376039505, 0.2506532371044159, 0.33053797483444214, 0.36523786187171936, 0.4623149633407593, 0.39772024750709534, 0.20366047322750092, 0.12011540681123734, 0.30271801352500916, 0.2692136764526367, 0.3871365487575531, 0.08822490274906158, 0.2992509603500366, 0.42407023906707764, 0.4053301513195038, 0.09463240951299667, 0.3256901502609253, 0.15235310792922974, 0.21915645897388458], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7b0a3e1528c62999dc8fd845923ae005(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 106, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([106], dtype='float32', min=0, max=0.5),
            paddle.uniform([106], dtype='float32', min=0, max=0.5),
            paddle.uniform([106], dtype='float32', min=0, max=0.5),
            paddle.uniform([106], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f8c50089a15d7496e67132317c5d0ca2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.0749916136264801, 0.2586941719055176, 0.3891110122203827, 0.10822196304798126, 0.009191655553877354, 0.2078712433576584, 0.17773069441318512, 0.4771146774291992, 0.08333209156990051, 0.38873156905174255, 0.3941482603549957, 0.4105556607246399, 0.18165934085845947, 0.36899861693382263, 0.18972653150558472, 0.2836834490299225, 0.33690860867500305, 0.4339326024055481, 0.45805370807647705, 0.31487971544265747, 0.02893453650176525, 0.40349262952804565, 0.025468917563557625, 0.20947088301181793, 0.34594154357910156, 0.14886769652366638, 0.17970986664295197, 0.09242844581604004, 0.4351655840873718, 0.22773391008377075], dtype='float32').reshape([30]),
            paddle.to_tensor([0.4371797442436218, 0.24716031551361084, 0.2696065306663513, 0.33096346259117126, 0.28234708309173584, 0.4661136865615845, 0.18691378831863403, 0.339977502822876, 0.09214723110198975, 0.09969515353441238, 0.21312421560287476, 0.28779080510139465, 0.17708712816238403, 0.32082879543304443, 0.19809632003307343, 0.42745521664619446, 0.21550831198692322, 0.24574431777000427, 0.43243318796157837, 0.15353597700595856, 0.3812140226364136, 0.16708539426326752, 0.3361106514930725, 0.1607191115617752, 0.49758410453796387, 0.3790379762649536, 0.08670401573181152, 0.3619580864906311, 0.3770557940006256, 0.3999057412147522], dtype='float32').reshape([30]),
            paddle.to_tensor([0.1558125913143158, 0.3840978741645813, 0.48036620020866394, 0.3699966073036194, 0.25889673829078674, 0.1449430286884308, 0.1432519108057022, 0.276384174823761, 0.45250412821769714, 0.05289848893880844, 0.039228830486536026, 0.0642600804567337, 0.15737156569957733, 0.03618163615465164, 0.49341216683387756, 0.2774372100830078, 0.30554893612861633, 0.014868918806314468, 0.4398733079433441, 0.007943627424538136, 0.12122421711683273, 0.10687579214572906, 0.005551312118768692, 0.3843604326248169, 0.14931121468544006, 0.39461541175842285, 0.06498315930366516, 0.2350742071866989, 0.2303539365530014, 0.4087149500846863], dtype='float32').reshape([30]),
            paddle.to_tensor([0.4889296293258667, 0.0766327902674675, 0.07247235625982285, 0.20748010277748108, 0.36604321002960205, 0.022045327350497246, 0.11885251849889755, 0.25140032172203064, 0.40142232179641724, 0.2507343888282776, 0.024756640195846558, 0.12823796272277832, 0.11372580379247665, 0.23343761265277863, 0.10304047912359238, 0.1226440817117691, 0.05179425701498985, 0.40257930755615234, 0.27059006690979004, 0.009756281040608883, 0.007447278592735529, 0.17163434624671936, 0.21548296511173248, 0.03615129366517067, 0.4727155864238739, 0.03673607483506203, 0.0651986226439476, 0.18476234376430511, 0.15824592113494873, 0.2803155779838562], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4489a16ce90ff3ce51b9f0c0abfa013b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a8301b810bccba10e4dfdee0df73334
    def get_inputs(self):
        return [
            paddle.uniform([1, 80, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_080fea0a5ec6ba173eb051c42d2ecc10(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 512, 256], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.4255516231060028, 0.4231322109699249, 0.2895432412624359, 0.28847819566726685, 0.026783805340528488, 0.1008128672838211, 0.20315846800804138, 0.01578342355787754, 0.43717247247695923, 0.42861607670783997, 0.4605225622653961, 0.16101863980293274, 0.4826284348964691, 0.48837581276893616, 0.049034904688596725, 0.33239302039146423], dtype='float32').reshape([16]),
            paddle.to_tensor([0.08090977370738983, 0.04859373718500137, 0.1308411806821823, 0.16175101697444916, 0.11790173500776291, 0.0025970004498958588, 0.16066871583461761, 0.37959060072898865, 0.4023817777633667, 0.30719056725502014, 0.19005313515663147, 0.37928545475006104, 0.09772489219903946, 0.325264036655426, 0.301150918006897, 0.3339805006980896], dtype='float32').reshape([16]),
            paddle.to_tensor([0.08530108630657196, 0.10496632754802704, 0.3115844428539276, 0.48213791847229004, 0.35310953855514526, 0.0318877175450325, 0.25944095849990845, 0.4639282524585724, 0.004224223084747791, 0.1338464617729187, 0.4881560802459717, 0.37655431032180786, 0.24476341903209686, 0.2895660102367401, 0.22267888486385345, 0.055588554590940475], dtype='float32').reshape([16]),
            paddle.to_tensor([0.2292121797800064, 0.22233110666275024, 0.3950595259666443, 0.31852322816848755, 0.33617573976516724, 0.3920779526233673, 0.17659059166908264, 0.015129986219108105, 0.144129678606987, 0.11998260021209717, 0.1369701325893402, 0.28593358397483826, 0.22145196795463562, 0.4472658038139343, 0.3489300310611725, 0.05906042084097862], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_60ac304885dcb2d44ec7dc1a1edcf864(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 1152, 6, 6], dtype='float16', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f5c5c845ec09e3015135ce2eb06d62c9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.18259374797344208, 0.4085251986980438, 0.1089303269982338, 0.48628145456314087, 0.440195232629776, 0.15696610510349274, 0.21561366319656372, 0.23742027580738068, 0.4570469856262207, 0.027110062539577484, 0.42669856548309326, 0.4696432054042816, 0.10416077822446823, 0.09568114578723907, 0.25829797983169556, 0.4392271935939789, 0.2781391739845276, 0.01362316869199276], dtype='float32').reshape([18]),
            paddle.to_tensor([0.32646143436431885, 0.35525640845298767, 0.4635622501373291, 0.08428657054901123, 0.21452213823795319, 0.17922380566596985, 0.30918166041374207, 0.30215445160865784, 0.30278852581977844, 0.4181583523750305, 0.12864089012145996, 0.013443408533930779, 0.19978176057338715, 0.04508528858423233, 0.4584565758705139, 0.17079512774944305, 0.23329998552799225, 0.24327681958675385], dtype='float32').reshape([18]),
            paddle.to_tensor([0.20536689460277557, 0.21330612897872925, 0.033831387758255005, 0.05628865957260132, 0.49972736835479736, 0.0842968299984932, 0.08776663988828659, 0.40109890699386597, 0.23727603256702423, 0.42673662304878235, 0.2571825683116913, 0.07406707108020782, 0.3522306978702545, 0.11721684038639069, 0.26538124680519104, 0.4048852324485779, 0.3555646240711212, 0.2400229275226593], dtype='float32').reshape([18]),
            paddle.to_tensor([0.33692246675491333, 0.33395135402679443, 0.44608134031295776, 0.23051434755325317, 0.3003527522087097, 0.014248264953494072, 0.22585558891296387, 0.28752920031547546, 0.4048687517642975, 0.4009430408477783, 0.3399961292743683, 0.05126936361193657, 0.18939927220344543, 0.49813973903656006, 0.48151156306266785, 0.11212966591119766, 0.22707317769527435, 0.2280614972114563], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_67e392c0725d6c60c2d63690e582977c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_fc6b6917e04b7b7cf9fdb39dde97ec40
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 40, 40], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ba8895dfd7dccb2b8993d845e19e7297(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.16399845480918884, 0.2404640018939972, 0.24381762742996216, 0.2439381331205368, 0.0621771439909935, 0.0013384507037699223, 0.2801983058452606, 0.07429124414920807, 0.020497236400842667, 0.04809979349374771, 0.05965937674045563, 0.2638045847415924, 0.2790442407131195, 0.44404855370521545, 0.39140111207962036, 0.2257222831249237, 0.33990880846977234, 0.38902631402015686, 0.018275754526257515, 0.2956917881965637, 0.31083112955093384, 0.28667327761650085, 0.024355189874768257, 0.0444234199821949, 0.00270942784845829, 0.2073499858379364, 0.12328065186738968, 0.18542809784412384, 0.47150614857673645, 0.3648444712162018], dtype='float32').reshape([30]),
            paddle.to_tensor([0.20499742031097412, 0.40303710103034973, 0.11167933791875839, 0.3405824601650238, 0.06489981710910797, 0.4735325574874878, 0.34213507175445557, 0.04698256403207779, 0.012843124568462372, 0.2443201243877411, 0.3867464065551758, 0.14646200835704803, 0.30117207765579224, 0.46716341376304626, 0.3041098415851593, 0.11460963636636734, 0.05789152532815933, 0.09096875041723251, 0.17436151206493378, 0.1717977523803711, 0.4582911729812622, 0.18033941090106964, 0.08182056248188019, 0.3895654082298279, 0.060744550079107285, 0.1678178757429123, 0.18917641043663025, 0.44623643159866333, 0.07475266605615616, 0.08123111724853516], dtype='float32').reshape([30]),
            paddle.to_tensor([0.45858511328697205, 0.4247009754180908, 0.17894487082958221, 0.18959587812423706, 0.3717837333679199, 0.38163483142852783, 0.24795617163181305, 0.34470945596694946, 0.09599808603525162, 0.2731936275959015, 0.3775630295276642, 0.29073017835617065, 0.020735913887619972, 0.10041225701570511, 0.11594590544700623, 0.1368611454963684, 0.36200737953186035, 0.4950973093509674, 0.39513856172561646, 0.20864543318748474, 0.3603171706199646, 0.014297861605882645, 0.3792736530303955, 0.20715303719043732, 0.03902134299278259, 0.04472476616501808, 0.11250361800193787, 0.2805189788341522, 0.20727761089801788, 0.4041970670223236], dtype='float32').reshape([30]),
            paddle.to_tensor([0.3469441533088684, 0.19408190250396729, 0.20903654396533966, 0.07845636457204819, 0.3746297359466553, 0.2105080634355545, 0.4717240631580353, 0.3208276927471161, 0.20740891993045807, 0.05041210353374481, 0.009231135249137878, 0.2752262055873871, 0.41710755228996277, 0.03919763118028641, 0.4539635479450226, 0.19002504646778107, 0.023025155067443848, 0.13001832365989685, 0.48158037662506104, 0.48505696654319763, 0.3699522316455841, 0.1160900890827179, 0.04847303405404091, 0.203139528632164, 0.22018417716026306, 0.255956768989563, 0.4043542444705963, 0.2159426510334015, 0.48467811942100525, 0.13052888214588165], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9f45d081c0b6f9e483d080e1b3c5115c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_da4d85bc213f70819a72e68f21214cdb
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 32, 16, 16], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2f5fc2b98d6821f038f60e03fd3443d4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b157f9ec6f93c8cc54683495a3dac815(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 2, 240, 240], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.45399239659309387, 0.22998182475566864], dtype='float32').reshape([2]),
            paddle.to_tensor([0.2100883275270462, 0.05046786367893219], dtype='float32').reshape([2]),
            paddle.to_tensor([0.05155219882726669, 0.39865222573280334], dtype='float32').reshape([2]),
            paddle.to_tensor([0.3846759796142578, 0.09005904942750931], dtype='float32').reshape([2]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_09203dcc29a15f88b9ae20bea9b1cfa6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3246a3ac6b7180fe33f9e0064cbe6286(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 120, 120], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_41e6492ae77b03382530d58ff48d6701(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 28, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.0045716105960309505, 0.045423734933137894, 0.41233548521995544, 0.04305374249815941, 0.405421644449234, 0.12740258872509003, 0.08640184998512268, 0.43306100368499756, 0.03490476682782173, 0.43183258175849915, 0.2638389766216278, 0.4681968092918396, 0.39874395728111267, 0.17758800089359283, 0.29519978165626526, 0.16507725417613983], dtype='float32').reshape([16]),
            paddle.to_tensor([0.10466986149549484, 0.013436736539006233, 0.26819297671318054, 0.3685533404350281, 0.2441333532333374, 0.1229715421795845, 0.3345063030719757, 0.42189839482307434, 0.1881270855665207, 0.2971818447113037, 0.017800798639655113, 0.06131036579608917, 0.47847139835357666, 0.13916882872581482, 0.37128037214279175, 0.18709196150302887], dtype='float32').reshape([16]),
            paddle.to_tensor([0.19321830570697784, 0.22014284133911133, 0.3107776343822479, 0.04309835657477379, 0.43933722376823425, 0.001127013936638832, 0.012033388949930668, 0.18563085794448853, 0.3397946059703827, 0.021671190857887268, 0.1749008297920227, 0.03546139970421791, 0.24814514815807343, 0.1883895844221115, 0.48297518491744995, 0.4672946333885193], dtype='float32').reshape([16]),
            paddle.to_tensor([0.40521079301834106, 0.3464856445789337, 0.4395517408847809, 0.08502809703350067, 0.2480107992887497, 0.2457999885082245, 0.08824734389781952, 0.3875960111618042, 0.12565337121486664, 0.2575700879096985, 0.31295692920684814, 0.4909612238407135, 0.3826860785484314, 0.2640228867530823, 0.18744860589504242, 0.47527286410331726], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f2cda932dcb43c99d1b2ddf4d8595bf5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 160, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_16299ad9801d20e9fa7e0d407067ebe2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 80, 80], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a05e2f30ac6fac3cc1ccb22d88405d95(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 160, 160], dtype='float16', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8ca51cbf600a2490b7ca40b7d02a9741(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2b470e817e801a61466d52708374cb2f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 384, 8, 8], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4dbe1df24e24eacf815abd30731be171(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 64, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9e7c17f9ba7391848adf4d3942ea6d30(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 64, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9be6db102a8dd919b7f6d098dcc8bcde(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 60, 60], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b920f1cf16e8e4fe5f8b0c9d30af357b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 4, 256, 512], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.3656165599822998, 0.42815274000167847, 0.39188456535339355, 0.01810758002102375], dtype='float32').reshape([4]),
            paddle.to_tensor([0.2508295476436615, 0.48163095116615295, 0.4749487340450287, 0.3260308504104614], dtype='float32').reshape([4]),
            paddle.to_tensor([0.12493555247783661, 0.18390388786792755, 0.06030896306037903, 0.42432454228401184], dtype='float32').reshape([4]),
            paddle.to_tensor([0.1228451132774353, 0.1199958473443985, 0.4755318760871887, 0.37337401509284973], dtype='float32').reshape([4]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f2292afb0c54437f367224fa6ac5b5af(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 40, 40], dtype='float16', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d250e99e11ab868a27cc44f37b47b2a0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 15, 15], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e848f2b621fc5233c5504096602a895a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 160, 160], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8a0c5342c6a9e56fdfcfcf259d92bcf1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 384, 16, 16], dtype='float16', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_630a0e162ff03fca2afad19f97d6aa2b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 56, 40, 40], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4f998d3d9ac3feb4d8b3e7092686ec07(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 16, 16], dtype='float16', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_392f345d39880094263f542fc25200c0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 160, 160], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fc659a2ea6091af53e425e7806f98003(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 336, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_827c7ecc3a40367cc58a098f13ba69a8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 70, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.uniform([70], dtype='float32', min=0, max=0.5),
            paddle.uniform([70], dtype='float32', min=0, max=0.5),
            paddle.uniform([70], dtype='float32', min=0, max=0.5),
            paddle.uniform([70], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d31a86b4694fd55f5c32dc43f8aed96a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_247d4d320f24ecb0ba90a3caa5a8584e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 4, 12], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.02726844884455204, 0.24533595144748688, 0.3035529851913452, 0.16750717163085938, 0.37050846219062805, 0.32534095644950867, 0.00863158144056797, 0.09066656976938248, 0.1759112924337387, 0.21630948781967163, 0.258842796087265, 0.476285457611084, 0.3233356177806854, 0.010586138814687729, 0.4334815740585327, 0.07127764075994492], dtype='float32').reshape([16]),
            paddle.to_tensor([0.1153712198138237, 0.4668097496032715, 0.29396751523017883, 0.3890458941459656, 0.23841099441051483, 0.49508488178253174, 0.4577741026878357, 0.14883477985858917, 0.134760320186615, 0.2816912531852722, 0.3728104829788208, 0.24161799252033234, 0.08672372251749039, 0.07870382815599442, 0.12793567776679993, 0.1538892686367035], dtype='float32').reshape([16]),
            paddle.to_tensor([0.11096246540546417, 0.10850880295038223, 0.20899905264377594, 0.4279943108558655, 0.0893169716000557, 0.2448926419019699, 0.1584528535604477, 0.03257063403725624, 0.27958157658576965, 0.30091097950935364, 0.47312280535697937, 0.3893722891807556, 0.09120351076126099, 0.11645195633172989, 0.4954029321670532, 0.18162596225738525], dtype='float32').reshape([16]),
            paddle.to_tensor([0.48693978786468506, 0.37754151225090027, 0.2769007980823517, 0.3931829631328583, 0.23503895103931427, 0.4437948167324066, 0.36952173709869385, 0.29553669691085815, 0.31962764263153076, 0.4709952771663666, 0.17457273602485657, 0.06157110258936882, 0.19224418699741364, 0.04908110573887825, 0.11571643501520157, 0.32442671060562134], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ef898b02c7d645fc360c1daf96998b13(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 60, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c0be56c9e55ad7c7068d1c27b836fdaa(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 19, 19], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_830e79444449ca0030f2c0a615d1c424(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 320, 320], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.18544933199882507, 0.16757647693157196, 0.12330713123083115, 0.32292649149894714, 0.08420965075492859, 0.39660584926605225, 0.36453452706336975, 0.004206067882478237], dtype='float32').reshape([8]),
            paddle.to_tensor([0.14069576561450958, 0.33263131976127625, 0.11245729774236679, 0.208872452378273, 0.12313417345285416, 0.2172430455684662, 0.322262167930603, 0.018338017165660858], dtype='float32').reshape([8]),
            paddle.to_tensor([0.05004438757896423, 0.14237071573734283, 0.1776028275489807, 0.4650935232639313, 0.3018916845321655, 0.44040605425834656, 0.08581718802452087, 0.37027743458747864], dtype='float32').reshape([8]),
            paddle.to_tensor([0.04282227158546448, 0.3317930996417999, 0.31633028388023376, 0.25346335768699646, 0.007173238322138786, 0.3020053803920746, 0.25545284152030945, 0.42258620262145996], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_47aa6f452775a174c4c49835e7ff1a3f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 720, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([720], dtype='float32', min=0, max=0.5),
            paddle.uniform([720], dtype='float32', min=0, max=0.5),
            paddle.uniform([720], dtype='float32', min=0, max=0.5),
            paddle.uniform([720], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6ef8c5c6fd3cacb543ec996455da8a55(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 480, 2, 50], dtype='float16', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_92f03dcfb4c66860d3e7f0daeb2f3360(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a8301b810bccba10e4dfdee0df73334
    def get_inputs(self):
        return [
            paddle.uniform([1, 480, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_82036f2bd4962271b9aafaf32c4e9a06(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 512, 1024], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.03352080658078194, 0.35629135370254517, 0.01132333092391491, 0.3050730526447296, 0.12188662588596344, 0.4580756723880768, 0.07531677931547165, 0.17553263902664185, 0.1186167299747467, 0.15305474400520325, 0.2103474885225296, 0.31598618626594543, 0.1247405856847763, 0.23369033634662628, 0.4322598874568939, 0.0941770076751709], dtype='float32').reshape([16]),
            paddle.to_tensor([0.21334822475910187, 0.44051462411880493, 0.30511996150016785, 0.16497154533863068, 0.030685413628816605, 0.08645215630531311, 0.44952821731567383, 0.34004777669906616, 0.12782418727874756, 0.3944151997566223, 0.3452022075653076, 0.04516186937689781, 0.13394935429096222, 0.20004236698150635, 0.25094136595726013, 0.009488187730312347], dtype='float32').reshape([16]),
            paddle.to_tensor([0.1718989610671997, 0.33421218395233154, 0.33320489525794983, 0.4905157685279846, 0.25441861152648926, 0.2736818194389343, 0.36692410707473755, 0.015080969780683517, 0.2385127693414688, 0.13391080498695374, 0.28774452209472656, 0.4439184367656708, 0.03384813666343689, 0.4475237727165222, 0.16243647038936615, 0.017299124971032143], dtype='float32').reshape([16]),
            paddle.to_tensor([0.3427654802799225, 0.13387593626976013, 0.4352778494358063, 0.4793868660926819, 0.3651403486728668, 0.3587148189544678, 0.38683587312698364, 0.3482120931148529, 0.2512379586696625, 0.015559040009975433, 0.2671531140804291, 0.2631310224533081, 0.36989879608154297, 0.3225630521774292, 0.28788837790489197, 0.30517256259918213], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5aa1d301a9c0bc7f66585b24d6eed571(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 16, 16], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_021252efb52ede89c8b44ec6099311e5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 112, 112], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_9d5dde09230a5209a57565a3b7304f49(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_95e811825649fd76b0bef5efe3ef21de(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_9d5dde09230a5209a57565a3b7304f49
    def get_inputs(self):
        return [
            paddle.uniform([1, 384, 49], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0cb5f9ac26a8ae3e49a61b7ce1777b6b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 160, 160], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.19600005447864532, 0.2036721259355545, 0.33741092681884766, 0.21523873507976532, 0.19147878885269165, 0.1534062623977661, 0.29075607657432556, 0.31594014167785645, 0.48001283407211304, 0.3512401282787323, 0.08804967999458313, 0.3663502335548401, 0.22722233831882477, 0.08629202842712402, 0.30363649129867554, 0.017439935356378555], dtype='float32').reshape([16]),
            paddle.to_tensor([0.46672049164772034, 0.046182867139577866, 0.12187594920396805, 0.4180228114128113, 0.21541647613048553, 0.3183838427066803, 0.018199510872364044, 0.06316173821687698, 0.1253015249967575, 0.19635571539402008, 0.05461616441607475, 0.12789025902748108, 0.025365492329001427, 0.3194059729576111, 0.22731824219226837, 0.3490305542945862], dtype='float32').reshape([16]),
            paddle.to_tensor([0.325710654258728, 0.2273411750793457, 0.03375910595059395, 0.022199256345629692, 0.05569368973374367, 0.20088958740234375, 0.30367475748062134, 0.19801880419254303, 0.26455381512641907, 0.08651712536811829, 0.38289982080459595, 0.26321932673454285, 0.35007864236831665, 0.29122698307037354, 0.4621368944644928, 0.25792986154556274], dtype='float32').reshape([16]),
            paddle.to_tensor([0.3617393970489502, 0.26979008316993713, 0.16214647889137268, 0.42341071367263794, 0.1793726086616516, 0.23039063811302185, 0.4288100600242615, 0.4402119517326355, 0.0504966601729393, 0.3801281154155731, 0.3379749059677124, 0.343100368976593, 0.24351978302001953, 0.28179681301116943, 0.17863860726356506, 0.33930012583732605], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f63b8d5cfa4b7d85342d08f52f460855(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 256, 128], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_89ff2cd406d34d720d4ca2b66914247f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 8, 6], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_618426c1ee0a4782a9c8da27fe24310d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_33854835b810a092767209029c27b75e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 4, 50], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.09382927417755127, 0.013923672027885914, 0.3737158477306366, 0.4199538230895996, 0.31038719415664673, 0.3246493339538574, 0.4496781527996063, 0.3349776864051819, 0.24084411561489105, 0.4159623682498932, 0.1840788871049881, 0.21814383566379547, 0.08314530551433563, 0.4219638705253601, 0.007167173083871603, 0.3310451805591583, 0.177744522690773, 0.13316841423511505, 0.4346555173397064, 0.32987692952156067, 0.4427587687969208, 0.06355508416891098, 0.0367901436984539, 0.0553707554936409], dtype='float32').reshape([24]),
            paddle.to_tensor([0.30222731828689575, 0.48318734765052795, 0.4338987171649933, 0.49148237705230713, 0.13065993785858154, 0.09375381469726562, 0.3572636544704437, 0.27716386318206787, 0.41288381814956665, 0.010841242037713528, 0.2674608528614044, 0.1357332468032837, 0.39809444546699524, 0.08293922245502472, 0.13845425844192505, 0.10423257201910019, 0.32765650749206543, 0.4200916588306427, 0.1371724009513855, 0.22758102416992188, 0.4383895993232727, 0.37884506583213806, 0.3592287003993988, 0.18625545501708984], dtype='float32').reshape([24]),
            paddle.to_tensor([0.35819190740585327, 0.442201167345047, 0.3945466876029968, 0.36265307664871216, 0.28631770610809326, 0.025803685188293457, 0.10918521881103516, 0.442057341337204, 0.1825094372034073, 0.029490454122424126, 0.4281764626502991, 0.27675747871398926, 0.14769969880580902, 0.07585112750530243, 0.39507633447647095, 0.15849988162517548, 0.10780719667673111, 0.32728856801986694, 0.047449998557567596, 0.1591385453939438, 0.3408155143260956, 0.48666852712631226, 0.287593811750412, 0.4471302926540375], dtype='float32').reshape([24]),
            paddle.to_tensor([0.11557583510875702, 0.04131440073251724, 0.2903302311897278, 0.32648929953575134, 0.20948415994644165, 0.28108397126197815, 0.2599722445011139, 0.34444403648376465, 0.18891635537147522, 0.4134698808193207, 0.3838910758495331, 0.34263211488723755, 0.16387741267681122, 0.3945707678794861, 0.17826655507087708, 0.3608483076095581, 0.3526183068752289, 0.03534838929772377, 0.34896495938301086, 0.15855813026428223, 0.3716098964214325, 0.3937720060348511, 0.02745838090777397, 0.38240233063697815], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_272337da5cd377ec31aa063d9972df49(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 144, 48, 48], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_111103e0a0bf699cff53016ed53857e4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.21402019262313843, 0.1466914564371109, 0.0035649058409035206, 0.09048622846603394, 0.03946123272180557, 0.19009563326835632, 0.05202793702483177, 0.1598873734474182, 0.32179757952690125, 0.4895516633987427, 0.16860203444957733, 0.3585912585258484, 0.14059151709079742, 0.4926604628562927, 0.4996148943901062, 0.26722583174705505, 0.19966022670269012, 0.39879658818244934, 0.21773545444011688, 0.10680035501718521, 0.046763401478528976, 0.33018577098846436, 0.25030696392059326, 0.17585788667201996, 0.053819864988327026, 0.44331789016723633, 0.12435492873191833, 0.09392057359218597, 0.17083264887332916, 0.4535784423351288], dtype='float32').reshape([30]),
            paddle.to_tensor([0.33366963267326355, 0.20881183445453644, 0.4372459948062897, 0.23790118098258972, 0.08117862045764923, 0.12898211181163788, 0.05492214486002922, 0.4114609360694885, 0.15854430198669434, 0.288425087928772, 0.4214284121990204, 0.3277503252029419, 0.03962825611233711, 0.35074663162231445, 0.0708763524889946, 3.7362740840762854e-05, 0.15592536330223083, 0.17818036675453186, 0.24628932774066925, 0.19648638367652893, 0.40951821208000183, 0.3507620394229889, 0.01943395286798477, 0.3554719388484955, 0.3198881149291992, 0.4944996237754822, 0.35040804743766785, 0.30084607005119324, 0.0638328492641449, 0.20020683109760284], dtype='float32').reshape([30]),
            paddle.to_tensor([0.10313302278518677, 0.26741382479667664, 0.027339031919836998, 0.43707916140556335, 0.32351982593536377, 0.25476202368736267, 0.283334881067276, 0.30594557523727417, 0.017720285803079605, 0.27848556637763977, 0.33386507630348206, 0.11453032493591309, 0.01672341674566269, 0.47750476002693176, 0.08873601257801056, 0.1631530523300171, 0.44685661792755127, 0.08450484275817871, 0.019798923283815384, 0.3803630471229553, 0.3187696039676666, 0.45234647393226624, 0.2914895713329315, 0.20562000572681427, 0.1873488575220108, 0.37730297446250916, 0.1406209021806717, 0.27974411845207214, 0.4557887613773346, 0.2946954071521759], dtype='float32').reshape([30]),
            paddle.to_tensor([0.3504442572593689, 0.0032600839622318745, 0.18049055337905884, 0.2347131371498108, 0.0884445309638977, 0.17793139815330505, 0.2476532757282257, 0.010512241162359715, 0.43992021679878235, 0.38651415705680847, 0.14777033030986786, 0.4260682761669159, 0.3089846074581146, 0.15112408995628357, 0.17415320873260498, 0.21456046402454376, 0.21321864426136017, 0.47993749380111694, 0.22814030945301056, 0.44389718770980835, 0.11435183882713318, 0.2863817512989044, 0.2395448237657547, 0.22024500370025635, 0.2709064185619354, 0.08703627437353134, 0.23631633818149567, 0.3597263991832733, 0.1915159821510315, 0.2308279573917389], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d4146e174f55e19b80c227eaeb1581c9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 96, 96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d15729d99984906665bc0c45fcba5d19(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 40, 40], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.16241367161273956, 0.24400870501995087, 0.37236377596855164, 0.1486658751964569, 0.4409189522266388, 0.3332028388977051, 0.15980534255504608, 0.07866628468036652, 0.37452059984207153, 0.022507281973958015, 0.23375868797302246, 0.3915415108203888, 0.07968006283044815, 0.22856071591377258, 0.29718533158302307, 0.042553193867206573, 0.08837682753801346, 0.19708620011806488, 0.45858636498451233, 0.40476861596107483, 0.19238623976707458, 0.06638409197330475, 0.4899965524673462, 0.2337360978126526], dtype='float32').reshape([24]),
            paddle.to_tensor([0.42486807703971863, 0.19280152022838593, 0.05603177845478058, 0.01109668891876936, 0.38336846232414246, 0.17851033806800842, 0.20009034872055054, 0.4624825716018677, 0.12280966341495514, 0.27782636880874634, 0.07671385258436203, 0.09857022762298584, 0.17406392097473145, 0.013625215739011765, 0.4230678081512451, 0.08604655414819717, 0.3449844419956207, 0.4944409132003784, 0.48517313599586487, 0.4790329933166504, 0.22801925241947174, 0.36716052889823914, 0.4287828803062439, 0.06665152311325073], dtype='float32').reshape([24]),
            paddle.to_tensor([0.4461602568626404, 0.3974844217300415, 0.2612663805484772, 0.15323041379451752, 0.28726938366889954, 0.4538538455963135, 0.28792575001716614, 0.08261249214410782, 0.26895108819007874, 0.4385603666305542, 0.10211924463510513, 0.02160009741783142, 0.06273556500673294, 0.024074506014585495, 0.4618380069732666, 0.13197524845600128, 0.20203180611133575, 0.2632647156715393, 0.3480294346809387, 0.0034505256917327642, 0.4148857593536377, 0.21068762242794037, 0.15256018936634064, 0.2907209098339081], dtype='float32').reshape([24]),
            paddle.to_tensor([0.4083537757396698, 0.4032212197780609, 0.3353309631347656, 0.08376337587833405, 0.09515605866909027, 0.266471803188324, 0.3449908494949341, 0.22843332588672638, 0.3549248278141022, 0.10954760015010834, 0.014322096481919289, 0.214319109916687, 0.2193635106086731, 0.136201873421669, 0.39014172554016113, 0.04197251796722412, 0.15927065908908844, 0.044412098824977875, 0.22973568737506866, 0.2859838008880615, 0.15236346423625946, 0.25816527009010315, 0.012760145589709282, 0.34901881217956543], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bcbc422043b1e98fe1533870f3face89(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 240, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_26efaf8d5bcabc8f8b9ee03ab166b082(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 16, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9ebd08b35877c937f8f7edfbd60efc98(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 80, 2, 50], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_aa47231b4bb80ea3e6aea444b79a2cbc(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 152, 152], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_925515d29d60fc29be68dc0c3b464edd(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 40, 40], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_82e4eeaa6471083fb8d5e1b145b0d696(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 16, 16], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c241f252de1ab2ef5efabc745ef603bf(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 224, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([224], dtype='float32', min=0, max=0.5),
            paddle.uniform([224], dtype='float32', min=0, max=0.5),
            paddle.uniform([224], dtype='float32', min=0, max=0.5),
            paddle.uniform([224], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_af7555471a5fa47dd9d2d25adeb36ed1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 480, 30, 30], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d0ab2186627111251b8e63d180616f51(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 8, 25], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_be2634e3e0da946dca4cd0e651ae6105(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 240, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d735fc795c4e2ce3d05291b689bce9a5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 480, 480], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.26108404994010925, 0.13688668608665466, 0.2806992828845978, 0.14217863976955414, 0.05099765956401825, 0.19836385548114777, 0.4619746804237366, 0.45390987396240234], dtype='float32').reshape([8]),
            paddle.to_tensor([0.4442991316318512, 0.18329182267189026, 0.0466657392680645, 0.3801465332508087, 0.0569138340651989, 0.18036076426506042, 0.33655446767807007, 0.11985690146684647], dtype='float32').reshape([8]),
            paddle.to_tensor([0.4400867223739624, 0.4628371298313141, 0.32694652676582336, 0.2830110490322113, 0.31247004866600037, 0.341048926115036, 0.4967258870601654, 0.4735828638076782], dtype='float32').reshape([8]),
            paddle.to_tensor([0.057662446051836014, 0.16119126975536346, 0.07548695057630539, 0.2587089538574219, 0.4749530851840973, 0.05890709534287453, 0.20034083724021912, 0.41949743032455444], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_58764fb3b6ca3aa152d527df6fb29f4f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([8, 256, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b628a784a403ca544f738436358df2cf(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a8301b810bccba10e4dfdee0df73334
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 75, 75], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_77b1444ca311f7bcccf9e01e9356fb46(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 224, 224], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.061745014041662216, 0.21299420297145844, 0.13439323008060455, 0.3533748686313629, 0.17212256789207458, 0.15484371781349182, 0.17983652651309967, 0.22086702287197113, 0.0837005227804184, 0.09088533371686935, 0.11943323910236359, 0.020522821694612503, 0.3471880257129669, 0.21674463152885437, 0.3023648262023926, 0.298610121011734], dtype='float32').reshape([16]),
            paddle.to_tensor([0.0927942544221878, 0.34238725900650024, 0.37741515040397644, 0.35395756363868713, 0.27597329020500183, 0.02197078801691532, 0.45542559027671814, 0.426889568567276, 0.45241478085517883, 0.3853662610054016, 0.2484750598669052, 0.2172677367925644, 0.48359665274620056, 0.3239840269088745, 0.02206643484532833, 0.11136708408594131], dtype='float32').reshape([16]),
            paddle.to_tensor([0.059405822306871414, 0.4293557107448578, 0.4704582989215851, 0.16788703203201294, 0.19063760340213776, 0.026369083672761917, 0.18755652010440826, 0.28156763315200806, 0.09139866381883621, 0.20979700982570648, 0.22803525626659393, 0.043210577219724655, 0.2810676693916321, 0.284392386674881, 0.00834321603178978, 0.4951589107513428], dtype='float32').reshape([16]),
            paddle.to_tensor([0.13093921542167664, 0.04816926643252373, 0.36239030957221985, 0.32296454906463623, 0.3439209461212158, 0.2581348121166229, 0.040738847106695175, 0.07005134224891663, 0.01796099729835987, 0.017273176461458206, 0.4234446883201599, 0.4235125780105591, 0.01114070974290371, 0.24813854694366455, 0.009428962133824825, 0.025051040574908257], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_17ae718d53546d0de454fbd0f64b08bb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 120, 128, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5d69bccb7c01c16d2bda74af1c075f03(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.40353861451148987, 0.1287468820810318, 0.3585670590400696, 0.3975450098514557, 0.4080098867416382, 0.3634926974773407, 0.11903267353773117, 0.10125161707401276, 0.13632939755916595, 0.4381296932697296, 0.09779699891805649, 0.28419819474220276, 0.31922447681427, 0.39204612374305725, 0.4514358937740326, 0.25482627749443054, 0.18926958739757538, 0.46212252974510193], dtype='float32').reshape([18]),
            paddle.to_tensor([0.14441543817520142, 0.4136934280395508, 0.23224005103111267, 0.26336604356765747, 0.2026285082101822, 0.38915392756462097, 0.14926056563854218, 0.03703823313117027, 0.046363264322280884, 0.16835901141166687, 0.4790221154689789, 0.32299694418907166, 0.13374841213226318, 0.28229042887687683, 0.4634975492954254, 0.4293116629123688, 0.28834760189056396, 0.09676836431026459], dtype='float32').reshape([18]),
            paddle.to_tensor([0.03672629967331886, 0.3450254499912262, 0.35381343960762024, 0.0408601313829422, 0.07839040458202362, 0.015869777649641037, 0.285561203956604, 0.13052968680858612, 0.2698056399822235, 0.2772862911224365, 0.4939928650856018, 0.01435636356472969, 0.1623958796262741, 0.2019098848104477, 0.0539642833173275, 0.41280972957611084, 0.20011930167675018, 0.17528308928012848], dtype='float32').reshape([18]),
            paddle.to_tensor([0.3898523449897766, 0.3202081322669983, 0.4851730763912201, 0.08269208669662476, 0.059936441481113434, 0.3649379014968872, 0.42139431834220886, 0.38482627272605896, 0.09958247095346451, 0.28773972392082214, 0.43020007014274597, 0.2963814437389374, 0.3230534493923187, 0.3878896236419678, 0.37778806686401367, 0.49492624402046204, 0.06015690043568611, 0.23885835707187653], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_38f4324dc9938150114f7adb99467e8c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e955537f4e8f61f85f209244cfb2b955(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 160, 8, 6], dtype='float16', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3f1af0edc55578d283898971c75ec701(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_3a47a120d62a3f3a422a747f511df8d9
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ec0b50dace25a5d56f9e8dd1a50dd13f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.26441967487335205, 0.400797963142395, 0.32189616560935974, 0.03343677520751953, 0.3669450581073761, 0.36389002203941345, 0.0693427249789238, 0.1071946918964386, 0.37493571639060974, 0.32792603969573975, 0.04730504751205444, 0.4649019241333008, 0.4974727928638458, 0.03971689194440842, 0.23974451422691345, 0.45936644077301025, 0.09402476251125336, 0.12047324329614639, 0.0791395977139473, 0.299368679523468, 0.06217147782444954, 0.18611982464790344, 0.06082457676529884, 0.06389934569597244, 0.4847378134727478, 0.28335490822792053, 0.36894455552101135, 0.27845752239227295, 0.2336706817150116, 0.03725961595773697], dtype='float32').reshape([30]),
            paddle.to_tensor([0.2599186599254608, 0.2673373520374298, 0.17009159922599792, 0.27322423458099365, 0.05311407148838043, 0.02067316323518753, 0.14877746999263763, 0.4008859097957611, 0.4061547517776489, 0.41723009943962097, 0.3842483460903168, 0.08199834823608398, 0.26133477687835693, 0.024718742817640305, 0.2506139576435089, 0.2415999174118042, 0.4423159956932068, 0.2367708832025528, 0.4556477963924408, 0.31005844473838806, 0.11702758073806763, 0.3520086109638214, 0.21047347784042358, 0.27635154128074646, 0.05662911757826805, 0.44848206639289856, 0.011156082153320312, 0.15121053159236908, 0.10948070138692856, 0.23171132802963257], dtype='float32').reshape([30]),
            paddle.to_tensor([0.07517832517623901, 0.11541207879781723, 0.13117583096027374, 0.06835722178220749, 0.10785321891307831, 0.4313819706439972, 0.12328417599201202, 0.20291109383106232, 0.21402402222156525, 0.30570366978645325, 0.4216410517692566, 0.1342441588640213, 0.015351907350122929, 0.4262888431549072, 0.32592684030532837, 0.057858679443597794, 0.058727554976940155, 0.016458582133054733, 0.16047434508800507, 0.49370259046554565, 0.4688994288444519, 0.1766429990530014, 0.42234349250793457, 0.43097424507141113, 0.23138101398944855, 0.45591288805007935, 0.3170773386955261, 0.21363721787929535, 0.27751821279525757, 0.3688867688179016], dtype='float32').reshape([30]),
            paddle.to_tensor([0.20165464282035828, 0.16864706575870514, 0.41334229707717896, 0.4366007149219513, 0.3569766879081726, 0.021353095769882202, 0.028883909806609154, 0.007390080019831657, 0.180404931306839, 0.14004164934158325, 0.17887334525585175, 0.28567075729370117, 0.42039045691490173, 0.37440115213394165, 0.0525478720664978, 0.002409519162029028, 0.009970324113965034, 0.34037336707115173, 0.02307318150997162, 0.17462439835071564, 0.07540147751569748, 0.4886007606983185, 0.37437719106674194, 0.355653315782547, 0.3644709587097168, 0.05363044887781143, 0.38285472989082336, 0.1294749528169632, 0.2941986918449402, 0.22099629044532776], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9c49c3d4cdd1a3e7e6672d44cea385a3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_af91295b59c5fca17f890a802deb001f
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 4, 32, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_01a234bf8fb86b474035e84be1dd91cd(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 64, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_00526e8d39b4e12afc9f95f0f6665000(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 56, 60, 60], dtype='float16', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a16b2bdf1df903228b5ef33a8469352b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 80, 80], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0b559b8922a11993fe83bc1ec8944464(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 1, 1], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3531e18ba32dd35bbf5fc765a3766d83(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 10, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.37950339913368225, 0.21626633405685425, 0.48929351568222046, 0.4697406589984894, 0.3213151693344116, 0.15769171714782715, 0.2675318717956543, 0.09406115114688873, 0.2791203260421753, 0.17665386199951172], dtype='float32').reshape([10]),
            paddle.to_tensor([0.31777825951576233, 0.2679947018623352, 0.04056335985660553, 0.37562036514282227, 0.43083369731903076, 0.21046017110347748, 0.05602866783738136, 0.0063438815996050835, 0.29167503118515015, 0.29151421785354614], dtype='float32').reshape([10]),
            paddle.to_tensor([0.025308137759566307, 0.48205071687698364, 0.21954034268856049, 0.4140191674232483, 0.16930854320526123, 0.25025081634521484, 0.3622882068157196, 0.17894487082958221, 0.3530023694038391, 0.19668987393379211], dtype='float32').reshape([10]),
            paddle.to_tensor([0.40462610125541687, 0.14574438333511353, 0.4822179079055786, 0.23215843737125397, 0.23188835382461548, 0.029511112719774246, 0.46695205569267273, 0.052392490208148956, 0.33479732275009155, 0.2634850740432739], dtype='float32').reshape([10]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9e838cd2acbe2bf97d80ea45cf02bb8b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.2329176664352417, 0.24347996711730957, 0.39981576800346375, 0.036035601049661636, 0.3429111838340759, 0.22188174724578857, 0.10258188098669052, 0.26849329471588135, 0.25077831745147705, 0.33316946029663086, 0.38344573974609375, 0.04717376455664635, 0.24998162686824799, 0.4901919960975647, 0.3503701686859131, 0.2355627566576004, 0.2984090745449066, 0.11771420389413834, 0.09237749129533768, 0.3137788772583008, 0.37179240584373474, 0.2950061857700348, 0.10735951364040375, 0.29386207461357117], dtype='float32').reshape([24]),
            paddle.to_tensor([0.34123626351356506, 0.26149025559425354, 0.2596191167831421, 0.3089025020599365, 0.15319356322288513, 0.058190152049064636, 0.3467237651348114, 0.3581699728965759, 0.3520074784755707, 0.11946061998605728, 0.14965593814849854, 0.311836838722229, 0.42895185947418213, 0.19957409799098969, 0.36238646507263184, 0.30756041407585144, 0.14096035063266754, 0.08428459614515305, 0.45090797543525696, 0.14561890065670013, 0.0027772081084549427, 0.08623896539211273, 0.14534175395965576, 0.03858519345521927], dtype='float32').reshape([24]),
            paddle.to_tensor([0.2456604689359665, 0.44621458649635315, 0.24816064536571503, 0.4888000190258026, 0.46865734457969666, 0.3132683336734772, 0.20632688701152802, 0.1522596776485443, 0.08636925369501114, 0.02737516537308693, 0.25598499178886414, 0.4687446355819702, 0.03100395016372204, 0.08559106290340424, 0.2767636477947235, 0.011887485161423683, 0.00598140899091959, 0.35263457894325256, 0.17091374099254608, 0.07434356212615967, 0.35870862007141113, 0.2467120736837387, 0.32632914185523987, 0.16920003294944763], dtype='float32').reshape([24]),
            paddle.to_tensor([0.4473332464694977, 0.3941554129123688, 0.2357049584388733, 0.2840167284011841, 0.005974336061626673, 0.3698354661464691, 0.2924852967262268, 0.03438565135002136, 0.23126578330993652, 0.3748207092285156, 0.4321582317352295, 0.43371036648750305, 0.49207136034965515, 0.26692476868629456, 0.23816542327404022, 0.46984344720840454, 0.21017788350582123, 0.39623159170150757, 0.4264078438282013, 0.04858892410993576, 0.35539618134498596, 0.13120748102664948, 0.09707553684711456, 0.2966628968715668], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3dfa4f4b593e67a3abe9883b23005416(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 64, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e4dada82336f848d47d879b66705e648(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 52, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([52], dtype='float32', min=0, max=0.5),
            paddle.uniform([52], dtype='float32', min=0, max=0.5),
            paddle.uniform([52], dtype='float32', min=0, max=0.5),
            paddle.uniform([52], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0dc18fb58b442e885e791aef4f565826(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 56, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f23763a3cace8ba81c8e2627ed031282(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_af91295b59c5fca17f890a802deb001f
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 4, 16, 16], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f94351e8057ddda42da72242498d97e2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 160, 8, 8], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_dc6e95c8341343313cd460ca93d3650a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 112, 64, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ad4d612056f345faf3006e65b42ce7d6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 512, 512], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_75f9402f7dc321423baaa6b108b94ff5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 144, 112, 112], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b7e67a9ae86fbbe874ea4b5585b0a952(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 10, 10], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a20578f63cd6d70174569ed4772d0682(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a8301b810bccba10e4dfdee0df73334
    def get_inputs(self):
        return [
            paddle.uniform([1, 1536, 10, 10], dtype='float32', min=0, max=0.5),
            paddle.uniform([1536], dtype='float32', min=0, max=0.5),
            paddle.uniform([1536], dtype='float32', min=0, max=0.5),
            paddle.uniform([1536], dtype='float32', min=0, max=0.5),
            paddle.uniform([1536], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_81f125284c93d2e7ee4f52ae60a21e47(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 112, 112], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.47906097769737244, 0.41989609599113464, 0.2573464512825012, 0.09233111143112183, 0.3028377890586853, 0.09284808486700058, 0.008453646674752235, 0.1368981897830963], dtype='float32').reshape([8]),
            paddle.to_tensor([0.2805635333061218, 0.16430173814296722, 0.4506084620952606, 0.06830686330795288, 0.06947527825832367, 0.21781498193740845, 0.16489820182323456, 0.1135161891579628], dtype='float32').reshape([8]),
            paddle.to_tensor([0.02251162752509117, 0.4748457372188568, 0.1897750049829483, 0.4102262556552887, 0.22392390668392181, 0.3508565425872803, 0.04509745165705681, 0.28645843267440796], dtype='float32').reshape([8]),
            paddle.to_tensor([0.45073553919792175, 0.19618602097034454, 0.05206960067152977, 0.0898507833480835, 0.27487218379974365, 0.006297703366726637, 0.17569001019001007, 0.3080354630947113], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_34519629d2b01e9728f353054bfb1426(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 60, 60], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2e213d3cdbcd171194337bc1fa9bfc64(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 480, 480], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_f78910a24ab00d45797c31273495c7e0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-06'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0cfa176e260cfbf04d01e6196989c538(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_f78910a24ab00d45797c31273495c7e0
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 32, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_50d6ce2427ec9c3f1d9a88519a9ee400(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 16, 50], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c39274b51611c3487a46588573ee57c5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_34b6c144900c0580025a24f3bb4bb4b8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.25412991642951965, 0.02385694533586502, 0.3847348988056183, 0.04514255374670029, 0.46033284068107605, 0.20547454059123993, 0.3124009370803833, 0.19481025636196136, 0.06705836951732635, 0.34540534019470215, 0.4510466754436493, 0.39287269115448, 0.36088475584983826, 0.33221691846847534, 0.4628603756427765, 0.2043643444776535, 0.18706706166267395, 0.4303748607635498, 0.1620526909828186, 0.12951123714447021], dtype='float32').reshape([20]),
            paddle.to_tensor([0.41139572858810425, 0.2956985533237457, 0.25239935517311096, 0.33724507689476013, 0.0704926922917366, 0.3464636206626892, 0.16716323792934418, 0.4250926673412323, 0.36956021189689636, 0.3146270215511322, 0.3119129538536072, 0.16940683126449585, 0.39299824833869934, 0.09683139622211456, 0.07834561914205551, 0.4453752040863037, 0.3967249393463135, 0.15156972408294678, 0.1831861138343811, 0.13650399446487427], dtype='float32').reshape([20]),
            paddle.to_tensor([0.31168267130851746, 0.21710073947906494, 0.11379430443048477, 0.1442590206861496, 0.22867637872695923, 0.21758747100830078, 0.30935776233673096, 0.1599850356578827, 0.4884132146835327, 0.21778008341789246, 0.3327822983264923, 0.4175768494606018, 0.003887894796207547, 0.0072195641696453094, 0.3178310692310333, 0.25442802906036377, 0.45509055256843567, 0.31154531240463257, 0.023448387160897255, 0.32926562428474426], dtype='float32').reshape([20]),
            paddle.to_tensor([0.3909437656402588, 0.2579604685306549, 0.1251833587884903, 0.30650416016578674, 0.35306763648986816, 0.2456931173801422, 0.46624791622161865, 0.39701971411705017, 0.25642818212509155, 0.03306075185537338, 0.1158277690410614, 0.4671279489994049, 0.04688810184597969, 0.16039419174194336, 0.145517498254776, 0.23826105892658234, 0.49234068393707275, 0.45154282450675964, 0.19300344586372375, 0.29864299297332764], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_93aa7a2bad1d4e12e33d81d8763cc60d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 512, 512], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.4550718665122986, 0.41370782256126404, 0.33372217416763306, 0.3339776396751404, 0.46554505825042725, 0.23137634992599487, 0.12887997925281525, 0.015635132789611816, 0.4353283941745758, 0.2849836051464081, 0.44881540536880493, 0.38148024678230286, 0.10071111470460892, 0.3789460361003876, 0.3869490325450897, 0.24799421429634094], dtype='float32').reshape([16]),
            paddle.to_tensor([0.10968460887670517, 0.43274232745170593, 0.3030139207839966, 0.12382859736680984, 0.005335038062185049, 0.2909625470638275, 0.349938303232193, 0.19566600024700165, 0.10010066628456116, 0.028515229001641273, 0.43107685446739197, 0.0479285791516304, 0.0534762404859066, 0.23736371099948883, 0.0654272735118866, 0.34369146823883057], dtype='float32').reshape([16]),
            paddle.to_tensor([0.49858975410461426, 0.4826054573059082, 0.08281946182250977, 0.18160171806812286, 0.13202030956745148, 0.0739433541893959, 0.013315057381987572, 0.43539679050445557, 0.39345717430114746, 0.053271155804395676, 0.13982489705085754, 0.036309219896793365, 0.3846737742424011, 0.41012200713157654, 0.3760847747325897, 0.4855828583240509], dtype='float32').reshape([16]),
            paddle.to_tensor([0.20328225195407867, 0.058614782989025116, 0.3591151237487793, 0.12535624206066132, 0.10295655578374863, 0.009527457877993584, 0.07983384281396866, 0.28145644068717957, 0.16998682916164398, 0.18134665489196777, 0.169947549700737, 0.48554474115371704, 0.13750728964805603, 0.23983415961265564, 0.35279005765914917, 0.4496489465236664], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ff337c11653720880c8b42d176c0ed19(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_af91295b59c5fca17f890a802deb001f
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 4, 64, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_81957a704c1947d4666f67228f9b85d1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1338ad9c441b1b98472de3a40ac9043e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cdc3b0a148b551f86897e4668e0bdb6b
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 8, 8], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_370df69b9dcc85dc0804c17dc8de431e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 82, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([82], dtype='float32', min=0, max=0.5),
            paddle.uniform([82], dtype='float32', min=0, max=0.5),
            paddle.uniform([82], dtype='float32', min=0, max=0.5),
            paddle.uniform([82], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bd78e080520db67791c9f82f48afc04c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_23c2b10d7a8abf9a67611a173566f3d2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 112, 112], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3b3a72aba639d468cca3f029698622fb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([8, 64, 112, 112], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2534e8748255d0ee99e710935077f497(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 61, 61], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.10895329713821411, 0.00906925555318594, 0.033235885202884674, 0.3010096549987793, 0.236174538731575, 0.36005792021751404, 0.06732512265443802, 0.041319191455841064, 0.30202949047088623, 0.28687795996665955, 0.022787168622016907, 0.28484073281288147, 0.346984326839447, 0.4043194651603699, 0.18011663854122162, 0.4892244338989258, 0.48963338136672974, 0.0314447395503521, 0.024590253829956055, 0.2714334726333618, 0.4471524953842163, 0.17977839708328247, 0.2835845351219177, 0.3648011088371277], dtype='float32').reshape([24]),
            paddle.to_tensor([0.24087338149547577, 0.007147615775465965, 0.03439495712518692, 0.3011871874332428, 0.11692220717668533, 0.4912026524543762, 0.4016604721546173, 0.21003089845180511, 0.09124299138784409, 0.4945415258407593, 0.22092866897583008, 0.1155676394701004, 0.19308358430862427, 0.2724136710166931, 0.34852200746536255, 0.1298036128282547, 0.3290345072746277, 0.2131098508834839, 0.1778017282485962, 0.052170660346746445, 0.19019238650798798, 0.3062232434749603, 0.05411669611930847, 0.11314238607883453], dtype='float32').reshape([24]),
            paddle.to_tensor([0.025483891367912292, 0.23460613191127777, 0.057831261307001114, 0.40048596262931824, 0.0719541683793068, 0.39800313115119934, 0.19471922516822815, 0.34513115882873535, 0.22936631739139557, 0.02085006795823574, 0.09969840943813324, 0.42790353298187256, 0.11601109057664871, 0.1845252513885498, 0.137134850025177, 0.35706058144569397, 0.1146119087934494, 0.23551829159259796, 0.05239298939704895, 0.3464323878288269, 0.3150660991668701, 0.16601717472076416, 0.0973089411854744, 0.32385894656181335], dtype='float32').reshape([24]),
            paddle.to_tensor([0.2212992012500763, 0.14983709156513214, 0.43771111965179443, 0.29126057028770447, 0.01684538647532463, 0.07967467606067657, 0.3974253535270691, 0.28224486112594604, 0.29510048031806946, 0.1310444325208664, 0.48727115988731384, 0.07688788324594498, 0.23688288033008575, 0.006586452946066856, 0.09960410743951797, 0.44496262073516846, 0.0006710484740324318, 0.06426156312227249, 0.13539931178092957, 0.0028976276516914368, 0.1394786238670349, 0.05843433737754822, 0.13081173598766327, 0.13302519917488098], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_be815a2b0ac5251ae6194b6067b61244(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 112, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d3eeb39e5f446c318b345b5858d89ea0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 8, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f87a6c6ea0d67316896e4c49a13ad1fd(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 120, 120], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ad0d2524e0aedb228de6741698de9592(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 160, 17, 17], dtype='float16', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9668e41c5601fb0a6edfdeb3ed5cbbfb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_d589af777f5a8ae699611515c41499fb
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 20, 20], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_537d37acef9d8cb1d89e9412036ad2a5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_af91295b59c5fca17f890a802deb001f
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 32, 64, 64], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.39313843846321106, 0.16045673191547394, 0.1870376169681549, 0.35316166281700134, 0.3037180006504059, 0.49152833223342896, 0.24310965836048126, 0.4224529564380646], dtype='float32').reshape([8]),
            paddle.to_tensor([0.4742755591869354, 0.06680431962013245, 0.44356828927993774, 0.1584046185016632, 0.026921046897768974, 0.3597659468650818, 0.25138306617736816, 0.3941396474838257], dtype='float32').reshape([8]),
            paddle.to_tensor([0.137590691447258, 0.28593456745147705, 0.1305766999721527, 0.09978697448968887, 0.09989245980978012, 0.19435417652130127, 0.4422304928302765, 0.22271506488323212], dtype='float32').reshape([8]),
            paddle.to_tensor([0.18962666392326355, 0.49055778980255127, 0.23860836029052734, 0.2885068356990814, 0.061820633709430695, 0.22304563224315643, 0.4658585786819458, 0.12308655679225922], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4c34af641aa3289578d1eac7a962a61e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 52, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([52], dtype='float32', min=0, max=0.5),
            paddle.uniform([52], dtype='float32', min=0, max=0.5),
            paddle.uniform([52], dtype='float32', min=0, max=0.5),
            paddle.uniform([52], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c3662562a628c119b524f35360abe0dd(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 12, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.3743457496166229, 0.009468959644436836, 0.1556912511587143, 0.37550604343414307, 0.03658967465162277, 0.0977468490600586, 0.05976448580622673, 0.4083317518234253, 0.11373963952064514, 0.49968504905700684, 0.29839539527893066, 0.4902683198451996], dtype='float32').reshape([12]),
            paddle.to_tensor([0.30525386333465576, 0.47005850076675415, 0.11386071890592575, 0.37339332699775696, 0.3075491189956665, 0.3329053819179535, 0.1147083193063736, 0.008213683031499386, 0.11970895528793335, 0.38564005494117737, 0.2810395359992981, 0.029422033578157425], dtype='float32').reshape([12]),
            paddle.to_tensor([0.4033266305923462, 0.05020246282219887, 0.1357366293668747, 0.36612680554389954, 0.16730168461799622, 0.2821468412876129, 0.4664531648159027, 0.056441742926836014, 0.02314227633178234, 0.36528170108795166, 0.09111397713422775, 0.4531182050704956], dtype='float32').reshape([12]),
            paddle.to_tensor([0.2728407680988312, 0.38409656286239624, 0.33459019660949707, 0.343993604183197, 0.31434011459350586, 0.3250885605812073, 0.4605875611305237, 0.4958953559398651, 0.16149155795574188, 0.22024372220039368, 0.11175180971622467, 0.4794943630695343], dtype='float32').reshape([12]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_858d65e46287c794b67220f85c9914e8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 120, 120], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.36306777596473694, 0.2690947353839874, 0.4976884424686432, 0.10012584924697876, 0.45040056109428406, 0.029437555000185966, 0.3054971992969513, 0.03000130131840706, 0.4450204074382782, 0.39216625690460205, 0.23087286949157715, 0.4620014727115631, 0.4871152639389038, 0.005821265745908022, 0.36096060276031494, 0.4618978500366211, 0.3312505781650543, 0.1334226280450821, 0.15498517453670502, 0.13826119899749756, 0.0056248800829052925, 0.402182012796402, 0.3135780096054077, 0.4422033727169037], dtype='float32').reshape([24]),
            paddle.to_tensor([0.37878283858299255, 0.38235506415367126, 0.16634690761566162, 0.3142869472503662, 0.35956570506095886, 0.02225092425942421, 0.17308127880096436, 0.458438515663147, 0.04254662245512009, 0.28682273626327515, 0.22662900388240814, 0.090608611702919, 0.4523729383945465, 0.35123446583747864, 0.3773278295993805, 0.3966478705406189, 0.23888280987739563, 0.3117237389087677, 0.49997538328170776, 0.21587732434272766, 0.1387782245874405, 0.029606401920318604, 0.20790065824985504, 0.2811637818813324], dtype='float32').reshape([24]),
            paddle.to_tensor([0.4081020653247833, 0.07408353686332703, 0.24157506227493286, 0.1756494641304016, 0.1676199734210968, 0.4316323697566986, 0.44449445605278015, 0.4159913659095764, 0.18982520699501038, 0.3192250728607178, 0.4392993748188019, 0.2161184698343277, 0.1783730834722519, 0.19529272615909576, 0.007026421371847391, 0.23024515807628632, 0.24910932779312134, 0.10809545964002609, 0.061574555933475494, 0.07383545488119125, 0.14046400785446167, 0.22264492511749268, 0.13837769627571106, 0.47203683853149414], dtype='float32').reshape([24]),
            paddle.to_tensor([0.35698288679122925, 0.4998815357685089, 0.3226321041584015, 0.012122170068323612, 0.19707898795604706, 0.08914119005203247, 0.347301185131073, 0.2285715937614441, 0.0878133773803711, 0.38680949807167053, 0.2408146858215332, 0.00788144301623106, 0.17967967689037323, 0.2811948359012604, 0.48724690079689026, 0.13073283433914185, 0.39697134494781494, 0.4109756052494049, 0.141393780708313, 0.2768557071685791, 0.2506132125854492, 0.05749130621552467, 0.40039122104644775, 0.11294783651828766], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_16baf02b4527b73c17598177911ff1b4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 64, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ad1f3ec9740f5898d60480d344f97741(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 8, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_157a00729c898c8994289990d1cee6ab(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 2048, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a5ab9e67816d12695fe668a698f0232e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a8301b810bccba10e4dfdee0df73334
    def get_inputs(self):
        return [
            paddle.uniform([1, 1152, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9677fc4c8dfb93fd81b71474bd5576ea(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 36, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3a8f5211399a84f10a158bea8c6903b2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 576, 32, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([576], dtype='float32', min=0, max=0.5),
            paddle.uniform([576], dtype='float32', min=0, max=0.5),
            paddle.uniform([576], dtype='float32', min=0, max=0.5),
            paddle.uniform([576], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3c7de9a2e7d41a4f6ae81e6c61f3459c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 64, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b1d47ff2eff492db1f04baeb20986e06(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.1322103589773178, 0.2804710865020752, 0.23041345179080963, 0.1291937381029129, 0.058328233659267426, 0.28158238530158997, 0.15430401265621185, 0.09889527410268784, 0.017807362601161003, 0.31191757321357727, 0.15401490032672882, 0.41857218742370605, 0.19480706751346588, 0.46354231238365173, 0.3141874074935913, 0.03454534336924553, 0.4140228033065796, 0.02395336702466011, 0.18701356649398804, 0.1058463379740715, 0.07620127499103546, 0.13244128227233887, 0.30998495221138, 0.2085055708885193, 0.25896209478378296, 0.04911467060446739, 0.3715927004814148, 0.08206304907798767, 0.07539113610982895, 0.36956512928009033], dtype='float32').reshape([30]),
            paddle.to_tensor([0.44166964292526245, 0.020387079566717148, 0.07045596837997437, 0.1807401180267334, 0.09122838079929352, 0.4542003571987152, 0.13913410902023315, 0.23635411262512207, 0.4504850208759308, 0.11943605542182922, 0.039861664175987244, 0.3729601800441742, 0.23349672555923462, 0.22422194480895996, 0.2630307078361511, 0.3739776015281677, 0.363247811794281, 0.37113291025161743, 0.47176268696784973, 0.42693862318992615, 0.03465990722179413, 0.32452303171157837, 0.17878003418445587, 0.24740147590637207, 0.055041003972291946, 0.2746107280254364, 0.2110728770494461, 0.062252964824438095, 0.10878756642341614, 0.3855856657028198], dtype='float32').reshape([30]),
            paddle.to_tensor([0.013817848637700081, 0.1189427524805069, 0.067310631275177, 0.2381470948457718, 0.15203900635242462, 0.3242835998535156, 0.10900793224573135, 0.1606302112340927, 0.3749154210090637, 0.1794881373643875, 0.4396103620529175, 0.17252753674983978, 0.14909550547599792, 0.4636463224887848, 0.2455207109451294, 0.0700341984629631, 0.3456348776817322, 0.2567574381828308, 0.17507825791835785, 0.020893478766083717, 0.09394102543592453, 0.08021682500839233, 0.27481332421302795, 0.26502150297164917, 0.4394316077232361, 0.05419924110174179, 0.027943691238760948, 0.17745184898376465, 0.44200053811073303, 0.4181545376777649], dtype='float32').reshape([30]),
            paddle.to_tensor([0.25855639576911926, 0.22265958786010742, 0.4043427109718323, 0.08135953545570374, 0.17152924835681915, 0.2646926939487457, 0.24216146767139435, 0.15693682432174683, 0.3921918570995331, 0.21735979616641998, 0.36652663350105286, 0.3894475996494293, 0.47917234897613525, 0.3594135046005249, 0.0681404396891594, 0.40846362709999084, 0.26523542404174805, 0.40057361125946045, 0.2562642991542816, 0.1957482546567917, 0.053209852427244186, 0.3910295367240906, 0.11563707888126373, 0.4292328953742981, 0.06434877216815948, 0.41504180431365967, 0.49702590703964233, 0.06817902624607086, 0.10101111233234406, 0.14598703384399414], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b31703b7bb1999e0429b34c03ceda206(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 4, 8], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_59823bd730fdb371a44ab7d697f3cd27(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 36, 16, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d6835ac83567a801ef03852447ed78fe(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 128, 128], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7f2d91bea1dc3d2a79875d4a25395722(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 40, 40], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e974c50b77aea63b55b5d8f8715227b0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 80, 30, 30], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_48979174d183fd5afb16613b80589c74(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_145ad20e7a3b3c8aa31f6b40d4fe3d45
    def get_inputs(self):
        return [
            paddle.uniform([49, 256], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_313cac7372f7a45a3800a21e81914d64(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 20, 20], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1affd0431979b46df629ebeeb76c6cdb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 1, 160, 160], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.3273170292377472], dtype='float32').reshape([1]),
            paddle.to_tensor([0.4739636480808258], dtype='float32').reshape([1]),
            paddle.to_tensor([0.4590248763561249], dtype='float32').reshape([1]),
            paddle.to_tensor([0.2607845067977905], dtype='float32').reshape([1]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7e58733432019e545cb4a40927925308(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 480, 20, 20], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_696bd4e4e5cb5a173b3b1653904e9662(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 768, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_86da30f7829434ed595e9f5d588b0929(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 336, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cfad5db87791c62f91fd9d9604dfc71c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 104, 4, 50], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9e60825b03f828fb83877bb718213267(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_034a1695ec9189e02b78c9dd1be6b62e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.1352824568748474, 0.2920786738395691, 0.05143813416361809, 0.2980138957500458, 0.28850021958351135, 0.25352469086647034, 0.06136700510978699, 0.2518545091152191, 0.40796515345573425, 0.24996191263198853, 0.17127323150634766, 0.05093095079064369, 0.12469229102134705, 0.37476545572280884, 0.3080587387084961, 0.08149466663599014, 0.3046749532222748, 0.12745611369609833, 0.284034788608551, 0.09729496389627457], dtype='float32').reshape([20]),
            paddle.to_tensor([0.28304237127304077, 0.12484613060951233, 0.29079389572143555, 0.20991940796375275, 0.04074139520525932, 0.15228335559368134, 0.18714648485183716, 0.08872715383768082, 0.4618009328842163, 0.33300039172172546, 0.22753790020942688, 0.08539560437202454, 0.2203209549188614, 0.2891276776790619, 0.12281353026628494, 0.2703274190425873, 0.097843237221241, 0.28858426213264465, 0.24201007187366486, 0.14708179235458374], dtype='float32').reshape([20]),
            paddle.to_tensor([0.08351276069879532, 0.20800882577896118, 0.19653241336345673, 0.12112154066562653, 0.38597002625465393, 0.021831592544913292, 0.09813667833805084, 0.43498489260673523, 0.0474398098886013, 0.10014061629772186, 0.3759903311729431, 0.309452623128891, 0.050432510673999786, 0.18121622502803802, 0.45665669441223145, 0.04425518959760666, 0.2681833505630493, 0.21431845426559448, 0.07177272439002991, 0.4291974604129791], dtype='float32').reshape([20]),
            paddle.to_tensor([0.04710410535335541, 0.15888293087482452, 0.303500771522522, 0.268726110458374, 0.23720833659172058, 0.43780890107154846, 0.38555586338043213, 0.16392216086387634, 0.24702413380146027, 0.4520972967147827, 0.1469184309244156, 0.47539955377578735, 0.28414198756217957, 0.4294169247150421, 0.2704788148403168, 0.128520667552948, 0.36020416021347046, 0.4380747377872467, 0.36384305357933044, 0.2055242359638214], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d681682d8cd99b38c1cfe3605e3c21dd(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 19, 19], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_97b47dc24d5154d982ef8cc701e9017d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cdf543476e4156bf1d278fb16b955099(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([8, 512, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bd0e7e23c6ce6a9f22f550ad1822303d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 240, 4, 50], dtype='float16', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_75b1040599ed84d9de2a5eeda5646b48(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 576, 16, 16], dtype='float32', min=0, max=0.5),
            paddle.uniform([576], dtype='float32', min=0, max=0.5),
            paddle.uniform([576], dtype='float32', min=0, max=0.5),
            paddle.uniform([576], dtype='float32', min=0, max=0.5),
            paddle.uniform([576], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d5e97d6678c93966451711f0f8081558(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 104, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_89eade693c25932d04c67f3c7ba6845d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 50, 84], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_385d768ff95cf75f94de6693ba2a2e51(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 4, 8], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_94335ef4e5897047138df29683c959f1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 60, 60], dtype='float16', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c8078e380043875b3588ad09686cbf93(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 256, 512], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.329547256231308, 0.2848684787750244, 0.11109817773103714, 0.4157187044620514, 0.30273398756980896, 0.30095887184143066, 0.2822042405605316, 0.10036992281675339, 0.3697260618209839, 0.39589637517929077, 0.13555464148521423, 0.20108656585216522, 0.15885883569717407, 0.14635048806667328, 0.43952688574790955, 0.22068461775779724], dtype='float32').reshape([16]),
            paddle.to_tensor([0.41492125391960144, 0.011763772927224636, 0.21555961668491364, 0.07498002797365189, 0.10331275314092636, 0.11850602924823761, 0.2588692307472229, 0.4882969260215759, 0.24295629560947418, 0.32622408866882324, 0.19372570514678955, 0.0537978895008564, 0.23064061999320984, 0.17515090107917786, 0.05809511989355087, 0.3969699740409851], dtype='float32').reshape([16]),
            paddle.to_tensor([0.08961832523345947, 0.24270376563072205, 0.4908726215362549, 0.34982699155807495, 0.12415555864572525, 0.4553658962249756, 0.011531970463693142, 0.1783525049686432, 0.39674481749534607, 0.13270999491214752, 0.1908043473958969, 0.41278496384620667, 0.06589687615633011, 0.34048712253570557, 0.05196723714470863, 0.16137872636318207], dtype='float32').reshape([16]),
            paddle.to_tensor([0.216734379529953, 0.061299268156290054, 0.17603875696659088, 0.28940045833587646, 0.25601422786712646, 0.07243331521749496, 0.37609604001045227, 0.05637349560856819, 0.38327252864837646, 0.21327371895313263, 0.27291539311408997, 0.47754138708114624, 0.3039422631263733, 0.39540448784828186, 0.4808788597583771, 0.408945232629776], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d6f64443a16b3d8be301cf24c4af4dbf(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.45348942279815674, 0.16067558526992798, 0.22962898015975952, 0.16985203325748444, 0.04806722700595856, 0.4737635552883148, 0.4615829885005951, 0.4044893682003021, 0.11922353506088257, 0.15410609543323517, 0.06818042695522308, 0.009825507178902626, 0.08404921740293503, 0.1430760622024536, 0.3834688663482666, 0.30167946219444275, 0.00650938181206584, 0.2847558557987213, 0.02478633262217045, 0.011871539987623692], dtype='float32').reshape([20]),
            paddle.to_tensor([0.4261593222618103, 0.3746693730354309, 0.4560988247394562, 0.24784709513187408, 0.28229573369026184, 0.24394431710243225, 0.027687763795256615, 0.3224054276943207, 0.4754179120063782, 0.11910192668437958, 0.48031049966812134, 0.18337495625019073, 0.12663033604621887, 0.2828427851200104, 0.026914969086647034, 0.377528578042984, 0.04106845706701279, 0.13853013515472412, 0.0009615799062885344, 0.002546534640714526], dtype='float32').reshape([20]),
            paddle.to_tensor([0.13433608412742615, 0.028669700026512146, 0.2787470519542694, 0.42054128646850586, 0.20336872339248657, 0.4539901614189148, 0.04665616527199745, 0.18336442112922668, 0.4698548913002014, 0.4120282530784607, 0.3529101014137268, 0.4239991009235382, 0.027357880026102066, 0.43092793226242065, 0.28883272409439087, 0.40277570486068726, 0.4302489161491394, 0.34852996468544006, 0.2335488200187683, 0.2531748414039612], dtype='float32').reshape([20]),
            paddle.to_tensor([0.02067338302731514, 0.4851873815059662, 0.019993308931589127, 0.46539074182510376, 0.29397594928741455, 0.14977309107780457, 0.11342643946409225, 0.41982170939445496, 0.41619962453842163, 0.19764861464500427, 0.07712185382843018, 0.3132857382297516, 0.07764627784490585, 0.14666618406772614, 0.17271557450294495, 0.0988960936665535, 0.229585200548172, 0.07819118350744247, 0.42216089367866516, 0.4788617491722107], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5b758db868987a1e9fa38aaaac010e03(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 480, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9896baa85c577d8626e158f27caf5d23(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 128, 128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4e58107c4ec45c81f79d4bdb74ef80e1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 160, 160], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.07916048914194107, 0.27139320969581604, 0.4882740080356598, 0.006801160052418709, 0.054684918373823166, 0.050217561423778534, 0.056347884237766266, 0.27247902750968933, 0.45962607860565186, 0.033036235719919205, 0.002782482886686921, 0.3982536792755127, 0.3416350483894348, 0.3158995807170868, 0.3857426643371582, 0.4069036543369293], dtype='float32').reshape([16]),
            paddle.to_tensor([0.1920602172613144, 0.17715635895729065, 0.31675800681114197, 0.3812764286994934, 0.21515168249607086, 0.2489480823278427, 0.009096253663301468, 0.07948672026395798, 0.020799120888113976, 0.3458079695701599, 0.21769866347312927, 0.007383330725133419, 0.3102510869503021, 0.2866818308830261, 0.34838515520095825, 0.1129232794046402], dtype='float32').reshape([16]),
            paddle.to_tensor([0.49950098991394043, 0.23496079444885254, 0.34140145778656006, 0.19330745935440063, 0.4317185878753662, 0.3561887741088867, 0.22588816285133362, 0.4771284759044647, 0.16962915658950806, 0.04593111574649811, 0.08735745400190353, 0.34957319498062134, 0.43354490399360657, 0.3993197977542877, 0.023913035169243813, 0.2121867686510086], dtype='float32').reshape([16]),
            paddle.to_tensor([0.02988334372639656, 0.3078455328941345, 0.49039942026138306, 0.24723707139492035, 0.043110720813274384, 0.19954125583171844, 0.4654048979282379, 0.1746518462896347, 0.2854040563106537, 0.12866725027561188, 0.3828587234020233, 0.04515746235847473, 0.4152211546897888, 0.49245962500572205, 0.13476230204105377, 0.4832511842250824], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3021aadd967b81f7590a701cc2e42a34(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 480, 480], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.36440590023994446, 0.37056371569633484, 0.026695191860198975, 0.44568949937820435, 0.10985417664051056, 0.4093693196773529, 0.2603263258934021, 0.14002278447151184], dtype='float32').reshape([8]),
            paddle.to_tensor([0.3673546016216278, 0.15459762513637543, 0.23137828707695007, 0.0908164381980896, 0.23178166151046753, 0.32082119584083557, 0.12804965674877167, 0.11877913773059845], dtype='float32').reshape([8]),
            paddle.to_tensor([0.39576029777526855, 0.42614051699638367, 0.41391512751579285, 0.4762840270996094, 0.03429170697927475, 0.1694352924823761, 0.19998984038829803, 0.410860151052475], dtype='float32').reshape([8]),
            paddle.to_tensor([0.03868536278605461, 0.3107284605503082, 0.2423674613237381, 0.2456987500190735, 0.2634062170982361, 0.13733048737049103, 0.16420191526412964, 0.27479422092437744], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6be3a5d5032d77d89ea93f6cdfc9a5fa(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 50, 84], dtype='float16', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_765095a358e3cb8a79ab260d3782d182(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 36, 32, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d32d5087fd068eafe1179401dcef04f0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 20, 20], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_688d1223a457607d1f272357c350ca62(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cdc3b0a148b551f86897e4668e0bdb6b
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 16, 16], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8ea2f82c3d2eb35c3e74801a6243f2a4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 112, 112], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.21126112341880798, 0.4283188283443451, 0.4401627480983734, 0.4467340409755707, 0.36864468455314636, 0.3532082438468933, 0.10822270065546036, 0.029347889125347137, 0.18655091524124146, 0.1543671041727066, 0.3140633702278137, 0.273730605840683, 0.3398236334323883, 0.4843180477619171, 0.0684351921081543, 0.2919601798057556, 0.06649018824100494, 0.3480968177318573, 0.3538670539855957, 0.4051172733306885, 0.1755024641752243, 0.08771463483572006, 0.43674173951148987, 0.09894733130931854], dtype='float32').reshape([24]),
            paddle.to_tensor([0.42619559168815613, 0.4808920621871948, 0.29439154267311096, 0.48641079664230347, 0.0897786021232605, 0.4259833097457886, 0.15576820075511932, 0.006203067488968372, 0.44162717461586, 0.2953447997570038, 0.00752591947093606, 0.27708861231803894, 0.22005623579025269, 0.2658248841762543, 0.24656321108341217, 0.1270330548286438, 0.14613065123558044, 0.14201092720031738, 0.25558730959892273, 0.17724287509918213, 0.3560510575771332, 0.3596431612968445, 0.3658629357814789, 0.20026209950447083], dtype='float32').reshape([24]),
            paddle.to_tensor([0.10680696368217468, 0.3035002648830414, 0.4064292907714844, 0.2853600084781647, 0.19625625014305115, 0.24606318771839142, 0.20152904093265533, 0.19966916739940643, 0.13488586246967316, 0.41553956270217896, 0.35027581453323364, 0.09136824309825897, 0.47051993012428284, 0.03822645545005798, 0.41189104318618774, 0.2715423107147217, 0.3093157708644867, 0.47379904985427856, 0.05151728540658951, 0.33607661724090576, 0.17714035511016846, 0.14498771727085114, 0.0023417058400809765, 0.20245251059532166], dtype='float32').reshape([24]),
            paddle.to_tensor([0.3774067163467407, 0.2074337601661682, 0.32383739948272705, 0.39249420166015625, 0.038049016147851944, 0.13680998980998993, 0.18366901576519012, 0.1398233026266098, 0.36108165979385376, 0.2417699545621872, 0.36831843852996826, 0.3635411560535431, 0.28475356101989746, 0.01547797117382288, 0.4583629071712494, 0.3410591781139374, 0.09443138539791107, 0.17669391632080078, 0.22723177075386047, 0.4349837005138397, 0.48199784755706787, 0.3854341208934784, 0.0483490414917469, 0.3493651747703552], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b6f742eb65f154fd00ea0a6fff45d60a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.39314451813697815, 0.4599660038948059, 0.3781331777572632, 0.23827201128005981, 0.4251258075237274, 0.003748533548787236, 0.43743041157722473, 0.08916044980287552, 0.31672170758247375, 0.4605009853839874, 0.1333920657634735, 0.42853474617004395, 0.14831705391407013, 0.4408542215824127, 0.1822982132434845, 0.2864839732646942, 0.40947604179382324, 0.0816086009144783], dtype='float32').reshape([18]),
            paddle.to_tensor([0.06618586927652359, 0.19308891892433167, 0.0609285943210125, 0.3139077126979828, 0.3054437041282654, 0.44556400179862976, 0.10565634071826935, 0.3028530180454254, 0.406686931848526, 0.2164294719696045, 0.38114091753959656, 0.38958221673965454, 0.11969336867332458, 0.4721581041812897, 0.16399477422237396, 0.16120436787605286, 0.003384957555681467, 0.23979392647743225], dtype='float32').reshape([18]),
            paddle.to_tensor([0.4201204478740692, 0.107307568192482, 0.46573731303215027, 0.046922046691179276, 0.08863113075494766, 0.2614041864871979, 0.25260910391807556, 0.1929769665002823, 0.4653538465499878, 0.28907275199890137, 0.4318239986896515, 0.14732106029987335, 0.1277649700641632, 0.04087037220597267, 0.3241438567638397, 0.18868502974510193, 0.18620090186595917, 0.4677983820438385], dtype='float32').reshape([18]),
            paddle.to_tensor([0.21780063211917877, 0.3718349039554596, 0.295825719833374, 0.030130116268992424, 0.351777046918869, 0.12041956186294556, 0.2095797210931778, 0.23559188842773438, 0.3558923006057739, 0.0798761248588562, 0.20268863439559937, 0.3491114377975464, 0.3980414569377899, 0.0241745263338089, 0.34477493166923523, 0.45093467831611633, 0.04160735756158829, 0.16659824550151825], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_92401916800b421a48c54bfd781a24bc(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 76, 76], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9f48d70023c69288d00c6f0215424282(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 768, 32, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_f5f2213be92f20fda5378beeed72688f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, True), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_376c09b1c6d0d3d7b9522d222d9c0031(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_f5f2213be92f20fda5378beeed72688f
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 32, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4fa6e0f95a1fb8606ad00df10b41b4a4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 4, 12], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f02a17f12e2b54c933a736920883be49(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 128, 128], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.2231007069349289, 0.18502891063690186, 0.24376961588859558, 0.13712447881698608, 0.48065659403800964, 0.1432787925004959, 0.18766993284225464, 0.49401262402534485, 0.3536176383495331, 0.3272247016429901, 0.13349053263664246, 0.2359900027513504, 0.49345988035202026, 0.0817212462425232, 0.3254345953464508, 0.09560759365558624], dtype='float32').reshape([16]),
            paddle.to_tensor([0.4280860722064972, 0.4358181357383728, 0.05336986482143402, 0.4155842065811157, 0.289206862449646, 0.23480768501758575, 0.23487971723079681, 0.19774064421653748, 0.47552987933158875, 0.23057031631469727, 0.257182240486145, 0.009275244548916817, 0.4695081412792206, 0.29150503873825073, 0.10361383855342865, 0.3635387718677521], dtype='float32').reshape([16]),
            paddle.to_tensor([0.04361017048358917, 0.12360075116157532, 0.1316872388124466, 0.3821200728416443, 0.41409727931022644, 0.28513428568840027, 0.46565666794776917, 0.364957332611084, 0.29052793979644775, 0.343162477016449, 0.23310521245002747, 0.3373126685619354, 0.30797672271728516, 0.15876778960227966, 0.3728724420070648, 0.40990155935287476], dtype='float32').reshape([16]),
            paddle.to_tensor([0.21285563707351685, 0.3664393126964569, 0.4232928156852722, 0.13898999989032745, 0.3343909978866577, 0.29991674423217773, 0.1642090380191803, 0.2910762429237366, 0.2778429388999939, 0.43737873435020447, 0.49338120222091675, 0.00036379153607413173, 0.009664149023592472, 0.4287237823009491, 0.29895687103271484, 0.12512362003326416], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_11f9f2f76dd37e62fdfb7a44bd3a4800(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 720, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([720], dtype='float32', min=0, max=0.5),
            paddle.uniform([720], dtype='float32', min=0, max=0.5),
            paddle.uniform([720], dtype='float32', min=0, max=0.5),
            paddle.uniform([720], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8d9b6f39ebce47925eac43b36062c56b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 176, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([176], dtype='float32', min=0, max=0.5),
            paddle.uniform([176], dtype='float32', min=0, max=0.5),
            paddle.uniform([176], dtype='float32', min=0, max=0.5),
            paddle.uniform([176], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_66670f1f5df6692283d64b91cc892eb3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 120, 120], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.01840779557824135, 0.45539528131484985, 0.23038950562477112, 0.20502060651779175, 0.18074972927570343, 0.06023581326007843, 0.03475057706236839, 0.4483773708343506, 0.30921852588653564, 0.3898394703865051, 0.2194790244102478, 0.18526090681552887, 0.2163100391626358, 0.10418150573968887, 0.027903780341148376, 0.3969295024871826, 0.4253484606742859, 0.3976270854473114, 0.3270990252494812, 0.16637460887432098, 0.3176858127117157, 0.026864472776651382, 0.0615813322365284, 0.1431671679019928], dtype='float32').reshape([24]),
            paddle.to_tensor([0.1520693153142929, 0.09052349627017975, 0.1419188678264618, 0.02387971617281437, 0.2743993401527405, 0.21588677167892456, 0.37907636165618896, 0.33300045132637024, 0.35920125246047974, 0.3679009675979614, 0.4574959874153137, 0.3098401129245758, 0.3576323390007019, 0.041784897446632385, 0.4538671672344208, 0.09249839186668396, 0.15646149218082428, 0.3787562847137451, 0.44192707538604736, 0.21257124841213226, 0.13515488803386688, 0.0841054916381836, 0.16055408120155334, 0.015035642310976982], dtype='float32').reshape([24]),
            paddle.to_tensor([0.2956428527832031, 0.42451784014701843, 0.4531940221786499, 0.2578786313533783, 0.3035416007041931, 0.06867966055870056, 0.475728839635849, 0.21165335178375244, 0.05866732820868492, 0.10724475979804993, 0.19220957159996033, 0.2156657725572586, 0.41004136204719543, 0.08595824986696243, 0.06671605259180069, 0.018579863011837006, 0.39302220940589905, 0.33599549531936646, 0.28442421555519104, 0.044066544622182846, 0.22522179782390594, 0.02430805005133152, 0.4544665217399597, 0.1908160150051117], dtype='float32').reshape([24]),
            paddle.to_tensor([0.006508199498057365, 0.15868137776851654, 0.23103801906108856, 0.25497645139694214, 0.47516366839408875, 0.21072430908679962, 0.181526318192482, 0.018506277352571487, 0.4660625755786896, 0.16820932924747467, 0.4918876588344574, 0.23407119512557983, 0.00966869480907917, 0.21670086681842804, 0.2042684257030487, 0.20703649520874023, 0.21853995323181152, 0.31493493914604187, 0.4057353436946869, 0.15161773562431335, 0.4415472447872162, 0.03433630242943764, 0.10935144126415253, 0.17248360812664032], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ac2dc0dcb2c9677e66cde6ebfaa80abb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_af91295b59c5fca17f890a802deb001f
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 32, 16, 16], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4b5894e9bfbc62753f54e3f60ccddba0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 30, 30], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_069e7cf64d651f596cba675b352cd136(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 160, 160], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f2f4ebae1e4886e48c34ba5c8a99b823(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 56, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_641fa009ab43def83afabee0e551174b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6ec867365773346d0357cf40b44eb66a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 12, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.4322442412376404, 0.04864511638879776, 0.3987094759941101, 0.307258278131485, 0.13022664189338684, 0.3014925718307495, 0.13664531707763672, 0.1701299250125885, 0.08125872164964676, 0.232864648103714, 0.08187858760356903, 0.2810075283050537], dtype='float32').reshape([12]),
            paddle.to_tensor([0.12901513278484344, 0.35048025846481323, 0.2920651137828827, 0.3412916362285614, 0.09064579010009766, 0.04892925173044205, 0.17226581275463104, 0.3885163366794586, 0.06571658700704575, 0.07844587415456772, 0.21426960825920105, 0.1822868287563324], dtype='float32').reshape([12]),
            paddle.to_tensor([0.05398201197385788, 0.46375134587287903, 0.401768296957016, 0.22184854745864868, 0.16106417775154114, 0.07135413587093353, 0.2654202878475189, 0.4590928554534912, 0.3977630138397217, 0.21517571806907654, 0.34382447600364685, 0.04925220087170601], dtype='float32').reshape([12]),
            paddle.to_tensor([0.010091434232890606, 0.317593514919281, 0.059577420353889465, 0.3731178641319275, 0.324392706155777, 0.10817454755306244, 0.22038814425468445, 0.21259434521198273, 0.26418137550354004, 0.29025980830192566, 0.2285507172346115, 0.4798913598060608], dtype='float32').reshape([12]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8cf7316702ee2c8c243bc7088e656a3e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 16, 16], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_049d2ff1a5ec8e89620ca7eb34911158(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 320, 320], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.27988946437835693, 0.2603711485862732, 0.31851261854171753, 0.14981311559677124, 0.022670026868581772, 0.4563475549221039, 0.23651686310768127, 0.24511678516864777], dtype='float32').reshape([8]),
            paddle.to_tensor([0.1328979730606079, 0.0630093365907669, 0.12558799982070923, 0.0570141077041626, 0.02526608482003212, 0.4416303336620331, 0.41093629598617554, 0.21639582514762878], dtype='float32').reshape([8]),
            paddle.to_tensor([0.09479358792304993, 0.0045245615765452385, 0.17640797793865204, 0.11315283179283142, 0.47189387679100037, 0.062198005616664886, 0.007565158419311047, 0.3230186402797699], dtype='float32').reshape([8]),
            paddle.to_tensor([0.29531344771385193, 0.0057490551844239235, 0.3526110053062439, 0.059799451380968094, 0.301454097032547, 0.3856559693813324, 0.17261618375778198, 0.47371166944503784], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_56b20a41d9228af88f464bf483c04b84(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.43643197417259216, 0.3963334560394287, 0.4514227509498596, 0.19779455661773682, 0.021147968247532845, 0.10266975313425064, 0.32391905784606934, 0.4748091399669647, 0.4135870933532715, 0.23885025084018707, 0.25081688165664673, 0.11464767903089523, 0.29463306069374084, 0.3354695439338684, 0.15579171478748322, 0.30710235238075256], dtype='float32').reshape([16]),
            paddle.to_tensor([0.03433319181203842, 0.385825514793396, 0.33555805683135986, 0.275264173746109, 0.24985940754413605, 0.07544080913066864, 0.028445996344089508, 0.04836322367191315, 0.4705592095851898, 0.2758505940437317, 0.026846695691347122, 0.011406432837247849, 0.24667620658874512, 0.40344828367233276, 0.34630289673805237, 0.4645976722240448], dtype='float32').reshape([16]),
            paddle.to_tensor([0.042593665421009064, 0.04722423106431961, 0.07049285620450974, 0.1959182620048523, 0.41185399889945984, 0.2769944667816162, 0.35787999629974365, 0.2004050761461258, 0.32537898421287537, 0.22795690596103668, 0.39052891731262207, 0.18466022610664368, 0.3250558078289032, 0.34502077102661133, 0.26224273443222046, 0.36004185676574707], dtype='float32').reshape([16]),
            paddle.to_tensor([0.16450385749340057, 0.41999590396881104, 0.444204717874527, 0.3212105631828308, 0.07819963246583939, 0.32788681983947754, 0.19121645390987396, 0.3690261244773865, 0.16040682792663574, 0.473329097032547, 0.18803292512893677, 0.25552645325660706, 0.3174404501914978, 0.10968272387981415, 0.4839073419570923, 0.21555131673812866], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_928e27ab2f293c507d0daeda610cb0c2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 15, 15], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b111c331e9bab02e6ac6eb5ca7dea2e4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 64, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f79cd68f447c3558499dbff26a3ee317(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 384, 64, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_738daedfb3a4fb3f6bf032c7d7f24af2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 56, 40, 40], dtype='float16', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e08a467cbf68d452f9404daccfaecf04(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 2048, 15, 15], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_19b7b514ac37ca965bb47e3e2c472fa3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.22447185218334198, 0.21973329782485962, 0.027645966038107872, 0.32527920603752136, 0.49040621519088745, 0.04357774928212166, 0.125724658370018, 0.39687347412109375, 0.497315376996994, 0.32775115966796875, 0.012595389038324356, 0.2561432123184204, 0.33798927068710327, 0.23702472448349, 0.21191418170928955, 0.42213675379753113, 0.227565735578537, 0.05142282322049141, 0.4636231064796448, 0.2755650579929352], dtype='float32').reshape([20]),
            paddle.to_tensor([0.10184735804796219, 0.16425038874149323, 0.2001734972000122, 0.008703303523361683, 0.4919816553592682, 0.011551572941243649, 0.3346436321735382, 0.2459072470664978, 0.20735286176204681, 0.2584517300128937, 0.09223655611276627, 0.4026569724082947, 0.2380266785621643, 0.09836969524621964, 0.36735785007476807, 0.028012441471219063, 0.23153918981552124, 0.3028513491153717, 0.048183783888816833, 0.4767899513244629], dtype='float32').reshape([20]),
            paddle.to_tensor([0.2686491310596466, 0.47737959027290344, 0.012089080177247524, 0.011243142187595367, 0.06833209842443466, 0.3858482539653778, 0.31677937507629395, 0.3928394317626953, 0.30527395009994507, 0.019334908574819565, 0.18579645454883575, 0.0735325813293457, 0.48812955617904663, 0.28316739201545715, 0.16726882755756378, 0.4354407489299774, 0.24238984286785126, 0.07193552702665329, 0.3334459662437439, 0.17670534551143646], dtype='float32').reshape([20]),
            paddle.to_tensor([0.08402828127145767, 0.33329811692237854, 0.41955432295799255, 0.3273000419139862, 0.17154839634895325, 0.48900219798088074, 0.053848136216402054, 0.19605989754199982, 0.2699480652809143, 0.13155168294906616, 0.3799665570259094, 0.4017705023288727, 0.28835025429725647, 0.2118023931980133, 0.39385777711868286, 0.19406546652317047, 0.1642310619354248, 0.4630109369754791, 0.38233625888824463, 0.4714687764644623], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_241a94705f67bb972a83d5527bfab8de(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 480, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_58d1065aba52e05d23a9eefdd612bf64(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 80, 2, 50], dtype='float16', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_54a62a2b6084718ed110d0fd0c7d4061(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 160, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7508ecbd57e112b80691f2375dec8ce2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_da4d85bc213f70819a72e68f21214cdb
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 4, 128, 128], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1cb430b66ccacfcb869a4ff91950eb55(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 1664, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([1664], dtype='float32', min=0, max=0.5),
            paddle.uniform([1664], dtype='float32', min=0, max=0.5),
            paddle.uniform([1664], dtype='float32', min=0, max=0.5),
            paddle.uniform([1664], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_77138f08f0b14978e46436683db57d7a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 112, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fd8e7cdda5e4010e6cc34a82e97ef7fe(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 64, 128], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.26496338844299316, 0.2972908318042755, 0.3724692761898041, 0.09763563424348831, 0.07761351764202118, 0.219270259141922, 0.4696052670478821, 0.07004518061876297, 0.49108755588531494, 0.4411297142505646, 0.07912620157003403, 0.0162693802267313, 0.05183353275060654, 0.3675820827484131, 0.029315654188394547, 0.47163066267967224, 0.33183398842811584, 0.21567942202091217], dtype='float32').reshape([18]),
            paddle.to_tensor([0.03981848806142807, 0.371021568775177, 0.3952966034412384, 0.46033111214637756, 0.22696813941001892, 0.20997817814350128, 0.13725300133228302, 0.38586485385894775, 0.20709197223186493, 0.217545747756958, 0.11457378417253494, 0.3585159480571747, 0.3298707902431488, 0.1756279468536377, 0.3675217032432556, 0.07068858295679092, 0.3706066608428955, 0.49905523657798767], dtype='float32').reshape([18]),
            paddle.to_tensor([0.35349586606025696, 0.334501177072525, 0.008974436670541763, 0.03190970793366432, 0.2655629813671112, 0.41247060894966125, 0.13235072791576385, 0.08688291162252426, 0.2094496637582779, 0.421000599861145, 0.006848364137113094, 0.1286783069372177, 0.06797197461128235, 0.4819715619087219, 0.16681213676929474, 0.19585606455802917, 0.41740870475769043, 0.3775601387023926], dtype='float32').reshape([18]),
            paddle.to_tensor([0.48290005326271057, 0.3501799702644348, 0.2752837538719177, 0.4675981402397156, 0.15786126255989075, 0.009835971519351006, 0.15859481692314148, 0.05180632323026657, 0.29695194959640503, 0.34557393193244934, 0.224629744887352, 0.2129479944705963, 0.313425213098526, 0.44852861762046814, 0.09304116666316986, 0.15593227744102478, 0.1037101149559021, 0.07001549005508423], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6fa22984d305dc31090619ba02a64a93(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 1, 1], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c7f54f72b6fb08bb1f45abb71dc1e605(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 1680, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([1680], dtype='float32', min=0, max=0.5),
            paddle.uniform([1680], dtype='float32', min=0, max=0.5),
            paddle.uniform([1680], dtype='float32', min=0, max=0.5),
            paddle.uniform([1680], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_66589ffcfd7e73c6ef8314e10f5b8f0b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 10, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.15028129518032074, 0.47591090202331543, 0.00889661256223917, 0.10549061000347137, 0.06549010425806046, 0.28227511048316956, 0.2174094021320343, 0.002852062229067087, 0.2860603630542755, 0.24916929006576538], dtype='float32').reshape([10]),
            paddle.to_tensor([0.0601394847035408, 0.18202298879623413, 0.07852259278297424, 0.06785383820533752, 0.025602491572499275, 0.06109911575913429, 0.058986663818359375, 0.40821367502212524, 0.41814470291137695, 0.33597666025161743], dtype='float32').reshape([10]),
            paddle.to_tensor([0.39447030425071716, 0.24405927956104279, 0.0035249432548880577, 0.06662650406360626, 0.2561022937297821, 0.32906395196914673, 0.44881221652030945, 0.22926093637943268, 0.1322701871395111, 0.011315337382256985], dtype='float32').reshape([10]),
            paddle.to_tensor([0.04195598140358925, 0.13478004932403564, 0.3271886706352234, 0.06348717957735062, 0.4692187011241913, 0.20095527172088623, 0.14096152782440186, 0.448213666677475, 0.10590357333421707, 0.4801138937473297], dtype='float32').reshape([10]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_efa044b8c49bc663d99ae2b96ab11e8d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_d589af777f5a8ae699611515c41499fb
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 80, 80], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4ea669bf34021b132b3c36471aa0e272(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 160, 160], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_22f9b9bea7c1506874f7de7c6dc4a213(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7c768321aa2dd0365749c3c67257aa46(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_58b4b74648ba171b249d7bef639c29fc(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_af91295b59c5fca17f890a802deb001f
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 32, 8, 8], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b80ae4e260cda84a803e2503ecb4ef36(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 672, 12, 12], dtype='float16', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3b3a52cdeb4482c1cd87c036e969dbbc(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 416, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([416], dtype='float32', min=0, max=0.5),
            paddle.uniform([416], dtype='float32', min=0, max=0.5),
            paddle.uniform([416], dtype='float32', min=0, max=0.5),
            paddle.uniform([416], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5104908147c704864a59222def9c5d87(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.4928409159183502, 0.22896990180015564, 0.07584698498249054, 0.19533076882362366, 0.27107587456703186, 0.04337500035762787, 0.21579918265342712, 0.31481286883354187, 0.3633716106414795, 0.2746520936489105, 0.42349162697792053, 0.22808176279067993, 0.012877324596047401, 0.07174945622682571, 0.3748004138469696, 0.35145094990730286, 0.1719290167093277, 0.30869147181510925, 0.16644181311130524, 0.04633460193872452, 0.030315186828374863, 0.07586579024791718, 0.24114446341991425, 0.047278277575969696, 0.13960805535316467, 0.16722683608531952, 0.1185540184378624, 0.31936419010162354, 0.36607038974761963, 0.28883880376815796], dtype='float32').reshape([30]),
            paddle.to_tensor([0.0802396759390831, 0.4501416087150574, 0.3779323995113373, 0.11374939233064651, 0.3886365294456482, 0.31391453742980957, 0.20747831463813782, 0.35555458068847656, 0.3224277198314667, 0.4278357923030853, 0.31415268778800964, 0.2802194654941559, 0.06666496396064758, 0.04102974757552147, 0.015612111426889896, 0.23234929144382477, 0.3498028516769409, 0.4522473216056824, 0.23312318325042725, 0.22854866087436676, 0.07020477205514908, 0.4411989748477936, 0.3194291591644287, 0.2799014747142792, 0.1950015127658844, 0.28779304027557373, 0.12152352929115295, 0.09361444413661957, 0.47148457169532776, 0.2703528106212616], dtype='float32').reshape([30]),
            paddle.to_tensor([0.2107180505990982, 0.16063255071640015, 0.07781888544559479, 0.1688406765460968, 0.0010215621441602707, 0.03209686651825905, 0.4408649802207947, 0.09314772486686707, 0.32177111506462097, 0.06189033389091492, 0.48509812355041504, 0.2670761048793793, 0.037508394569158554, 0.2584661841392517, 0.00021180097246542573, 0.25096723437309265, 0.32162249088287354, 0.46630167961120605, 0.3904605805873871, 0.05995101481676102, 0.38543617725372314, 0.42094117403030396, 0.40168583393096924, 0.12470588088035583, 0.3991365432739258, 0.49117541313171387, 0.02759387530386448, 0.42048734426498413, 0.30166032910346985, 0.19635052978992462], dtype='float32').reshape([30]),
            paddle.to_tensor([0.13931973278522491, 0.07569915056228638, 0.02088482305407524, 0.14947138726711273, 0.11990851908922195, 0.11353087425231934, 0.22846084833145142, 0.17429187893867493, 0.08356445282697678, 0.22041422128677368, 0.305438369512558, 0.4783155620098114, 0.040114834904670715, 0.3554477393627167, 0.4751630127429962, 0.2640507221221924, 0.3343299627304077, 0.15539821982383728, 0.1214064210653305, 0.0020714672282338142, 0.30039671063423157, 0.10728956758975983, 0.1573037952184677, 0.4384215474128723, 0.15041138231754303, 0.1818394958972931, 0.4790681004524231, 0.3932455778121948, 0.410095751285553, 0.0033418191596865654], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d03e09abd0c9c3fcf4a66ab90e09616e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 36, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_96a16a730cb55bf28e6b0c242f8e2b7e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 120, 4, 50], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_754cf3adee4fd03f0fe313736991c4ee(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 2, 2], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_98c15139922aceb200465cbc7954fb2e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 80, 80], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.30249297618865967, 0.11355943977832794, 0.4772561192512512, 0.49511703848838806, 0.38168227672576904, 0.034931544214487076, 0.37160035967826843, 0.3788912296295166, 0.25675177574157715, 0.014741676859557629, 0.11190555989742279, 0.317011296749115, 0.4820844233036041, 0.34787967801094055, 0.13713744282722473, 0.1520097553730011, 0.028068384155631065, 0.11728990077972412, 0.4697389602661133, 0.407149076461792, 0.2713889181613922, 0.3810649812221527, 0.13783520460128784, 0.12012918293476105], dtype='float32').reshape([24]),
            paddle.to_tensor([0.41297173500061035, 0.2667405307292938, 0.29612407088279724, 0.10207716375589371, 0.4606281518936157, 0.23910722136497498, 0.07706855982542038, 0.1737852841615677, 0.4559272527694702, 0.29220616817474365, 0.38932010531425476, 0.3469085991382599, 0.2748037874698639, 0.4670930504798889, 0.15490972995758057, 0.4432780146598816, 0.22370034456253052, 0.35747915506362915, 0.41315096616744995, 0.2284596562385559, 0.4662860035896301, 0.11629758775234222, 0.4764147698879242, 0.24779701232910156], dtype='float32').reshape([24]),
            paddle.to_tensor([0.16156277060508728, 0.3546343743801117, 0.01253304723650217, 0.26038697361946106, 0.03854971006512642, 0.3362502157688141, 0.4508483409881592, 0.38367536664009094, 0.2523672580718994, 0.1782645434141159, 0.21038204431533813, 0.37631794810295105, 0.4050368368625641, 0.04144002124667168, 0.0012008636258542538, 0.43541625142097473, 0.33825674653053284, 0.1803426742553711, 0.18958039581775665, 0.3635411858558655, 0.18563535809516907, 0.0881105363368988, 0.1846134513616562, 0.2815433442592621], dtype='float32').reshape([24]),
            paddle.to_tensor([0.15702416002750397, 0.21898074448108673, 0.288463830947876, 0.399130642414093, 0.3814859092235565, 0.17316307127475739, 0.09406304359436035, 0.36156928539276123, 0.13720425963401794, 0.11418565362691879, 0.12344804406166077, 0.041178248822689056, 0.3431643545627594, 0.4672764837741852, 0.19047802686691284, 0.33750468492507935, 0.39767616987228394, 0.28225862979888916, 0.4881206154823303, 0.14752435684204102, 0.3805473744869232, 0.3126978576183319, 0.0008220086456276476, 0.25430330634117126], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_317c8f8f7135772cc70f1a3a0345aac3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 112, 112], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.17633028328418732, 0.21872058510780334, 0.15558315813541412, 0.33140671253204346, 0.03953108564019203, 0.32116299867630005, 0.13458649814128876, 0.15611524879932404, 0.16399148106575012, 0.1210896298289299, 0.37618640065193176, 0.43157535791397095, 0.4998197853565216, 0.12984566390514374, 0.4305696189403534, 0.4021529257297516, 0.16390183568000793, 0.30473798513412476, 0.2232414186000824, 0.47647911310195923, 0.4187442660331726, 0.3190372884273529, 0.43024036288261414, 0.40230128169059753], dtype='float32').reshape([24]),
            paddle.to_tensor([0.08881540596485138, 0.3096369802951813, 0.27549228072166443, 0.24456191062927246, 0.31484222412109375, 0.35744643211364746, 0.02469097077846527, 0.08213642239570618, 0.02998872846364975, 0.052509866654872894, 0.2819086015224457, 0.3050672113895416, 0.35808321833610535, 0.4552973806858063, 0.13004346191883087, 0.310628741979599, 0.38807323575019836, 0.03700525313615799, 0.45803457498550415, 0.18870744109153748, 0.1317881941795349, 0.13255740702152252, 0.07075396925210953, 0.008162816986441612], dtype='float32').reshape([24]),
            paddle.to_tensor([0.41784071922302246, 0.3193518817424774, 0.21065689623355865, 0.0751485750079155, 0.35456371307373047, 0.46978145837783813, 0.30372217297554016, 0.22842267155647278, 0.33263739943504333, 0.3520331084728241, 0.3854953646659851, 0.40520623326301575, 0.4231434464454651, 0.08954119682312012, 0.18041718006134033, 0.24558104574680328, 0.020576797425746918, 0.34379950165748596, 0.14746662974357605, 0.3187287449836731, 0.4569394290447235, 0.15649712085723877, 0.24106191098690033, 0.0994420200586319], dtype='float32').reshape([24]),
            paddle.to_tensor([0.33388751745224, 0.11645489931106567, 0.0779375359416008, 0.2848173975944519, 0.13152161240577698, 0.26374995708465576, 0.2901724576950073, 0.43466031551361084, 0.19769032299518585, 0.3475465178489685, 0.44169875979423523, 0.1985556185245514, 0.34395861625671387, 0.40856000781059265, 0.037686388939619064, 0.2298801988363266, 0.3373197019100189, 0.31733718514442444, 0.2467355728149414, 0.01115700975060463, 0.4150007367134094, 0.24796603620052338, 0.44087767601013184, 0.46331116557121277], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_dcd93da19c2eefca2283ef7e3191b897(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 480, 480], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.31007277965545654, 0.058822665363550186, 0.09222743660211563, 0.015980158001184464, 0.2866608798503876, 0.1758219450712204, 0.07340361177921295, 0.04492966830730438], dtype='float32').reshape([8]),
            paddle.to_tensor([0.39616522192955017, 0.48521456122398376, 0.3127937316894531, 0.032970182597637177, 0.4005991220474243, 0.08724988996982574, 0.0472724623978138, 0.005209662020206451], dtype='float32').reshape([8]),
            paddle.to_tensor([0.25299471616744995, 0.08108344674110413, 0.04416962340474129, 0.07155699282884598, 0.46086201071739197, 0.1773301512002945, 0.39847397804260254, 0.2381216436624527], dtype='float32').reshape([8]),
            paddle.to_tensor([0.06143587827682495, 0.1258818656206131, 0.3312748074531555, 0.2694762647151947, 0.333989679813385, 0.3777967095375061, 0.2212355136871338, 0.015156242996454239], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_63af24eae4570bdfbddb352e1ca7ef2a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 120, 120], dtype='float16', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_da92cfe51222301c1b9780d8f9b1d1dc(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 36, 16, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_60d91a1f6a54f1e5c92c51f470bdc3ad(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.2627514898777008, 0.21115174889564514, 0.20485824346542358, 0.36845293641090393, 0.4391691982746124, 0.46678224205970764, 0.48600074648857117, 0.4147647023200989, 0.4080573618412018, 0.22322778403759003, 0.49482983350753784, 0.3249717354774475, 0.1887868046760559, 0.17403696477413177, 0.21706561744213104, 0.34001433849334717, 0.26930558681488037, 0.48696520924568176, 0.10900566726922989, 0.4353107810020447, 0.16995741426944733, 0.3345513939857483, 0.30333536863327026, 0.4662652611732483, 0.20178908109664917, 0.36802738904953003, 0.009690135717391968, 0.40981927514076233, 0.08200665563344955, 0.06805691868066788], dtype='float32').reshape([30]),
            paddle.to_tensor([0.12963730096817017, 0.14792612195014954, 0.10502013564109802, 0.2911872863769531, 0.013843351975083351, 0.0371481217443943, 0.007028048858046532, 0.3140680193901062, 0.2370944768190384, 0.4079413115978241, 0.3657311797142029, 0.1821155846118927, 0.05111830681562424, 0.4273659884929657, 0.2892679274082184, 0.44822412729263306, 0.033371537923812866, 0.41519877314567566, 0.33004483580589294, 0.2470506727695465, 0.06790616363286972, 0.3273901343345642, 0.40288111567497253, 0.2091692090034485, 0.15732160210609436, 0.4014148712158203, 0.12455886602401733, 0.43665972352027893, 0.04837901517748833, 0.15949106216430664], dtype='float32').reshape([30]),
            paddle.to_tensor([0.4591239094734192, 0.38706856966018677, 0.09168414026498795, 0.008766005747020245, 0.31802961230278015, 0.04879459738731384, 0.4623764455318451, 0.28705358505249023, 0.1284656971693039, 0.40995556116104126, 0.2229968011379242, 0.15881676971912384, 0.37395527958869934, 0.10939879715442657, 0.08288102596998215, 0.4484594762325287, 0.12372680008411407, 0.06542158126831055, 0.4072180390357971, 0.4032382369041443, 0.3354165554046631, 0.09277613461017609, 0.29320600628852844, 0.12270508706569672, 0.03467109426856041, 0.2321852743625641, 0.035024549812078476, 0.14776460826396942, 0.45581892132759094, 0.0917457863688469], dtype='float32').reshape([30]),
            paddle.to_tensor([0.19814229011535645, 0.05889482423663139, 0.18676993250846863, 0.07821746170520782, 0.4848555624485016, 0.29866358637809753, 0.2550853192806244, 0.13069620728492737, 0.24153539538383484, 0.4729022979736328, 0.26495733857154846, 0.2766358554363251, 0.3984483778476715, 0.0011252667754888535, 0.18387766182422638, 0.28478315472602844, 0.31687456369400024, 0.015585926361382008, 0.35784393548965454, 0.26135188341140747, 0.4916359782218933, 0.3550657331943512, 0.04063725844025612, 0.26592957973480225, 0.37632066011428833, 0.4822559058666229, 0.09539429098367691, 0.45762869715690613, 0.3705243766307831, 0.15101008117198944], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_39b00cc22dcbbaaf9a8f4a55a3a1fc48(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 112, 112], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.4097617566585541, 0.09458955377340317, 0.33906373381614685, 0.22265835106372833, 0.11348097026348114, 0.16870732605457306, 0.4186270236968994, 0.29449597001075745, 0.11057962477207184, 0.29189354181289673, 0.19681097567081451, 0.042554475367069244, 0.28649142384529114, 0.4465053975582123, 0.08091296255588531, 0.14437605440616608, 0.09533826261758804, 0.04676584154367447, 0.008455024100840092, 0.06561629474163055, 0.4841045141220093, 0.08127422630786896, 0.025879420340061188, 0.0753546878695488], dtype='float32').reshape([24]),
            paddle.to_tensor([0.28843286633491516, 0.4322287142276764, 0.4939550459384918, 0.21971455216407776, 0.3442756235599518, 0.33751627802848816, 0.2207833230495453, 0.29300549626350403, 0.0177707951515913, 0.44331371784210205, 0.3640502691268921, 0.3428798317909241, 0.20480366051197052, 0.48372307419776917, 0.38095077872276306, 0.3769291341304779, 0.4217926263809204, 0.4640868604183197, 0.07905891537666321, 0.3131892681121826, 0.10363460332155228, 0.45179635286331177, 0.34839150309562683, 0.2140384167432785], dtype='float32').reshape([24]),
            paddle.to_tensor([0.4568922221660614, 0.33761370182037354, 0.46472713351249695, 0.2575172483921051, 0.46366873383522034, 0.2577960193157196, 0.3994840383529663, 0.4798702895641327, 0.07552581280469894, 0.45372432470321655, 0.09200819581747055, 0.4212249219417572, 0.4940321743488312, 0.29870015382766724, 0.19940774142742157, 0.39421626925468445, 0.37062060832977295, 0.13621099293231964, 0.24580498039722443, 0.32158902287483215, 0.4831142723560333, 0.2700813412666321, 0.27077996730804443, 0.09577896445989609], dtype='float32').reshape([24]),
            paddle.to_tensor([0.49171414971351624, 0.07725318521261215, 0.2432122677564621, 0.21447302401065826, 0.42493003606796265, 0.09861457347869873, 0.43190494179725647, 0.34571781754493713, 0.0175618976354599, 0.4421401619911194, 0.18620264530181885, 0.1799582988023758, 0.4785948693752289, 0.44360554218292236, 0.2679729163646698, 0.18853306770324707, 0.30236849188804626, 0.12243147939443588, 0.2725989520549774, 0.32437199354171753, 0.0861288383603096, 0.1957557499408722, 0.27540382742881775, 0.37241479754447937], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_34bd9fea540da17a48d9a3f426b1b296(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_5e7cb23119f65b33bdbad71952f6dd12
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 64, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7061e34fb50d012f405d220ba63c46fe(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 80, 80], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.06217839568853378, 0.17434661090373993, 0.40915223956108093, 0.239015594124794, 0.026865720748901367, 0.4442203938961029, 0.23185403645038605, 0.46945318579673767, 0.42938292026519775, 0.3811568021774292, 0.09525604546070099, 0.4601953327655792, 0.04925230145454407, 0.16968098282814026, 0.3440597653388977, 0.27313148975372314, 0.05414755269885063, 0.04755423590540886, 0.29626214504241943, 0.11607541888952255, 0.17390772700309753, 0.09242639690637589, 0.09491623938083649, 0.15898776054382324], dtype='float32').reshape([24]),
            paddle.to_tensor([0.06211050972342491, 0.23697370290756226, 0.4266040623188019, 0.23833182454109192, 0.2700023651123047, 0.4147307872772217, 0.1251267045736313, 0.2551150918006897, 0.03862244635820389, 0.10578912496566772, 0.1150517389178276, 0.08761604130268097, 0.26895198225975037, 0.452370285987854, 0.225095734000206, 0.4069090783596039, 0.3954732418060303, 0.23557734489440918, 0.11032270640134811, 0.2557813227176666, 0.21124468743801117, 0.05168711021542549, 0.4399621784687042, 0.49200811982154846], dtype='float32').reshape([24]),
            paddle.to_tensor([0.07207116484642029, 0.39779534935951233, 0.19644500315189362, 0.016146250069141388, 0.21361851692199707, 0.37908288836479187, 0.48565635085105896, 0.04786604642868042, 0.11251630634069443, 0.2304804027080536, 0.42808815836906433, 0.04755404219031334, 0.21410001814365387, 0.367327481508255, 0.09780934453010559, 0.49029895663261414, 0.35973477363586426, 0.23098783195018768, 0.47721290588378906, 0.35741180181503296, 0.4472007155418396, 0.17579275369644165, 0.06289932876825333, 0.25739550590515137], dtype='float32').reshape([24]),
            paddle.to_tensor([0.3412887156009674, 0.16968169808387756, 0.08242212235927582, 0.4018085300922394, 0.3617388904094696, 0.10269128531217575, 0.2956283688545227, 0.16734659671783447, 0.1020292118191719, 0.20403242111206055, 0.39959412813186646, 0.19691215455532074, 0.06699305027723312, 0.18687117099761963, 0.2949245274066925, 0.22040380537509918, 0.42492449283599854, 0.01074352115392685, 0.1197039857506752, 0.1789707988500595, 0.27209675312042236, 0.3868264853954315, 0.3630625009536743, 0.45720770955085754], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_83c05042ed51ebfca06ef86feace4333(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 8, 25], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fbe86416b600ba76164e7603dc6b5205(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 104, 60, 60], dtype='float16', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_12a49777084c3fd57a3b0bd2c3e35dfb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 4, 3], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1ad2b47f7bf039f89465d61626278397(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 32, 16], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5b00e9ea7b514a7422f590dd020f9e65(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 304, 128, 128], dtype='float32', min=0, max=0.5),
            paddle.uniform([304], dtype='float32', min=0, max=0.5),
            paddle.uniform([304], dtype='float32', min=0, max=0.5),
            paddle.uniform([304], dtype='float32', min=0, max=0.5),
            paddle.uniform([304], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6c6b68ea9a0398247c50cd0eaa9422b3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 112, 112], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.23957455158233643, 0.45435765385627747, 0.28170645236968994, 0.3163650631904602, 0.2808629274368286, 0.26258033514022827, 0.23666781187057495, 0.4692282974720001, 0.16712023317813873, 0.3463567793369293, 0.4590313136577606, 0.3191949129104614, 0.3861517310142517, 0.4762353301048279, 0.30888038873672485, 0.2795802652835846], dtype='float32').reshape([16]),
            paddle.to_tensor([0.15519146621227264, 0.38465407490730286, 0.17624056339263916, 0.278263658285141, 0.2060510218143463, 0.34487712383270264, 0.2638036906719208, 0.48974278569221497, 0.19749855995178223, 0.16427527368068695, 0.033044617623090744, 0.20757639408111572, 0.01962878368794918, 0.21695661544799805, 0.4249672293663025, 0.27894845604896545], dtype='float32').reshape([16]),
            paddle.to_tensor([0.09816180169582367, 0.43001800775527954, 0.03717147558927536, 0.4127984941005707, 0.0997525006532669, 0.09079436957836151, 0.18960745632648468, 0.4698806703090668, 0.176710307598114, 0.05565538629889488, 0.26553744077682495, 0.43027397990226746, 0.3592459559440613, 0.04886505380272865, 0.40887606143951416, 0.08064982295036316], dtype='float32').reshape([16]),
            paddle.to_tensor([0.37806418538093567, 0.2714061737060547, 0.257228285074234, 0.10387379676103592, 0.05406917631626129, 0.045616913586854935, 0.23502151668071747, 0.10371998697519302, 0.3690358102321625, 0.46208488941192627, 0.013166481629014015, 0.36004599928855896, 0.24331024289131165, 0.4296925663948059, 0.1599361151456833, 0.021428793668746948], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5ccdb266cd7d6e0c1f579801be4b2684(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 160, 160], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.08092986047267914, 0.37102630734443665, 0.1801169067621231, 0.31091588735580444, 0.35203614830970764, 0.1512066125869751, 0.3833499550819397, 0.3461308777332306, 0.439221054315567, 0.16748075187206268, 0.3010881245136261, 0.1874019205570221, 0.09450902044773102, 0.29772594571113586, 0.33815762400627136, 0.14339791238307953], dtype='float32').reshape([16]),
            paddle.to_tensor([0.4386523962020874, 0.004416198469698429, 0.18109343945980072, 0.012295550666749477, 0.48052313923835754, 0.2521965205669403, 0.3388831615447998, 0.2653703987598419, 0.17399927973747253, 0.20315207540988922, 0.06088726595044136, 0.11936283856630325, 0.0039021954871714115, 0.2719041407108307, 0.45733070373535156, 0.46028515696525574], dtype='float32').reshape([16]),
            paddle.to_tensor([0.20590314269065857, 0.3992694318294525, 0.07564336806535721, 0.20684130489826202, 0.23722396790981293, 0.43823957443237305, 0.28176653385162354, 0.2773595154285431, 0.484781414270401, 0.2660670280456543, 0.4033338725566864, 0.1787540167570114, 0.062026675790548325, 0.12378326058387756, 0.41438305377960205, 0.19853344559669495], dtype='float32').reshape([16]),
            paddle.to_tensor([0.04662278667092323, 0.20239119231700897, 0.41968411207199097, 0.2605265974998474, 0.45708054304122925, 0.3272174000740051, 0.2530694007873535, 0.07612293213605881, 0.23791182041168213, 0.44946733117103577, 0.43478158116340637, 0.4056780934333801, 0.18937243521213531, 0.08520928025245667, 0.3086008131504059, 0.4024122953414917], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_801c3bd15ee9bc04c8894b70cf10ec01(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 72, 128, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ffabe0c92c64e2c4984d7d0c45acc687(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 28, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.1493670493364334, 0.09113415330648422, 0.45099857449531555, 0.23191675543785095, 0.2910432815551758, 0.0266634039580822, 0.13793881237506866, 0.1217636913061142, 0.22016145288944244, 0.02474447898566723, 0.2626163363456726, 0.2263556867837906, 0.17441388964653015, 0.2064911425113678, 0.3762383759021759, 0.472977876663208, 0.3381614685058594, 0.2793746888637543, 0.19888435304164886, 0.11426053196191788, 0.41380396485328674, 0.034201622009277344, 0.35997533798217773, 0.08164528012275696, 0.30764511227607727, 0.25089117884635925, 0.3844558894634247, 0.1489783525466919], dtype='float32').reshape([28]),
            paddle.to_tensor([0.06950594484806061, 0.21534276008605957, 0.022854408249258995, 0.41236963868141174, 0.4273856282234192, 0.33205583691596985, 0.3659440875053406, 0.16877128183841705, 0.4356785714626312, 0.4084685146808624, 0.323316365480423, 0.22505883872509003, 0.4267944097518921, 0.025249334052205086, 0.33276674151420593, 0.3403460383415222, 0.08896832913160324, 0.16237181425094604, 0.27643778920173645, 0.2663053870201111, 0.41973569989204407, 0.06105020269751549, 0.15563039481639862, 0.3616771399974823, 0.1258506178855896, 0.125112846493721, 0.3303389847278595, 0.16351106762886047], dtype='float32').reshape([28]),
            paddle.to_tensor([0.1963120549917221, 0.35902708768844604, 0.26192808151245117, 0.02044500596821308, 0.24887903034687042, 0.33811110258102417, 0.47758516669273376, 0.2589877247810364, 0.016688907518982887, 0.10508806258440018, 0.3774617910385132, 0.29163652658462524, 0.2586050033569336, 0.23101113736629486, 0.20810037851333618, 0.3492630124092102, 0.12156247347593307, 0.2818840742111206, 0.36076363921165466, 0.16300325095653534, 0.3088720142841339, 0.0817243680357933, 0.016154665499925613, 0.35397836565971375, 0.33284348249435425, 0.04652072861790657, 0.0990288257598877, 0.45389342308044434], dtype='float32').reshape([28]),
            paddle.to_tensor([0.4798162579536438, 0.24627920985221863, 0.13697673380374908, 0.43234720826148987, 0.3345481753349304, 0.14263667166233063, 0.4341813623905182, 0.006240067537873983, 0.4296366274356842, 0.14257758855819702, 0.10614722222089767, 0.15778864920139313, 0.36734503507614136, 0.37556278705596924, 0.15557442605495453, 0.02111239731311798, 0.20403221249580383, 0.34264034032821655, 0.27965399622917175, 0.20735591650009155, 0.3357667326927185, 0.13147932291030884, 0.322735458612442, 0.019481461495161057, 0.15434803068637848, 0.06254776567220688, 0.07752416282892227, 0.22477415204048157], dtype='float32').reshape([28]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ed165700b5329da03cb6b81fd0321eb2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 122, 122], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0133dc19d35aae9a3106e74920fc7661(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.24439983069896698, 0.32719239592552185, 0.4670220911502838, 0.33418890833854675, 0.25545287132263184, 0.2783588171005249, 0.35297974944114685, 0.30093225836753845, 0.14993324875831604, 0.10938718914985657, 0.24264317750930786, 0.3310059905052185, 0.3307950496673584, 0.48399364948272705, 0.16846483945846558, 0.48714011907577515, 0.1687203347682953, 0.32047200202941895, 0.4146924912929535, 0.19182537496089935], dtype='float32').reshape([20]),
            paddle.to_tensor([0.354154497385025, 0.12699103355407715, 0.2602686285972595, 0.1710706502199173, 0.42422646284103394, 0.2462318241596222, 0.43030422925949097, 0.0960296019911766, 0.48307302594184875, 0.391110897064209, 0.38748058676719666, 0.020352620631456375, 0.3440220057964325, 0.12679339945316315, 0.141795352101326, 0.07044890522956848, 0.3333257734775543, 0.47876444458961487, 0.07057839632034302, 0.39034679532051086], dtype='float32').reshape([20]),
            paddle.to_tensor([0.2634875774383545, 0.16203899681568146, 0.4105278551578522, 0.008966580964624882, 0.0713103711605072, 0.2940788269042969, 0.06467113643884659, 0.1598655879497528, 0.34413596987724304, 0.2596001625061035, 0.3229948878288269, 0.20389747619628906, 0.23513060808181763, 0.423848032951355, 0.16150636970996857, 0.26615509390830994, 0.4798116087913513, 0.06987319886684418, 0.09330473840236664, 0.38321182131767273], dtype='float32').reshape([20]),
            paddle.to_tensor([0.38438519835472107, 0.17353391647338867, 0.24029505252838135, 0.14382392168045044, 0.21597251296043396, 0.2501615285873413, 0.22955761849880219, 0.3849538564682007, 0.1982201784849167, 0.14677253365516663, 0.4574095606803894, 0.32243454456329346, 0.4202606678009033, 0.21410590410232544, 0.30320194363594055, 0.20807847380638123, 0.36719948053359985, 0.4882798194885254, 0.10381124168634415, 0.14566366374492645], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_42f8662f9e4c46b472a43ad00f1ba74b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 28, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.00621001236140728, 0.20263248682022095, 0.4225784242153168, 0.06491104513406754, 0.08003610372543335, 0.1774970144033432, 0.40237653255462646, 0.32196468114852905, 0.42761996388435364, 0.42561283707618713, 0.23882979154586792, 0.26910147070884705, 0.24509529769420624, 0.43028417229652405, 0.4404027462005615, 0.07492554187774658, 0.18851618468761444, 0.1264285147190094, 0.033453233540058136, 0.06884648650884628, 0.37335482239723206, 0.2554592490196228, 0.2185487598180771, 0.4698372483253479, 0.405706524848938, 0.23810282349586487, 0.4094085991382599, 0.3661101460456848], dtype='float32').reshape([28]),
            paddle.to_tensor([0.08165165781974792, 0.44856202602386475, 0.2837381958961487, 0.46206018328666687, 0.1470825970172882, 0.20170412957668304, 0.15081313252449036, 0.11455728113651276, 0.29461345076560974, 0.03986431658267975, 0.023470522835850716, 0.4369771182537079, 0.2866117060184479, 0.4748392105102539, 0.37243494391441345, 0.12191830575466156, 0.31557825207710266, 0.09673464298248291, 0.34432804584503174, 0.16184134781360626, 0.06151413917541504, 0.23674826323986053, 0.21198280155658722, 0.19338522851467133, 0.0055104512721300125, 0.2348119020462036, 0.33929872512817383, 0.03143832087516785], dtype='float32').reshape([28]),
            paddle.to_tensor([0.33404943346977234, 0.4667835831642151, 0.0854426920413971, 0.18558216094970703, 0.3297099471092224, 0.22731073200702667, 0.42321085929870605, 0.22973525524139404, 0.21139779686927795, 0.39679691195487976, 0.18025808036327362, 0.25149354338645935, 0.41367897391319275, 0.32293540239334106, 0.016856171190738678, 0.0539996400475502, 0.07143385708332062, 0.27873316407203674, 0.2050338089466095, 0.28245076537132263, 0.3548544943332672, 0.3158939778804779, 0.06711799651384354, 0.018092496320605278, 0.3046860098838806, 0.39186811447143555, 0.3776279091835022, 0.05936526134610176], dtype='float32').reshape([28]),
            paddle.to_tensor([0.29736506938934326, 0.2012912482023239, 0.07877262681722641, 0.28543752431869507, 0.320054829120636, 0.40225279331207275, 0.15067936480045319, 0.46511393785476685, 0.016959382221102715, 0.3870818614959717, 0.2699055075645447, 0.030455976724624634, 0.16666744649410248, 0.036828745156526566, 0.03358025103807449, 0.08174576610326767, 0.36753639578819275, 0.08910748362541199, 0.2819206416606903, 0.08514327555894852, 0.09073937684297562, 0.0767909437417984, 0.18142256140708923, 0.1670469492673874, 0.4453076422214508, 0.07705481350421906, 0.29583030939102173, 0.2659308612346649], dtype='float32').reshape([28]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bf6bb842a03f16570ef598b7fc789a32(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 1, 1], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_018c90d2406f056ca90ca40699938dc9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 480, 12, 12], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d585aca6cceb2924e36e898cb68c93c2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_af91295b59c5fca17f890a802deb001f
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 4, 16, 16], dtype='float16', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_01d361c32a0ccc44897e6a1c3387465a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 160, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9741da1500161c6c6d265764f2566383(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 75, 1, 1], dtype='float16', min=0, max=0.5),
            paddle.uniform([75], dtype='float32', min=0, max=0.5),
            paddle.uniform([75], dtype='float32', min=0, max=0.5),
            paddle.uniform([75], dtype='float32', min=0, max=0.5),
            paddle.uniform([75], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_152fd115ab4a06dff3ec0e607b85f99f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 480, 480], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7bc83fbc276de34f688755e5e9350a6d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.18659405410289764, 0.03629680722951889, 0.2143319994211197, 0.3453591763973236, 0.33797889947891235, 0.02526887319982052, 0.09593883156776428, 0.436806321144104, 0.0007963397656567395, 0.12180653214454651, 0.11912357807159424, 0.41701409220695496, 0.012425538152456284, 0.16381101310253143, 0.3010331392288208, 0.04830651730298996, 0.3675149083137512, 0.2435840368270874, 0.32316654920578003, 0.3607720732688904, 0.0507134385406971, 0.3812359571456909, 0.38620761036872864, 0.12886692583560944], dtype='float32').reshape([24]),
            paddle.to_tensor([0.23069889843463898, 0.23122215270996094, 0.012518583796918392, 0.11404285579919815, 0.47444188594818115, 0.12018825858831406, 0.486263632774353, 0.35989999771118164, 0.4583083689212799, 0.024304520338773727, 0.3457167148590088, 0.4135466516017914, 0.29361072182655334, 0.2814573645591736, 0.12035145610570908, 0.14108878374099731, 0.09390386939048767, 0.3308905363082886, 0.13233594596385956, 0.1297570914030075, 0.14279578626155853, 0.07753060013055801, 0.07693688571453094, 0.15482118725776672], dtype='float32').reshape([24]),
            paddle.to_tensor([0.41138991713523865, 0.3315090239048004, 0.3733488619327545, 0.1576889008283615, 0.23197947442531586, 0.4749694764614105, 0.12549373507499695, 0.1741439402103424, 0.18655771017074585, 0.14910228550434113, 0.2791988253593445, 0.27568382024765015, 0.3879408836364746, 0.37170273065567017, 0.37471839785575867, 0.11037469655275345, 0.45778924226760864, 0.399903804063797, 0.16056622564792633, 0.17906954884529114, 0.10313059389591217, 0.1263173669576645, 0.08006152510643005, 0.4309770464897156], dtype='float32').reshape([24]),
            paddle.to_tensor([0.09664557874202728, 0.06526639312505722, 0.43458086252212524, 0.2693490982055664, 0.2865065336227417, 0.34570178389549255, 0.4226965010166168, 0.4095163643360138, 0.3646068274974823, 0.21240000426769257, 0.06410803645849228, 0.3660205006599426, 0.28209197521209717, 0.059141017496585846, 0.022679010406136513, 0.27537038922309875, 0.28076645731925964, 0.27248916029930115, 0.06792590022087097, 0.039787352085113525, 0.10920265316963196, 0.2772050201892853, 0.41916364431381226, 0.07634775340557098], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_85e4bdf21e37fe5111aaba9002133618(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.15788091719150543, 0.3508795499801636, 0.09467359632253647, 0.02434619516134262, 0.0855463445186615, 0.156760111451149, 0.46493270993232727, 0.39720529317855835, 0.31904006004333496, 0.3031759262084961, 0.25738269090652466, 0.12698659300804138, 0.11644061654806137, 0.22346225380897522, 0.4563758969306946, 0.14998620748519897, 0.37452757358551025, 0.03358478844165802, 0.058278489857912064, 0.2889169454574585, 0.43628397583961487, 0.3140583336353302, 0.282046377658844, 0.29930657148361206], dtype='float32').reshape([24]),
            paddle.to_tensor([0.03995988145470619, 0.24211062490940094, 0.31877100467681885, 0.271573930978775, 0.01904418133199215, 0.08313663303852081, 0.3042351007461548, 0.20721203088760376, 0.4197848439216614, 0.20963993668556213, 0.09659605473279953, 0.2536642849445343, 0.00473600160330534, 0.14880934357643127, 0.08806253969669342, 0.07660847902297974, 0.1332772970199585, 0.09715630114078522, 0.2809678614139557, 0.12906675040721893, 0.3426148593425751, 0.3299294114112854, 0.3791047930717468, 0.22120855748653412], dtype='float32').reshape([24]),
            paddle.to_tensor([0.06654594838619232, 0.170172780752182, 0.291686087846756, 0.41197696328163147, 0.22742797434329987, 0.1682027280330658, 0.09087128192186356, 0.21541878581047058, 0.2691357731819153, 0.16417691111564636, 0.21010743081569672, 0.42802372574806213, 0.0863688513636589, 0.3224288821220398, 0.1593412160873413, 0.47301578521728516, 0.2353733330965042, 0.24774642288684845, 0.43778422474861145, 0.2126297801733017, 0.014676236547529697, 0.24791255593299866, 0.03070955164730549, 0.3755151927471161], dtype='float32').reshape([24]),
            paddle.to_tensor([0.04950820654630661, 0.2706904411315918, 0.03657208010554314, 0.035822536796331406, 0.2053348571062088, 0.11946326494216919, 0.34108173847198486, 0.21790741384029388, 0.16967351734638214, 0.043122798204422, 0.29549816250801086, 0.2347605675458908, 0.1619689017534256, 0.3299315571784973, 0.1833745688199997, 0.310228168964386, 0.26366448402404785, 0.36747387051582336, 0.2831515669822693, 0.06357947736978531, 0.20938068628311157, 0.42751652002334595, 0.11778010427951813, 0.23040807247161865], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_54c0cfd4e99e748dbf11f2f2a74d4d69(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 185, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([185], dtype='float32', min=0, max=0.5),
            paddle.uniform([185], dtype='float32', min=0, max=0.5),
            paddle.uniform([185], dtype='float32', min=0, max=0.5),
            paddle.uniform([185], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d132145e142c2df216d881929bdf1b19(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 200, 336], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9e103bc53eeac66739f28d309f081895(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 288, 31, 31], dtype='float16', min=0, max=0.5),
            paddle.uniform([288], dtype='float32', min=0, max=0.5),
            paddle.uniform([288], dtype='float32', min=0, max=0.5),
            paddle.uniform([288], dtype='float32', min=0, max=0.5),
            paddle.uniform([288], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_18e19a5d619c3894970f09f71abdd453(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_af91295b59c5fca17f890a802deb001f
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 32, 32, 32], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.114125557243824, 0.4854830205440521, 0.2096257507801056, 0.13036814332008362, 0.12375780194997787, 0.43848884105682373, 0.48808133602142334, 0.11122971028089523, 0.25777190923690796, 0.45025792717933655, 0.07503136247396469, 0.286948561668396, 0.34115201234817505, 0.270973801612854, 0.028887739405035973, 0.18785224854946136], dtype='float32').reshape([16]),
            paddle.to_tensor([0.43578284978866577, 0.3607334494590759, 0.207236185669899, 0.3656141459941864, 0.14659841358661652, 0.05965490639209747, 0.48728829622268677, 0.11606650054454803, 0.22249513864517212, 0.4434763789176941, 0.44783493876457214, 0.09224905073642731, 0.13477645814418793, 0.24436524510383606, 0.15505848824977875, 0.15917125344276428], dtype='float32').reshape([16]),
            paddle.to_tensor([0.07546582818031311, 0.37078776955604553, 0.4588535726070404, 0.27402839064598083, 0.22592593729496002, 0.46729615330696106, 0.04736865311861038, 0.49328431487083435, 0.2764210104942322, 0.2677370309829712, 0.4103936553001404, 0.27021700143814087, 0.03641615808010101, 0.2525809705257416, 0.3209069073200226, 0.21106766164302826], dtype='float32').reshape([16]),
            paddle.to_tensor([0.4549770653247833, 0.3046953082084656, 0.4081331789493561, 0.13410288095474243, 0.19217821955680847, 0.46195000410079956, 0.3838760256767273, 0.18365171551704407, 0.22234749794006348, 0.2951038181781769, 0.1061311662197113, 0.3200892210006714, 0.08004448562860489, 0.05379766598343849, 0.33871257305145264, 0.11549220234155655], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8290f5307cc0ab1963d8cb66795133c7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 1536, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([1536], dtype='float32', min=0, max=0.5),
            paddle.uniform([1536], dtype='float32', min=0, max=0.5),
            paddle.uniform([1536], dtype='float32', min=0, max=0.5),
            paddle.uniform([1536], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2c45114306fa6afa815848304919f2a8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.3393900692462921, 0.3388262391090393, 0.07942190766334534, 0.22774553298950195, 0.2820407748222351, 0.29989174008369446, 0.08396875113248825, 0.2535710632801056, 0.26908260583877563, 0.1155693382024765, 0.03200984746217728, 0.035218652337789536, 0.24604368209838867, 0.00895899347960949, 0.3735349178314209, 0.13591323792934418, 0.001967156073078513, 0.04887299984693527, 0.04650691896677017, 0.21965427696704865], dtype='float32').reshape([20]),
            paddle.to_tensor([0.44200602173805237, 0.4502740800380707, 0.225916787981987, 0.3340221047401428, 0.00794049073010683, 0.21958529949188232, 0.4138735234737396, 0.2783854901790619, 0.44863247871398926, 0.26086804270744324, 0.3911871910095215, 0.08733558654785156, 0.3277989625930786, 0.07406765967607498, 0.2546151578426361, 0.3243257403373718, 0.41925856471061707, 0.1888410747051239, 0.39500200748443604, 0.059550873935222626], dtype='float32').reshape([20]),
            paddle.to_tensor([0.013604477047920227, 0.24619808793067932, 0.2435893714427948, 0.3136483430862427, 0.24625632166862488, 0.3366907835006714, 0.47454869747161865, 0.13048335909843445, 0.4751063883304596, 0.40458032488822937, 0.013241177424788475, 0.43362173438072205, 0.44538459181785583, 0.30068260431289673, 0.25720375776290894, 0.3974987864494324, 0.07354407012462616, 0.001562756486237049, 0.3318425118923187, 0.29239824414253235], dtype='float32').reshape([20]),
            paddle.to_tensor([0.179845929145813, 0.16627326607704163, 0.3300633728504181, 0.25882187485694885, 0.186519593000412, 0.10501028597354889, 0.24036754667758942, 0.3894607126712799, 0.1858811378479004, 0.11451524496078491, 0.19104169309139252, 0.08755571395158768, 0.4437141716480255, 0.202797532081604, 0.39961737394332886, 0.1140284463763237, 0.4265117645263672, 0.4246911108493805, 0.3100816309452057, 0.05597519502043724], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_56ba344d63318f77354f22823329049c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 10, 10], dtype='float16', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4413f2bfa67e8c8099a3db9fb1e17aea(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.4577527642250061, 0.3372310698032379, 0.3208444118499756, 0.0843098983168602, 0.05528125911951065, 0.0037000859156250954, 0.1242104098200798, 0.08514466881752014, 0.4763590395450592, 0.16854876279830933, 0.01990431733429432, 0.4364660382270813, 0.10345721244812012, 0.44449111819267273, 0.21996274590492249, 0.04569253697991371, 0.46942993998527527, 0.36120766401290894, 0.16633319854736328, 0.37981387972831726, 0.07172677665948868, 0.3208087086677551, 0.49026376008987427, 0.3223631680011749, 0.21771851181983948, 0.0013868592213839293, 0.44343599677085876, 0.03497037664055824, 0.2649887800216675, 0.07308541983366013], dtype='float32').reshape([30]),
            paddle.to_tensor([0.3559046983718872, 0.03106006048619747, 0.27021604776382446, 0.06500060111284256, 0.2548217177391052, 0.01736469939351082, 0.19790001213550568, 0.01718398928642273, 0.13127566874027252, 0.2510372996330261, 0.48000380396842957, 0.4260639548301697, 0.22764113545417786, 0.4172035753726959, 0.01515568420290947, 0.3985373079776764, 0.396241158246994, 0.16105283796787262, 0.03257499635219574, 0.010083645582199097, 0.2888050079345703, 0.23669974505901337, 0.12802693247795105, 0.11595942080020905, 0.04890697821974754, 0.2801388204097748, 0.08538449555635452, 0.13094748556613922, 0.12119536101818085, 0.3844210207462311], dtype='float32').reshape([30]),
            paddle.to_tensor([0.16969722509384155, 0.45594140887260437, 0.24187694489955902, 0.05031588673591614, 0.0972483903169632, 0.14506517350673676, 0.05535920336842537, 0.1572759747505188, 0.2947603166103363, 0.04322848841547966, 0.2860631048679352, 0.1080021783709526, 0.46845516562461853, 0.09455391764640808, 0.32695600390434265, 0.2852799892425537, 0.4836895167827606, 0.3158441185951233, 0.14454491436481476, 0.4183730185031891, 0.19515615701675415, 0.4733825623989105, 0.32881832122802734, 0.06918854266405106, 0.3663780093193054, 0.08432583510875702, 0.10502544045448303, 0.09275277704000473, 0.18701191246509552, 0.36994507908821106], dtype='float32').reshape([30]),
            paddle.to_tensor([0.08713789284229279, 0.47033968567848206, 0.4395045042037964, 0.44220998883247375, 0.44126734137535095, 0.19148387014865875, 0.3258768320083618, 0.12535659968852997, 0.494975745677948, 0.2068002074956894, 0.1558862030506134, 0.14766660332679749, 0.3835045397281647, 0.42042359709739685, 0.4699355363845825, 0.0341758206486702, 0.038366567343473434, 0.17998528480529785, 0.2824552655220032, 0.1861828863620758, 0.47810497879981995, 0.20900453627109528, 0.13305151462554932, 0.48749789595603943, 0.13135525584220886, 0.14739583432674408, 0.0021590865217149258, 0.03476182371377945, 0.08724593371152878, 0.16660942137241364], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_03b304ea015102d5c344dce761f77e2a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_5e7cb23119f65b33bdbad71952f6dd12
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 128, 128], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a520a0f5a5033c015f96363c4fff598c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 240, 240], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.15780720114707947, 0.48819997906684875, 0.09101609885692596, 0.12542937695980072, 0.15585961937904358, 0.16905832290649414, 0.3829984664916992, 0.16553403437137604, 0.111152283847332, 0.07036982476711273, 0.4720243215560913, 0.047032032161951065, 0.2718449831008911, 0.11581042408943176, 0.2346470057964325, 0.3951854109764099], dtype='float32').reshape([16]),
            paddle.to_tensor([0.2139454036951065, 0.45486733317375183, 0.0631176009774208, 0.19527149200439453, 0.04791705310344696, 0.36493006348609924, 0.2318398505449295, 0.4521031677722931, 0.4088461995124817, 0.2364354133605957, 0.0350506529211998, 0.34255513548851013, 0.05678589642047882, 0.12031543254852295, 0.3507777154445648, 0.3477412462234497], dtype='float32').reshape([16]),
            paddle.to_tensor([0.08948279917240143, 0.16300594806671143, 0.1434856504201889, 0.020972851663827896, 0.4404192268848419, 0.1913590282201767, 0.48525506258010864, 0.25063782930374146, 0.12843440473079681, 0.16027092933654785, 0.47617846727371216, 0.10407168418169022, 0.03947976231575012, 0.35069549083709717, 0.4936457574367523, 0.01780129037797451], dtype='float32').reshape([16]),
            paddle.to_tensor([0.18533366918563843, 0.30121153593063354, 0.2641516327857971, 0.13940000534057617, 0.11302049458026886, 0.1909816712141037, 0.33292481303215027, 0.1774769276380539, 0.1653730273246765, 0.36051487922668457, 0.026537815108895302, 0.1377873569726944, 0.13918232917785645, 0.14937280118465424, 0.298691987991333, 0.2441306859254837], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_30877f4063a08621273c9c9a5be40654(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 120, 120], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f1fec932848472eef5cc919be5c3758c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a8301b810bccba10e4dfdee0df73334
    def get_inputs(self):
        return [
            paddle.uniform([1, 240, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_523959420b2303693f728d445dac6d32(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 1, 1], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6ba9498368459996ec19f6bf11ea615f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.22050030529499054, 0.3671926259994507, 0.3958962559700012, 0.03174429014325142, 0.2586764991283417, 0.18003827333450317, 0.09428568184375763, 0.18818604946136475, 0.27987024188041687, 0.3605958819389343, 0.0047078863717615604, 0.32240456342697144, 0.007685920223593712, 0.2781139314174652, 0.3705100417137146, 0.07604477554559708, 0.3137524425983429, 0.23390863835811615, 0.4979064166545868, 0.31893330812454224, 0.10535436123609543, 0.31570807099342346, 0.2683057188987732, 0.11258439719676971, 0.04253170266747475, 0.4247582256793976, 0.11919809877872467, 0.024730021134018898, 0.25608760118484497, 0.30767950415611267], dtype='float32').reshape([30]),
            paddle.to_tensor([0.007244510110467672, 0.20404011011123657, 0.3321455121040344, 0.07140690833330154, 0.41671693325042725, 0.008985323831439018, 0.28867170214653015, 0.13970111310482025, 0.20704972743988037, 0.15811733901500702, 0.016453297808766365, 0.4504226744174957, 0.4508401155471802, 0.12153155356645584, 0.22999699413776398, 0.31507641077041626, 0.3212212920188904, 0.18016229569911957, 0.07472847402095795, 0.05007390305399895, 0.06298872083425522, 0.4006333649158478, 0.04105037823319435, 0.11255033314228058, 0.31806111335754395, 0.302214652299881, 0.2561933696269989, 0.4750291705131531, 0.0831935852766037, 0.36524665355682373], dtype='float32').reshape([30]),
            paddle.to_tensor([0.27899426221847534, 0.07714927196502686, 0.24415040016174316, 0.17514294385910034, 0.10560896247625351, 0.4265347719192505, 0.21806615591049194, 0.30032065510749817, 0.39363664388656616, 0.37906399369239807, 0.12128915637731552, 0.16666831076145172, 0.21911072731018066, 0.24695518612861633, 0.24012871086597443, 0.44125574827194214, 0.2898350656032562, 0.19150196015834808, 0.35966169834136963, 0.04958784207701683, 0.046853020787239075, 0.11958404630422592, 0.2901526391506195, 0.2729218900203705, 0.3591463565826416, 0.31354820728302, 0.4189304709434509, 0.24385011196136475, 0.20972076058387756, 0.34541797637939453], dtype='float32').reshape([30]),
            paddle.to_tensor([0.41054391860961914, 0.2933647334575653, 0.21819530427455902, 0.30983778834342957, 0.08664628863334656, 0.22815191745758057, 0.10585109889507294, 0.289359450340271, 0.28195446729660034, 0.29599204659461975, 0.04083501175045967, 0.03485802561044693, 0.12673154473304749, 0.2684904932975769, 0.19212640821933746, 0.009883476421236992, 0.38716286420822144, 0.2552385628223419, 0.04993973672389984, 0.3536730706691742, 0.4823920726776123, 0.3436894714832306, 0.30950021743774414, 0.08117291331291199, 0.4249170422554016, 0.41286030411720276, 0.22459155321121216, 0.03648703917860985, 0.14414994418621063, 0.03424256294965744], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9b453f3e419c4743f3b9c0ac9035dafc(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 28, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.1283276379108429, 0.3226843774318695, 0.48695066571235657, 0.13400979340076447, 0.46686646342277527, 0.3729768991470337, 0.3689449429512024, 0.4932597577571869, 0.10041507333517075, 0.17438746988773346, 0.2543770968914032, 0.30401113629341125, 0.4242132008075714, 0.47996291518211365, 0.13294513523578644, 0.13683077692985535, 0.47654634714126587, 0.01971309259533882, 0.14319124817848206, 0.017703641206026077, 0.3434208631515503, 0.43045833706855774, 0.38596099615097046, 0.4854184687137604, 0.2714529037475586, 0.46599167585372925, 0.053662870079278946, 0.3330203890800476], dtype='float32').reshape([28]),
            paddle.to_tensor([0.24252836406230927, 0.3504610061645508, 0.2122430056333542, 0.20719014108181, 0.2239374965429306, 0.14831465482711792, 0.3470367193222046, 0.28455981612205505, 0.4040951132774353, 0.46919089555740356, 0.4701363146305084, 0.02157479338347912, 0.19923366606235504, 0.09453103691339493, 0.282856822013855, 0.10647266358137131, 0.044188205152750015, 0.08981247246265411, 0.16261257231235504, 0.3854384422302246, 0.3360549807548523, 0.4081093668937683, 0.44560083746910095, 0.2484002709388733, 0.35310760140419006, 0.3267250061035156, 0.4664863049983978, 0.1956745982170105], dtype='float32').reshape([28]),
            paddle.to_tensor([0.23618891835212708, 0.04072863981127739, 0.290251225233078, 0.24272531270980835, 0.15458810329437256, 0.0024890764616429806, 0.054676640778779984, 0.2818641662597656, 0.4038761258125305, 0.3351127505302429, 0.36978164315223694, 0.4114375114440918, 0.34756922721862793, 0.2960200309753418, 0.09249842911958694, 0.28218019008636475, 0.17837367951869965, 0.03767392784357071, 0.4407725930213928, 0.2100331038236618, 0.08707057684659958, 0.4398488700389862, 0.10458250343799591, 0.2534412443637848, 0.16842150688171387, 0.04548536241054535, 0.20759552717208862, 0.13109208643436432], dtype='float32').reshape([28]),
            paddle.to_tensor([0.4032537639141083, 0.48633894324302673, 0.36696964502334595, 0.1576785147190094, 0.2454216629266739, 0.12524719536304474, 0.38452956080436707, 0.22805872559547424, 0.15656599402427673, 0.24161572754383087, 0.2467762678861618, 0.004506982397288084, 0.25949525833129883, 0.4674016237258911, 0.1348506361246109, 0.45936453342437744, 0.1640263944864273, 0.08678247779607773, 0.3143314719200134, 0.387423038482666, 0.1532387137413025, 0.04605317860841751, 0.09596926718950272, 0.43002817034721375, 0.04643852263689041, 0.11113464832305908, 0.3819144368171692, 0.46931785345077515], dtype='float32').reshape([28]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e292c3f686aba07e55502a3a4b4e216a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 608, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([608], dtype='float32', min=0, max=0.5),
            paddle.uniform([608], dtype='float32', min=0, max=0.5),
            paddle.uniform([608], dtype='float32', min=0, max=0.5),
            paddle.uniform([608], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3e1776a35b304beb3665e9338c946bb3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.3651144206523895, 0.0882391482591629, 0.36797577142715454, 0.01141933910548687, 0.13116499781608582, 0.23055769503116608, 0.49951601028442383, 0.2985590100288391, 0.3163812756538391, 0.30161160230636597, 0.302583783864975, 0.05916013941168785, 0.05721502751111984, 0.4786120057106018, 0.07772421091794968, 0.357241153717041], dtype='float32').reshape([16]),
            paddle.to_tensor([0.21360594034194946, 0.3065153956413269, 0.47093501687049866, 0.42402008175849915, 0.24202416837215424, 0.24126297235488892, 0.2158903330564499, 0.09016832709312439, 0.23954501748085022, 0.45098504424095154, 0.3051846921443939, 0.04447971284389496, 0.42048168182373047, 0.3681279718875885, 0.05884617939591408, 0.17456623911857605], dtype='float32').reshape([16]),
            paddle.to_tensor([0.23221458494663239, 0.10512010753154755, 0.3951946198940277, 0.20363427698612213, 0.2642157971858978, 0.3445555865764618, 0.3884691894054413, 0.327589213848114, 0.305016428232193, 0.05111880227923393, 0.20212630927562714, 0.3117969334125519, 0.4190014600753784, 0.39845845103263855, 0.20968852937221527, 0.2722732722759247], dtype='float32').reshape([16]),
            paddle.to_tensor([0.3233183026313782, 0.1028774306178093, 0.017447758466005325, 0.2822291851043701, 0.10676706582307816, 0.4433262348175049, 0.28246909379959106, 0.25965046882629395, 0.25281065702438354, 0.11877944320440292, 0.0867767259478569, 0.07454802840948105, 0.18636341392993927, 0.4384312629699707, 0.1617601364850998, 0.43942320346832275], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4f81eb475f9ceb8fd6358ef73a947efb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 16, 16], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e3998578c3662eed154471ae3513e67f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_fc6b6917e04b7b7cf9fdb39dde97ec40
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 20, 20], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_768526d9576df15012bea1298cbcff0a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 80, 80], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.38944709300994873, 0.3280973732471466, 0.28399530053138733, 0.2092728614807129, 0.39446985721588135, 0.03772154450416565, 0.32180795073509216, 0.0012912438251078129, 0.12299682945013046, 0.2137710601091385, 0.43707796931266785, 0.4639143645763397, 0.2374630570411682, 0.28408393263816833, 0.4190574586391449, 0.321883887052536, 0.435223788022995, 0.41037145256996155, 0.18819913268089294, 0.2425873726606369, 0.041497062891721725, 0.463926762342453, 0.35645967721939087, 0.022079555317759514], dtype='float32').reshape([24]),
            paddle.to_tensor([0.21313080191612244, 0.2804737091064453, 0.22908319532871246, 0.36770495772361755, 0.18732869625091553, 0.3765239715576172, 0.2039775848388672, 0.03782902657985687, 0.2978714108467102, 0.1900061070919037, 0.086089588701725, 0.4063570499420166, 0.45568785071372986, 0.14996793866157532, 0.1898646205663681, 0.4779602885246277, 0.019319869577884674, 0.16919298470020294, 0.2004317194223404, 0.41983917355537415, 0.3560016453266144, 0.43654459714889526, 0.42466628551483154, 0.18148928880691528], dtype='float32').reshape([24]),
            paddle.to_tensor([0.2514045536518097, 0.2703879773616791, 0.05655166506767273, 0.4384765923023224, 0.122600257396698, 0.46454399824142456, 0.44668301939964294, 0.04631069302558899, 0.2733793258666992, 0.4106106162071228, 0.31871217489242554, 0.06445533782243729, 0.3858126699924469, 0.45146122574806213, 0.4491504430770874, 0.41306453943252563, 0.11381332576274872, 0.34212997555732727, 0.36316153407096863, 0.4125382602214813, 0.3167121410369873, 0.23662589490413666, 0.43855321407318115, 0.37932267785072327], dtype='float32').reshape([24]),
            paddle.to_tensor([0.2893454134464264, 0.004025190137326717, 0.36044198274612427, 0.0844312459230423, 0.373089998960495, 0.48064348101615906, 0.08215950429439545, 0.22345887124538422, 0.29385408759117126, 0.2553630769252777, 0.1001133844256401, 0.29437845945358276, 0.2591431438922882, 0.24397891759872437, 0.09054266661405563, 0.4078838527202606, 0.45849841833114624, 0.34963110089302063, 0.49850472807884216, 0.028750158846378326, 0.28257641196250916, 0.3857893645763397, 0.1918763667345047, 0.13625489175319672], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_696ac6583a0c789762ac239e1e0c7431(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 76, 76], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_72a56ffb44843cc0c3b1967cb6ffde03(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_d589af777f5a8ae699611515c41499fb
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 160, 160], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e37710a031515f2d119a0501331ae10d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 1776, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([1776], dtype='float32', min=0, max=0.5),
            paddle.uniform([1776], dtype='float32', min=0, max=0.5),
            paddle.uniform([1776], dtype='float32', min=0, max=0.5),
            paddle.uniform([1776], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e25acc85d7cbc5d5fca017c34d4fd7f3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 48, 160], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_810283540631737518ee8d40fd1652a7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 1200, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([1200], dtype='float32', min=0, max=0.5),
            paddle.uniform([1200], dtype='float32', min=0, max=0.5),
            paddle.uniform([1200], dtype='float32', min=0, max=0.5),
            paddle.uniform([1200], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d191a054145823f5d69526872265eb77(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_051c3b6e28e9241544f10fe3f9bd7276(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 144, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_533b48e0a2606fa61cfb2a15727e4d92(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 82, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([82], dtype='float32', min=0, max=0.5),
            paddle.uniform([82], dtype='float32', min=0, max=0.5),
            paddle.uniform([82], dtype='float32', min=0, max=0.5),
            paddle.uniform([82], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4da988e4382ce76a787669bfaebcadfa(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 256, 512], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.44577115774154663, 0.39566829800605774, 0.22731250524520874, 0.39593222737312317, 0.14390532672405243, 0.47030001878738403, 0.03866826370358467, 0.3578156530857086, 0.41620713472366333, 0.3590887486934662, 0.04807818681001663, 0.40557682514190674, 0.45385900139808655, 0.008726567029953003, 0.42733967304229736, 0.21735180914402008], dtype='float32').reshape([16]),
            paddle.to_tensor([0.012486400082707405, 0.3401853144168854, 0.29439935088157654, 0.3749031126499176, 0.4124242961406708, 0.1084800660610199, 0.3113930821418762, 0.2922774851322174, 0.2611250579357147, 0.02675182744860649, 0.0649852305650711, 0.47797122597694397, 0.32001256942749023, 0.15052832663059235, 0.46016812324523926, 0.1669343113899231], dtype='float32').reshape([16]),
            paddle.to_tensor([0.3423815369606018, 0.1496453583240509, 0.4379251003265381, 0.32544806599617004, 0.2678588032722473, 0.45065200328826904, 0.36952802538871765, 0.3235076367855072, 0.3213295042514801, 0.22467893362045288, 0.08086381107568741, 0.2789789140224457, 0.36219051480293274, 0.22784622013568878, 0.23766908049583435, 0.35998860001564026], dtype='float32').reshape([16]),
            paddle.to_tensor([0.42039862275123596, 0.3250035345554352, 0.2256115972995758, 0.08171594142913818, 0.4213009476661682, 0.3814806342124939, 0.21280638873577118, 0.04354841262102127, 0.41531863808631897, 0.48589465022087097, 0.0194315817207098, 0.33512789011001587, 0.27960529923439026, 0.11501504480838776, 0.23734334111213684, 0.2991483509540558], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3caf77e370b9f4872b4e2fa9edfc8afb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.4312146008014679, 0.4742890000343323, 0.43163055181503296, 0.22694553434848785, 0.131358802318573, 0.28430864214897156, 0.474852979183197, 0.3675721287727356, 0.14836707711219788, 0.42648470401763916, 0.22329849004745483, 0.4328728914260864, 0.11405447125434875, 0.4658973217010498, 0.05251690745353699, 0.3976726233959198], dtype='float32').reshape([16]),
            paddle.to_tensor([0.2097080796957016, 0.4800876975059509, 0.3373163640499115, 0.2708302140235901, 0.3571700155735016, 0.3537500500679016, 0.34798455238342285, 0.3079121708869934, 0.39670753479003906, 0.12406250834465027, 0.4753083884716034, 0.14500491321086884, 0.3755010962486267, 0.18257202208042145, 0.3185714781284332, 0.0562412329018116], dtype='float32').reshape([16]),
            paddle.to_tensor([0.002688902895897627, 0.03025159239768982, 0.4274826645851135, 0.3781077265739441, 0.3297080397605896, 0.20209182798862457, 0.2855003774166107, 0.07376676052808762, 0.15445926785469055, 0.2421996146440506, 0.029856273904442787, 0.12785673141479492, 0.3139854073524475, 0.18222588300704956, 0.43780630826950073, 0.40398135781288147], dtype='float32').reshape([16]),
            paddle.to_tensor([0.4998985528945923, 0.2091922163963318, 0.04142843931913376, 0.1663609892129898, 0.22759810090065002, 0.40037673711776733, 0.18996724486351013, 0.05855124816298485, 0.23489783704280853, 0.11639908701181412, 0.06853991746902466, 0.06243191659450531, 0.379476934671402, 0.4811037480831146, 0.43028438091278076, 0.25277435779571533], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_230d2719f862e7eb2b0d6a05fd139f65(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 320, 4, 3], dtype='float16', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_80a922d01c51b42428c4a611167a7986(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 16, 50], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6604ca43e007a966a0b18784d6699c6e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 10, 10], dtype='float16', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_972534663799a28f3f84ce6eac5c4db2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 120, 60, 60], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ccba862fa9c2ecd5256194d5f8ff2b8d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 2, 50], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b2d8d882ee010ee619c03dc47f378b5e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 576, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([576], dtype='float32', min=0, max=0.5),
            paddle.uniform([576], dtype='float32', min=0, max=0.5),
            paddle.uniform([576], dtype='float32', min=0, max=0.5),
            paddle.uniform([576], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6f39a2ba7999290be4197b59d3de5012(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.41347208619117737, 0.29383349418640137, 0.41602078080177307, 0.43893104791641235, 0.459938108921051, 0.3554992079734802, 0.084181047976017, 0.036860838532447815, 0.3954058587551117, 0.23203875124454498, 0.288729190826416, 0.1533457487821579, 0.2969277799129486, 0.2553754448890686, 0.3427737355232239, 0.21690958738327026, 0.1971326619386673, 0.3754446804523468, 0.3193955719470978, 0.3661932349205017], dtype='float32').reshape([20]),
            paddle.to_tensor([0.41227272152900696, 0.47602951526641846, 0.3800518810749054, 0.03468501940369606, 0.35515648126602173, 0.00913701020181179, 0.2521088421344757, 0.37171852588653564, 0.4185291528701782, 0.39886683225631714, 0.09738162904977798, 0.35932493209838867, 0.05903603509068489, 0.0956546738743782, 0.15837040543556213, 0.10992621630430222, 0.26300615072250366, 0.024076182395219803, 0.040789905935525894, 0.4828060567378998], dtype='float32').reshape([20]),
            paddle.to_tensor([0.08858732879161835, 0.19687297940254211, 0.016358718276023865, 0.4563679099082947, 0.09211448580026627, 0.44061633944511414, 0.36562225222587585, 0.08755926787853241, 0.49286141991615295, 0.34655076265335083, 0.4811857342720032, 0.20228904485702515, 0.26951679587364197, 0.28206202387809753, 0.2903507649898529, 0.17443598806858063, 0.312032014131546, 0.18654990196228027, 0.03600037470459938, 0.40801987051963806], dtype='float32').reshape([20]),
            paddle.to_tensor([0.11733990907669067, 0.09778156131505966, 0.4706839323043823, 0.2127319574356079, 0.0325450599193573, 0.16823457181453705, 0.3189295828342438, 0.2299555391073227, 0.29640907049179077, 0.05303497239947319, 0.11371637135744095, 0.3129447400569916, 0.30526232719421387, 0.22804908454418182, 0.3351592421531677, 0.4790705740451813, 0.4333845376968384, 0.2144872099161148, 0.19238825142383575, 0.41160258650779724], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6465c79ca6acfd43baa242b224839b21(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 64, 128], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.3847651481628418, 0.19944427907466888, 0.307864785194397, 0.030473096296191216, 0.16678059101104736, 0.10649072378873825, 0.25500211119651794, 0.06649942696094513, 0.15777096152305603, 0.19599264860153198, 0.18537463247776031, 0.08077847957611084, 0.33211347460746765, 0.24179363250732422, 0.20318567752838135, 0.44100284576416016, 0.0660051628947258, 0.19208082556724548], dtype='float32').reshape([18]),
            paddle.to_tensor([0.12517736852169037, 0.1375611275434494, 0.4132956862449646, 0.11723282188177109, 0.11799471825361252, 0.008988922461867332, 0.08454054594039917, 0.45831331610679626, 0.35888853669166565, 0.48383548855781555, 0.29849374294281006, 0.3154946267604828, 0.05578823387622833, 0.42856988310813904, 0.33178991079330444, 0.39133772253990173, 0.3696206212043762, 0.4136086702346802], dtype='float32').reshape([18]),
            paddle.to_tensor([0.12125828117132187, 0.1927914172410965, 0.2999873757362366, 0.024059871211647987, 0.3278735876083374, 0.26028186082839966, 0.1985173225402832, 0.1476614773273468, 0.13350717723369598, 0.06304993480443954, 0.44443279504776, 0.12589377164840698, 0.2477579116821289, 0.2405870258808136, 0.3298794627189636, 0.1203274130821228, 0.36652374267578125, 0.4072315990924835], dtype='float32').reshape([18]),
            paddle.to_tensor([0.1734778732061386, 0.40679436922073364, 0.4941641688346863, 0.1252550184726715, 0.1131853461265564, 0.38442888855934143, 0.255487322807312, 0.3645707666873932, 0.4606661796569824, 0.44739848375320435, 0.22072316706180573, 0.04860997200012207, 0.17456413805484772, 0.3929753005504608, 0.29720577597618103, 0.11293014138936996, 0.20914332568645477, 0.09372926503419876], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a15883703d87ba481bcb6cc6d8edebea(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.4011761248111725, 0.32011938095092773, 0.17844592034816742, 0.397501140832901, 0.3676236867904663, 0.33079463243484497, 0.10719772428274155, 0.44050702452659607, 0.34283334016799927, 0.39370137453079224, 0.464514821767807, 0.242221400141716, 0.2713748514652252, 0.034871701151132584, 0.222614586353302, 0.03861844539642334, 0.391266405582428, 0.4114864766597748, 0.4098804295063019, 0.43649959564208984], dtype='float32').reshape([20]),
            paddle.to_tensor([0.3733709454536438, 0.3414372205734253, 0.07970110327005386, 0.49446672201156616, 0.17035363614559174, 0.053231995552778244, 0.4300868809223175, 0.4103536009788513, 0.4115731418132782, 0.12944278120994568, 0.3590546250343323, 0.356656551361084, 0.19665639102458954, 0.3415619432926178, 0.09604714065790176, 0.39694368839263916, 0.26726916432380676, 0.21196788549423218, 0.3015221655368805, 0.37787580490112305], dtype='float32').reshape([20]),
            paddle.to_tensor([0.37117844820022583, 0.32755348086357117, 0.3510424792766571, 0.2912486493587494, 0.46269491314888, 0.46982908248901367, 0.005672322120517492, 0.19818827509880066, 0.158501535654068, 0.39230072498321533, 0.2776573896408081, 0.2788447141647339, 0.022699987515807152, 0.4327475428581238, 0.4266923666000366, 0.12924975156784058, 0.3903769254684448, 0.010401209816336632, 0.08650155365467072, 0.10681671649217606], dtype='float32').reshape([20]),
            paddle.to_tensor([0.2667631506919861, 0.44278544187545776, 0.24270372092723846, 0.1932862102985382, 0.4965023696422577, 0.3547614812850952, 0.23486141860485077, 0.3629063069820404, 0.4084583818912506, 0.23703524470329285, 0.4302549958229065, 0.3946177661418915, 0.3902589976787567, 0.12899915874004364, 0.23104353249073029, 0.272058367729187, 0.09303895384073257, 0.03264198452234268, 0.41951805353164673, 0.22446833550930023], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_78743eead2ce001a83c9b6764d1ed72c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_da4d85bc213f70819a72e68f21214cdb
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 32, 32, 32], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.43973150849342346, 0.09756865352392197, 0.22590458393096924, 0.34013330936431885, 0.06174638494849205, 0.44083693623542786, 0.16732190549373627, 0.2806962728500366, 0.13772280514240265, 0.10458280891180038, 0.4773227274417877, 0.007323093246668577, 0.44435396790504456, 0.3604944348335266, 0.04371915012598038, 0.21460264921188354], dtype='float32').reshape([16]),
            paddle.to_tensor([0.28775283694267273, 0.35427969694137573, 0.04457315802574158, 0.2523854970932007, 0.49093127250671387, 0.45954445004463196, 0.4884795546531677, 0.1674412339925766, 0.10592547804117203, 0.25109875202178955, 0.29337212443351746, 0.32168522477149963, 0.2773301899433136, 0.46232423186302185, 0.3957207500934601, 0.1185346320271492], dtype='float32').reshape([16]),
            paddle.to_tensor([0.19594134390354156, 0.14303164184093475, 0.31280961632728577, 0.09381533414125443, 0.4093624949455261, 0.47668370604515076, 0.45448359847068787, 0.31661251187324524, 0.28309980034828186, 0.4522305727005005, 0.07574841380119324, 0.47647422552108765, 0.000654770468827337, 0.43068429827690125, 0.03252711892127991, 0.4816611111164093], dtype='float32').reshape([16]),
            paddle.to_tensor([0.4499501883983612, 0.14277854561805725, 0.06784415245056152, 0.49609726667404175, 0.34238380193710327, 0.17203688621520996, 0.04511570185422897, 0.31823477149009705, 0.49655547738075256, 0.2357196807861328, 0.48374608159065247, 0.08198010921478271, 0.47624218463897705, 0.4883015751838684, 0.04117606207728386, 0.31314000487327576], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9806e63237171a6d0501b8d59308387f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 32, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b2ae34095d578abb1415e0733faed9d7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_f5f2213be92f20fda5378beeed72688f
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 2, 2], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_11e0bdaed2841d89b397581dce666cf7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 16, 16], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_834df3c50ffacdf5db56a6ab82ca7bc8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 672, 6, 6], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_78b00ddf8bf9622c92dd4c8689f5a1b0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 160, 160], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.11032833158969879, 0.10924028605222702, 0.24417898058891296, 0.4272798001766205, 0.1847052425146103, 0.08446477353572845, 0.042694900184869766, 0.38502582907676697, 0.0336170494556427, 0.3935771584510803, 0.1963050365447998, 0.3619960844516754, 0.44662320613861084, 0.10762548446655273, 0.22089453041553497, 0.2578562796115875, 0.22767093777656555, 0.34047868847846985, 0.38928014039993286, 0.08610205352306366, 0.029785271733999252, 0.29508283734321594, 0.15259897708892822, 0.19255965948104858], dtype='float32').reshape([24]),
            paddle.to_tensor([0.131862074136734, 0.028983458876609802, 0.3508981764316559, 0.40163466334342957, 0.1063859611749649, 0.2829912602901459, 0.27964743971824646, 0.04783313721418381, 0.1038016602396965, 0.28473687171936035, 0.3103012442588806, 0.18719345331192017, 0.1298651397228241, 0.494701087474823, 0.2825848460197449, 0.372779905796051, 0.32917943596839905, 0.012616410851478577, 0.3896331489086151, 0.46025902032852173, 0.2952137291431427, 0.4913910925388336, 0.4514574110507965, 0.039840634912252426], dtype='float32').reshape([24]),
            paddle.to_tensor([0.3220590054988861, 0.3772773742675781, 0.37320220470428467, 0.4532165825366974, 0.35782819986343384, 0.08679031580686569, 0.1906733512878418, 0.12410801649093628, 0.17742495238780975, 0.3599630296230316, 0.011366024613380432, 0.30099672079086304, 0.18767547607421875, 0.19552640616893768, 0.37378180027008057, 0.2884465754032135, 0.0960407555103302, 0.2185019552707672, 0.17755581438541412, 0.024281103163957596, 0.36955463886260986, 0.4252089560031891, 0.44646215438842773, 0.11705300956964493], dtype='float32').reshape([24]),
            paddle.to_tensor([0.2156875729560852, 0.23727744817733765, 0.26921844482421875, 0.4826906621456146, 0.47398820519447327, 0.18334129452705383, 0.3429417312145233, 0.3447273075580597, 0.11055729538202286, 0.44694194197654724, 0.04929821193218231, 0.04822696000337601, 0.240894615650177, 0.10438025742769241, 0.05256946012377739, 0.4832445979118347, 0.3234318494796753, 0.48239865899086, 0.02815179154276848, 0.2394094318151474, 0.07108020782470703, 0.31159332394599915, 0.05511998012661934, 0.33859631419181824], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fd0195fe5d6da2df0afc396153b8badf(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 40, 40], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.2671753466129303, 0.2985212504863739, 0.0786450132727623, 0.23035120964050293, 0.22255849838256836, 0.10668382048606873, 0.06628144532442093, 0.16823405027389526, 0.483241468667984, 0.26926636695861816, 0.08446669578552246, 0.05488164350390434, 0.09956527501344681, 0.37832963466644287, 0.2133396416902542, 0.3586159646511078, 0.26278039813041687, 0.35373052954673767, 0.09903550893068314, 0.20654122531414032, 0.2867748439311981, 0.05136852711439133, 0.18178188800811768, 0.38147425651550293], dtype='float32').reshape([24]),
            paddle.to_tensor([0.11919907480478287, 0.31779545545578003, 0.012063373811542988, 0.09392610937356949, 0.18014048039913177, 0.1431921422481537, 0.06605177372694016, 0.4093007743358612, 0.31674882769584656, 0.27368468046188354, 0.1503077745437622, 0.11430837959051132, 0.21069781482219696, 0.232119619846344, 0.29591771960258484, 0.33618301153182983, 0.31492093205451965, 0.48226025700569153, 0.04133498668670654, 0.29891011118888855, 0.2505962550640106, 0.2508977949619293, 0.44456395506858826, 0.44332537055015564], dtype='float32').reshape([24]),
            paddle.to_tensor([0.3549460470676422, 0.036604609340429306, 0.057025469839572906, 0.05867517367005348, 0.07622529566287994, 0.2784925103187561, 0.48624491691589355, 0.3409838378429413, 0.4939950704574585, 0.11060456186532974, 0.22019140422344208, 0.3640320897102356, 0.45464688539505005, 0.12195713073015213, 0.0008983819861896336, 0.04416680335998535, 0.12057271599769592, 0.4897483289241791, 0.4687597155570984, 0.2633667588233948, 0.24716269969940186, 0.46081969141960144, 0.23111429810523987, 0.239042729139328], dtype='float32').reshape([24]),
            paddle.to_tensor([0.2630873918533325, 0.11924484372138977, 0.26441842317581177, 0.19894158840179443, 0.1443178653717041, 0.2597549855709076, 0.49944278597831726, 0.2672204077243805, 0.40113893151283264, 0.30287355184555054, 0.09255538880825043, 0.43790721893310547, 0.33258485794067383, 0.07046422362327576, 0.33219048380851746, 0.4182090759277344, 0.09464473277330399, 0.3816259503364563, 0.22024044394493103, 0.3608449697494507, 0.23974773287773132, 0.39168238639831543, 0.1322517693042755, 0.013060728088021278], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e7a6eb8cc3dd0f5d4932a530a9806fc6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.061834514141082764, 0.22619196772575378, 0.08023224771022797, 0.10192368179559708, 0.32356172800064087, 0.059832245111465454, 0.3643089830875397, 0.23056453466415405, 0.005835066549479961, 0.26819342374801636, 0.09312686324119568, 0.004118580371141434, 0.3015420138835907, 0.12535825371742249, 0.26275601983070374, 0.2553333044052124, 0.48251378536224365, 0.29689475893974304, 0.39388638734817505, 0.06594567000865936], dtype='float32').reshape([20]),
            paddle.to_tensor([0.17915324866771698, 0.4579066336154938, 0.3988032639026642, 0.07428982108831406, 0.391898512840271, 0.46277785301208496, 0.09142760932445526, 0.023104161024093628, 0.30547043681144714, 0.34575098752975464, 0.17233887314796448, 0.006078898441046476, 0.23776564002037048, 0.24358518421649933, 0.4637078046798706, 0.29792290925979614, 0.12168560922145844, 0.014331714250147343, 0.28648829460144043, 0.0667475089430809], dtype='float32').reshape([20]),
            paddle.to_tensor([0.17633455991744995, 0.38724568486213684, 0.002033129334449768, 0.3147105574607849, 0.23004429042339325, 0.45280328392982483, 0.39832210540771484, 0.047024261206388474, 0.29280078411102295, 0.35667872428894043, 0.196747824549675, 0.3590520918369293, 0.4588366448879242, 0.42886486649513245, 0.1964731514453888, 0.0896824300289154, 0.2843666076660156, 0.4356047511100769, 0.09714400768280029, 0.4579862654209137], dtype='float32').reshape([20]),
            paddle.to_tensor([0.14899136126041412, 0.4126279950141907, 0.3700433373451233, 0.25022515654563904, 0.15508493781089783, 0.21142560243606567, 0.21498297154903412, 0.21080267429351807, 0.13645829260349274, 0.29129841923713684, 0.36569052934646606, 0.16545851528644562, 0.38600343465805054, 0.23734235763549805, 0.39950311183929443, 0.09978035092353821, 0.05264170467853546, 0.22585836052894592, 0.1268913447856903, 0.0525270439684391], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_767b5b1325b07e5e6f5d4999dbfa845c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.08209486305713654, 0.49836239218711853, 0.2909911274909973, 0.014799349941313267, 0.4520396888256073, 0.05117411911487579, 0.48856765031814575, 0.009960673749446869, 0.49123793840408325, 0.08779963105916977, 0.11625847965478897, 0.19352291524410248, 0.1840560883283615, 0.4032469093799591, 0.44188016653060913, 0.10185232758522034, 0.4219275712966919, 0.15509778261184692, 0.26766881346702576, 0.3070317208766937, 0.1790596842765808, 0.4943293333053589, 0.07050999999046326, 0.4282154142856598, 0.08634433895349503, 0.27039891481399536, 0.2822067141532898, 0.043040934950113297, 0.15970072150230408, 0.26608625054359436], dtype='float32').reshape([30]),
            paddle.to_tensor([0.07684993743896484, 0.0022107705008238554, 0.3715864419937134, 0.26798295974731445, 0.4656369388103485, 0.29998472332954407, 0.3453100621700287, 0.24964627623558044, 0.34386110305786133, 0.05055393651127815, 0.40926581621170044, 0.08405527472496033, 0.21588768064975739, 0.2051600217819214, 0.0319947749376297, 0.14802737534046173, 0.3093959093093872, 0.11616374552249908, 0.2477424442768097, 0.3945048749446869, 0.03580811247229576, 0.3660421669483185, 0.1549643874168396, 0.18213976919651031, 0.4978889524936676, 0.38549351692199707, 0.06704066693782806, 0.37247905135154724, 0.35876572132110596, 0.18113455176353455], dtype='float32').reshape([30]),
            paddle.to_tensor([0.41456228494644165, 0.059253208339214325, 0.04724399000406265, 0.21328836679458618, 0.44653022289276123, 0.030069073662161827, 0.007364499382674694, 0.22814133763313293, 0.2690564692020416, 0.06860941648483276, 0.15190917253494263, 0.48815640807151794, 0.08109138906002045, 0.08741611242294312, 0.4591277241706848, 0.09415818750858307, 0.28595879673957825, 0.036204949021339417, 0.2315991073846817, 0.02957354113459587, 0.37759825587272644, 0.007357641588896513, 0.02109421417117119, 0.04098528251051903, 0.3769019842147827, 0.30559614300727844, 0.12937240302562714, 0.40238267183303833, 0.3479362726211548, 0.12965042889118195], dtype='float32').reshape([30]),
            paddle.to_tensor([0.45392754673957825, 0.25736430287361145, 0.42438164353370667, 0.17865636944770813, 0.2806469798088074, 0.21544981002807617, 0.48162806034088135, 0.2630787491798401, 0.05586572363972664, 0.08607650548219681, 0.30360642075538635, 0.10993720591068268, 0.41666582226753235, 0.24780283868312836, 0.27790844440460205, 0.20425575971603394, 0.4502951204776764, 0.33898186683654785, 0.4593229293823242, 0.3977217674255371, 0.23283790051937103, 0.1962987333536148, 0.268294095993042, 0.4965430796146393, 0.4589674472808838, 0.3617449700832367, 0.006621411070227623, 0.3023260831832886, 0.3578583002090454, 0.32689186930656433], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_59495d05e1605d17661b79b02e9d0907(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 480, 480], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.4883669912815094, 0.2995281219482422, 0.3902973234653473, 0.3843948543071747, 0.26534801721572876, 0.41677823662757874, 0.25266891717910767, 0.12113163620233536], dtype='float32').reshape([8]),
            paddle.to_tensor([0.4163154065608978, 0.3389620780944824, 0.3872450292110443, 0.3897412419319153, 0.04867025464773178, 0.21918614208698273, 0.3038368821144104, 0.03696633130311966], dtype='float32').reshape([8]),
            paddle.to_tensor([0.25590142607688904, 0.3842760920524597, 0.13595978915691376, 0.2321775257587433, 0.023800237104296684, 0.3219466805458069, 0.46023935079574585, 0.36860209703445435], dtype='float32').reshape([8]),
            paddle.to_tensor([0.1459142416715622, 0.3057960271835327, 0.1133931502699852, 0.2745082378387451, 0.321371853351593, 0.21297036111354828, 0.4313597083091736, 0.17361019551753998], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_974fab82b3f50d4ca658e24084c56aa0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.31742334365844727, 0.2971697449684143, 0.1410028338432312, 0.3999844491481781, 0.42216911911964417, 0.20487207174301147, 0.005745150148868561, 0.427705317735672, 0.44592025876045227, 0.38654056191444397, 0.048698585480451584, 0.16863064467906952, 0.417771577835083, 0.3632287383079529, 0.06414433568716049, 0.33812737464904785, 0.13403773307800293, 0.23948001861572266, 0.013366875238716602, 0.41127100586891174, 0.4654061198234558, 0.2881764769554138, 0.021549256518483162, 0.43141651153564453], dtype='float32').reshape([24]),
            paddle.to_tensor([0.49165353178977966, 0.181182399392128, 0.3783831298351288, 0.056965749710798264, 0.3617995083332062, 0.33107998967170715, 0.03329802677035332, 0.39262837171554565, 0.2426413893699646, 0.4655109941959381, 0.056699659675359726, 0.42057153582572937, 0.26917678117752075, 0.16180168092250824, 0.42304226756095886, 0.32437795400619507, 0.1502923220396042, 0.09175462275743484, 0.20366449654102325, 0.2217867076396942, 0.07518859207630157, 0.37590038776397705, 0.23172684013843536, 0.30575689673423767], dtype='float32').reshape([24]),
            paddle.to_tensor([0.3683607876300812, 0.08423680067062378, 0.3709992468357086, 0.20650404691696167, 0.1155780777335167, 0.06824177503585815, 0.09293543547391891, 0.3214159607887268, 0.31733131408691406, 0.1509060263633728, 0.31360381841659546, 0.1645633578300476, 0.18788157403469086, 0.12950298190116882, 0.23766222596168518, 0.22415749728679657, 0.4012817144393921, 0.4125587046146393, 0.46671897172927856, 0.24351924657821655, 0.3893720507621765, 0.2976677119731903, 0.3526538014411926, 0.32590779662132263], dtype='float32').reshape([24]),
            paddle.to_tensor([0.17585410177707672, 0.3017427325248718, 0.1964399367570877, 0.31727561354637146, 0.04318413883447647, 0.4255254864692688, 0.4798084497451782, 0.22011569142341614, 0.1762627363204956, 0.2559968829154968, 0.3751591145992279, 0.34081321954727173, 0.38361385464668274, 0.10074777156114578, 0.044781941920518875, 0.4820496737957001, 0.04597784951329231, 0.21576453745365143, 0.12245554476976395, 0.36489930748939514, 0.45676764845848083, 0.18719913065433502, 0.10463843494653702, 0.41839519143104553], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ec70c7c394f42a08d00d717446e5c53f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 336, 60, 60], dtype='float16', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_534b6c2e1388c0bdc493bd9e9b16bfa9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 256, 256], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.3334837257862091, 0.3518320322036743, 0.21655912697315216, 0.4113002419471741, 0.004415825940668583, 0.10764109343290329, 0.2313968986272812, 0.41425079107284546, 0.3193182945251465, 0.33088982105255127, 0.15956522524356842, 0.23910394310951233, 0.0802808478474617, 0.4049643278121948, 0.0697953924536705, 0.4035106301307678], dtype='float32').reshape([16]),
            paddle.to_tensor([0.07557307183742523, 0.3337786793708801, 0.451619952917099, 0.09766321629285812, 0.43133869767189026, 0.11417524516582489, 0.2538381814956665, 0.4977555572986603, 0.31191104650497437, 0.49635350704193115, 0.15705935657024384, 0.3319602906703949, 0.046036411076784134, 0.1593388468027115, 0.37617409229278564, 0.49630025029182434], dtype='float32').reshape([16]),
            paddle.to_tensor([0.25374719500541687, 0.21715296804904938, 0.4992305636405945, 0.2117692232131958, 0.3873695731163025, 0.12187471240758896, 0.28908130526542664, 0.49202364683151245, 0.3365841507911682, 0.025722699239850044, 0.20982395112514496, 0.23546139895915985, 0.24360410869121552, 0.2432761788368225, 0.17495913803577423, 0.4728893041610718], dtype='float32').reshape([16]),
            paddle.to_tensor([0.43363064527511597, 0.46358534693717957, 0.010565065778791904, 0.1228339672088623, 0.07380397617816925, 0.3373286724090576, 0.3061682879924774, 0.3286319077014923, 0.47601085901260376, 0.42076170444488525, 0.03558576479554176, 0.3944116234779358, 0.48109200596809387, 0.08378613740205765, 0.15296097099781036, 0.4788699746131897], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c05e3af1597b4a6ae7b92f7e3e77941e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 336, 60, 60], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_70a18695eedfefde1f6a8246cc95ec55(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 528, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([528], dtype='float32', min=0, max=0.5),
            paddle.uniform([528], dtype='float32', min=0, max=0.5),
            paddle.uniform([528], dtype='float32', min=0, max=0.5),
            paddle.uniform([528], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cdca329a884c530c9e37a97c84353454(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 16, 16], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cd190daaaf836f21385cf3a7fc435599(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_af91295b59c5fca17f890a802deb001f
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 32, 16, 16], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8515fbb702653dd24718abeacc5dd679(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 256, 256], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c63be849fc18df5c4cb7c5d072d0752c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_f5f2213be92f20fda5378beeed72688f
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 4, 4], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_69e85a305d3624b1f56fe1fb7ed1e4ec(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.044888511300086975, 0.10473959147930145, 0.49584314227104187, 0.09655004739761353, 0.24640008807182312, 0.27119162678718567, 0.05898846313357353, 0.4981764256954193, 0.013010298833251, 0.4220082759857178, 0.29762405157089233, 0.39944925904273987, 0.34127670526504517, 0.19219104945659637, 0.17160911858081818, 0.49814167618751526, 0.19945745170116425, 0.4512283504009247, 0.33177992701530457, 0.2513963580131531], dtype='float32').reshape([20]),
            paddle.to_tensor([0.21260710060596466, 0.23010793328285217, 0.02563491277396679, 0.24325688183307648, 0.26421570777893066, 0.15423350036144257, 0.31390923261642456, 0.4198278486728668, 0.14212563633918762, 0.05793797969818115, 0.25021132826805115, 0.34523916244506836, 0.18672339618206024, 0.246022567152977, 0.20907782018184662, 0.1152840182185173, 0.0009882878512144089, 0.43633508682250977, 0.3410915434360504, 0.23620742559432983], dtype='float32').reshape([20]),
            paddle.to_tensor([0.2837742269039154, 0.43294674158096313, 0.26636022329330444, 0.16516545414924622, 0.16436657309532166, 0.17560908198356628, 0.479154109954834, 0.14028015732765198, 0.15495754778385162, 0.43470460176467896, 0.3428954482078552, 0.10936363786458969, 0.4553874433040619, 0.357020765542984, 0.037865977734327316, 0.4219798147678375, 0.2819938063621521, 0.08239880949258804, 0.4302317500114441, 0.09467585384845734], dtype='float32').reshape([20]),
            paddle.to_tensor([0.15578922629356384, 0.4240575134754181, 0.10513129830360413, 0.21312083303928375, 0.07766793668270111, 0.1220439150929451, 0.320139080286026, 0.0117146922275424, 0.1414574533700943, 0.18500564992427826, 0.18465906381607056, 0.16802512109279633, 0.4700430929660797, 0.22080783545970917, 0.14812374114990234, 0.3525026738643646, 0.47844648361206055, 0.4777699112892151, 0.4219719171524048, 0.2725646197795868], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_236d86d5f0638c40a3f1874a57050b35(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 64, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_100ce3bd2382abe40998da31ecbff016(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b6f51999d78b80bd5604ad310baf38f9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a8301b810bccba10e4dfdee0df73334
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 10, 10], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1ffa5384829b2f3d9f3a83be7d363910(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 31, 31], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b9af604330c78e72ba0b059f3740753c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 128, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_969df04f47281a56f03dc1a98e3f22f0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 480, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a4072b0552fbcd6e712cc5c5992de113(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 4, 50], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_59076b23a8a27d9d5e37f4a25a3ff217(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 8, 50], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6f0628572187b4d8e1ea08e7ea46cd87(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 12, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.3838593363761902, 0.29668283462524414, 0.4395146667957306, 0.04237068071961403, 0.07479088753461838, 0.2531692683696747, 0.12062156200408936, 0.29809823632240295, 0.3863595724105835, 0.4667162299156189, 0.4776158630847931, 0.38197264075279236], dtype='float32').reshape([12]),
            paddle.to_tensor([0.3118499517440796, 0.03922104835510254, 0.30548301339149475, 0.3135286271572113, 0.22478006780147552, 0.4973771274089813, 0.10264462977647781, 0.487001895904541, 0.04391053318977356, 0.3767332434654236, 0.27880966663360596, 0.4124498665332794], dtype='float32').reshape([12]),
            paddle.to_tensor([0.17794616520404816, 0.3477056324481964, 0.15869170427322388, 0.34024834632873535, 0.19699649512767792, 0.21018126606941223, 0.42660439014434814, 0.15887092053890228, 0.0038125035353004932, 0.37796181440353394, 0.29561251401901245, 0.08764997124671936], dtype='float32').reshape([12]),
            paddle.to_tensor([0.32446569204330444, 0.3591388165950775, 0.2992798089981079, 0.10177499800920486, 0.3046566843986511, 0.4377691149711609, 0.0034163843374699354, 0.3369686007499695, 0.13038092851638794, 0.0077196513302624226, 0.05080391466617584, 0.3542504906654358], dtype='float32').reshape([12]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_43080a6a5e2fc43edb3ff9404065bba0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 244, 244], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.07588472962379456, 0.4846370220184326, 0.40672850608825684, 0.2612215578556061, 0.3446924686431885, 0.0544445738196373, 0.3224084675312042, 0.28374844789505005, 0.15336409211158752, 0.23994171619415283, 0.15563097596168518, 0.4874389171600342, 0.10263676196336746, 0.019418152049183846, 0.06680711358785629, 0.11509575694799423], dtype='float32').reshape([16]),
            paddle.to_tensor([0.40743759274482727, 0.070948526263237, 0.11513575911521912, 0.16806291043758392, 0.4662342369556427, 0.05171279236674309, 0.2836918532848358, 0.4789728820323944, 0.25340285897254944, 0.15770265460014343, 0.4650043249130249, 0.02702047862112522, 0.14123080670833588, 0.41296398639678955, 0.20768161118030548, 0.1117977648973465], dtype='float32').reshape([16]),
            paddle.to_tensor([0.2462185025215149, 0.32208922505378723, 0.21510794758796692, 0.08832846581935883, 0.2809036374092102, 0.47713592648506165, 0.12555216252803802, 0.3437097370624542, 0.36624857783317566, 0.0893479511141777, 0.35157546401023865, 0.4650288224220276, 0.26937732100486755, 0.4016594886779785, 0.32244235277175903, 0.1470572054386139], dtype='float32').reshape([16]),
            paddle.to_tensor([0.17677582800388336, 0.1125824972987175, 0.4990972578525543, 0.12550292909145355, 0.3926006853580475, 0.3277732729911804, 0.46861782670021057, 0.1639135628938675, 0.3200981020927429, 0.483029305934906, 0.27791357040405273, 0.20047777891159058, 0.3300454914569855, 0.42842215299606323, 0.2546943128108978, 0.4917387068271637], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_30ccd94c7ddd30b81ae68040c6f33fd0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.41369110345840454, 0.3814903199672699, 0.49354687333106995, 0.26327475905418396, 0.4222036302089691, 0.43529826402664185, 0.43966853618621826, 0.08166873455047607, 0.16276173293590546, 0.4802529215812683, 0.31953150033950806, 0.19424955546855927, 0.37397071719169617, 0.3442646861076355, 0.4134688079357147, 0.44798600673675537, 0.39002370834350586, 0.027256671339273453, 0.3949076533317566, 0.4404676854610443, 0.38260695338249207, 0.49657148122787476, 0.3511790335178375, 0.2970421612262726, 0.37087276577949524, 0.404653936624527, 0.2561686038970947, 0.1199321299791336, 0.47989749908447266, 0.07543309032917023], dtype='float32').reshape([30]),
            paddle.to_tensor([0.13722115755081177, 0.3975870907306671, 0.3186759352684021, 0.4133853316307068, 0.36859723925590515, 0.28309324383735657, 0.05930656939744949, 0.13632266223430634, 0.32942086458206177, 0.2845383882522583, 0.321310818195343, 0.1034824401140213, 0.46724000573158264, 0.2856566607952118, 0.3742141127586365, 0.28022363781929016, 0.04309944063425064, 0.21915580332279205, 0.17533424496650696, 0.460345596075058, 0.017401648685336113, 0.2668786942958832, 0.35764989256858826, 0.11837352812290192, 0.08414934575557709, 0.3385494649410248, 0.453422486782074, 0.17726796865463257, 0.02400946617126465, 0.37969234585762024], dtype='float32').reshape([30]),
            paddle.to_tensor([0.2535606622695923, 0.28961434960365295, 0.45003852248191833, 0.3171856999397278, 0.3238743841648102, 0.047192372381687164, 0.010538097470998764, 0.44416776299476624, 0.4235128164291382, 0.14289632439613342, 0.1239352747797966, 0.4140476584434509, 0.2252204716205597, 0.0745038092136383, 0.11997461318969727, 0.08598403632640839, 0.2048860788345337, 0.19296254217624664, 0.35260045528411865, 0.3537917733192444, 0.2913926839828491, 0.3301297724246979, 0.31947678327560425, 0.2760647237300873, 0.07171758264303207, 0.1413899064064026, 0.037938233464956284, 0.2804413437843323, 0.09014812856912613, 0.2477000504732132], dtype='float32').reshape([30]),
            paddle.to_tensor([0.4694061279296875, 0.30627286434173584, 0.05063813179731369, 0.45129823684692383, 0.17090962827205658, 0.04427227005362511, 0.2995907664299011, 0.2955578565597534, 0.48800697922706604, 0.4620134234428406, 0.293535977602005, 0.42281574010849, 0.23269163072109222, 0.35459184646606445, 0.20549418032169342, 0.10957023501396179, 0.4234098792076111, 0.34606215357780457, 0.4719994366168976, 0.4693363308906555, 0.28651782870292664, 0.3464137017726898, 0.13303740322589874, 0.15164215862751007, 0.0780673623085022, 0.21965789794921875, 0.2881883978843689, 0.34771251678466797, 0.3489905595779419, 0.06969470530748367], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e5b02729044a2278eef5229a1654b347(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 24, 80], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c7dbcca464cc7079b4786a6e73dce42e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 64, 128], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.22745734453201294, 0.33764734864234924, 0.07022818177938461, 0.050965648144483566, 0.1320258527994156, 0.3300812840461731, 0.49012038111686707, 0.4106539785861969, 0.1712987720966339, 0.2664702832698822, 0.32317590713500977, 0.4252420961856842, 0.422230988740921, 0.2991640567779541, 0.35844579339027405, 0.1414300501346588, 0.20186026394367218, 0.2010975331068039], dtype='float32').reshape([18]),
            paddle.to_tensor([0.10756330192089081, 0.09977288544178009, 0.11425428837537766, 0.11175301671028137, 0.18685245513916016, 0.468270868062973, 0.4334993064403534, 0.08426034450531006, 0.31125348806381226, 0.12803058326244354, 0.17659907042980194, 0.18152692914009094, 0.17653892934322357, 0.3435102105140686, 0.4153093099594116, 0.11422044783830643, 0.14332005381584167, 0.27100712060928345], dtype='float32').reshape([18]),
            paddle.to_tensor([0.2967434525489807, 0.4493519067764282, 0.07340572029352188, 0.1892174333333969, 0.04989995062351227, 0.1594197154045105, 0.11001737415790558, 0.4399113655090332, 0.03131736442446709, 0.3115434944629669, 0.06181180104613304, 0.23250369727611542, 0.03009825013577938, 0.2280152142047882, 0.31083235144615173, 0.11010389775037766, 0.08643253892660141, 0.005916861351579428], dtype='float32').reshape([18]),
            paddle.to_tensor([0.06710270792245865, 0.45696908235549927, 0.13951382040977478, 0.21856102347373962, 0.3954005241394043, 0.377884179353714, 0.49055472016334534, 0.3028514087200165, 0.2713722288608551, 0.20982055366039276, 0.013455900363624096, 0.09547911584377289, 0.46764057874679565, 0.3618309497833252, 0.14801526069641113, 0.45648524165153503, 0.15998512506484985, 0.371847927570343], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6da099092ba45c1fd46afe28e73eaa81(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 350, 25], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f737ba5365b3d4a1db36ebf9a812e520(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 256, 512], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.09448792040348053, 0.2649999260902405, 0.18653689324855804, 0.45914310216903687, 0.4069046378135681, 0.07876340299844742, 0.055656202137470245, 0.24132263660430908, 0.3458399474620819, 0.09346352517604828, 0.39262303709983826, 0.39992907643318176, 0.13966630399227142, 0.48686662316322327, 0.40547457337379456, 0.403469443321228], dtype='float32').reshape([16]),
            paddle.to_tensor([0.23540814220905304, 0.45739442110061646, 0.2828409969806671, 0.4107024073600769, 0.10430298000574112, 0.351226270198822, 0.06727687269449234, 0.3736856281757355, 0.35941749811172485, 0.028087159618735313, 0.23949894309043884, 0.4461468458175659, 0.30125856399536133, 0.4812561273574829, 0.3641011118888855, 0.23252983391284943], dtype='float32').reshape([16]),
            paddle.to_tensor([0.09152613580226898, 0.1405533403158188, 0.4626368582248688, 0.3385795056819916, 0.32534030079841614, 0.06552748382091522, 0.2826915979385376, 0.0745476484298706, 0.008331935852766037, 0.4522445499897003, 0.34335291385650635, 0.2128240466117859, 0.4498026371002197, 0.04740706458687782, 0.1633337140083313, 0.4000057280063629], dtype='float32').reshape([16]),
            paddle.to_tensor([0.23864978551864624, 0.11168716102838516, 0.48336470127105713, 0.3153159022331238, 0.06214432045817375, 0.40136486291885376, 0.10828810185194016, 0.4344581961631775, 0.046655066311359406, 0.49190840125083923, 0.026051120832562447, 0.2951059341430664, 0.09204690158367157, 0.24695123732089996, 0.44630810618400574, 0.23747211694717407], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3976c4eb08613cad08878c78a534385f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 56, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a06dfc3f02a255e83050ba92c46f428c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.36113739013671875, 0.3314223885536194, 0.29634177684783936, 0.07041435688734055, 0.4630488157272339, 0.31375259160995483, 0.30570217967033386, 0.47768276929855347, 0.2435973882675171, 0.33000287413597107, 0.4272157847881317, 0.04978562146425247, 0.1596331149339676, 0.0716201663017273, 0.03428567200899124, 0.26616886258125305], dtype='float32').reshape([16]),
            paddle.to_tensor([0.12321504950523376, 0.23389631509780884, 0.48414552211761475, 0.4394075274467468, 0.36367273330688477, 0.256084680557251, 0.1353497952222824, 0.114454485476017, 0.06052340194582939, 0.26813268661499023, 0.2618582248687744, 0.28954726457595825, 0.4376397728919983, 0.39126652479171753, 0.3995493948459625, 0.34778082370758057], dtype='float32').reshape([16]),
            paddle.to_tensor([0.052164819091558456, 0.021931493654847145, 0.20373092591762543, 0.1539573222398758, 0.3240293860435486, 0.05767897516489029, 0.15721571445465088, 0.271405965089798, 0.10018837451934814, 0.04663096368312836, 0.31170135736465454, 0.19424982368946075, 0.1421544998884201, 0.07179072499275208, 0.15250174701213837, 0.35502585768699646], dtype='float32').reshape([16]),
            paddle.to_tensor([0.3765345513820648, 0.12918730080127716, 0.2902441918849945, 0.018433846533298492, 0.2807238698005676, 0.3933328092098236, 0.16573956608772278, 0.28616172075271606, 0.048160746693611145, 0.2908470332622528, 0.46181538701057434, 0.2914885878562927, 0.255960077047348, 0.3394358158111572, 0.14086796343326569, 0.2670028507709503], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a388e16955560c0ec2e8780947ce2ee5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_b4f4da3a8d44d1b5b3a2903a097df186
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 128, 128], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_96514297c4dcdafe71892fd6aa27812a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 32, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a3216f17f3ebae06e93b357d6cf0c915(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 80, 20, 20], dtype='float16', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_66dd600bf74cd6e470df63df0bd5aa83(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 432, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([432], dtype='float32', min=0, max=0.5),
            paddle.uniform([432], dtype='float32', min=0, max=0.5),
            paddle.uniform([432], dtype='float32', min=0, max=0.5),
            paddle.uniform([432], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d28ed095c9e7bfa24842b50df21517c3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 240, 240], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_83a513c035ef0f9d35a3e2b69487e779(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 25, 42], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b274ea83c927a37d520664248e3b7925(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 152, 152], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_26c63e8cdc316719fc0b123f1f056a03(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 80, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8935bb450540ec43e944c9172acafa70(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 112, 112], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.06433168798685074, 0.11626412719488144, 0.2150348573923111, 0.2452758103609085, 0.15363666415214539, 0.10721205919981003, 0.222610741853714, 0.29441583156585693], dtype='float32').reshape([8]),
            paddle.to_tensor([0.10192965716123581, 0.05817754194140434, 0.012424531392753124, 0.1497708559036255, 0.15934622287750244, 0.15180827677249908, 0.1770496815443039, 0.12648242712020874], dtype='float32').reshape([8]),
            paddle.to_tensor([0.34484270215034485, 0.26072657108306885, 0.17746420204639435, 0.27630993723869324, 0.32031071186065674, 0.4279755651950836, 0.34420400857925415, 0.4351651668548584], dtype='float32').reshape([8]),
            paddle.to_tensor([0.03289354592561722, 0.3232247233390808, 0.45398473739624023, 0.018144620582461357, 0.37882333993911743, 0.16725505888462067, 0.1884242743253708, 0.05923300236463547], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cf59bb699c89c2c5f5c63571444e485a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 117, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([117], dtype='float32', min=0, max=0.5),
            paddle.uniform([117], dtype='float32', min=0, max=0.5),
            paddle.uniform([117], dtype='float32', min=0, max=0.5),
            paddle.uniform([117], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_08412eb5799f5451a850dcea1ed32407(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_af91295b59c5fca17f890a802deb001f
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 32, 64, 64], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.46875879168510437, 0.027420924976468086, 0.4290071427822113, 0.13282814621925354, 0.1353924572467804, 0.45227521657943726, 0.235974982380867, 0.09804078191518784], dtype='float32').reshape([8]),
            paddle.to_tensor([0.11256542801856995, 0.4257642924785614, 0.3769093453884125, 0.09083671867847443, 0.21055911481380463, 0.15312838554382324, 0.3862474262714386, 0.22718383371829987], dtype='float32').reshape([8]),
            paddle.to_tensor([0.3292161226272583, 0.42363306879997253, 0.2951333522796631, 0.11003389954566956, 0.34620505571365356, 0.20095385611057281, 0.2279527187347412, 0.4105847477912903], dtype='float32').reshape([8]),
            paddle.to_tensor([0.23080627620220184, 0.2837369441986084, 0.4481946527957916, 0.484355092048645, 0.17467473447322845, 0.37074363231658936, 0.2171555608510971, 0.2843132019042969], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ec9c5efc4345e6d97fad0ed47fe1f58b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 16, 50], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_052cea90ac4c9779c77b9881d8acac1b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 28, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.4738827347755432, 0.06825429201126099, 0.26877281069755554, 0.36785706877708435, 0.11348215490579605, 0.033741746097803116, 0.4960252344608307, 0.02808813750743866, 0.2725287973880768, 0.05277208238840103, 0.36914074420928955, 0.30258309841156006, 0.19485554099082947, 0.4387684762477875, 0.35295864939689636, 0.4246627688407898, 0.14048171043395996, 0.4889780282974243, 0.37377557158470154, 0.14958691596984863, 0.20535901188850403, 0.26512736082077026, 0.040765512734651566, 0.4509645998477936, 0.00560881057754159, 0.3575863838195801, 0.01106269285082817, 0.35978931188583374], dtype='float32').reshape([28]),
            paddle.to_tensor([0.04369058087468147, 0.3848464787006378, 0.2797681391239166, 0.1790897101163864, 0.2386336326599121, 0.0220219474285841, 0.021354906260967255, 0.09065064042806625, 0.018354032188653946, 0.44863444566726685, 0.03900016471743584, 0.48588642477989197, 0.3780307471752167, 0.18904773890972137, 0.4762797951698303, 0.45140334963798523, 0.30382055044174194, 0.44550955295562744, 0.35588911175727844, 0.3673224449157715, 0.1746770143508911, 0.49497053027153015, 0.48522621393203735, 0.49834832549095154, 0.12297233194112778, 0.053371887654066086, 0.3652331233024597, 0.10906732082366943], dtype='float32').reshape([28]),
            paddle.to_tensor([0.03604652360081673, 0.34029874205589294, 0.3963838517665863, 0.058644331991672516, 0.3685651421546936, 0.3258552551269531, 0.11434771120548248, 0.37822169065475464, 0.18493039906024933, 0.25927016139030457, 0.4630127549171448, 0.13224849104881287, 0.07024253159761429, 0.06432881206274033, 0.2522733509540558, 0.4696978032588959, 0.45591723918914795, 0.47362226247787476, 0.03968874737620354, 0.47540032863616943, 0.2405589371919632, 0.1864505112171173, 0.3787265717983246, 0.006053003948181868, 0.13512958586215973, 0.2938534915447235, 0.24601994454860687, 0.48344525694847107], dtype='float32').reshape([28]),
            paddle.to_tensor([0.319614440202713, 0.0017183602321892977, 0.39459219574928284, 0.1862492561340332, 0.09586722403764725, 0.22512264549732208, 0.3286343216896057, 0.4857259690761566, 0.37547534704208374, 0.04131501168012619, 0.32517513632774353, 0.054513756185770035, 0.027947716414928436, 0.3950350284576416, 0.11912970244884491, 0.05668128281831741, 0.17885446548461914, 0.021303920075297356, 0.4298701286315918, 0.3528873026371002, 0.12497175484895706, 0.33305421471595764, 0.34705212712287903, 0.4214053153991699, 0.33925357460975647, 0.2885933816432953, 0.4926450252532959, 0.15231454372406006], dtype='float32').reshape([28]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_eaa8a2046fe72f8e4cb5177421dc8133(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.06566251069307327, 0.03962182253599167, 0.11036641150712967, 0.12454608082771301, 0.25995782017707825, 0.2128167748451233, 0.3051365613937378, 0.4677009880542755, 0.3714578449726105, 0.4522179067134857, 0.41115090250968933, 0.19031918048858643, 0.3524438440799713, 0.1692957580089569, 0.4617767333984375, 0.23799507319927216, 0.3782426714897156, 0.3078460395336151, 0.1880536824464798, 0.24095916748046875, 0.13740625977516174, 0.19142591953277588, 0.030507506802678108, 0.397615522146225, 0.1564503163099289, 0.11936193704605103, 0.1809808313846588, 0.46972891688346863, 0.25720351934432983, 0.48334941267967224], dtype='float32').reshape([30]),
            paddle.to_tensor([0.45385950803756714, 0.3820185661315918, 0.016604768112301826, 0.41567644476890564, 0.17168407142162323, 0.4435827434062958, 0.05415104702115059, 0.15782298147678375, 0.4800795316696167, 0.22221942245960236, 0.3099358379840851, 0.2078612595796585, 0.02889173850417137, 0.4673333466053009, 0.16697892546653748, 0.4072665572166443, 0.3701428174972534, 0.1897115707397461, 0.12582960724830627, 0.4925806224346161, 0.4215202033519745, 0.40219759941101074, 0.2406332641839981, 0.22738315165042877, 0.46488556265830994, 0.2505764365196228, 0.1155586764216423, 0.4417281746864319, 0.24869117140769958, 0.4664122462272644], dtype='float32').reshape([30]),
            paddle.to_tensor([0.43684136867523193, 0.37329521775245667, 0.47612398862838745, 0.38353610038757324, 0.14712271094322205, 0.29571977257728577, 0.2821098864078522, 0.12071599066257477, 0.3472431004047394, 0.26136186718940735, 0.03589288517832756, 0.3325636386871338, 0.06418383121490479, 0.09816611558198929, 0.437372088432312, 0.26680704951286316, 0.4499632716178894, 0.4600301682949066, 0.08078087866306305, 0.2249092012643814, 0.39521563053131104, 0.051150187849998474, 0.12130240350961685, 0.2308560311794281, 0.2957538664340973, 0.28643402457237244, 0.43936777114868164, 0.20426441729068756, 0.16226333379745483, 0.0850435271859169], dtype='float32').reshape([30]),
            paddle.to_tensor([0.4826904535293579, 0.4409853518009186, 0.23398776352405548, 0.12842921912670135, 0.22171908617019653, 0.35760223865509033, 0.2366923987865448, 0.223830446600914, 0.269063800573349, 0.21811543405056, 0.4419979155063629, 0.16613779962062836, 0.3727785348892212, 0.16986173391342163, 0.43032124638557434, 0.4610525667667389, 0.017131049185991287, 0.22118788957595825, 0.3703249394893646, 0.21737916767597198, 0.3996877670288086, 0.3288399279117584, 0.060613926500082016, 0.4295411705970764, 0.10269191116094589, 0.4271513819694519, 0.4579751789569855, 0.2664967179298401, 0.36669084429740906, 0.41988828778266907], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c3c5faf99de1ca42aa0607f79f4c1ba3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 38, 38], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_03eca6fb312f8659782bb2f9472c638c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 20, 20], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_64419d460ae403ede21c7410fd257f07(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 304, 304], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b9d13454b2707c913b1c3869bad72d3e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 14, 25], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_adc49fd1f7cc60f9458491425b0cfe7a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 480, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a085e37fa0df592c782fb69c75b0c5c5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d9920596b4d5edc00df3520f3985016d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 12, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.38437312841415405, 0.42359453439712524, 0.21617887914180756, 0.1485862284898758, 0.4878242015838623, 0.46347832679748535, 0.15632520616054535, 0.47339850664138794, 0.19799090921878815, 0.35691913962364197, 0.07989536225795746, 0.21860042214393616], dtype='float32').reshape([12]),
            paddle.to_tensor([0.29303473234176636, 0.4675277769565582, 0.06693517416715622, 0.1273754984140396, 0.4367048442363739, 0.08774849772453308, 0.38702747225761414, 0.39092665910720825, 0.047829680144786835, 0.08011934161186218, 0.05891163647174835, 0.044152434915304184], dtype='float32').reshape([12]),
            paddle.to_tensor([0.48475381731987, 0.29670554399490356, 0.05822567269206047, 0.4232693314552307, 0.3894974887371063, 0.12336938828229904, 0.15894627571105957, 0.29859188199043274, 0.1833675056695938, 0.30467844009399414, 0.08340134471654892, 0.45360657572746277], dtype='float32').reshape([12]),
            paddle.to_tensor([0.27258503437042236, 0.377961128950119, 0.3717609941959381, 0.21667233109474182, 0.10628403723239899, 0.4062383770942688, 0.18322286009788513, 0.056973278522491455, 0.08497221767902374, 0.2240370512008667, 0.03996735438704491, 0.3852653503417969], dtype='float32').reshape([12]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fc5a33bdfd3ddc5f182ca4954df7d387(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.45900824666023254, 0.35088616609573364, 0.331539124250412, 0.04060312733054161, 0.37852635979652405, 0.012482203543186188, 0.2171972692012787, 0.0521332286298275, 0.4382769763469696, 0.2953350841999054, 0.16612572968006134, 0.12815086543560028, 0.3187309801578522, 0.4583330750465393, 0.33876606822013855, 0.18847939372062683, 0.3825392425060272, 0.3361937999725342, 0.29727035760879517, 0.370209276676178, 0.2112560272216797, 0.3342324495315552, 0.27515435218811035, 0.0938490554690361, 0.057138871401548386, 0.2786286175251007, 0.21051864326000214, 0.2772325575351715, 0.042447395622730255, 0.39857861399650574], dtype='float32').reshape([30]),
            paddle.to_tensor([0.1653321534395218, 0.16377539932727814, 0.17935800552368164, 0.38022080063819885, 0.030542615801095963, 0.3220275044441223, 0.41179946064949036, 0.40590691566467285, 0.07538022845983505, 0.1769251823425293, 0.016323620453476906, 0.16273336112499237, 0.43107765913009644, 0.0019361283630132675, 0.21664203703403473, 0.3230942189693451, 0.13465742766857147, 0.45149222016334534, 0.2677553594112396, 0.4789964556694031, 0.3409373164176941, 0.48792099952697754, 0.15057043731212616, 0.3608086109161377, 0.3076217770576477, 0.2355276346206665, 0.03899066895246506, 0.038545817136764526, 0.4439835548400879, 0.37764066457748413], dtype='float32').reshape([30]),
            paddle.to_tensor([0.18848977982997894, 0.3853001892566681, 0.040915507823228836, 0.48418867588043213, 0.44855359196662903, 0.42085567116737366, 0.26753151416778564, 0.15235377848148346, 0.0949239656329155, 0.33363738656044006, 0.12713997066020966, 0.36278215050697327, 0.2403223216533661, 0.23473390936851501, 0.20090103149414062, 0.3313596248626709, 0.17384448647499084, 0.23800534009933472, 0.05719686299562454, 0.3923689126968384, 0.18969590961933136, 0.4417383372783661, 0.2457103729248047, 0.33110418915748596, 0.14535987377166748, 0.015838423743844032, 0.06337570399045944, 0.0182961318641901, 0.3286290764808655, 0.2821257710456848], dtype='float32').reshape([30]),
            paddle.to_tensor([0.40728721022605896, 0.37756747007369995, 0.23773647844791412, 0.24567317962646484, 0.23950086534023285, 0.3274039328098297, 0.2166733294725418, 0.15655997395515442, 0.49781689047813416, 0.05886993557214737, 0.1992531269788742, 0.4370840787887573, 0.03281734883785248, 0.48295125365257263, 0.41043907403945923, 0.27694767713546753, 0.10794296115636826, 0.2620195150375366, 0.21304680407047272, 0.33402523398399353, 0.11866915971040726, 0.10937110334634781, 0.40501827001571655, 0.14274270832538605, 0.05087456479668617, 0.4582284092903137, 0.4835478961467743, 0.29737475514411926, 0.43333151936531067, 0.04370691254734993], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_77eb72090d9031d6a9ec5700f4d325b9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 40, 40], dtype='float16', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a3d2691ff02817d98a893274575a64ae(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 80, 80], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.49753761291503906, 0.2101077437400818, 0.2929026782512665, 0.019566692411899567, 0.21658876538276672, 0.020331040024757385, 0.4106284976005554, 0.16891787946224213, 0.14383801817893982, 0.4887041449546814, 0.4338954985141754, 0.4507773518562317, 0.02602517604827881, 0.4700743556022644, 0.47554004192352295, 0.3271946907043457, 0.14948111772537231, 0.3257497549057007, 0.36819273233413696, 0.29974088072776794, 0.4960063397884369, 0.36027050018310547, 0.03852987661957741, 0.022351039573550224], dtype='float32').reshape([24]),
            paddle.to_tensor([0.2894405126571655, 0.11589375883340836, 0.1668020635843277, 0.4392237365245819, 0.42499762773513794, 0.3456117510795593, 0.24489176273345947, 0.4285829961299896, 0.11758106201887131, 0.40057262778282166, 0.16436058282852173, 0.054771535098552704, 0.38225051760673523, 0.47766315937042236, 0.1767028123140335, 0.47197094559669495, 0.03996993973851204, 0.054661378264427185, 0.22498345375061035, 0.47815898060798645, 0.28148436546325684, 0.05389073118567467, 0.3956248462200165, 0.29276636242866516], dtype='float32').reshape([24]),
            paddle.to_tensor([0.23058393597602844, 0.024551812559366226, 0.3718979060649872, 0.23369908332824707, 0.33457326889038086, 0.19196102023124695, 0.13102120161056519, 0.44139349460601807, 0.2016850858926773, 0.2702305018901825, 0.09611006081104279, 0.017088396474719048, 0.06343257427215576, 0.15171155333518982, 0.4155275821685791, 0.29273393750190735, 0.44279414415359497, 0.2564801871776581, 0.4902244210243225, 0.382762610912323, 0.2119286209344864, 0.2540394067764282, 0.28562131524086, 0.38435104489326477], dtype='float32').reshape([24]),
            paddle.to_tensor([0.25441238284111023, 0.46347036957740784, 0.03609493002295494, 0.17685018479824066, 0.4776209592819214, 0.2141065150499344, 0.23368476331233978, 0.3264371156692505, 0.4906326234340668, 0.03508026525378227, 0.3377577066421509, 0.05370650440454483, 0.10342833399772644, 0.12954075634479523, 0.41902977228164673, 0.25582605600357056, 0.2875809073448181, 0.19072940945625305, 0.4857949912548065, 0.358802855014801, 0.42689281702041626, 0.06386565417051315, 0.4421037435531616, 0.38218116760253906], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ab7ec1704e068c79e50a56e7602f729e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 1536, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([1536], dtype='float32', min=0, max=0.5),
            paddle.uniform([1536], dtype='float32', min=0, max=0.5),
            paddle.uniform([1536], dtype='float32', min=0, max=0.5),
            paddle.uniform([1536], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fe6396b8458678c74562c4decc7c44c0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 384, 32, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7d2968d0b978accf150d88385827fc2f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 320, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_04ee250d3f7b909faa7e4fb9b9bb33cc(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 160, 160], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_142bd3fe899df8a44e48bb02308d27b9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 512, 1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8cf5039fd582582d6abe3160d5cbe2d0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 320, 320], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.4645826518535614, 0.3472043573856354, 0.33839088678359985, 0.4192664325237274, 0.253743439912796, 0.4820691645145416, 0.21281638741493225, 0.16778214275836945], dtype='float32').reshape([8]),
            paddle.to_tensor([0.21509498357772827, 0.3097544014453888, 0.4725717008113861, 0.2744559645652771, 0.13827340304851532, 0.4321334958076477, 0.41856256127357483, 0.0253891721367836], dtype='float32').reshape([8]),
            paddle.to_tensor([0.07091166079044342, 0.45503154397010803, 0.30607226490974426, 0.23093080520629883, 0.07807046920061111, 0.19910193979740143, 0.28450554609298706, 0.059183519333601], dtype='float32').reshape([8]),
            paddle.to_tensor([0.31801605224609375, 0.4633132815361023, 0.1722378134727478, 0.2040041983127594, 0.08964771777391434, 0.0568072646856308, 0.15435616672039032, 0.21723438799381256], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e62051d30473a8835c28f0200d96df4f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 2048, 10, 10], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c42c0438ed5fbfb9adf134c82339690a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.4505515694618225, 0.22316744923591614, 0.4749869704246521, 0.49981579184532166, 0.3915441632270813, 0.4753076136112213, 0.26034829020500183, 0.22888652980327606, 0.018046021461486816, 0.18741662800312042, 0.0892471969127655, 0.4271770417690277, 0.20651808381080627, 0.4055662751197815, 0.37146660685539246, 0.3932384252548218, 0.1581479161977768, 0.07046151161193848, 0.23220238089561462, 0.22844034433364868, 0.43843260407447815, 0.21964862942695618, 0.12539365887641907, 0.49269038438796997, 0.10186173021793365, 0.22317299246788025, 0.20340411365032196, 0.23173262178897858, 0.3908565640449524, 0.22025030851364136], dtype='float32').reshape([30]),
            paddle.to_tensor([0.4229962229728699, 0.007352457847446203, 0.47365838289260864, 0.268498033285141, 0.36737534403800964, 0.22669461369514465, 0.349261075258255, 0.3063806891441345, 0.1419009268283844, 0.21391627192497253, 0.26761582493782043, 0.09718091785907745, 0.023347461596131325, 0.04777401685714722, 0.30343198776245117, 0.15685445070266724, 0.2897587716579437, 0.34699922800064087, 0.39604735374450684, 0.15426230430603027, 0.045028556138277054, 0.3206969201564789, 0.40161579847335815, 0.40596804022789, 0.23907320201396942, 0.2960168123245239, 0.4772741198539734, 0.3937113583087921, 0.10409269481897354, 0.04289533942937851], dtype='float32').reshape([30]),
            paddle.to_tensor([0.44806742668151855, 0.3270670771598816, 0.3332473337650299, 0.3377877473831177, 0.4653795659542084, 0.2929514944553375, 0.3968367874622345, 0.34529921412467957, 0.48523592948913574, 0.40970587730407715, 0.437796413898468, 0.19503602385520935, 0.31358006596565247, 0.342296302318573, 0.10723407566547394, 0.21393075585365295, 0.43409064412117004, 0.4205769896507263, 0.0926261842250824, 0.0032386723905801773, 0.19176620244979858, 0.23536162078380585, 0.1838054358959198, 0.28324824571609497, 0.002021642168983817, 0.08714741468429565, 0.4907427430152893, 0.24626709520816803, 0.35594117641448975, 0.44168540835380554], dtype='float32').reshape([30]),
            paddle.to_tensor([0.2091277539730072, 0.1781710386276245, 0.35560575127601624, 0.06796413660049438, 0.2863815426826477, 0.42233115434646606, 0.37065836787223816, 0.08094101399183273, 0.028482789173722267, 0.330558717250824, 0.12296363711357117, 0.1588757485151291, 0.11829111725091934, 0.2545422315597534, 0.41971588134765625, 0.2598586678504944, 0.30616700649261475, 0.2959647476673126, 0.11690250784158707, 0.24593031406402588, 0.3644527792930603, 0.30751365423202515, 0.49585601687431335, 0.3073747158050537, 0.026512151584029198, 0.3478871285915375, 0.30888572335243225, 0.3326703906059265, 0.14383253455162048, 0.3324868977069855], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_53c03582fdbe946c81856c99c10397e5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a2fa616a7f633e547afd4d6042bf2950(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 80, 80], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.42537251114845276, 0.38167276978492737, 0.048439037054777145, 0.3479856252670288, 0.4759884774684906, 0.003353372449055314, 0.03067563846707344, 0.1574259102344513, 0.2772923409938812, 0.4434056878089905, 0.012125715613365173, 0.07353668659925461, 0.16552993655204773, 0.4325833320617676, 0.3835062086582184, 0.2971155345439911, 0.423806756734848, 0.12586042284965515, 0.16657155752182007, 0.0663275346159935, 0.4657915532588959, 0.30554550886154175, 0.44385895133018494, 0.024752141907811165], dtype='float32').reshape([24]),
            paddle.to_tensor([0.1313217133283615, 0.22424693405628204, 0.39268168807029724, 0.15413393080234528, 0.12497349083423615, 0.4429291784763336, 0.14712311327457428, 0.22097212076187134, 0.23987692594528198, 0.351744681596756, 0.11637935042381287, 0.46826085448265076, 0.24085330963134766, 0.2207338511943817, 0.1566346436738968, 0.203997403383255, 0.07941398024559021, 0.4896209239959717, 0.27060309052467346, 0.41188889741897583, 0.23856939375400543, 0.47812044620513916, 0.37210336327552795, 0.07610848546028137], dtype='float32').reshape([24]),
            paddle.to_tensor([0.39803755283355713, 0.24617674946784973, 0.25974714756011963, 0.30574047565460205, 0.4697791635990143, 0.4106544852256775, 0.2039882242679596, 0.12000863254070282, 0.4439683258533478, 0.2495347261428833, 0.40724584460258484, 0.23533128201961517, 0.4187208414077759, 0.006475168280303478, 0.24781069159507751, 0.44089609384536743, 0.1518602818250656, 0.2704179883003235, 0.21382305026054382, 0.4756142199039459, 0.18131348490715027, 0.30896756052970886, 0.3499865233898163, 0.19152230024337769], dtype='float32').reshape([24]),
            paddle.to_tensor([0.3639483153820038, 0.07653755694627762, 0.1412375122308731, 0.2640598714351654, 0.4550042450428009, 0.45087742805480957, 0.3529861867427826, 0.006286390125751495, 0.1257268786430359, 0.25389569997787476, 0.17007359862327576, 0.12702924013137817, 0.4599754214286804, 0.1703663021326065, 0.24800044298171997, 0.05471066012978554, 0.34640175104141235, 0.0988759845495224, 0.4712427258491516, 0.4699699282646179, 0.20992206037044525, 0.21237683296203613, 0.10950934141874313, 0.19190391898155212], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cffbd8b3caf0d757da888be4dc1e5c4b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 52, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([52], dtype='float32', min=0, max=0.5),
            paddle.uniform([52], dtype='float32', min=0, max=0.5),
            paddle.uniform([52], dtype='float32', min=0, max=0.5),
            paddle.uniform([52], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b063944624c83e42c56f2bb702917bd1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 112, 112], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.40393710136413574, 0.42770060896873474, 0.13398438692092896, 0.019196433946490288, 0.28444716334342957, 0.18443992733955383, 0.16378775238990784, 0.2748048007488251], dtype='float32').reshape([8]),
            paddle.to_tensor([0.13093218207359314, 0.0016909849364310503, 0.4901002049446106, 0.027118699625134468, 0.45282310247421265, 0.09573368728160858, 0.25550615787506104, 0.13170507550239563], dtype='float32').reshape([8]),
            paddle.to_tensor([0.04224977269768715, 0.4950590431690216, 0.2090226262807846, 0.09649346023797989, 0.11767160892486572, 0.45237618684768677, 0.2761373817920685, 0.12765929102897644], dtype='float32').reshape([8]),
            paddle.to_tensor([0.17379756271839142, 0.4917494058609009, 0.1297222375869751, 0.03969841077923775, 0.45027732849121094, 0.47397613525390625, 0.24414603412151337, 0.4952484965324402], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1b73be516a2595c2a5bd5ff15aa5a346(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 168, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([168], dtype='float32', min=0, max=0.5),
            paddle.uniform([168], dtype='float32', min=0, max=0.5),
            paddle.uniform([168], dtype='float32', min=0, max=0.5),
            paddle.uniform([168], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f2b1aa3b9465437313283e091ce2d7ec(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a8301b810bccba10e4dfdee0df73334
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bdf39b87d742a27dc992d097a1f96540(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 768, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4d7b7d81a837269c3e4ec09bb17ee163(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 16, 16], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_51833a9a06b622832271bede3e3d925c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 80, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ead4e9297cfea0c20f4b600cf2ae66d7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 160, 160], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9acad7ae0ee9a7c57660267148c83a5c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.3247493803501129, 0.3410290479660034, 0.2418631911277771, 0.014815476723015308, 0.09579439461231232, 0.2791941463947296, 0.051667533814907074, 0.15621493756771088, 0.2889137864112854, 0.24687518179416656, 0.24443913996219635, 0.3103328049182892, 0.39130234718322754, 0.4014468491077423, 0.24492469429969788, 0.07792147248983383, 0.23567795753479004, 0.2958533465862274], dtype='float32').reshape([18]),
            paddle.to_tensor([0.353032648563385, 0.1733790934085846, 0.28266391158103943, 0.37521567940711975, 0.3610376715660095, 0.3710177540779114, 0.08845742791891098, 0.11090455949306488, 0.07772669941186905, 0.3723064064979553, 0.27244865894317627, 0.17404088377952576, 0.0492301844060421, 0.29516008496284485, 0.2295139580965042, 0.4351028800010681, 0.4377419948577881, 0.1630742996931076], dtype='float32').reshape([18]),
            paddle.to_tensor([0.06208319962024689, 0.29538795351982117, 0.09673447906970978, 0.4590131342411041, 0.1687469482421875, 0.3926222324371338, 0.2961215376853943, 0.294720858335495, 0.4356662333011627, 0.07092735171318054, 0.43708211183547974, 0.08870019763708115, 0.21009619534015656, 0.11984039843082428, 0.4558361768722534, 0.03942495584487915, 0.19271361827850342, 0.3368443548679352], dtype='float32').reshape([18]),
            paddle.to_tensor([0.08223345130681992, 0.3774470388889313, 0.3454022705554962, 0.16573412716388702, 0.15473057329654694, 0.03333558514714241, 0.07707877457141876, 0.41779419779777527, 0.4861306846141815, 0.15751010179519653, 0.4041177034378052, 0.37463417649269104, 0.06535550951957703, 0.26333868503570557, 0.2103705257177353, 0.40858566761016846, 0.3664426803588867, 0.37059253454208374], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_969c0cc600315bef2775a4373447a6c8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 80, 64, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_69bdfb57e52b409e4bc91e3fe92b0998(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.41972166299819946, 0.22119282186031342, 0.10360696166753769, 0.42853808403015137, 0.23743876814842224, 0.3746054172515869, 0.19740352034568787, 0.185701385140419, 0.4503405690193176, 0.22690357267856598, 0.4778973460197449, 0.4576164782047272, 0.21177449822425842, 0.49574846029281616, 0.3753088712692261, 0.12504446506500244, 0.20454876124858856, 0.3161174952983856], dtype='float32').reshape([18]),
            paddle.to_tensor([0.011295843869447708, 0.24247956275939941, 0.0903487429022789, 0.07228392362594604, 0.32077279686927795, 0.18325398862361908, 0.3087552785873413, 0.22799614071846008, 0.30468085408210754, 0.21097823977470398, 0.4615721106529236, 0.016125231981277466, 0.1719491332769394, 0.31620270013809204, 0.34223389625549316, 0.26151949167251587, 0.2910536825656891, 0.4940415918827057], dtype='float32').reshape([18]),
            paddle.to_tensor([0.17182672023773193, 0.453375905752182, 0.24648351967334747, 0.10856959968805313, 0.3270286023616791, 0.2559296488761902, 0.3666421175003052, 0.18610656261444092, 0.4261343479156494, 0.2625247836112976, 0.11457963287830353, 0.1691160649061203, 0.45487961173057556, 0.05973714217543602, 0.270675390958786, 0.3685804605484009, 0.03691960871219635, 0.49345582723617554], dtype='float32').reshape([18]),
            paddle.to_tensor([0.2219567596912384, 0.3421929180622101, 0.2086939513683319, 0.36541664600372314, 0.3723755478858948, 0.20521508157253265, 0.40072110295295715, 0.0743594765663147, 0.08198662847280502, 0.4485985338687897, 0.05000891536474228, 0.2648355960845947, 0.09431453049182892, 0.08701891452074051, 0.12739624083042145, 0.05731712654232979, 0.06841934472322464, 0.23387745022773743], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3c22e99a19551864e4c2c53dc9e25197(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_b4f4da3a8d44d1b5b3a2903a097df186
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 128, 128], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bb24c2846b037902869dff1123f33e09(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 512, 256], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.34398752450942993, 0.16478097438812256, 0.013258332386612892, 0.28085434436798096, 0.15349265933036804, 0.3783819377422333, 0.4394093155860901, 0.2790936231613159, 0.36658087372779846, 0.0462486632168293, 0.4064960777759552, 0.2951851189136505, 0.30362457036972046, 0.4845699667930603, 0.4621065557003021, 0.4863912761211395], dtype='float32').reshape([16]),
            paddle.to_tensor([0.37875235080718994, 0.4989354908466339, 0.27068448066711426, 0.11907541006803513, 0.2484893649816513, 0.39229443669319153, 0.4332391321659088, 0.06289886683225632, 0.25776684284210205, 0.04652851074934006, 0.2862085998058319, 0.4152562618255615, 0.10112620145082474, 0.4668711721897125, 0.2898155152797699, 0.2598496973514557], dtype='float32').reshape([16]),
            paddle.to_tensor([0.4818981885910034, 0.27806541323661804, 0.4452722370624542, 0.11428110301494598, 0.396393358707428, 0.21401835978031158, 0.3081253468990326, 0.17686869204044342, 0.09646168351173401, 0.4647543728351593, 0.3621642589569092, 0.2680127024650574, 0.053738467395305634, 0.0223684161901474, 0.15829332172870636, 0.3697681128978729], dtype='float32').reshape([16]),
            paddle.to_tensor([0.30004438757896423, 0.4954417645931244, 0.048408493399620056, 0.12823155522346497, 0.4659421741962433, 0.09346471726894379, 0.28264957666397095, 0.29121220111846924, 0.18727628886699677, 0.35106077790260315, 0.10676997154951096, 0.42882347106933594, 0.02459893934428692, 0.09940184652805328, 0.42533645033836365, 0.12224673479795456], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_92658b05560d4b89956c248facace849(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 702, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([702], dtype='float32', min=0, max=0.5),
            paddle.uniform([702], dtype='float32', min=0, max=0.5),
            paddle.uniform([702], dtype='float32', min=0, max=0.5),
            paddle.uniform([702], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_959f4eb8cf48d48b76194c825afd590b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 304, 304], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_38c7e36e5b80740950b23c4fb46c3cc5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 16, 16], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2337d3e6f0690d181df064dd0ebbf595(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 32, 100], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ee153eafdfdc41ee918b7884333c40d1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 256, 128], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.45648354291915894, 0.2715393304824829, 0.08847768604755402, 0.3856413662433624, 0.216139554977417, 0.48798877000808716, 0.42977356910705566, 0.39926791191101074, 0.3565323054790497, 0.13018916547298431, 0.45886191725730896, 0.2841354012489319, 0.07750681042671204, 0.09505537897348404, 0.3539663255214691, 0.17877818644046783, 0.30827397108078003, 0.36291247606277466, 0.23661498725414276, 0.43954983353614807, 0.17815057933330536, 0.2946692109107971, 0.49928605556488037, 0.4174374043941498], dtype='float32').reshape([24]),
            paddle.to_tensor([0.25915980339050293, 0.06832089275121689, 0.2717980742454529, 0.27226337790489197, 0.23769405484199524, 0.1979001760482788, 0.48746809363365173, 0.4199419617652893, 0.15543510019779205, 0.2915296256542206, 0.07024670392274857, 0.16426178812980652, 0.1756279468536377, 0.27649375796318054, 0.482456773519516, 0.37519198656082153, 0.1990574598312378, 0.2931423783302307, 0.2789490222930908, 0.12489677965641022, 0.25536736845970154, 0.3448886275291443, 0.2550352215766907, 0.34107694029808044], dtype='float32').reshape([24]),
            paddle.to_tensor([0.43755480647087097, 0.4762260615825653, 0.03007330372929573, 0.039920441806316376, 0.1727123111486435, 0.22438223659992218, 0.09382366389036179, 0.37848618626594543, 0.4813072085380554, 0.15360127389431, 0.39093706011772156, 0.3086056113243103, 0.24665352702140808, 0.3273041844367981, 0.3862830698490143, 0.48095569014549255, 0.12767179310321808, 0.3669237196445465, 0.23147498071193695, 0.03345390409231186, 0.03460521623492241, 0.2566245496273041, 0.38069871068000793, 0.3491151034832001], dtype='float32').reshape([24]),
            paddle.to_tensor([0.26945599913597107, 0.2883197069168091, 0.3778527081012726, 0.3104313611984253, 0.4895170032978058, 0.3262013792991638, 0.47340497374534607, 0.03165585547685623, 0.14533238112926483, 0.21249538660049438, 0.008664584718644619, 0.1312296837568283, 0.4587804079055786, 0.005589864682406187, 0.1900637298822403, 0.17859816551208496, 0.23323385417461395, 0.39500701427459717, 0.18737784028053284, 0.46244609355926514, 0.2601194381713867, 0.20766174793243408, 0.11174929887056351, 0.2653985023498535], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_766724a8d011b266035a1b1ed5c2d503(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_145ad20e7a3b3c8aa31f6b40d4fe3d45
    def get_inputs(self):
        return [
            paddle.uniform([49, 512], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_02e7800b11254683dcab17377e035093(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a8301b810bccba10e4dfdee0df73334
    def get_inputs(self):
        return [
            paddle.uniform([1, 112, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_959b2ca67282a9c7adeef590e1d96c15(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 2048, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_183fecd1ba0631fd4f29b967c2b6012d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 350, 25], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_36bf88563ced47919dca9ad4502d4a0f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.4705738425254822, 0.20563170313835144, 0.21544748544692993, 0.48658451437950134, 0.017780710011720657, 0.3812195956707001, 0.09096337109804153, 0.4256282150745392, 0.37283584475517273, 0.4010905623435974, 0.41508057713508606, 0.410849004983902, 0.42234984040260315, 0.2603086233139038, 0.13098230957984924, 0.41273170709609985, 0.2778257727622986, 0.06996414065361023, 0.1785626858472824, 0.2770371735095978], dtype='float32').reshape([20]),
            paddle.to_tensor([0.08582165092229843, 0.007479146588593721, 0.2835243344306946, 0.22579823434352875, 0.03834362328052521, 0.10208155959844589, 0.19898498058319092, 0.23674556612968445, 0.043152522295713425, 0.3802114427089691, 0.11495088785886765, 0.19431501626968384, 0.30619364976882935, 0.020488116890192032, 0.20741532742977142, 0.296043336391449, 0.30630460381507874, 0.41242527961730957, 0.4241424798965454, 0.012811211869120598], dtype='float32').reshape([20]),
            paddle.to_tensor([0.10321879386901855, 0.3413245975971222, 0.21098308265209198, 0.018059944733977318, 0.06355975568294525, 0.0705963671207428, 0.3187965750694275, 0.2784081995487213, 0.011543254368007183, 0.2863325774669647, 0.23729068040847778, 0.16913774609565735, 0.09876593202352524, 0.038499318063259125, 0.1016237884759903, 0.4523651897907257, 0.36986225843429565, 0.1086299866437912, 0.013502395711839199, 0.11056219786405563], dtype='float32').reshape([20]),
            paddle.to_tensor([0.11735747754573822, 0.009665017947554588, 0.2172432243824005, 0.3965957760810852, 0.10986741632223129, 0.22824175655841827, 0.3756791055202484, 0.199087455868721, 0.30083906650543213, 0.15396444499492645, 0.27013370394706726, 0.1667429655790329, 0.15130947530269623, 0.1304800808429718, 0.436555415391922, 0.44190236926078796, 0.4083445370197296, 0.34416234493255615, 0.18700534105300903, 0.06277256458997726], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a2b2113ffee33c2baa59700466ca2b47(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 480, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_34b0099e1201eee8b349702b71428da4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 80, 80], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4c1c77ca16ed7240c83da96cc4253e06(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 128, 128], dtype='float16', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f060e387800588640c9b0016da25d0e6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.04789397493004799, 0.3434475064277649, 0.4521530866622925, 0.32968854904174805, 0.2184416800737381, 0.30338990688323975, 0.04838941991329193, 0.10812339931726456, 0.13728737831115723, 0.1336604803800583, 0.0163519736379385, 0.010290002450346947, 0.23388785123825073, 0.3500322699546814, 0.49325111508369446, 0.31529363989830017, 0.37765055894851685, 0.1484086662530899], dtype='float32').reshape([18]),
            paddle.to_tensor([0.1477382332086563, 0.1552569419145584, 0.49571067094802856, 0.01740243099629879, 0.21493607759475708, 0.13905669748783112, 0.41749924421310425, 0.33817437291145325, 0.1568455994129181, 0.42697200179100037, 0.45129430294036865, 0.006329139228910208, 0.1866985708475113, 0.49749282002449036, 0.25639769434928894, 0.16801349818706512, 0.4964697062969208, 0.21695710718631744], dtype='float32').reshape([18]),
            paddle.to_tensor([0.05772045999765396, 0.11369907110929489, 0.41662880778312683, 0.4416993260383606, 0.4870401918888092, 0.48947808146476746, 0.44893965125083923, 0.06147105619311333, 0.154861181974411, 0.4020980894565582, 0.07120492309331894, 0.3723470866680145, 0.44711875915527344, 0.07245221734046936, 0.006226261146366596, 0.368020236492157, 0.44388991594314575, 0.27052128314971924], dtype='float32').reshape([18]),
            paddle.to_tensor([0.3733518719673157, 0.10621657967567444, 0.4417460560798645, 0.3042732775211334, 0.0073426030576229095, 0.17876248061656952, 0.2791590392589569, 0.43977779150009155, 0.3539111912250519, 0.22292523086071014, 0.09032437950372696, 0.45425331592559814, 0.1042778417468071, 0.47750788927078247, 0.07996497303247452, 0.09360441565513611, 0.0349278599023819, 0.02664920501410961], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_60f8b95aba499b85b6b22561e5c747f1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7a744866386aab0c7cba4745ce3875c8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 32, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4855484834c65f53c76738d448269bd9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 320, 320], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.22243256866931915, 0.30074915289878845, 0.3851374387741089, 0.2133142352104187, 0.1303640902042389, 0.11198440939188004, 0.4599904716014862, 0.31919053196907043], dtype='float32').reshape([8]),
            paddle.to_tensor([0.19814293086528778, 0.2009175568819046, 0.4694008529186249, 0.20730392634868622, 0.3525989353656769, 0.4638407826423645, 0.01689838618040085, 0.2886647582054138], dtype='float32').reshape([8]),
            paddle.to_tensor([0.11280502378940582, 0.21652334928512573, 0.21912871301174164, 0.0770564004778862, 0.4465886354446411, 0.42936238646507263, 0.12569616734981537, 0.26404228806495667], dtype='float32').reshape([8]),
            paddle.to_tensor([0.08750828355550766, 0.1375093013048172, 0.2525668144226074, 0.33178311586380005, 0.37002694606781006, 0.38394930958747864, 0.18842127919197083, 0.33900606632232666], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_a02f1e2888bea69fc79b31a122ed24eb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_373430720ed08419bcefd0b2c6786694(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_a02f1e2888bea69fc79b31a122ed24eb
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 26], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_167c7a73b6b24a5c0f86b90f54dacd0b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_3a47a120d62a3f3a422a747f511df8d9
    def get_inputs(self):
        return [
            paddle.uniform([1, 672, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d83f2b20463a52c863ab5e7795c885ef(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 2048, 20, 20], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_433c0397ab823465c5848d084a4a7858(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([8, 512, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a24b4c4cf2343ed1b655a3cc6f6c4ffc(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 512, 1024], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.4284777343273163, 0.46107444167137146, 0.16018129885196686, 0.300402969121933, 0.3286516070365906, 0.3103320896625519, 0.1373235136270523, 0.2789466381072998, 0.07557565718889236, 0.3513312041759491, 0.12668363749980927, 0.09847424179315567, 0.4915325343608856, 0.14386941492557526, 0.2850468158721924, 0.2994556427001953], dtype='float32').reshape([16]),
            paddle.to_tensor([0.04864060878753662, 0.21208150684833527, 0.43867620825767517, 0.33180344104766846, 0.05015525221824646, 0.24951289594173431, 0.14126233756542206, 0.20381633937358856, 0.43153688311576843, 0.20390456914901733, 0.413702130317688, 0.12756332755088806, 0.059672750532627106, 0.04379179701209068, 0.39610224962234497, 0.34660136699676514], dtype='float32').reshape([16]),
            paddle.to_tensor([0.03437139466404915, 0.13912032544612885, 0.26033908128738403, 0.2818862795829773, 0.04796707257628441, 0.10127667337656021, 0.0376809686422348, 0.0023544514551758766, 0.20385806262493134, 0.3021366000175476, 0.08053650707006454, 0.48254120349884033, 0.43268659710884094, 0.12504589557647705, 0.3226405084133148, 0.39094680547714233], dtype='float32').reshape([16]),
            paddle.to_tensor([0.06332529336214066, 0.24829843640327454, 0.08007565885782242, 0.1457909494638443, 0.04698813334107399, 0.045738205313682556, 0.22347360849380493, 0.10279962420463562, 0.375708669424057, 0.011785682290792465, 0.2850685119628906, 0.22272539138793945, 0.20157839357852936, 0.2920859754085541, 0.08078998327255249, 0.25987935066223145], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7a872ae421fedd547b7705a500b493c3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 160, 160], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.4795006513595581, 0.4192701578140259, 0.18814809620380402, 0.4978106915950775, 0.28273260593414307, 0.41978636384010315, 0.3585967719554901, 0.18395060300827026], dtype='float32').reshape([8]),
            paddle.to_tensor([0.2146615982055664, 0.2766416072845459, 0.08623068034648895, 0.2762782573699951, 0.23908771574497223, 0.21087610721588135, 0.08845720440149307, 0.00984217319637537], dtype='float32').reshape([8]),
            paddle.to_tensor([0.2332535684108734, 0.4160931408405304, 0.36906278133392334, 0.34109973907470703, 0.16490085422992706, 0.22543124854564667, 0.3242174983024597, 0.46922433376312256], dtype='float32').reshape([8]),
            paddle.to_tensor([0.15110084414482117, 0.037457164376974106, 0.34288424253463745, 0.4389215409755707, 0.2661478519439697, 0.11320039629936218, 0.09337110072374344, 0.3395006060600281], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_89a070e2ef3e820e7d37b9edee5e8bb7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 32, 64], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.24241270124912262, 0.07801158726215363, 0.20167364180088043, 0.17066608369350433, 0.13916254043579102, 0.4059564769268036, 0.005247496999800205, 0.010623233392834663, 0.23983319103717804, 0.20373107492923737, 0.40844258666038513, 0.27011236548423767, 0.3458002209663391, 0.20078451931476593, 0.485726922750473, 0.03421753644943237, 0.16265301406383514, 0.2433338463306427], dtype='float32').reshape([18]),
            paddle.to_tensor([0.038251377642154694, 0.4384799897670746, 0.2962875962257385, 0.39596396684646606, 0.29816001653671265, 0.38654154539108276, 0.11870256066322327, 0.3578633964061737, 0.20093336701393127, 0.3131216764450073, 0.44066229462623596, 0.05848781764507294, 0.19305771589279175, 0.31868767738342285, 0.11335960030555725, 0.020061297342181206, 0.2840416431427002, 0.08947249501943588], dtype='float32').reshape([18]),
            paddle.to_tensor([0.41215264797210693, 0.0961703509092331, 0.1890663206577301, 0.19412051141262054, 0.41759344935417175, 0.16341909766197205, 0.29527798295021057, 0.13816244900226593, 0.0882103368639946, 0.26967301964759827, 0.008747759275138378, 0.2680959403514862, 0.46397730708122253, 0.286346435546875, 0.10491140186786652, 0.17088444530963898, 0.169303297996521, 0.03865959122776985], dtype='float32').reshape([18]),
            paddle.to_tensor([0.21132998168468475, 0.34238308668136597, 0.21711423993110657, 0.3930853307247162, 0.1976301670074463, 0.1371118575334549, 0.24371781945228577, 0.09880411624908447, 0.021452490240335464, 0.16991885006427765, 0.1763685941696167, 0.28073447942733765, 0.22737690806388855, 0.014562537893652916, 0.0951680988073349, 0.05977337807416916, 0.17962130904197693, 0.17367872595787048], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f5a63c8367b4ebf3ff6c3b47ed78a531(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_af91295b59c5fca17f890a802deb001f
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 4, 32, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0d7ca0ad10dc704827e3025d50637730(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 160, 160], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.3563889265060425, 0.3032568395137787, 0.4540744125843048, 0.406520277261734, 0.2656550407409668, 0.44353345036506653, 0.4485912322998047, 0.4900515079498291, 0.011223330162465572, 0.3947514593601227, 0.4650079309940338, 0.3806697428226471, 0.013029300607740879, 0.06868098676204681, 0.37685781717300415, 0.12854602932929993], dtype='float32').reshape([16]),
            paddle.to_tensor([0.14154797792434692, 0.167294442653656, 0.4365919232368469, 0.37606799602508545, 0.1296333223581314, 0.43725818395614624, 0.07532908767461777, 0.006381009239703417, 0.46559903025627136, 0.3074858486652374, 0.05114764720201492, 0.28897401690483093, 0.47490260004997253, 0.19506141543388367, 0.12710246443748474, 0.2262718230485916], dtype='float32').reshape([16]),
            paddle.to_tensor([0.14649227261543274, 0.36666685342788696, 0.05517667159438133, 0.12284865975379944, 0.3060915172100067, 0.30966275930404663, 0.026630934327840805, 0.42852166295051575, 0.21290551126003265, 0.3298281729221344, 0.3956628739833832, 0.19932816922664642, 0.12476524710655212, 0.23360714316368103, 0.41960614919662476, 0.115492083132267], dtype='float32').reshape([16]),
            paddle.to_tensor([0.08101043850183487, 0.10379044711589813, 0.220769464969635, 0.13709953427314758, 0.3468698561191559, 0.22618283331394196, 0.35150158405303955, 0.2642698585987091, 0.10996241867542267, 0.06886697560548782, 0.42058098316192627, 0.23290188610553741, 0.23059234023094177, 0.1970161348581314, 0.037415217608213425, 0.36083361506462097], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_556ba203c0f6ab7c32b646ed49692769(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 128, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8941ea1ec5e7ba93b4933251d09e08ed(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_a02f1e2888bea69fc79b31a122ed24eb
    def get_inputs(self):
        return [
            paddle.uniform([1, 50, 350], dtype='float16', min=0, max=0.5),
            paddle.uniform([50], dtype='float32', min=0, max=0.5),
            paddle.uniform([50], dtype='float32', min=0, max=0.5),
            paddle.uniform([50], dtype='float32', min=0, max=0.5),
            paddle.uniform([50], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_60f00ef98ad29508dda849ddae457f6a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.30233243107795715, 0.4659610390663147, 0.382024347782135, 0.07420069724321365, 0.3812805712223053, 0.04273053631186485, 0.1934383660554886, 0.292336106300354, 0.13041548430919647, 0.42604103684425354, 0.42656660079956055, 0.42772117257118225, 0.2430124431848526, 0.1446714699268341, 0.16934062540531158, 0.26046663522720337, 0.19248172640800476, 0.24031469225883484, 0.4881838262081146, 0.46446409821510315], dtype='float32').reshape([20]),
            paddle.to_tensor([0.1474023312330246, 0.06979679316282272, 0.4716378450393677, 0.33735379576683044, 0.04159918054938316, 0.014670347794890404, 0.38351738452911377, 0.36172953248023987, 0.4563838839530945, 0.316997766494751, 0.4346388578414917, 0.22011356055736542, 0.2928714156150818, 0.04552137479186058, 0.2399880439043045, 0.10921899229288101, 0.13429395854473114, 0.4301634728908539, 0.10891404747962952, 0.04816976562142372], dtype='float32').reshape([20]),
            paddle.to_tensor([0.29084113240242004, 0.2524877190589905, 0.1175110936164856, 0.12403945624828339, 0.28224998712539673, 0.006043822504580021, 0.13604818284511566, 0.2008877843618393, 0.41491907835006714, 0.256686806678772, 0.29144152998924255, 0.007899204269051552, 0.4277402460575104, 0.21569716930389404, 0.4129391312599182, 0.313179612159729, 0.23354798555374146, 0.18280044198036194, 0.1289350688457489, 0.19576744735240936], dtype='float32').reshape([20]),
            paddle.to_tensor([0.05930742248892784, 0.41232651472091675, 0.04751007258892059, 0.45698970556259155, 0.35361552238464355, 0.22971665859222412, 0.47543951869010925, 0.477120578289032, 0.4112093448638916, 0.07413967698812485, 0.4472194314002991, 0.11824340373277664, 0.19578178226947784, 0.006655855569988489, 0.41427648067474365, 0.39922285079956055, 0.379658043384552, 0.3538905084133148, 0.12161541730165482, 0.20503826439380646], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_375855d7ddf992cb87205eee4f5ea525(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.13561749458312988, 0.34480077028274536, 0.29309797286987305, 0.3655812442302704, 0.02783374860882759, 0.38837549090385437, 0.32688409090042114, 0.11285417526960373, 0.07839041203260422, 0.22332483530044556, 0.4026404917240143, 0.061656344681978226, 0.18139927089214325, 0.32768988609313965, 0.4281405508518219, 0.4470050036907196, 0.49262574315071106, 0.0767674520611763], dtype='float32').reshape([18]),
            paddle.to_tensor([0.021931273862719536, 0.26629504561424255, 0.08497586846351624, 0.4964075982570648, 0.27716976404190063, 0.39082735776901245, 0.001635630615055561, 0.3252330720424652, 0.19813185930252075, 0.09965943545103073, 0.4712654948234558, 0.10690458118915558, 0.0788365826010704, 0.3976142108440399, 0.26868149638175964, 0.2693299353122711, 0.12108850479125977, 0.05693263188004494], dtype='float32').reshape([18]),
            paddle.to_tensor([0.3081601560115814, 0.15035176277160645, 0.11876150965690613, 0.2939392924308777, 0.19899389147758484, 0.4063831567764282, 0.2594914436340332, 0.18551792204380035, 0.10360196232795715, 0.08100258558988571, 0.31851035356521606, 0.35868126153945923, 0.24374602735042572, 0.4436042010784149, 0.12410090118646622, 0.2626171112060547, 0.363996297121048, 0.13769328594207764], dtype='float32').reshape([18]),
            paddle.to_tensor([0.2444526106119156, 0.03161003813147545, 0.10741061717271805, 0.06519508361816406, 0.35276204347610474, 0.025574222207069397, 0.4772929251194, 0.1672866940498352, 0.45576685667037964, 0.20476393401622772, 0.3855910003185272, 0.05040094256401062, 0.49829521775245667, 0.05307963117957115, 0.12552335858345032, 0.06064121425151825, 0.4652286469936371, 0.1788591742515564], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e1c984bc7e4322cf21a1844397da5b8d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 224, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([224], dtype='float32', min=0, max=0.5),
            paddle.uniform([224], dtype='float32', min=0, max=0.5),
            paddle.uniform([224], dtype='float32', min=0, max=0.5),
            paddle.uniform([224], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a6c2f22513c68bf4f835b9a97c9f8d0a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_af91295b59c5fca17f890a802deb001f
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 32, 64, 64], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.05704303830862045, 0.3429102599620819, 0.048069991171360016, 0.41409116983413696, 0.29746776819229126, 0.3548915684223175, 0.4075271487236023, 0.4728301167488098, 0.017294591292738914, 0.46770814061164856, 0.22781150043010712, 0.3088552951812744, 0.19137142598628998, 0.41078755259513855, 0.2101050168275833, 0.07647424936294556], dtype='float32').reshape([16]),
            paddle.to_tensor([0.059623681008815765, 0.42407163977622986, 0.21600431203842163, 0.2869966924190521, 0.14921370148658752, 0.14187569916248322, 0.23281769454479218, 0.22193238139152527, 0.10652448982000351, 0.45848357677459717, 0.02582589164376259, 0.0553295835852623, 0.43197089433670044, 0.45500829815864563, 0.3134944438934326, 0.061953164637088776], dtype='float32').reshape([16]),
            paddle.to_tensor([0.25863534212112427, 0.4418099820613861, 0.045295435935258865, 0.013259311206638813, 0.2625218629837036, 0.10032390058040619, 0.06094188988208771, 0.0022867252118885517, 0.32873883843421936, 0.14938676357269287, 0.015275565907359123, 0.1034589484333992, 0.38455647230148315, 0.19250090420246124, 0.49311020970344543, 0.28907856345176697], dtype='float32').reshape([16]),
            paddle.to_tensor([0.41166552901268005, 0.4314247965812683, 0.4415489137172699, 0.037331387400627136, 0.42040175199508667, 0.28789469599723816, 0.010961384512484074, 0.336641788482666, 0.11537421494722366, 0.09609737247228622, 0.24933350086212158, 0.1550600230693817, 0.1966736614704132, 0.40630197525024414, 0.1697378158569336, 0.11849628388881683], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_dd6a89057c25040bf86b0e8170f3dd40(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 26, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.12333758920431137, 0.08854036033153534, 0.38510021567344666, 0.12571673095226288, 0.338591605424881, 0.4279244840145111, 0.23931649327278137, 0.49025797843933105, 0.3870171308517456, 0.4776844084262848, 0.2650206983089447, 0.02551327459514141, 0.22646918892860413, 0.17348851263523102, 0.18400970101356506, 0.20177645981311798, 0.09866630285978317, 0.3745114505290985, 0.3180556893348694, 0.3094898462295532, 0.19916988909244537, 0.12162921577692032, 0.3685307204723358, 0.042971767485141754, 0.2556985020637512, 0.2664383351802826], dtype='float32').reshape([26]),
            paddle.to_tensor([0.0714883804321289, 0.22728043794631958, 0.10113278031349182, 0.35603803396224976, 0.16407082974910736, 0.24022208154201508, 0.4191322922706604, 0.256430983543396, 0.4086243510246277, 0.09251262247562408, 0.34693145751953125, 0.2668253183364868, 0.41247203946113586, 0.1360337883234024, 0.28171443939208984, 0.1869589239358902, 0.4897201657295227, 0.2539082169532776, 0.22445300221443176, 0.0032597112003713846, 0.23918643593788147, 0.4735250771045685, 0.29461121559143066, 0.0822555422782898, 0.47514209151268005, 0.4420781433582306], dtype='float32').reshape([26]),
            paddle.to_tensor([0.4232931435108185, 0.31521743535995483, 0.15759992599487305, 0.2082228809595108, 0.47389093041419983, 0.3198143243789673, 0.3145211637020111, 0.2744571268558502, 0.31483790278434753, 0.12793152034282684, 0.09963727742433548, 0.36761799454689026, 0.3007851243019104, 0.09542866051197052, 0.18476299941539764, 0.025642966851592064, 0.40552452206611633, 0.35712751746177673, 0.07356993854045868, 0.3471055030822754, 0.11661532521247864, 0.40304943919181824, 0.12053009122610092, 0.13021430373191833, 0.29265400767326355, 0.2708078920841217], dtype='float32').reshape([26]),
            paddle.to_tensor([0.097763292491436, 0.054452620446681976, 0.18151907622814178, 0.46789610385894775, 0.45066308975219727, 0.3034222722053528, 0.07751467823982239, 0.36121660470962524, 0.42680227756500244, 0.07783139497041702, 0.15334472060203552, 0.3829832077026367, 0.37121641635894775, 0.34851354360580444, 0.45160973072052, 0.33927246928215027, 0.2732488811016083, 0.294894278049469, 0.1639954000711441, 0.489069402217865, 0.36236563324928284, 0.01817527413368225, 0.15302930772304535, 0.19777004420757294, 0.2942997217178345, 0.14857454597949982], dtype='float32').reshape([26]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e734d68a46b3e29944359d6743a396be(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 636, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([636], dtype='float32', min=0, max=0.5),
            paddle.uniform([636], dtype='float32', min=0, max=0.5),
            paddle.uniform([636], dtype='float32', min=0, max=0.5),
            paddle.uniform([636], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ff9d9376218dd715e87083cc86b90a56(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 160, 160], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.11446868628263474, 0.03927907720208168, 0.0024818896781653166, 0.14562562108039856, 0.39831167459487915, 0.33169642090797424, 0.00894886627793312, 0.07045763731002808, 0.033750418573617935, 0.3561961054801941, 0.1095770075917244, 0.4791378676891327, 0.14215169847011566, 0.1764347106218338, 0.14007200300693512, 0.13225744664669037, 0.359733521938324, 0.22953994572162628, 0.4397531747817993, 0.1657656580209732, 0.17581342160701752, 0.29767048358917236, 0.4569110870361328, 0.2519034743309021], dtype='float32').reshape([24]),
            paddle.to_tensor([0.35996538400650024, 0.12369630485773087, 0.10422909259796143, 0.18151648342609406, 0.3357682228088379, 0.09762001782655716, 0.24364103376865387, 0.35251376032829285, 0.24548333883285522, 0.4742034673690796, 0.24483168125152588, 0.39242786169052124, 0.29778367280960083, 0.4048921763896942, 0.41214659810066223, 0.15717388689517975, 0.15996943414211273, 0.4050731360912323, 0.14921633899211884, 0.07077416777610779, 0.3253207504749298, 0.4372186064720154, 0.2657449245452881, 0.354824960231781], dtype='float32').reshape([24]),
            paddle.to_tensor([0.09127771854400635, 0.3673323690891266, 0.06853628903627396, 0.10410182923078537, 0.07314323633909225, 0.22084730863571167, 0.31292879581451416, 0.2826370298862457, 0.08473382890224457, 0.27287164330482483, 0.3093733489513397, 0.12087998539209366, 0.0991312563419342, 0.014092492870986462, 0.11177550256252289, 0.07031892985105515, 0.4483656883239746, 0.04550626128911972, 0.4690464437007904, 0.21627840399742126, 0.031534552574157715, 0.11467035859823227, 0.4197693467140198, 0.4468223452568054], dtype='float32').reshape([24]),
            paddle.to_tensor([0.34479033946990967, 0.1432499885559082, 0.1854700744152069, 0.0829690545797348, 0.3480140268802643, 0.4203314185142517, 0.2430073618888855, 0.2132807970046997, 0.3616867959499359, 0.04038165137171745, 0.15029121935367584, 0.19754484295845032, 0.20320045948028564, 0.2796321213245392, 0.18105874955654144, 0.2704549729824066, 0.475747287273407, 0.11201021820306778, 0.3813433051109314, 0.2494119554758072, 0.10283364355564117, 0.42244604229927063, 0.08390121906995773, 0.34156206250190735], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3a58ed8f846d363a0924bef90fd0da86(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.32636359333992004, 0.39576947689056396, 0.24312715232372284, 0.3648664057254791, 0.2771623432636261, 0.24714268743991852, 0.07840219885110855, 0.23002798855304718, 0.40153053402900696, 0.20736852288246155, 0.42782360315322876, 0.1463126242160797, 0.43731626868247986, 0.35056254267692566, 0.37185221910476685, 0.4343043565750122, 0.41534677147865295, 0.2672363221645355], dtype='float32').reshape([18]),
            paddle.to_tensor([0.4849882125854492, 0.34432733058929443, 0.44428423047065735, 0.4382339417934418, 0.42543303966522217, 0.30893370509147644, 0.3181474804878235, 0.37522196769714355, 0.25586822628974915, 0.3978688418865204, 0.11023305356502533, 0.3380576968193054, 0.2790479063987732, 0.0865875706076622, 0.40044721961021423, 0.09138946235179901, 0.11607969552278519, 0.3013041317462921], dtype='float32').reshape([18]),
            paddle.to_tensor([0.3658145070075989, 0.2939428687095642, 0.002369193360209465, 0.10566973686218262, 0.09208326041698456, 0.018731877207756042, 0.3557839095592499, 0.11957988142967224, 0.3070424497127533, 0.459367573261261, 0.39870503544807434, 0.4151509404182434, 0.1745065152645111, 0.4761123061180115, 0.23881441354751587, 0.0752042829990387, 0.12274198979139328, 0.22705025970935822], dtype='float32').reshape([18]),
            paddle.to_tensor([0.2845688462257385, 0.29278093576431274, 0.02717246301472187, 0.22824224829673767, 0.17637908458709717, 0.14183993637561798, 0.47086116671562195, 0.11774308234453201, 0.17122574150562286, 0.4865861237049103, 0.06751228123903275, 0.39676007628440857, 0.19639095664024353, 0.3301732838153839, 0.224294975399971, 0.3902205526828766, 0.33323028683662415, 0.2966246008872986], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5df9ec34282d229187056c10339a1c32(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 160, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d4d7b7f27bc1ec501550c5de43cb33b1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_3a47a120d62a3f3a422a747f511df8d9
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 10, 10], dtype='float16', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1ed1f2cdb23239d3f5e6fb8aeceb71a1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 256, 128], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.11581320315599442, 0.4968602657318115, 0.48283007740974426, 0.26889678835868835, 0.2836992144584656, 0.10151009261608124, 0.4445919692516327, 0.2404319941997528, 0.0008788500563241541, 0.36112257838249207, 0.08356752246618271, 0.3844813108444214, 0.49170371890068054, 0.49415123462677, 0.08875254541635513, 0.39381715655326843, 0.39377808570861816, 0.10406597703695297, 0.3702535331249237, 0.4348941445350647, 0.1039610430598259, 0.1370151787996292, 0.2797279357910156, 0.49718207120895386], dtype='float32').reshape([24]),
            paddle.to_tensor([0.2678377032279968, 0.1904393881559372, 0.25052979588508606, 0.3874213397502899, 0.037108372896909714, 0.30193600058555603, 0.3798035681247711, 0.048927947878837585, 0.14183050394058228, 0.03922201320528984, 0.38383111357688904, 0.0313086174428463, 0.11471103131771088, 0.2545369565486908, 0.06225024163722992, 0.19369705021381378, 0.33207857608795166, 0.21042092144489288, 0.0020924783311784267, 0.3414302170276642, 0.3137853741645813, 0.40259212255477905, 0.4096565842628479, 0.12251327186822891], dtype='float32').reshape([24]),
            paddle.to_tensor([0.44373977184295654, 0.03899434953927994, 0.43761175870895386, 0.14327624440193176, 0.029626112431287766, 0.18798714876174927, 0.3296409845352173, 0.173353374004364, 0.20771251618862152, 0.1575009971857071, 0.308480829000473, 0.4700124263763428, 0.21166209876537323, 0.12595033645629883, 0.038304366171360016, 0.3919476568698883, 0.09370799362659454, 0.3959525227546692, 0.05453727766871452, 0.4200788736343384, 0.21600456535816193, 0.40976327657699585, 0.4375747740268707, 0.35956382751464844], dtype='float32').reshape([24]),
            paddle.to_tensor([0.11589076370000839, 0.03853585571050644, 0.07849404215812683, 0.43316951394081116, 0.2253715693950653, 0.3023190498352051, 0.4172990918159485, 0.043855220079422, 0.45615488290786743, 0.033597927540540695, 0.30937862396240234, 0.1647314876317978, 0.2676487863063812, 0.4605195224285126, 0.11137685924768448, 0.48635610938072205, 0.4899517297744751, 0.21928349137306213, 0.28968244791030884, 0.36643344163894653, 0.4432797133922577, 0.09165667742490768, 0.24954356253147125, 0.4280867874622345], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5eeea7eca8e17938f64222dc57b75930(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 240, 240], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.41299837827682495, 0.3860825300216675, 0.16331292688846588, 0.04528946802020073, 0.014731625095009804, 0.18787159025669098, 0.16840338706970215, 0.1059647873044014, 0.47811436653137207, 0.29439663887023926, 0.05005771294236183, 0.29808324575424194, 0.28677284717559814, 0.4342748522758484, 0.02554941736161709, 0.13686047494411469], dtype='float32').reshape([16]),
            paddle.to_tensor([0.22343188524246216, 0.4797534942626953, 0.49549880623817444, 0.11044672131538391, 0.06128701567649841, 0.3207375705242157, 0.03609634190797806, 0.2752283811569214, 0.1533578634262085, 0.29804086685180664, 0.08986719697713852, 0.4485247731208801, 0.2917874753475189, 0.06844504177570343, 0.3203842341899872, 0.11664433032274246], dtype='float32').reshape([16]),
            paddle.to_tensor([0.1583825796842575, 0.1303863674402237, 0.017481572926044464, 0.09682594984769821, 0.45414263010025024, 0.03049306385219097, 0.1619362235069275, 0.1402808278799057, 0.08353244513273239, 0.31537187099456787, 0.48645493388175964, 0.09390440583229065, 0.40590202808380127, 0.10868364572525024, 0.43011271953582764, 0.018106283619999886], dtype='float32').reshape([16]),
            paddle.to_tensor([0.16002674400806427, 0.35431966185569763, 0.0807928815484047, 0.41788598895072937, 0.08784855157136917, 0.022889062762260437, 0.4973422884941101, 0.017493462190032005, 0.3579718768596649, 0.3839259743690491, 0.10580223053693771, 0.37718889117240906, 0.10755158960819244, 0.4111896753311157, 0.43192100524902344, 0.03684293106198311], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_25b79a7c8839d4c2883cc38f49e917f6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 12, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.2256605327129364, 0.36584168672561646, 0.1469140499830246, 0.02609957940876484, 0.3126712143421173, 0.047517307102680206, 0.3147978186607361, 0.18147510290145874, 0.4065723717212677, 0.18736764788627625, 0.4813879728317261, 0.4883688688278198], dtype='float32').reshape([12]),
            paddle.to_tensor([0.023502741008996964, 0.400027334690094, 0.09813759475946426, 0.4561724364757538, 0.22834047675132751, 0.13355165719985962, 0.09937845170497894, 0.42862939834594727, 0.43659478425979614, 0.22034157812595367, 0.4900567829608917, 0.26324549317359924], dtype='float32').reshape([12]),
            paddle.to_tensor([0.43129533529281616, 0.4816831946372986, 0.36150866746902466, 0.43539515137672424, 0.3261048495769501, 0.15301749110221863, 0.22857221961021423, 0.3957005739212036, 0.29969748854637146, 0.3687248229980469, 0.21582446992397308, 0.18168407678604126], dtype='float32').reshape([12]),
            paddle.to_tensor([0.4274587333202362, 0.3218536674976349, 0.064915731549263, 0.3717610836029053, 0.22100697457790375, 0.29881876707077026, 0.33098286390304565, 0.4200202226638794, 0.3315402567386627, 0.26447251439094543, 0.24580784142017365, 0.11266914010047913], dtype='float32').reshape([12]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1a08767a77d64f118029fe79e482d18b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 40, 40], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.1141095831990242, 0.11832092702388763, 0.10926369577646255, 0.15830470621585846, 0.2971542477607727, 0.4197443127632141, 0.2696124017238617, 0.21883173286914825, 0.2492988109588623, 0.10151580721139908, 0.1566789597272873, 0.013187129981815815, 0.3914187550544739, 0.0958016887307167, 0.2083076536655426, 0.3233848214149475, 0.36440855264663696, 0.4756644666194916, 0.001008894992992282, 0.4010638892650604, 0.48153364658355713, 0.18255336582660675, 0.19951550662517548, 0.27610647678375244], dtype='float32').reshape([24]),
            paddle.to_tensor([0.19234521687030792, 0.12541644275188446, 0.05378353223204613, 0.02109653316438198, 0.4342310428619385, 0.45827123522758484, 0.29706627130508423, 0.06185787543654442, 0.44253870844841003, 0.20937323570251465, 0.04599647596478462, 0.4367910325527191, 0.42189866304397583, 0.4572339653968811, 0.1778675764799118, 0.04411757364869118, 0.336529016494751, 0.06380776315927505, 0.25721415877342224, 0.3623991310596466, 0.4089316725730896, 0.28385835886001587, 0.1323477178812027, 0.17476455867290497], dtype='float32').reshape([24]),
            paddle.to_tensor([0.07508815824985504, 0.2416270524263382, 0.47870558500289917, 0.371420294046402, 0.03104512393474579, 0.31699395179748535, 0.2998288869857788, 0.4728754460811615, 0.3900682032108307, 0.3920108377933502, 0.1382286250591278, 0.46411603689193726, 0.453296035528183, 0.2577800154685974, 0.33363717794418335, 0.3600800037384033, 0.13006767630577087, 0.30931153893470764, 0.29564976692199707, 0.08971969783306122, 0.29008013010025024, 0.4275183081626892, 0.15458545088768005, 0.32470017671585083], dtype='float32').reshape([24]),
            paddle.to_tensor([0.08630868047475815, 0.05413864180445671, 0.01692281849682331, 0.46546778082847595, 0.43759119510650635, 0.22371432185173035, 0.37239155173301697, 0.4371836185455322, 0.3767794668674469, 0.26282402873039246, 0.09204583615064621, 0.339626282453537, 0.2996046245098114, 0.4413006603717804, 0.18869467079639435, 0.07390960305929184, 0.488384485244751, 0.3387542963027954, 0.1985122114419937, 0.2957294285297394, 0.17421483993530273, 0.29662758111953735, 0.4462672472000122, 0.47360652685165405], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8ab29aa41f40d83dd1f017e1d79c2e74(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 25, 42], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8ce42903cf9b154c45cc91fc11b908d4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 132, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([132], dtype='float32', min=0, max=0.5),
            paddle.uniform([132], dtype='float32', min=0, max=0.5),
            paddle.uniform([132], dtype='float32', min=0, max=0.5),
            paddle.uniform([132], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c2928656c51c1ba6ab13381a19867213(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 80, 80], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_862256c62a1a22852c828c1826b16055(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 2048, 25, 42], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d1bbbbf2245cb8134806992e36f8dd59(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a8301b810bccba10e4dfdee0df73334
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.07611335068941116, 0.09105866402387619, 0.3617575764656067, 0.34548813104629517, 0.27601006627082825, 0.18613120913505554, 0.41194459795951843, 0.22330957651138306, 0.26313304901123047, 0.3956887423992157, 0.49266350269317627, 0.007684779353439808, 0.3925292491912842, 0.23787663877010345, 0.22923611104488373, 0.18588833510875702, 0.10735301673412323, 0.3851848244667053, 0.3866106867790222, 0.26461276412010193, 0.05755780264735222, 0.37089699506759644, 0.373028963804245, 0.37469080090522766], dtype='float32').reshape([24]),
            paddle.to_tensor([0.3274034857749939, 0.4917196035385132, 0.3278880715370178, 0.194943368434906, 0.32123908400535583, 0.24896448850631714, 0.2575792968273163, 0.4123009145259857, 0.2676023542881012, 0.02020014449954033, 0.357424259185791, 0.43632081151008606, 0.17196215689182281, 0.39870792627334595, 0.27510160207748413, 0.45688608288764954, 0.44239866733551025, 0.20079122483730316, 0.28260183334350586, 0.2263387143611908, 0.3701542615890503, 0.12607011198997498, 0.3224470317363739, 0.39646485447883606], dtype='float32').reshape([24]),
            paddle.to_tensor([0.13242316246032715, 0.1558089256286621, 0.12372346222400665, 0.4686868488788605, 0.08248097449541092, 0.4319133758544922, 0.34307974576950073, 0.4624893069267273, 0.13326099514961243, 0.45284441113471985, 0.3926941454410553, 0.26101911067962646, 0.2584582567214966, 0.13649117946624756, 0.25593945384025574, 0.4696531295776367, 0.04659867286682129, 0.311350554227829, 0.24682074785232544, 0.35520586371421814, 0.08974983543157578, 0.24977797269821167, 0.015525237657129765, 0.40085849165916443], dtype='float32').reshape([24]),
            paddle.to_tensor([0.2554839849472046, 0.34761279821395874, 0.39824122190475464, 0.3269955813884735, 0.09288453310728073, 0.13703788816928864, 0.3354799747467041, 0.49729853868484497, 0.44330334663391113, 0.3151915967464447, 0.43129169940948486, 0.4398171901702881, 0.48771587014198303, 0.20875653624534607, 0.4819283187389374, 0.3506512939929962, 0.4745369553565979, 0.3144335150718689, 0.1615680307149887, 0.4458697736263275, 0.2695107161998749, 0.19926929473876953, 0.4535290002822876, 0.2271231859922409], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1e59e7224cfe32d84634d904a309a75b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 2048, 10, 10], dtype='float16', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f8a8dd35be62d5174a829c2d9a123d4a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_aace3e61c17184d9d634af90b0fd2dcb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 16, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d95540850b53dead5ba46c67e3b24694(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 320, 320], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.42108023166656494, 0.22564919292926788, 0.36123350262641907, 0.36037153005599976, 0.4959980249404907, 0.22247451543807983, 0.1589920073747635, 0.46831417083740234], dtype='float32').reshape([8]),
            paddle.to_tensor([0.49923744797706604, 0.16562704741954803, 0.454353928565979, 0.08077599853277206, 0.045732855796813965, 0.08013362437486649, 0.07913873344659805, 0.3717062771320343], dtype='float32').reshape([8]),
            paddle.to_tensor([0.26305267214775085, 0.3056526780128479, 0.15373589098453522, 0.40235400199890137, 0.15032970905303955, 0.4678816795349121, 0.49123769998550415, 0.011519082821905613], dtype='float32').reshape([8]),
            paddle.to_tensor([0.46686816215515137, 0.15136836469173431, 0.20065104961395264, 0.439725786447525, 0.3742474615573883, 0.303303062915802, 0.46545088291168213, 0.2453698217868805], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c44a22b74ee31fb7f1ce33abbdad5a27(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 336, 40, 40], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9f19bb7cf6ae49def8a578ada27fa018(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 144, 112, 112], dtype='float16', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b62888419ace5e97051364cca9bf9d50(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 60, 60], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5e50f838f6b70eb0dd84353c87661338(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 118, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.uniform([118], dtype='float32', min=0, max=0.5),
            paddle.uniform([118], dtype='float32', min=0, max=0.5),
            paddle.uniform([118], dtype='float32', min=0, max=0.5),
            paddle.uniform([118], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8e16c54faa89fa85397fd5ea44a36201(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.12905371189117432, 0.4092091917991638, 0.259787380695343, 0.09379085153341293, 0.43783852458000183, 0.17793689668178558, 0.47643473744392395, 0.04601183161139488, 0.21385549008846283, 0.3026127219200134, 0.37340307235717773, 0.3467088043689728, 0.2189156860113144, 0.3679374158382416, 0.34718436002731323, 0.21471907198429108, 0.4306008517742157, 0.35065704584121704, 0.47125107049942017, 0.08789914101362228, 0.3003585934638977, 0.28794848918914795, 0.0794730931520462, 0.44197869300842285, 0.0832148864865303, 0.0018369769677519798, 0.3264237642288208, 0.4509466886520386, 0.4152722656726837, 0.13635101914405823], dtype='float32').reshape([30]),
            paddle.to_tensor([0.34620538353919983, 0.11851092427968979, 0.4251331686973572, 0.2749614715576172, 0.39425089955329895, 0.10146646946668625, 0.4718330502510071, 0.2804933488368988, 0.42640066146850586, 0.0004072407609783113, 0.4817887544631958, 0.3058774769306183, 0.41402971744537354, 0.39735615253448486, 0.21147505939006805, 0.35778844356536865, 0.08044949918985367, 0.4143346846103668, 0.02140890620648861, 0.3272014856338501, 0.3096579909324646, 0.24111583828926086, 0.17287002503871918, 0.24215568602085114, 0.11968066543340683, 0.13505451381206512, 0.09280480444431305, 0.3119690716266632, 0.18336856365203857, 0.24660634994506836], dtype='float32').reshape([30]),
            paddle.to_tensor([0.18219062685966492, 0.49935436248779297, 0.3713228106498718, 0.1532364785671234, 0.34637120366096497, 0.2894476056098938, 0.36300548911094666, 0.2524467408657074, 0.2231205850839615, 0.4927220344543457, 0.023778036236763, 0.3716006278991699, 0.2195112109184265, 0.2535853385925293, 0.1467220038175583, 0.1384289413690567, 0.0677107721567154, 0.19668060541152954, 0.10118619352579117, 0.09578195959329605, 0.27816563844680786, 0.4822952151298523, 0.3493124842643738, 0.26194053888320923, 0.29043665528297424, 0.1839601993560791, 0.34663519263267517, 0.2205948382616043, 0.029664482921361923, 0.1939428746700287], dtype='float32').reshape([30]),
            paddle.to_tensor([0.23002879321575165, 0.06681539863348007, 0.1686365157365799, 0.1939288228750229, 0.13767121732234955, 0.1719658374786377, 0.4886939823627472, 0.03407924622297287, 0.11919726431369781, 0.3760576546192169, 0.38906988501548767, 0.47839421033859253, 0.1830388456583023, 0.4486812651157379, 0.32232555747032166, 0.21013212203979492, 0.30703696608543396, 0.24632687866687775, 0.033547088503837585, 0.4506857395172119, 0.2488563060760498, 0.4215278625488281, 0.11064202338457108, 0.09146048128604889, 0.4660150408744812, 0.18856245279312134, 0.39771273732185364, 0.2659538686275482, 0.27183112502098083, 0.4696770906448364], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_54ff35b87436bcf9e09918f40f3c50af(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_da4d85bc213f70819a72e68f21214cdb
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 32, 64, 64], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.32598403096199036, 0.0852632075548172, 0.16763745248317719, 0.08270502835512161, 0.0553133524954319, 0.061860036104917526, 0.21277277171611786, 0.26100289821624756], dtype='float32').reshape([8]),
            paddle.to_tensor([0.2890593409538269, 0.004232926294207573, 0.24888166785240173, 0.29167380928993225, 0.4347476065158844, 0.42767491936683655, 0.2710593044757843, 0.13438807427883148], dtype='float32').reshape([8]),
            paddle.to_tensor([0.3063638210296631, 0.1474483609199524, 0.16855917870998383, 0.31942158937454224, 0.3820127546787262, 0.41692107915878296, 0.22636575996875763, 0.0367923304438591], dtype='float32').reshape([8]),
            paddle.to_tensor([0.21928852796554565, 0.05624881014227867, 0.07532712072134018, 0.0855078473687172, 0.1312331110239029, 0.18500377237796783, 0.3106692135334015, 0.420186847448349], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3f3dc8f352a2cf33171544c1637a9646(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 10, 10], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a6c6d1dfe49475dd7b94d34fc091fc5a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 1952, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([1952], dtype='float32', min=0, max=0.5),
            paddle.uniform([1952], dtype='float32', min=0, max=0.5),
            paddle.uniform([1952], dtype='float32', min=0, max=0.5),
            paddle.uniform([1952], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b97b62dc7aa6b7d13df3f479c27b3cdc(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 112, 112], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.10415060818195343, 0.24882648885250092, 0.36528289318084717, 0.11325518786907196, 0.3454976975917816, 0.13371437788009644, 0.008257010951638222, 0.32619890570640564, 0.34657198190689087, 0.2557815611362457, 0.08141759037971497, 0.34924739599227905, 0.20994681119918823, 0.05629885941743851, 0.0063413046300411224, 0.025433028116822243, 0.44325414299964905, 0.29371845722198486, 0.057943038642406464, 0.2387848198413849, 0.0690142884850502, 0.38196781277656555, 0.14433494210243225, 0.3400508463382721], dtype='float32').reshape([24]),
            paddle.to_tensor([0.08569648861885071, 0.45676735043525696, 0.21612407267093658, 0.1818484365940094, 0.033052437007427216, 0.08415708690881729, 0.2812015116214752, 0.036635544151067734, 0.3181907832622528, 0.008028870448470116, 0.10108751803636551, 0.08795126527547836, 0.052070558071136475, 0.21322031319141388, 0.32893720269203186, 0.17109724879264832, 0.1645505726337433, 0.48473644256591797, 0.4144148528575897, 0.45852795243263245, 0.3676927089691162, 0.18491360545158386, 0.2557606101036072, 0.02797730639576912], dtype='float32').reshape([24]),
            paddle.to_tensor([0.2956199645996094, 0.33430609107017517, 0.1698489487171173, 0.03773022070527077, 0.4314742982387543, 0.344724178314209, 0.47622594237327576, 0.41830870509147644, 0.34024277329444885, 0.24486637115478516, 0.45235806703567505, 0.39486685395240784, 0.009599373675882816, 0.43324851989746094, 0.24111145734786987, 0.393125981092453, 0.44802725315093994, 0.27825504541397095, 0.17731444537639618, 0.25024473667144775, 0.08992922306060791, 0.016541628167033195, 0.09046430140733719, 0.3248574137687683], dtype='float32').reshape([24]),
            paddle.to_tensor([0.1873622089624405, 0.230330228805542, 0.44097641110420227, 0.3874419927597046, 0.07794477790594101, 0.19848817586898804, 0.43226373195648193, 0.15555340051651, 0.0787917822599411, 0.49282416701316833, 0.3559035360813141, 0.031680792570114136, 0.29639679193496704, 0.3238430917263031, 0.38273733854293823, 0.47300559282302856, 0.021949026733636856, 0.33455324172973633, 0.0713271051645279, 0.10203766822814941, 0.1588720828294754, 0.16953763365745544, 0.4113348126411438, 0.4218734800815582], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ce82bee3da8532e30ef48fb0d9aa9cae(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 28, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.29881954193115234, 0.21639975905418396, 0.3655684292316437, 0.17891450226306915, 0.2308436930179596, 0.06983558088541031, 0.4437817633152008, 0.23655414581298828, 0.0024737087078392506, 0.008118042722344398, 0.21759259700775146, 0.39628344774246216, 0.24668343365192413, 0.3773318827152252, 0.37654417753219604, 0.349447101354599, 0.31558308005332947, 0.11005203425884247, 0.06502465903759003, 0.3265439569950104, 0.1622447967529297, 0.37091439962387085, 0.18757113814353943, 0.12134452909231186, 0.12375837564468384, 0.3064180910587311, 0.4088982939720154, 0.2735852301120758], dtype='float32').reshape([28]),
            paddle.to_tensor([0.4991105794906616, 0.21489037573337555, 0.12154044210910797, 0.000550609256606549, 0.0037135197781026363, 0.030189381912350655, 0.21164481341838837, 0.1659241020679474, 0.40223759412765503, 0.10758019238710403, 0.3513813614845276, 0.229306161403656, 0.37425360083580017, 0.44641920924186707, 0.2729538083076477, 0.25947073101997375, 0.47710877656936646, 0.12910835444927216, 0.20348981022834778, 0.09829936921596527, 0.3998199701309204, 0.15074513852596283, 0.12221581488847733, 0.40342384576797485, 0.04137352481484413, 0.325662761926651, 0.4185892939567566, 0.005755157209932804], dtype='float32').reshape([28]),
            paddle.to_tensor([0.1928473860025406, 0.2761058211326599, 0.08859268575906754, 0.014811230823397636, 0.2700061798095703, 0.4687464237213135, 0.4976065456867218, 0.01993691734969616, 0.39248785376548767, 0.36473917961120605, 0.20917214453220367, 0.24068427085876465, 0.2835683226585388, 0.47535085678100586, 0.4163864254951477, 0.05647014454007149, 0.3321876525878906, 0.13707540929317474, 0.36605915427207947, 0.026302946731448174, 0.35359689593315125, 0.20943845808506012, 0.17095565795898438, 0.21969902515411377, 0.30840396881103516, 0.416667103767395, 0.3083786368370056, 0.3796353340148926], dtype='float32').reshape([28]),
            paddle.to_tensor([0.08576415479183197, 0.2090025395154953, 0.08367922902107239, 0.4179242253303528, 0.05571960285305977, 0.17839917540550232, 0.04753010347485542, 0.2291572540998459, 0.009482658468186855, 0.4266543388366699, 0.008638981729745865, 0.49483707547187805, 0.3301343619823456, 0.3057580888271332, 0.13395653665065765, 0.37405937910079956, 0.12452485412359238, 0.4576030969619751, 0.42267149686813354, 0.49011489748954773, 0.36738911271095276, 0.4767633080482483, 0.042820364236831665, 0.1327442079782486, 0.11817667633295059, 0.09088900685310364, 0.30830976366996765, 0.21643316745758057], dtype='float32').reshape([28]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d9a64632e19e1ed1dc3bc4f19b426e83(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.2078259438276291, 0.42871707677841187, 0.20467983186244965, 0.39866718649864197, 0.11433719098567963, 0.3938496708869934, 0.49091842770576477, 0.38484007120132446, 0.43632832169532776, 0.16921019554138184, 0.34473085403442383, 0.13219709694385529, 0.27167025208473206, 0.21253226697444916, 0.15550102293491364, 0.4836958050727844, 0.32571837306022644, 0.030484959483146667, 0.3360854685306549, 0.37282806634902954, 0.03380356729030609, 0.20491360127925873, 0.2677552402019501, 0.05628976598381996], dtype='float32').reshape([24]),
            paddle.to_tensor([0.07893761247396469, 0.28167277574539185, 0.49543827772140503, 0.3674749732017517, 0.18449412286281586, 0.1874522715806961, 0.39346298575401306, 0.47475337982177734, 0.3314509987831116, 0.3706306517124176, 0.33422765135765076, 0.1144292950630188, 0.46426647901535034, 0.030068546533584595, 0.31605422496795654, 0.4572479724884033, 0.01589660532772541, 0.3277362585067749, 0.3224765956401825, 0.2108854055404663, 0.39818641543388367, 0.32555899024009705, 0.03142363950610161, 0.2718063294887543], dtype='float32').reshape([24]),
            paddle.to_tensor([0.39289066195487976, 0.19869421422481537, 0.03374427929520607, 0.27824726700782776, 0.18731026351451874, 0.32764774560928345, 0.3521103858947754, 0.3182874619960785, 0.016876066103577614, 0.3463550806045532, 0.15992282330989838, 0.47791486978530884, 0.3713153004646301, 0.3030273914337158, 0.46610575914382935, 0.40472114086151123, 0.3406710922718048, 0.19949939846992493, 0.27761614322662354, 0.3573512136936188, 0.09674327820539474, 0.2809768319129944, 0.4119584858417511, 0.31179139018058777], dtype='float32').reshape([24]),
            paddle.to_tensor([0.34247204661369324, 0.20089353621006012, 0.38176047801971436, 0.27918002009391785, 0.05546856299042702, 0.11732389777898788, 0.16744491457939148, 0.4603182077407837, 0.375469446182251, 0.12114372104406357, 0.004552116151899099, 0.39431366324424744, 0.010476561263203621, 0.12320521473884583, 0.3822156488895416, 0.4536249339580536, 0.2682075798511505, 0.00044362893095239997, 0.06205836683511734, 0.4598107933998108, 0.024909814819693565, 0.4835103452205658, 0.45256176590919495, 0.4145205318927765], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b8c6510d6434a89c7fdcb6da1192fd02(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 320, 320], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.1526193767786026, 0.2788945138454437, 0.1339210718870163, 0.43815433979034424, 0.13603191077709198, 0.13670280575752258, 0.21018768846988678, 0.25907981395721436, 0.2534116506576538, 0.2372812181711197, 0.05073714256286621, 0.07842441648244858, 0.2121991515159607, 0.27179229259490967, 0.2821822464466095, 0.06382311135530472], dtype='float32').reshape([16]),
            paddle.to_tensor([0.26512670516967773, 0.2829035222530365, 0.08068960905075073, 0.16047337651252747, 0.36087751388549805, 0.2561247944831848, 0.024175604805350304, 0.44393590092658997, 0.4678354859352112, 0.49139416217803955, 0.4910052418708801, 0.43162551522254944, 0.1655144840478897, 0.3146911561489105, 0.21862106025218964, 0.2651293873786926], dtype='float32').reshape([16]),
            paddle.to_tensor([0.43543750047683716, 0.0994437113404274, 0.428078293800354, 0.3767509162425995, 0.12453264743089676, 0.19102409482002258, 0.2772151529788971, 0.4689965844154358, 0.415798157453537, 0.14537882804870605, 0.20820026099681854, 0.21066801249980927, 0.49024248123168945, 0.32959070801734924, 0.01477680541574955, 0.08487680554389954], dtype='float32').reshape([16]),
            paddle.to_tensor([0.2814749479293823, 0.014233915135264397, 0.3626501262187958, 0.05109503120183945, 0.2972381114959717, 0.20378147065639496, 0.3896709382534027, 0.3897171914577484, 0.4296037256717682, 0.42746633291244507, 0.411650687456131, 0.4636446535587311, 0.45421165227890015, 0.03278440237045288, 0.07882873713970184, 0.3199107348918915], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_760ed7d32ac47ad32d6055eaae31c64b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 160, 160], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3356ee67f00e933157756a80b8af66d8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.18059095740318298, 0.18955349922180176, 0.32658952474594116, 0.4776618182659149, 0.49905264377593994, 0.27918288111686707, 0.07083173841238022, 0.21309514343738556, 0.18055470287799835, 0.03587616607546806, 0.04948710277676582, 0.12266669422388077, 0.10386032611131668, 0.4075074791908264, 0.05190643295645714, 0.15817973017692566, 0.31756091117858887, 0.40590900182724, 0.2276248186826706, 0.13533268868923187, 0.43236202001571655, 0.25694596767425537, 0.22596922516822815, 0.4743647575378418, 0.1936042606830597, 0.2572762668132782, 0.1134687140583992, 0.12295405566692352, 0.30726462602615356, 0.08215554058551788], dtype='float32').reshape([30]),
            paddle.to_tensor([0.3975622057914734, 0.4946851432323456, 0.4625355899333954, 0.4874991178512573, 0.38941097259521484, 0.08871269226074219, 0.056975655257701874, 0.47163838148117065, 0.3277202844619751, 0.21153037250041962, 0.4050731360912323, 0.3150824010372162, 0.06316623091697693, 0.1036401018500328, 0.3629972040653229, 0.08687453716993332, 0.3853907287120819, 0.044420938938856125, 0.172090083360672, 0.08593294769525528, 0.4178370535373688, 0.27836692333221436, 0.3249053657054901, 0.04967160522937775, 0.1194298192858696, 0.06576427072286606, 0.4374134838581085, 0.3406253457069397, 0.266253262758255, 0.44368502497673035], dtype='float32').reshape([30]),
            paddle.to_tensor([0.017802592366933823, 0.19370941817760468, 0.0009607369429431856, 0.27711382508277893, 0.29715967178344727, 0.4956324100494385, 0.3436105251312256, 0.4989051818847656, 0.4764615595340729, 0.2931784987449646, 0.2806859314441681, 0.42462679743766785, 0.27715981006622314, 0.23796318471431732, 0.1934647113084793, 0.01457165740430355, 0.35885390639305115, 0.03598238527774811, 0.3159010112285614, 0.2586718201637268, 0.3663419783115387, 0.28353098034858704, 0.055743880569934845, 0.3691697120666504, 0.25979042053222656, 0.3319607079029083, 0.17638203501701355, 0.1547524631023407, 0.3757913112640381, 0.011920756660401821], dtype='float32').reshape([30]),
            paddle.to_tensor([0.3720235526561737, 0.378380686044693, 0.452938437461853, 0.06490259617567062, 0.4384477734565735, 0.2254178822040558, 0.06436537951231003, 0.25620150566101074, 0.3440331518650055, 0.3901161253452301, 0.24393971264362335, 0.1298554241657257, 0.2393917441368103, 0.44153469800949097, 0.46699222922325134, 0.10361884534358978, 0.10054168105125427, 0.0928398072719574, 0.23356425762176514, 0.05355126038193703, 0.211734801530838, 0.2168319970369339, 0.48200297355651855, 0.13157092034816742, 0.12353730946779251, 0.4523060917854309, 0.2690187990665436, 0.25151246786117554, 0.18904873728752136, 0.31825053691864014], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a46aa433fb12367ccbc71281ddac552c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_3a47a120d62a3f3a422a747f511df8d9
    def get_inputs(self):
        return [
            paddle.uniform([1, 112, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_caeacc87b4d90d97e522748f99f4df12(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.0038667256012558937, 0.4589039087295532, 0.43511953949928284, 0.19761812686920166, 0.2016749083995819, 0.07736199349164963, 0.09523256868124008, 0.24170683324337006, 0.35474103689193726, 0.3845702111721039, 0.05046585202217102, 0.18125876784324646, 0.3937279284000397, 0.128569558262825, 0.3169102370738983, 0.4180319607257843, 0.4631290137767792, 0.4791495203971863, 0.4722711145877838, 0.030624013394117355], dtype='float32').reshape([20]),
            paddle.to_tensor([0.18454769253730774, 0.04840372875332832, 0.28101813793182373, 0.4408937692642212, 0.3423677980899811, 0.19088147580623627, 0.34125757217407227, 0.45802056789398193, 0.04545050486922264, 0.33526432514190674, 0.4801170229911804, 0.4307335317134857, 0.08727956563234329, 0.16941338777542114, 0.2774244546890259, 0.05888189747929573, 0.13168767094612122, 0.4036271870136261, 0.030697744339704514, 0.3363786041736603], dtype='float32').reshape([20]),
            paddle.to_tensor([0.3131299614906311, 0.3838464319705963, 0.309426486492157, 0.17734110355377197, 0.2872415781021118, 0.11202433705329895, 0.2827616035938263, 0.4391478896141052, 0.2535804808139801, 0.08958616852760315, 0.4568862318992615, 0.27046671509742737, 0.23768730461597443, 0.4160301387310028, 0.26901963353157043, 0.10869483649730682, 0.3058047294616699, 0.2637307345867157, 0.10218816250562668, 0.31330952048301697], dtype='float32').reshape([20]),
            paddle.to_tensor([0.44186413288116455, 0.09138737618923187, 0.02921830117702484, 0.1294514387845993, 0.40655040740966797, 0.09839808195829391, 0.36634185910224915, 0.2676932215690613, 0.3501228094100952, 0.37127044796943665, 0.2796008884906769, 0.43022915720939636, 0.03007734753191471, 0.22443746030330658, 0.37313076853752136, 0.24459847807884216, 0.41644778847694397, 0.3306645452976227, 0.4869764447212219, 0.08592691272497177], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a62766a8b419dd50ff93483076c4c8ea(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 60, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fc051f1af2a05ba0d128806f2542b1cf(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.17892353236675262, 0.3706689178943634, 0.07933389395475388, 0.1434531807899475, 0.288107305765152, 0.44161444902420044, 0.30571702122688293, 0.45735564827919006, 0.35051843523979187, 0.46866536140441895, 0.2768462300300598, 0.35519134998321533, 0.46920454502105713, 0.10429553687572479, 0.13756296038627625, 0.2383536547422409, 0.3213820457458496, 0.23897117376327515, 0.26442474126815796, 0.3822486102581024, 0.4762175381183624, 0.23248891532421112, 0.10216955095529556, 0.45548710227012634, 0.019766364246606827, 0.4749506413936615, 0.34433093667030334, 0.28836730122566223, 0.4086092710494995, 0.39728403091430664], dtype='float32').reshape([30]),
            paddle.to_tensor([0.14503523707389832, 0.01577935926616192, 0.4653479754924774, 0.37042367458343506, 0.0744231566786766, 0.20179422199726105, 0.42591017484664917, 0.4595940411090851, 0.23864348232746124, 0.2761327624320984, 0.13297632336616516, 0.4874962866306305, 0.0683610737323761, 0.09539586305618286, 0.23225754499435425, 0.2590576112270355, 0.4642287492752075, 0.20673717558383942, 0.01920245587825775, 0.18677867949008942, 0.3702487647533417, 0.26570308208465576, 0.05264879763126373, 0.2642207443714142, 0.22275348007678986, 0.15513299405574799, 0.25806528329849243, 0.4741220474243164, 0.07293623685836792, 0.25667884945869446], dtype='float32').reshape([30]),
            paddle.to_tensor([0.21162673830986023, 0.41276541352272034, 0.39643511176109314, 0.18363435566425323, 0.1689509004354477, 0.21283826231956482, 0.036413583904504776, 0.17739352583885193, 0.020426690578460693, 0.09980760514736176, 0.37326768040657043, 0.40313535928726196, 0.3386167585849762, 0.4277117848396301, 0.16386374831199646, 0.10739386826753616, 0.04183894395828247, 0.4331704080104828, 0.371814489364624, 0.3639955520629883, 0.35019823908805847, 0.37761029601097107, 0.2658708989620209, 0.44880834221839905, 0.07883346825838089, 0.3486723303794861, 0.4368586838245392, 0.3549593687057495, 0.44085201621055603, 0.17708875238895416], dtype='float32').reshape([30]),
            paddle.to_tensor([0.2709510922431946, 0.11650124937295914, 0.3220979869365692, 0.37363871932029724, 0.17955231666564941, 0.38769569993019104, 0.2118532955646515, 0.08286692947149277, 0.28000473976135254, 0.4530046284198761, 0.0512850359082222, 0.3863612115383148, 0.18553763628005981, 0.44741320610046387, 0.474849134683609, 0.0652807354927063, 0.17609648406505585, 0.12974913418293, 0.15782354772090912, 0.0010844236239790916, 0.3686765134334564, 0.08547931164503098, 0.303763210773468, 0.37399497628211975, 0.36321401596069336, 0.0039985207840800285, 0.3248908221721649, 0.48484674096107483, 0.14332666993141174, 0.4441267251968384], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_22054cfe1b40d036c9394b86a6369ea8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 32, 100], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f2e8719839d3208c0460662fc07b323e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.03651074692606926, 0.30171212553977966, 0.1308024525642395, 0.16972173750400543, 0.10215238481760025, 0.11769241839647293, 0.4737139642238617, 0.42201751470565796, 0.10802474617958069, 0.23315449059009552, 0.44621697068214417, 0.49771326780319214, 0.3992898464202881, 0.32648637890815735, 0.4193698465824127, 0.3206517696380615, 0.2975436747074127, 0.1243671253323555], dtype='float32').reshape([18]),
            paddle.to_tensor([0.3679265081882477, 0.11215385794639587, 0.4490414261817932, 0.20361840724945068, 0.44763100147247314, 0.04964185878634453, 0.1455838829278946, 0.2869335114955902, 0.46830418705940247, 0.4989267587661743, 0.4295351803302765, 0.130560040473938, 0.21624700725078583, 0.4134405255317688, 0.031825900077819824, 0.11902056634426117, 0.2627210319042206, 0.401478111743927], dtype='float32').reshape([18]),
            paddle.to_tensor([0.03279169276356697, 0.3049417734146118, 0.23287439346313477, 0.4053563177585602, 0.2160302847623825, 0.02917146123945713, 0.087269127368927, 0.10275544226169586, 0.3912270963191986, 0.21256226301193237, 0.16670475900173187, 0.14219126105308533, 0.49718016386032104, 0.2229965478181839, 0.45889633893966675, 0.09592794626951218, 0.10555662959814072, 0.4347348213195801], dtype='float32').reshape([18]),
            paddle.to_tensor([0.07456816732883453, 0.45394599437713623, 0.3408163785934448, 0.039395350962877274, 0.12489242106676102, 0.19622555375099182, 0.03461526334285736, 0.3955343961715698, 0.09837362915277481, 0.07914918661117554, 0.43480074405670166, 0.09815926104784012, 0.0004501666990108788, 0.35697612166404724, 0.09441280364990234, 0.32991454005241394, 0.013183625414967537, 0.32501670718193054], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f38ff1eaa5c486b213fe9202fd1478c7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.47507238388061523, 0.10650339722633362, 0.0013348266948014498, 0.021183105185627937, 0.3677918016910553, 0.4341046214103699, 0.37161046266555786, 0.4411526918411255, 0.14755362272262573, 0.021485647186636925, 0.3777063488960266, 0.16588114202022552, 0.10767202079296112, 0.2101241648197174, 0.06500232219696045, 0.4904562532901764], dtype='float32').reshape([16]),
            paddle.to_tensor([0.2701576054096222, 0.13901999592781067, 0.006354456767439842, 0.3203023076057434, 0.13255305588245392, 0.34448423981666565, 0.31357133388519287, 0.24809923768043518, 0.33044689893722534, 0.4779037535190582, 0.3801335096359253, 0.39468419551849365, 0.3003036379814148, 0.10691720992326736, 0.49245938658714294, 0.21720288693904877], dtype='float32').reshape([16]),
            paddle.to_tensor([0.16127647459506989, 0.34273678064346313, 0.38302963972091675, 0.34513241052627563, 0.499715119600296, 0.03034556284546852, 0.2414916306734085, 0.4205968677997589, 0.29261013865470886, 0.40749260783195496, 0.05107565224170685, 0.3492416441440582, 0.44727247953414917, 0.2675158381462097, 0.2256353795528412, 0.40917977690696716], dtype='float32').reshape([16]),
            paddle.to_tensor([0.13069164752960205, 0.14435896277427673, 0.004266246221959591, 0.21089375019073486, 0.035106103867292404, 0.03889197111129761, 0.4186393618583679, 0.4806765019893646, 0.31525173783302307, 0.2072782963514328, 0.21645605564117432, 0.157770037651062, 0.3285449147224426, 0.028676899150013924, 0.400581955909729, 0.06073477119207382], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1f9467a3dadd9f4760532f14e907fa47(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([8, 512, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1c518e06dd275b6467bb1199fca40deb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.25312474370002747, 0.31094422936439514, 0.42355263233184814, 0.17196817696094513, 0.3904770016670227, 0.03405730798840523, 0.2999753952026367, 0.31674516201019287, 0.3621712028980255, 0.26817741990089417, 0.4619775116443634, 0.3443056643009186, 0.011941615492105484, 0.24920855462551117, 0.29961973428726196, 0.43466776609420776, 0.34477415680885315, 0.21808543801307678], dtype='float32').reshape([18]),
            paddle.to_tensor([0.28532055020332336, 0.3255930244922638, 0.44306033849716187, 0.11514774709939957, 0.2588565945625305, 0.27178844809532166, 0.33385807275772095, 0.471576988697052, 0.31942299008369446, 0.3305700719356537, 0.10390359908342361, 0.4434327781200409, 0.2420853078365326, 0.24737240374088287, 0.46933022141456604, 0.321088969707489, 0.20300722122192383, 0.4965173602104187], dtype='float32').reshape([18]),
            paddle.to_tensor([0.236445814371109, 0.2235025316476822, 0.031257349997758865, 0.24398794770240784, 0.3081139624118805, 0.3752857744693756, 0.22183962166309357, 0.0036156168207526207, 0.4257838726043701, 0.14748622477054596, 0.21723607182502747, 0.4623856246471405, 0.07398368418216705, 0.13426199555397034, 0.19332218170166016, 0.49936795234680176, 0.275071382522583, 0.1835303157567978], dtype='float32').reshape([18]),
            paddle.to_tensor([0.25731515884399414, 0.09269435703754425, 0.11897477507591248, 0.18735159933567047, 0.21622143685817719, 0.26926717162132263, 0.4589811861515045, 0.17764003574848175, 0.2449231743812561, 0.14428330957889557, 0.4391374886035919, 0.2265203446149826, 0.16790439188480377, 0.09587622433900833, 0.20260608196258545, 0.25836503505706787, 0.38456615805625916, 0.44333845376968384], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1f8becb14dd7aeaa8de0b9af33d21b6d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.1998978555202484, 0.12362444400787354, 0.16107015311717987, 0.018231483176350594, 0.4583028256893158, 0.003133330959826708, 0.19945698976516724, 0.3789213001728058, 0.060842759907245636, 0.3063253164291382, 0.24600982666015625, 0.0037903599441051483, 0.40324103832244873, 0.21340450644493103, 0.4994472563266754, 0.13489054143428802, 0.4786355495452881, 0.3699316382408142, 0.25494077801704407, 0.10842914879322052, 0.4802493155002594, 0.02423088625073433, 0.3414387106895447, 0.22865915298461914], dtype='float32').reshape([24]),
            paddle.to_tensor([0.36810290813446045, 0.22187493741512299, 0.34065327048301697, 0.03921988606452942, 0.07802174240350723, 0.329226553440094, 0.24840301275253296, 0.33678045868873596, 0.1117049902677536, 0.38223204016685486, 0.40307530760765076, 0.4420374929904938, 0.20979854464530945, 0.2643914818763733, 0.05287957191467285, 0.28333184123039246, 0.27146026492118835, 0.13684476912021637, 0.04609256982803345, 0.2741280794143677, 0.06891939043998718, 0.15024316310882568, 0.28192415833473206, 0.02858157828450203], dtype='float32').reshape([24]),
            paddle.to_tensor([0.04731213301420212, 0.4794905483722687, 0.2697920501232147, 0.35082367062568665, 0.3376865088939667, 0.3664553463459015, 0.21978554129600525, 0.05777141451835632, 0.43820634484291077, 0.39628925919532776, 0.23047897219657898, 0.10676592588424683, 0.32254141569137573, 0.08805098384618759, 0.146097794175148, 0.08696495741605759, 0.360483318567276, 0.15657949447631836, 0.18534055352210999, 0.0032295421697199345, 0.1736108660697937, 0.055955830961465836, 0.4090861976146698, 0.012943539768457413], dtype='float32').reshape([24]),
            paddle.to_tensor([0.248565673828125, 0.10810276865959167, 0.2483198642730713, 0.3618195950984955, 0.10959819704294205, 0.1617036610841751, 0.19241076707839966, 0.31494736671447754, 0.3906707465648651, 0.3919852077960968, 0.44890421628952026, 0.4163610339164734, 0.03740459308028221, 0.42789509892463684, 0.3102172315120697, 0.29709917306900024, 0.2988679111003876, 0.012168646790087223, 0.40830332040786743, 0.32131677865982056, 0.16753429174423218, 0.4875803291797638, 0.17859964072704315, 0.35997694730758667], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_04e184098d1665a7bb3a3d3b51d7d29a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 8, 8], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8256765909154d5f83366d5bdeefe1ee(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_c8eca113b3029d88b25158f95c03e4fe
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 32, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b9ea214f5d60d2b3634847ed9445f97c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.30616509914398193, 0.15766246616840363, 0.03562776371836662, 0.382250040769577, 0.38814547657966614, 0.1263597458600998, 0.31824564933776855, 0.13390883803367615, 0.40276187658309937, 0.4597693383693695, 0.02324611321091652, 0.1648586392402649, 0.2567349672317505, 0.4398885667324066, 0.37235987186431885, 0.4570695459842682, 0.464815616607666, 0.40395858883857727, 0.08217759430408478, 0.31312501430511475, 0.008845209144055843, 0.27089112997055054, 0.49601444602012634, 0.3509877920150757], dtype='float32').reshape([24]),
            paddle.to_tensor([0.3038356900215149, 0.4198894500732422, 0.4514906406402588, 0.06477317214012146, 0.4869994521141052, 0.03496736288070679, 0.06293220818042755, 0.165839284658432, 0.1993873417377472, 0.39890772104263306, 0.4442134201526642, 0.1319156140089035, 0.21841587126255035, 0.32169416546821594, 0.36094895005226135, 0.26207226514816284, 0.15072192251682281, 0.24009357392787933, 0.042457256466150284, 0.15382517874240875, 0.30104950070381165, 0.4935096502304077, 0.33164238929748535, 0.3020458519458771], dtype='float32').reshape([24]),
            paddle.to_tensor([0.47356611490249634, 0.3358015716075897, 0.14469611644744873, 0.474718302488327, 0.46312761306762695, 0.12930917739868164, 0.07518491894006729, 0.2867065370082855, 0.41799691319465637, 0.15522649884223938, 0.09909787029027939, 0.03816346079111099, 0.2273324579000473, 0.4372256398200989, 0.3285614550113678, 0.4976872503757477, 0.11401532590389252, 0.3463636636734009, 0.36758458614349365, 0.15629105269908905, 0.28816336393356323, 0.006998254917562008, 0.3828086853027344, 0.21319617331027985], dtype='float32').reshape([24]),
            paddle.to_tensor([0.26814383268356323, 0.14454981684684753, 0.05244500935077667, 0.149990051984787, 0.3885079324245453, 0.03834591433405876, 0.3480512201786041, 0.2226889580488205, 0.08908480405807495, 0.012865313328802586, 0.34071412682533264, 0.2809261679649353, 0.289828360080719, 0.2321060746908188, 0.19031469523906708, 0.4677809476852417, 0.20188705623149872, 0.2625446021556854, 0.18747170269489288, 0.25393298268318176, 0.338068425655365, 0.4893329441547394, 0.07177077978849411, 0.42856165766716003], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_76fab50cdf5245bdbced2ef6c5022d95(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 8, 25], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_eb2a519f04601244cfdb59c497a966d4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 128, 128], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_98edef8dc64cc56432351b9e658c82c4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 672, 64, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c54541fac09d4234cd8280aeba8d76f0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 80, 80], dtype='float16', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c16430a27a59d547d8cd02057a5cd2b5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 1392, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([1392], dtype='float32', min=0, max=0.5),
            paddle.uniform([1392], dtype='float32', min=0, max=0.5),
            paddle.uniform([1392], dtype='float32', min=0, max=0.5),
            paddle.uniform([1392], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_85086989d63185813228fb58ce28c230(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 1152, 6, 6], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_dc29e0ec463c49a3e40b58aa58a172d9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.37974944710731506, 0.12919467687606812, 0.32958418130874634, 0.28441792726516724, 0.08879976719617844, 0.18633945286273956, 0.4571581184864044, 0.27088624238967896, 0.23191115260124207, 0.34606650471687317, 0.11345963180065155, 0.41393452882766724, 0.011472790502011776, 0.22872786223888397, 0.16496287286281586, 0.22801698744297028, 0.08586663007736206, 0.315822035074234], dtype='float32').reshape([18]),
            paddle.to_tensor([0.31868693232536316, 0.22175270318984985, 0.40996620059013367, 0.27437883615493774, 0.40299588441848755, 0.47637996077537537, 0.3608102798461914, 0.19538410007953644, 0.18073323369026184, 0.02781829610466957, 0.18843801319599152, 0.15293359756469727, 0.15829315781593323, 0.06553582847118378, 0.1793903410434723, 0.2671497166156769, 0.37142011523246765, 0.3411811590194702], dtype='float32').reshape([18]),
            paddle.to_tensor([0.3687387704849243, 0.12118670344352722, 0.111240454018116, 0.3971363604068756, 0.0573231503367424, 0.17867307364940643, 0.4546777606010437, 0.33479082584381104, 0.22848093509674072, 0.17571130394935608, 0.36559319496154785, 0.1552169770002365, 0.2935861051082611, 0.37916579842567444, 0.2707187831401825, 0.31556808948516846, 0.4976908266544342, 0.029912149533629417], dtype='float32').reshape([18]),
            paddle.to_tensor([0.24465391039848328, 0.4519183933734894, 0.438223659992218, 0.30645638704299927, 0.3340911269187927, 0.043960198760032654, 0.2221589982509613, 0.01007167249917984, 0.4675752520561218, 0.1920231580734253, 0.45709681510925293, 0.23798023164272308, 0.4107954204082489, 0.15229162573814392, 0.018047723919153214, 0.4151166081428528, 0.24028950929641724, 0.14825968444347382], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5c5e6a64543906aa2b4c85d2e311d23c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 504, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([504], dtype='float32', min=0, max=0.5),
            paddle.uniform([504], dtype='float32', min=0, max=0.5),
            paddle.uniform([504], dtype='float32', min=0, max=0.5),
            paddle.uniform([504], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9b5050678541797f213f49ac9e2d61f2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 36, 1, 1], dtype='float16', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cce457b69300d9a918b49c2cf96cc15e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_da4d85bc213f70819a72e68f21214cdb
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 32, 64, 64], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.3838655650615692, 0.1154559925198555, 0.17280684411525726, 0.25075823068618774, 0.1430753767490387, 0.15555016696453094, 0.0853348895907402, 0.17420141398906708, 0.1692989021539688, 0.4887908399105072, 0.015782101079821587, 0.3448207974433899, 0.1932305097579956, 0.2849448025226593, 0.44282615184783936, 0.38120928406715393], dtype='float32').reshape([16]),
            paddle.to_tensor([0.12175905704498291, 0.14464648067951202, 0.05974676460027695, 0.48960569500923157, 0.00496782548725605, 0.37809276580810547, 0.07350926101207733, 0.29537665843963623, 0.17004968225955963, 0.1313815414905548, 0.1771947592496872, 0.30157241225242615, 0.2676008939743042, 0.19037990272045135, 0.09991689771413803, 0.4790151119232178], dtype='float32').reshape([16]),
            paddle.to_tensor([0.3141116499900818, 0.1122407391667366, 0.13065074384212494, 0.04547267407178879, 0.04027220234274864, 0.4748428761959076, 0.07286226004362106, 0.31726402044296265, 0.28679007291793823, 0.31464481353759766, 0.03929821029305458, 0.49664032459259033, 0.19830746948719025, 0.16324366629123688, 0.41909292340278625, 0.29533955454826355], dtype='float32').reshape([16]),
            paddle.to_tensor([0.2794303297996521, 0.06714371591806412, 0.017304465174674988, 0.08488695323467255, 0.4945657551288605, 0.3952922821044922, 0.2516091763973236, 0.21856294572353363, 0.4216279089450836, 0.024867234751582146, 0.4682060182094574, 0.393513023853302, 0.21815919876098633, 0.20548701286315918, 0.0348869152367115, 0.1149684488773346], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_aa8f3110f32d9b563fe6595090d52fbb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 104, 60, 60], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_063dc7e01f236c8604db4f7a20cdb29e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 61, 61], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_39dcf59154ee6e52a1142fce238da819(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 1872, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([1872], dtype='float32', min=0, max=0.5),
            paddle.uniform([1872], dtype='float32', min=0, max=0.5),
            paddle.uniform([1872], dtype='float32', min=0, max=0.5),
            paddle.uniform([1872], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_570c7f44d46bc406d963f37b031fbd2e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 162, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.uniform([162], dtype='float32', min=0, max=0.5),
            paddle.uniform([162], dtype='float32', min=0, max=0.5),
            paddle.uniform([162], dtype='float32', min=0, max=0.5),
            paddle.uniform([162], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_994885f947bf35890cd54100a71ca8e4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_da4d85bc213f70819a72e68f21214cdb
    def get_inputs(self):
        return [
            paddle.uniform([1, 2048, 4, 8, 8], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f0719b88687343f07eab56b575f76eaf(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.24433265626430511, 0.27442318201065063, 0.03634001314640045, 0.21303419768810272, 0.4120781421661377, 0.3004835844039917, 0.44235146045684814, 0.03277001157402992, 0.11827816814184189, 0.166807621717453, 0.36202946305274963, 0.04956641048192978, 0.3002263307571411, 0.40408164262771606, 0.3522970974445343, 0.2617500126361847], dtype='float32').reshape([16]),
            paddle.to_tensor([0.022047756239771843, 0.2850152254104614, 0.09016140550374985, 0.1281631588935852, 0.11755047738552094, 0.20958344638347626, 0.24742966890335083, 0.21641860902309418, 0.14366307854652405, 0.2861343324184418, 0.051881276071071625, 0.005036961752921343, 0.43820950388908386, 0.4128875434398651, 0.4012168347835541, 0.13025589287281036], dtype='float32').reshape([16]),
            paddle.to_tensor([0.4489218592643738, 0.07106854021549225, 0.38099372386932373, 0.1451171487569809, 0.16379129886627197, 0.48204439878463745, 0.1607525646686554, 0.3051784038543701, 0.43841949105262756, 0.28764867782592773, 0.37061214447021484, 0.3050004243850708, 0.11216222494840622, 0.1927890181541443, 0.1337268054485321, 0.38784053921699524], dtype='float32').reshape([16]),
            paddle.to_tensor([0.219825878739357, 0.3962729275226593, 0.46567055583000183, 0.22032058238983154, 0.10436174273490906, 0.20760910212993622, 0.45926082134246826, 0.40343496203422546, 0.21563170850276947, 0.06224920600652695, 0.08944693207740784, 0.35945841670036316, 0.47101807594299316, 0.33620452880859375, 0.19794683158397675, 0.1604223996400833], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_70ea3757da4a657548c4170c7e31d59a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.19748544692993164, 0.27423229813575745, 0.10435247421264648, 0.29362040758132935, 0.4784052073955536, 0.2703041732311249, 0.32278040051460266, 0.4471975564956665, 0.4072631895542145, 0.22734764218330383, 0.05163241922855377, 0.055969882756471634, 0.006001504138112068, 0.11102073639631271, 0.4032745659351349, 0.20977430045604706, 0.3191928267478943, 0.369559645652771, 0.10679714381694794, 0.09526124596595764, 0.20427168905735016, 0.3114016056060791, 0.44441884756088257, 0.2935844361782074, 0.1927611082792282, 0.3770480453968048, 0.4012726843357086, 0.12396539747714996, 0.25596657395362854, 0.3191894292831421], dtype='float32').reshape([30]),
            paddle.to_tensor([0.20833918452262878, 0.41678279638290405, 0.4464854300022125, 0.20541292428970337, 0.27537134289741516, 0.37337130308151245, 0.31401365995407104, 0.243509441614151, 0.09565217047929764, 0.33054113388061523, 0.3620304465293884, 0.40285468101501465, 0.37801578640937805, 0.36833620071411133, 0.40494316816329956, 0.21538034081459045, 0.07445616275072098, 0.3630247414112091, 0.33966726064682007, 0.4624037742614746, 0.46149739623069763, 0.4087621569633484, 0.4552155137062073, 0.19449706375598907, 0.2149040549993515, 0.19380395114421844, 0.3667335510253906, 0.058683570474386215, 0.4500146806240082, 0.3696526288986206], dtype='float32').reshape([30]),
            paddle.to_tensor([0.16406869888305664, 0.4239423871040344, 0.2375280112028122, 0.3905852735042572, 0.36763182282447815, 0.4788285493850708, 0.06380380690097809, 0.3045419156551361, 0.35123756527900696, 0.17986072599887848, 0.445084810256958, 0.22296695411205292, 0.1734769344329834, 0.2668260931968689, 0.39682552218437195, 0.49443623423576355, 0.301013708114624, 0.18019579350948334, 0.335468053817749, 0.3016899824142456, 0.4809960424900055, 0.40348565578460693, 0.43554577231407166, 0.4003519117832184, 0.3204575181007385, 0.039887234568595886, 0.47184598445892334, 0.019923504441976547, 0.3520955741405487, 0.25799787044525146], dtype='float32').reshape([30]),
            paddle.to_tensor([0.01808692142367363, 0.19285979866981506, 0.28130024671554565, 0.048660457134246826, 0.4367268979549408, 0.026213524863123894, 0.4739508032798767, 0.04099549725651741, 0.39463600516319275, 0.020318863913416862, 0.3824334442615509, 0.08219753205776215, 0.1618235856294632, 0.32853609323501587, 0.27838438749313354, 0.111103355884552, 0.3706511855125427, 0.020460326224565506, 0.06564012169837952, 0.2661818265914917, 0.10665673017501831, 0.49024781584739685, 0.3116324245929718, 0.009964159689843655, 0.12378298491239548, 0.48790496587753296, 0.11059758812189102, 0.1791321188211441, 0.3849344253540039, 0.36530324816703796], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8c028efd3f171936765f3e188f1e9e33(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 160, 160], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_34152ce4b44d9a26633f523ee44dd689(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 240, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4c7d79093a6b598f47ee1b3be6842cd4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 240, 240], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4d64e541eaa3172d377e3939574b932c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.1980719417333603, 0.28939881920814514, 0.4588346481323242, 0.03776064142584801, 0.28323131799697876, 0.4816710650920868, 0.11628179252147675, 0.23306351900100708, 0.34512242674827576, 0.36523187160491943, 0.27598050236701965, 0.2888296842575073, 0.11436421424150467, 0.07227860391139984, 0.44614091515541077, 0.488645076751709, 0.29680168628692627, 0.023312289267778397, 0.42234301567077637, 0.2520920932292938, 0.2526829242706299, 0.16118749976158142, 0.04526011645793915, 0.42158395051956177, 0.019158706068992615, 0.46372082829475403, 0.41663071513175964, 0.27836644649505615, 0.1741972118616104, 0.2766244113445282], dtype='float32').reshape([30]),
            paddle.to_tensor([0.04742368310689926, 0.3796897828578949, 0.4007290303707123, 0.3011454939842224, 0.18920738995075226, 0.20186300575733185, 0.18177390098571777, 0.29829874634742737, 0.01057353988289833, 0.35012364387512207, 0.3616415560245514, 0.23962195217609406, 0.27453818917274475, 0.17451779544353485, 0.17331352829933167, 0.07450713217258453, 0.4692911207675934, 0.14638185501098633, 0.0723787397146225, 0.18639174103736877, 0.05117063596844673, 0.06390534341335297, 0.0076410770416259766, 0.18458695709705353, 0.03397807106375694, 0.14851529896259308, 0.2822800576686859, 0.35500290989875793, 0.4536307156085968, 0.2137594223022461], dtype='float32').reshape([30]),
            paddle.to_tensor([0.4149235188961029, 0.1356964111328125, 0.2659274935722351, 0.05452903360128403, 0.1415282040834427, 0.27007102966308594, 0.0512087345123291, 0.3490000367164612, 0.053303975611925125, 0.2592430114746094, 0.2766621708869934, 0.10802057385444641, 0.4846370220184326, 0.4651467800140381, 0.3115025460720062, 0.3458351790904999, 0.21196642518043518, 0.3581230342388153, 0.32256850600242615, 0.4259183704853058, 0.42163732647895813, 0.36835476756095886, 0.2083393782377243, 0.3955543041229248, 0.44076451659202576, 0.3289565145969391, 0.4373003840446472, 0.16153620183467865, 0.27497580647468567, 0.3289368450641632], dtype='float32').reshape([30]),
            paddle.to_tensor([0.4653976261615753, 0.18020273745059967, 0.3992103040218353, 0.11768738925457001, 0.25970348715782166, 0.28027763962745667, 0.3213998079299927, 0.15143559873104095, 0.023288672789931297, 0.0428549125790596, 0.28651753067970276, 0.21900267899036407, 0.39612677693367004, 0.26099854707717896, 0.25167861580848694, 0.4004180431365967, 0.42559462785720825, 0.11668176203966141, 0.36577603220939636, 0.21943514049053192, 0.38293957710266113, 0.20329463481903076, 0.020299825817346573, 0.35189497470855713, 0.4117942154407501, 0.39741194248199463, 0.3799959719181061, 0.02058669552206993, 0.3968898355960846, 0.3095887303352356], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4ab55bc8ee696ee0f529e0716c3a50ea(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 32, 24], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.3435420095920563, 0.33443036675453186, 0.43114742636680603, 0.05282336473464966, 0.011289047077298164, 0.37219688296318054, 0.10944506525993347, 0.37743571400642395, 0.05655491352081299, 0.29039865732192993, 0.020151814445853233, 0.4725147783756256, 0.2115432322025299, 0.49435821175575256, 0.3774022161960602, 0.36852943897247314], dtype='float32').reshape([16]),
            paddle.to_tensor([0.14943338930606842, 0.32488110661506653, 0.22695210576057434, 0.19144216179847717, 0.30587098002433777, 0.09107621014118195, 0.24171096086502075, 0.1594906896352768, 0.10779493302106857, 0.1187695786356926, 0.16658331453800201, 0.44229403138160706, 0.18303954601287842, 0.16598641872406006, 0.2842232286930084, 0.15357011556625366], dtype='float32').reshape([16]),
            paddle.to_tensor([0.3124149441719055, 0.12114586681127548, 0.030603960156440735, 0.031328171491622925, 0.1657383143901825, 0.3635835647583008, 0.1218927726149559, 0.24025610089302063, 0.20163963735103607, 0.2855541408061981, 0.4553740918636322, 0.29157698154449463, 0.26335224509239197, 0.03310807794332504, 0.4158026874065399, 0.39460137486457825], dtype='float32').reshape([16]),
            paddle.to_tensor([0.05071403831243515, 0.008040792308747768, 0.4553684890270233, 0.1260537952184677, 0.11567500978708267, 0.010005384683609009, 0.07801558077335358, 0.05562770739197731, 0.4691351652145386, 0.1560678631067276, 0.3362386226654053, 0.2028784602880478, 0.16202634572982788, 0.2823983132839203, 0.28709933161735535, 0.0663115605711937], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_24c13f53efc7297015e9ef3d08efa560(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 1, 2], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0954cc827885a7ad3ea97e271e6cf8b5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.3911122977733612, 0.15955907106399536, 0.3967170715332031, 0.13092392683029175, 0.14553356170654297, 0.18500855565071106, 0.3043668270111084, 0.40786677598953247, 0.21051576733589172, 0.2482946664094925, 0.2011587917804718, 0.19426050782203674, 0.43576952815055847, 0.3503718972206116, 0.47772711515426636, 0.41730108857154846, 0.1711617261171341, 0.03871529549360275, 0.022481881082057953, 0.47630682587623596, 0.3229910731315613, 0.4811232388019562, 0.06321455538272858, 0.39005231857299805, 0.23759868741035461, 0.3136274814605713, 0.2591721713542938, 0.27485573291778564, 0.22540339827537537, 0.07457008957862854], dtype='float32').reshape([30]),
            paddle.to_tensor([0.2557578682899475, 0.07835765928030014, 0.4524857699871063, 0.23555928468704224, 0.15231996774673462, 0.04668191820383072, 0.21309693157672882, 0.15226387977600098, 0.46583685278892517, 0.11866946518421173, 0.33158227801322937, 0.3142385482788086, 0.4351675808429718, 0.3973683714866638, 0.20908240973949432, 0.17378884553909302, 0.44093480706214905, 0.1460753232240677, 0.32351958751678467, 0.23093026876449585, 0.47255799174308777, 0.3594677746295929, 0.09252207726240158, 0.06802935153245926, 0.01413801871240139, 0.09065774828195572, 0.1810365468263626, 0.4207238256931305, 0.17748045921325684, 0.38892173767089844], dtype='float32').reshape([30]),
            paddle.to_tensor([0.34567564725875854, 0.3121284246444702, 0.2591061294078827, 0.4743500053882599, 0.36392372846603394, 0.055444978177547455, 0.19476768374443054, 0.25267845392227173, 0.29150840640068054, 0.45555850863456726, 0.38138842582702637, 0.14311908185482025, 0.4306715428829193, 0.3116821050643921, 0.32055574655532837, 0.4240725338459015, 0.46867501735687256, 0.08636916428804398, 0.06297468394041061, 0.45397281646728516, 0.11355147510766983, 0.3234642446041107, 0.21045362949371338, 0.0977947860956192, 0.11223752796649933, 0.40007317066192627, 0.17952531576156616, 0.05116913095116615, 0.24657151103019714, 0.12049125880002975], dtype='float32').reshape([30]),
            paddle.to_tensor([0.15914961695671082, 0.019845863804221153, 0.34873753786087036, 0.005141241475939751, 0.14055819809436798, 0.11263454705476761, 0.030125075951218605, 0.42348891496658325, 0.43289878964424133, 0.19073078036308289, 0.44282135367393494, 0.4529755115509033, 0.04320532828569412, 0.3614347279071808, 0.1424296647310257, 0.46201109886169434, 0.05817436799407005, 0.12936177849769592, 0.04637052118778229, 0.3564915657043457, 0.06308256089687347, 0.12547190487384796, 0.4529971778392792, 0.00535017903894186, 0.15463890135288239, 0.4041121006011963, 0.16216778755187988, 0.3126104474067688, 0.3504442870616913, 0.35255685448646545], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c42a23b980578ffac4603921e0282918(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 184, 64, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([184], dtype='float32', min=0, max=0.5),
            paddle.uniform([184], dtype='float32', min=0, max=0.5),
            paddle.uniform([184], dtype='float32', min=0, max=0.5),
            paddle.uniform([184], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4f683497f258f869d3291d480f4d70ac(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 256, 512], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_53131ae6c761fa92a51d6e06ecd6e1db(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.47444161772727966, 0.1591617465019226, 0.3990812599658966, 0.338527113199234, 0.46803387999534607, 0.06679663062095642, 0.17570962011814117, 0.3338978588581085, 0.3845205307006836, 0.008834635838866234, 0.24777907133102417, 0.27141669392585754, 0.19100235402584076, 0.07138187438249588, 0.026999015361070633, 0.08261184394359589, 0.19421452283859253, 0.2775016725063324, 0.1195351630449295, 0.42031097412109375], dtype='float32').reshape([20]),
            paddle.to_tensor([0.4702778458595276, 0.25587815046310425, 0.2303013950586319, 0.41361352801322937, 0.004720847588032484, 0.021109523251652718, 0.4317154288291931, 0.23693238198757172, 0.15840305387973785, 0.029487011954188347, 0.031994860619306564, 0.22509698569774628, 0.37721166014671326, 0.3129490613937378, 0.07966429740190506, 0.3261410892009735, 0.31663015484809875, 0.08442819863557816, 0.4628206193447113, 0.29429641366004944], dtype='float32').reshape([20]),
            paddle.to_tensor([0.3386797606945038, 0.1839207112789154, 0.38970425724983215, 0.32343941926956177, 0.17962628602981567, 0.2779383659362793, 0.22533077001571655, 0.2952568233013153, 0.1623145341873169, 0.14433743059635162, 0.3261815011501312, 0.4454609453678131, 0.10044735670089722, 0.2152683436870575, 0.1620291918516159, 0.1816502958536148, 0.4173394739627838, 0.09476162493228912, 0.43134617805480957, 0.48459792137145996], dtype='float32').reshape([20]),
            paddle.to_tensor([0.15908169746398926, 0.19123123586177826, 0.025758061558008194, 0.2716786861419678, 0.12334021180868149, 0.40199825167655945, 0.10359456390142441, 0.23737500607967377, 0.24539147317409515, 0.04860329255461693, 0.3657427132129669, 0.3582308888435364, 0.18817923963069916, 0.3536708950996399, 0.4293716847896576, 0.23306120932102203, 0.43731698393821716, 0.12518611550331116, 0.18997882306575775, 0.13262799382209778], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_80d2444e3a11d01823a67c5414e04b56(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 640, 8, 8], dtype='float16', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fe7b3ff6d0b1395d90c480317062d53e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 64, 256], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d985fc0eb93210146232b9ad6f30ed2a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 10, 112, 112], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.3717256486415863, 0.08081915974617004, 0.09539090842008591, 0.39595577120780945, 0.19712243974208832, 0.38184720277786255, 0.49260929226875305, 0.46921855211257935, 0.4114868938922882, 0.25716888904571533], dtype='float32').reshape([10]),
            paddle.to_tensor([0.03115926682949066, 0.18914088606834412, 0.14264419674873352, 0.34506186842918396, 0.006653400603681803, 0.12339568138122559, 0.25118640065193176, 0.13508489727973938, 0.4085679352283478, 0.27697473764419556], dtype='float32').reshape([10]),
            paddle.to_tensor([0.3944834768772125, 0.46105948090553284, 0.36512309312820435, 0.20812895894050598, 0.27944931387901306, 0.4480791985988617, 0.3012244999408722, 0.03925306722521782, 0.03515929728746414, 0.3272913992404938], dtype='float32').reshape([10]),
            paddle.to_tensor([0.28677910566329956, 0.12695448100566864, 0.22852563858032227, 0.11228697001934052, 0.29894956946372986, 0.011541884392499924, 0.2451280802488327, 0.42557641863822937, 0.13876977562904358, 0.46477776765823364], dtype='float32').reshape([10]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7f491496970c567562a5dc2ca893bf2f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 56, 60, 60], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f53a80c1faf4bc16242ddf168b186247(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 672, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7a38d344bc704910d461605b2cfcb3dd(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([8, 2048, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1a2f94c47a382194886a6a7cd8480f8c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 8, 50], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_59984a112d7c9fa1470b7f9be32bb7c5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 8, 8], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f427a906e176988dea051edf777bfae6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.192805215716362, 0.35526636242866516, 0.30124184489250183, 0.40338972210884094, 0.4821919798851013, 0.42466744780540466, 0.09799116849899292, 0.20460613071918488, 0.3542121946811676, 0.2944636940956116, 0.1639050394296646, 0.365085244178772, 0.04188895970582962, 0.18684960901737213, 0.17618317902088165, 0.4886121451854706, 0.08324167877435684, 0.03696664422750473, 0.45387977361679077, 0.08462551981210709, 0.43999335169792175, 0.0686124637722969, 0.06392715871334076, 0.44684898853302, 0.03960210829973221, 0.48240020871162415, 0.08174718171358109, 0.022429466247558594, 0.10198633372783661, 0.36355826258659363], dtype='float32').reshape([30]),
            paddle.to_tensor([0.2934301793575287, 0.015959151089191437, 0.3602610230445862, 0.3786645233631134, 0.4205222427845001, 0.27224278450012207, 0.25928795337677, 0.08705682307481766, 0.48851436376571655, 0.39059990644454956, 0.28560370206832886, 0.07448990643024445, 0.468485027551651, 0.053818229585886, 0.18444150686264038, 0.4335614740848541, 0.17474223673343658, 0.29979366064071655, 0.07935453951358795, 0.3829919397830963, 0.07364420592784882, 0.45859405398368835, 0.1651611179113388, 0.36810049414634705, 0.32833993434906006, 0.1608838438987732, 0.1806519329547882, 0.04264251887798309, 0.19939252734184265, 0.43037471175193787], dtype='float32').reshape([30]),
            paddle.to_tensor([0.46782663464546204, 0.1098644882440567, 0.09225092828273773, 0.36223796010017395, 0.4715999364852905, 0.45246055722236633, 0.4476400315761566, 0.1997315138578415, 0.07145160436630249, 0.32724496722221375, 0.44290485978126526, 0.03177937865257263, 0.010533330962061882, 0.2985863983631134, 0.49087703227996826, 0.20446865260601044, 0.036086294800043106, 0.2361890822649002, 0.15135079622268677, 0.3710441291332245, 0.14802931249141693, 0.19348016381263733, 0.07929319143295288, 0.3820655345916748, 0.16292160749435425, 0.48985353112220764, 0.34585893154144287, 0.20528432726860046, 0.12554393708705902, 0.34592950344085693], dtype='float32').reshape([30]),
            paddle.to_tensor([0.4893237352371216, 0.4451453983783722, 0.3106544017791748, 0.11202707886695862, 0.48591068387031555, 0.23364529013633728, 0.012316733598709106, 0.2397662103176117, 0.41005802154541016, 0.2092682421207428, 0.3203527629375458, 0.36130428314208984, 0.4877152740955353, 0.19909048080444336, 0.30571237206459045, 0.42039212584495544, 0.15404026210308075, 0.42482301592826843, 0.4327198565006256, 0.3811364471912384, 0.4191868305206299, 0.4809163510799408, 0.10855907201766968, 0.23403896391391754, 0.3986537456512451, 0.39747288823127747, 0.4260278046131134, 0.4276508092880249, 0.31921514868736267, 0.1270046979188919], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_14250fb6dea63a62d3e88c31659def05(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 768, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6e9722e5fe9417de968b4affc3340cb0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 1104, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([1104], dtype='float32', min=0, max=0.5),
            paddle.uniform([1104], dtype='float32', min=0, max=0.5),
            paddle.uniform([1104], dtype='float32', min=0, max=0.5),
            paddle.uniform([1104], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_18e6bb4bf43a8fe37c085d0010d34938(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_af91295b59c5fca17f890a802deb001f
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 32, 64, 64], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.15148939192295074, 0.144209086894989, 0.45478254556655884, 0.36380377411842346, 0.01823536865413189, 0.2531432509422302, 0.4772278964519501, 0.35004737973213196], dtype='float32').reshape([8]),
            paddle.to_tensor([0.11294316500425339, 0.15683157742023468, 0.36638420820236206, 0.2593262791633606, 0.08926774561405182, 0.3127719759941101, 0.29133039712905884, 0.4903009235858917], dtype='float32').reshape([8]),
            paddle.to_tensor([0.40333589911460876, 0.13203422725200653, 0.47752246260643005, 0.06035367399454117, 0.03655576333403587, 0.0241165179759264, 0.4224397838115692, 0.0969579666852951], dtype='float32').reshape([8]),
            paddle.to_tensor([0.43846917152404785, 0.08263880759477615, 0.34880632162094116, 0.13945621252059937, 0.1263681799173355, 0.4485490322113037, 0.24238070845603943, 0.2625528872013092], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9e855878e0c3e2d87c143f63acb77104(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 80, 80], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_43431341833c123aa06cbf465e385d34(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.08021476864814758, 0.27414387464523315, 0.04546325281262398, 0.4781801998615265, 0.2837056517601013, 0.06378845125436783, 0.26258403062820435, 0.07769828289747238, 0.30105075240135193, 0.4879925847053528, 0.07040490210056305, 0.10272170603275299, 0.025938404724001884, 0.415116548538208, 0.07192938029766083, 0.03382072597742081, 0.2604634463787079, 0.4234068989753723, 0.42141038179397583, 0.18613578379154205, 0.12261275947093964, 0.4798867702484131, 0.08739151805639267, 0.15422748029232025, 0.1812404990196228, 0.2102467566728592, 0.39151495695114136, 0.4892406165599823, 0.3752964437007904, 0.13516531884670258], dtype='float32').reshape([30]),
            paddle.to_tensor([0.09423165768384933, 0.0011230413801968098, 0.33917492628097534, 0.483847439289093, 0.3765794336795807, 0.1242627277970314, 0.3920849561691284, 0.04820004105567932, 0.1436096727848053, 0.2201821655035019, 0.23751814663410187, 0.22828781604766846, 0.15016503632068634, 0.35210540890693665, 0.036619868129491806, 0.4374983310699463, 0.16420961916446686, 0.14219608902931213, 0.07321611791849136, 0.3848591446876526, 0.3480044901371002, 0.06429323554039001, 0.09948483854532242, 0.2619515657424927, 0.06719548255205154, 0.4859853982925415, 0.2993282675743103, 0.18929794430732727, 0.1020660474896431, 0.10703957080841064], dtype='float32').reshape([30]),
            paddle.to_tensor([0.2686021625995636, 0.21779493987560272, 0.4773389399051666, 0.07651317864656448, 0.06710873544216156, 0.21744225919246674, 0.02692696452140808, 0.36553090810775757, 0.02725292183458805, 0.18655864894390106, 0.21588528156280518, 0.3839036524295807, 0.25834497809410095, 0.40582138299942017, 0.18223945796489716, 0.1615314483642578, 0.2495582550764084, 0.19862820208072662, 0.3276585042476654, 0.17748817801475525, 0.055702243000268936, 0.28807777166366577, 0.12395507097244263, 0.03985818475484848, 0.08329025655984879, 0.41806185245513916, 0.46683749556541443, 0.014509288594126701, 0.3020307123661041, 0.40850603580474854], dtype='float32').reshape([30]),
            paddle.to_tensor([0.14795075356960297, 0.4206283390522003, 0.36450809240341187, 0.4520075023174286, 0.4243854582309723, 0.3845732808113098, 0.021769115701317787, 0.08737724274396896, 0.03191540762782097, 0.4921789765357971, 0.11335844546556473, 0.09577890485525131, 0.26843884587287903, 0.264884889125824, 0.10165289044380188, 0.3732130825519562, 0.14383526146411896, 0.23234978318214417, 0.3258231580257416, 0.4374389946460724, 0.49973437190055847, 0.276792049407959, 0.47159478068351746, 0.4979018568992615, 0.060941461473703384, 0.13012509047985077, 0.4791678190231323, 0.24957861006259918, 0.02136511728167534, 0.48948565125465393], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_73d43573dd5365a61eac0fda48bfb886(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 480, 64, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a0adc5753af7fe143075eb6222fc1173(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 400, 672], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4dfc4478e11e6177c1336708cbdbe4e3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 672, 32, 16], dtype='float16', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0963df03aaf9f43e41471a9fd232d274(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ddb66dd698e2685c3da31568dbbed598(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.47600993514060974, 0.4170152246952057, 0.22850088775157928, 0.3900355398654938, 0.39398086071014404, 0.3621656894683838, 0.4680665135383606, 0.1627739667892456, 0.32839149236679077, 0.3620128035545349, 0.29606401920318604, 0.17476175725460052, 0.10655997693538666, 0.3261024057865143, 0.43330633640289307, 0.15624400973320007, 0.13491059839725494, 0.3437681496143341, 0.17625802755355835, 0.3745349943637848, 0.20645345747470856, 0.4727238714694977, 0.49627387523651123, 0.4689374566078186], dtype='float32').reshape([24]),
            paddle.to_tensor([0.2966381311416626, 0.38610219955444336, 0.2396789789199829, 0.4040902256965637, 0.40513888001441956, 0.34770140051841736, 0.16731087863445282, 0.43840491771698, 0.10346867889165878, 0.06290566921234131, 0.11905591934919357, 0.36090347170829773, 0.18738825619220734, 0.17535749077796936, 0.15104110538959503, 0.48352232575416565, 0.21334101259708405, 0.42223960161209106, 0.09328746050596237, 0.21303123235702515, 0.13383208215236664, 0.1790798306465149, 0.010348992422223091, 0.35897183418273926], dtype='float32').reshape([24]),
            paddle.to_tensor([0.17636004090309143, 0.35107508301734924, 0.353959321975708, 0.17800065875053406, 0.4909330904483795, 0.3199385404586792, 0.3564479351043701, 0.49414053559303284, 0.33878642320632935, 0.21316169202327728, 0.25527966022491455, 0.44976866245269775, 0.381997674703598, 0.20445023477077484, 0.4078424572944641, 0.27138733863830566, 0.11968892067670822, 0.2928289473056793, 0.23176121711730957, 0.04660051688551903, 0.43533581495285034, 0.12095025926828384, 0.41007745265960693, 0.16245579719543457], dtype='float32').reshape([24]),
            paddle.to_tensor([0.372608482837677, 0.42285260558128357, 0.014790920540690422, 0.18342845141887665, 0.21217410266399384, 0.008914442732930183, 0.23845519125461578, 0.2574824094772339, 0.4735235571861267, 0.0966782495379448, 0.16848483681678772, 0.17195318639278412, 0.20286570489406586, 0.2259790301322937, 0.20951911807060242, 0.3717772960662842, 0.12105284631252289, 0.23982511460781097, 0.05583252012729645, 0.20486724376678467, 0.413443386554718, 0.1269172877073288, 0.3654063642024994, 0.137170672416687], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5af480c9fc6bdf26ce9c7e4bbca9cb72(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([8, 64, 112, 112], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_303e28bf8daa20964b62c99c19c50cd4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 32, 24], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_23711ec782b668352827f63497675d30(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 128, 128], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.150798961520195, 0.45949506759643555, 0.23441174626350403, 0.4972591996192932, 0.21911099553108215, 0.4139579236507416, 0.12981411814689636, 0.05108543857932091, 0.41133913397789, 0.20440572500228882, 0.2872354984283447, 0.2780376076698303, 0.1459750086069107, 0.32488566637039185, 0.27171018719673157, 0.2997005879878998], dtype='float32').reshape([16]),
            paddle.to_tensor([0.28991395235061646, 0.05663905665278435, 0.2135269045829773, 0.3619864881038666, 0.27144765853881836, 0.10765597969293594, 0.0008626332855783403, 0.380809485912323, 0.06571624428033829, 0.414414644241333, 0.4956873655319214, 0.34521153569221497, 0.4527947008609772, 0.23429132997989655, 0.30335941910743713, 0.11005418747663498], dtype='float32').reshape([16]),
            paddle.to_tensor([0.32458701729774475, 0.45248427987098694, 0.011806637048721313, 0.4596409797668457, 0.007939057424664497, 0.3379165232181549, 0.31084996461868286, 0.32076025009155273, 0.21064013242721558, 0.3544023633003235, 0.1914782077074051, 0.11436682194471359, 0.22944574058055878, 0.027607152238488197, 0.4184814691543579, 0.11240677535533905], dtype='float32').reshape([16]),
            paddle.to_tensor([0.0101047707721591, 0.36426857113838196, 0.13162700831890106, 0.10401444882154465, 0.4119277894496918, 0.3722578287124634, 0.3576442003250122, 0.022659890353679657, 0.295747846364975, 0.19334609806537628, 0.016314152628183365, 0.3868591785430908, 0.3482239246368408, 0.15553858876228333, 0.07407084107398987, 0.001501691760495305], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5f7d4591ab0b12280abc4cb98151ee1b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 64, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7664b7ad73a2e97b56493691ed3463d5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_0edd578f48a07aa5d1d79f9f678b472e
    def get_inputs(self):
        return [
            paddle.uniform([16, 512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_48604b98b1458fb0e9218ec4a4763146(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 128, 128], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d0361ade9877207d476284d4592bb17c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 240, 240], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.04248944669961929, 0.3781268894672394, 0.05896715074777603, 0.035997625440359116, 0.10935524851083755, 0.45833608508110046, 0.10486705601215363, 0.051199257373809814, 0.2613065540790558, 0.4825364053249359, 0.3930376470088959, 0.41446107625961304, 0.15581031143665314, 0.2724825441837311, 0.3524502217769623, 0.35543543100357056, 0.18555188179016113, 0.4638783633708954, 0.41837576031684875, 0.47212180495262146, 0.30218803882598877, 0.2002769112586975, 0.024280235171318054, 0.47662922739982605], dtype='float32').reshape([24]),
            paddle.to_tensor([0.2901941239833832, 0.05289885774254799, 0.38204896450042725, 0.04921421781182289, 0.4260837733745575, 0.2891382575035095, 0.3448220491409302, 0.4807244837284088, 0.470004677772522, 0.08723217248916626, 0.02924683503806591, 0.31626489758491516, 0.008816590532660484, 0.12859119474887848, 0.14507627487182617, 0.49326878786087036, 0.23073072731494904, 0.15096057951450348, 0.3350931406021118, 0.09597928076982498, 0.35576727986335754, 0.26850664615631104, 0.053761690855026245, 0.0898466482758522], dtype='float32').reshape([24]),
            paddle.to_tensor([0.138846293091774, 0.319439172744751, 0.41868236660957336, 0.2924056649208069, 0.4716469645500183, 0.24543708562850952, 0.13587437570095062, 0.41566726565361023, 0.475408673286438, 0.14695952832698822, 0.23507775366306305, 0.4063407778739929, 0.0693902000784874, 0.14710597693920135, 0.3489690124988556, 0.2754063606262207, 0.34496763348579407, 0.3364245891571045, 0.3749992549419403, 0.0931539312005043, 0.2399061918258667, 0.24965296685695648, 0.4937789738178253, 0.4913973808288574], dtype='float32').reshape([24]),
            paddle.to_tensor([0.2840586006641388, 0.40737810730934143, 0.4273706376552582, 0.11165743321180344, 0.030132118612527847, 0.07948171347379684, 0.47669801115989685, 0.4030431807041168, 0.3227931261062622, 0.39064648747444153, 0.01172649022191763, 0.41855865716934204, 0.1347058266401291, 0.4122360944747925, 0.3500693440437317, 0.47472208738327026, 0.16725990176200867, 0.04965321719646454, 0.22681470215320587, 0.4333457946777344, 0.2727568447589874, 0.4732567071914673, 0.10324613749980927, 0.26704534888267517], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a3d9cd28fdb6291cc4bd37e252785569(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 1152, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_39330267090c991ed50c9159098040e8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 160, 32, 16], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6d776a4944b1367335a29ea8d976cd39(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 640, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_600593c2b0292f2202ad0502d6793939(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.035456668585538864, 0.4604385495185852, 0.21035660803318024, 0.4136507213115692, 0.05438355728983879, 0.22814756631851196, 0.33795222640037537, 0.04430039972066879, 0.46385812759399414, 0.3424520194530487, 0.2970336377620697, 0.3617819845676422, 0.2948587238788605, 0.07695483416318893, 0.45574817061424255, 0.05739535763859749, 0.10260828584432602, 0.3831775188446045, 0.12319599837064743, 0.36499273777008057], dtype='float32').reshape([20]),
            paddle.to_tensor([0.025641843676567078, 0.10739785432815552, 0.37986519932746887, 0.15187476575374603, 0.26188796758651733, 0.15228411555290222, 0.37579911947250366, 0.023361902683973312, 0.37536999583244324, 0.19917403161525726, 0.43192258477211, 0.28341570496559143, 0.45647597312927246, 0.2816809117794037, 0.35553988814353943, 0.2684299349784851, 0.47368913888931274, 0.29012253880500793, 0.14913058280944824, 0.4095962643623352], dtype='float32').reshape([20]),
            paddle.to_tensor([0.25083237886428833, 0.13186265528202057, 0.04494642838835716, 0.12431075423955917, 0.1035265251994133, 0.385871559381485, 0.30567553639411926, 0.3907862901687622, 0.14531458914279938, 0.029803350567817688, 0.10575627535581589, 0.17087744176387787, 0.4585890471935272, 0.4354744851589203, 0.03460942581295967, 0.08591319620609283, 0.001242886297404766, 0.38966599106788635, 0.0931314006447792, 0.46648186445236206], dtype='float32').reshape([20]),
            paddle.to_tensor([0.21199403703212738, 0.38014110922813416, 0.0749797448515892, 0.28728652000427246, 0.17547310888767242, 0.21068346500396729, 0.2330324649810791, 0.3190278708934784, 0.04329591244459152, 0.3321252465248108, 0.2644270062446594, 0.19571396708488464, 0.42855456471443176, 0.009881996549665928, 0.2061919867992401, 0.19780580699443817, 0.04848647490143776, 0.19407880306243896, 0.2318233698606491, 0.3537740111351013], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d0bacb7e09ad0cc97564821ab7bdb739(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.3816847801208496, 0.02181198261678219, 0.10352189838886261, 0.08986427634954453, 0.20025473833084106, 0.1794925034046173, 0.03440628573298454, 0.36371049284935, 0.3365410566329956, 0.32987865805625916, 0.0756264328956604, 0.3623286187648773, 0.489488422870636, 0.1495952606201172, 0.030261602252721786, 0.45233264565467834], dtype='float32').reshape([16]),
            paddle.to_tensor([0.46862930059432983, 0.42152148485183716, 0.2891199290752411, 0.24945218861103058, 0.2255338579416275, 0.21631641685962677, 0.38770151138305664, 0.27204591035842896, 0.29642874002456665, 0.1563350111246109, 0.10978911817073822, 0.3067081570625305, 0.0074460492469370365, 0.05133385583758354, 0.09292851388454437, 0.31609925627708435], dtype='float32').reshape([16]),
            paddle.to_tensor([0.16612863540649414, 0.03568657115101814, 0.35677313804626465, 0.38657745718955994, 0.4300957918167114, 0.304736852645874, 0.17872294783592224, 0.10438213497400284, 0.2214546799659729, 0.12765248119831085, 0.3158443570137024, 0.3256169855594635, 0.2525842785835266, 0.4195830821990967, 0.3445826470851898, 0.48850002884864807], dtype='float32').reshape([16]),
            paddle.to_tensor([0.003169605042785406, 0.38070836663246155, 0.11232328414916992, 0.20077699422836304, 0.3128184378147125, 0.10042677074670792, 0.22343984246253967, 0.08843235671520233, 0.3309127986431122, 0.4805622696876526, 0.4203294515609741, 0.4172738492488861, 0.38421180844306946, 0.17287980020046234, 0.17866387963294983, 0.32160845398902893], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_479de920af3a7a230d821f84346267b3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 12, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.007253045681864023, 0.13398349285125732, 0.022103067487478256, 0.455102801322937, 0.08683495968580246, 0.11802490055561066, 0.20553912222385406, 0.15791650116443634, 0.4594203233718872, 0.01245712861418724, 0.2890196442604065, 0.10091811418533325], dtype='float32').reshape([12]),
            paddle.to_tensor([0.2645426094532013, 0.4052819609642029, 0.1551814079284668, 0.1718829721212387, 0.29517123103141785, 0.45630812644958496, 0.29887986183166504, 0.12694521248340607, 0.10479580610990524, 0.43047434091567993, 0.22698725759983063, 0.2223849892616272], dtype='float32').reshape([12]),
            paddle.to_tensor([0.1748577356338501, 0.08949490636587143, 0.3540988266468048, 0.178551584482193, 0.4000144600868225, 0.3046111464500427, 0.10681083798408508, 0.19636228680610657, 0.3965534567832947, 0.007328768260776997, 0.2688955068588257, 0.27986106276512146], dtype='float32').reshape([12]),
            paddle.to_tensor([0.04152224585413933, 0.17444439232349396, 0.16349956393241882, 0.018627343699336052, 0.3280639946460724, 0.02621043100953102, 0.22038815915584564, 0.008411535993218422, 0.4792163670063019, 0.3924293518066406, 0.11412005871534348, 0.34516626596450806], dtype='float32').reshape([12]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b65f09185d22a4169610985f12ef280b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 27, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.356604665517807, 0.2952965199947357, 0.0018679017666727304, 0.4479313790798187, 0.29773563146591187, 0.008204861544072628, 0.4783957004547119, 0.16015039384365082, 0.08435014635324478, 0.20416057109832764, 0.10572109371423721, 0.480151504278183, 0.0625251904129982, 0.16638854146003723, 0.09844403713941574, 0.2598424255847931, 0.4538741111755371, 0.3357138931751251, 0.2911880612373352, 0.05825580283999443, 0.4993787109851837, 0.10924089699983597, 0.13541851937770844, 0.1339695155620575, 0.10300285369157791, 0.4650343358516693, 0.4891642928123474], dtype='float32').reshape([27]),
            paddle.to_tensor([0.047645453363657, 0.49968937039375305, 0.2205384522676468, 0.08187433332204819, 0.3777133822441101, 0.2146642655134201, 0.44628626108169556, 0.34929513931274414, 0.4167434573173523, 0.09561894088983536, 0.13988609611988068, 0.263818621635437, 0.05059932917356491, 0.14284281432628632, 0.18316200375556946, 0.3804284930229187, 0.3736989498138428, 0.05010021850466728, 0.2635937035083771, 0.05187182128429413, 0.028766946867108345, 0.4996192455291748, 0.11495363712310791, 0.2996688187122345, 0.1990792155265808, 0.20308564603328705, 0.3628558814525604], dtype='float32').reshape([27]),
            paddle.to_tensor([0.480356901884079, 0.20032912492752075, 0.006138856057077646, 0.20090915262699127, 0.3958694338798523, 0.30890801548957825, 0.12210425734519958, 0.08586175739765167, 0.19877581298351288, 0.14229170978069305, 0.012692454271018505, 0.26176634430885315, 0.1388024240732193, 0.14381711184978485, 0.05594129487872124, 0.4785190522670746, 0.09739884734153748, 0.1555596888065338, 0.06284470856189728, 0.16906942427158356, 0.1556536853313446, 0.1735067069530487, 0.3231653571128845, 0.3325263261795044, 0.13201919198036194, 0.11805332452058792, 0.45026037096977234], dtype='float32').reshape([27]),
            paddle.to_tensor([0.474040687084198, 0.3706958293914795, 0.1653076410293579, 0.09075488895177841, 0.054532427340745926, 0.05978156626224518, 0.33407342433929443, 0.2017279863357544, 0.2753491699695587, 0.13239175081253052, 0.3226805031299591, 0.25228220224380493, 0.21126584708690643, 0.26978328824043274, 0.1322588175535202, 0.2480655461549759, 0.38971421122550964, 0.44419944286346436, 0.3630048334598541, 0.01046934723854065, 0.2992617189884186, 0.0795329287648201, 0.16472402215003967, 0.2980928421020508, 0.37430277466773987, 0.3023405373096466, 0.1838178038597107], dtype='float32').reshape([27]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8ecdb97679ee8ee21726f90edcc691ba(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 64, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_dc43dd0015eecaab463b62a64d5fa105(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 19, 19], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_efd5912aec0fd056bdbe0750959261c9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 64, 128], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.013250698335468769, 0.03448149561882019, 0.46103042364120483, 0.007309118751436472, 0.1790359914302826, 0.46839556097984314, 0.4669089913368225, 0.4921947121620178, 0.3427894413471222, 0.026102175936102867, 0.20109888911247253, 0.05650439113378525, 0.3480648100376129, 0.4281626343727112, 0.3285505175590515, 0.08557987958192825, 0.02023952081799507, 0.34524771571159363], dtype='float32').reshape([18]),
            paddle.to_tensor([0.10887904465198517, 0.20104020833969116, 0.09400524944067001, 0.012786940671503544, 0.40330904722213745, 0.20290176570415497, 0.36753153800964355, 0.2144942432641983, 0.4115990102291107, 0.20183628797531128, 0.4519379436969757, 0.28253889083862305, 0.2867276072502136, 0.34237930178642273, 0.3402431011199951, 0.21554847061634064, 0.4981032609939575, 0.2768542468547821], dtype='float32').reshape([18]),
            paddle.to_tensor([0.19842512905597687, 0.39381512999534607, 0.34736114740371704, 0.29469138383865356, 0.13138307631015778, 0.38581985235214233, 0.10130443423986435, 0.0868246778845787, 0.46221473813056946, 0.3448137640953064, 0.3266165554523468, 0.1130964383482933, 0.4082415699958801, 0.35411161184310913, 0.0055508604273200035, 0.47147244215011597, 0.16188454627990723, 0.07074441760778427], dtype='float32').reshape([18]),
            paddle.to_tensor([0.17527921497821808, 0.025949275121092796, 0.15741606056690216, 0.23241008818149567, 0.4280724823474884, 0.2908552885055542, 0.3941352367401123, 0.4819227159023285, 0.3232670724391937, 0.14762575924396515, 0.04557683691382408, 0.13139274716377258, 0.2839910686016083, 0.3011496067047119, 0.454660028219223, 0.41325509548187256, 0.08323366940021515, 0.27233773469924927], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_78ee2e26ae0f78d9914b70443d5e057e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 200, 336], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0c222ad3df2714168bbbea5f1ccf5f0b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_b4f4da3a8d44d1b5b3a2903a097df186
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 32, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b5b8e9ff916c554ce7265e7c8a766543(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 4, 50], dtype='float16', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_026a1368cd4f20a985083c719213675a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 64, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_634fbe3fe8596f1608e8d2bffa59c189(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0c90e6f36ee62d83e7a99e0bfa0cb061(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 28, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1f0ade45bd152ef82b588f2a11bdaab9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 32, 8, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0b4b8879c39d03f646f9a91b3184e36c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_373728b1490bc33e9f944e7dc2e95787(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 16, 512, 1024], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b22610661c3aba4c91d66f9c02532a06(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b37a9d0f63cb1d8171d304dd8f545e82(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5f89151d5524d3ee789d2b4cd1a607c9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e28a464948bafd11a8012207f50f6fb3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 16, 12], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_192bfa443e2ebf2c6bfa87b81cc89e2d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_93cb196bfa65ee5c3f96ce05d181a31a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2c9a65811391b48c16663893efad06f2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7368ec1ffb57fc339f15151366d9e8e3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 4, 26], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_399ad121ba8b7abf1eedd38cf9d4f3c3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b7377c3d7f0a477016a5dc92e1a42451(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f5b7b0fafa1d70e4588dab9864032d2a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 40, 40], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_02d2de9f3d7b29bc3390442d83ae46a2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1776, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1776], dtype='float32'),
            paddle.static.InputSpec(shape=[1776], dtype='float32'),
            paddle.static.InputSpec(shape=[1776], dtype='float32'),
            paddle.static.InputSpec(shape=[1776], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d50b2ed5144cbdd1ca5042fed72047fc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d685f2531a4e8e1ab9346744dc305143(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 8, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_801eff40f6ce09cb8d1af376fd7f30ac(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_795d9cc3f8e663f9f242086e329c6e31(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 20, 20], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ba5d32036b331464bee1a454f001a9f5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c264d3b0c0b1a61a28086f770ee56d74(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 8, 6], dtype='float16'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bcbd2bd27060e5444ea5c4872a8dff65(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 32, 32, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_83f47e87307cfd4dd8a195d9d35fd4d1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e1b19a292b33d2adecafe5164671e766(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 640, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_639564a6a54579140c8cf73996e45a72(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 60, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c23f660599def9fbcf07210411b7c578(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d888a268af70f20a6e8eb7f00a9b04c0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_11a1f24d7a4be003f6b0979b40654476(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2048, 8, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_47253853fe7e4c594fd18626e3ff76f7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 80, 80], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d86166d29866b8690a7ca1699e996024(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 104, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9437e9a2ff510eb57d949f967612d7d4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f29f53619829634831b832b49653cd59(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2cfd04874ac8e2deccd00b885bbd9725(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 36, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_093e50fc34ac9d700f8462a8951e7dd6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6857ca0c0268dc8b0debff2e586c173e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3b30e6c17eef017676cfb2b3323b0c80(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b38f647774f7456158edb7bbe2614217(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 128, 175, 25], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9b9a6a813dd98d200bcb55dbd392a4aa(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 120, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1245b3cb3e107d262d7ecf1acbf2e1e0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_455292ca3cef8e477c828481e3a2100d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9289ce5059ee500d56412e66c3fb5acc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 160, 160], dtype='float16'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7ca719eb7eda5d71abaf08e1fe892374(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1044, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1044], dtype='float32'),
            paddle.static.InputSpec(shape=[1044], dtype='float32'),
            paddle.static.InputSpec(shape=[1044], dtype='float32'),
            paddle.static.InputSpec(shape=[1044], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f632f6b71e76f85dde9e6a9e74d99131(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1c0d78493af12479591cd66912c25938(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_efcdcd8cb72d0add1f8e32f942773658(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 140, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[140], dtype='float32'),
            paddle.static.InputSpec(shape=[140], dtype='float32'),
            paddle.static.InputSpec(shape=[140], dtype='float32'),
            paddle.static.InputSpec(shape=[140], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_831f90d1946f5d88b73a11bf9e88126f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 36, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_93b7eb5220c982e74801e19028ab6cea(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, 4, 3], dtype='float16'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9f84fbdce742b74de5544e057cd28838(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e0667d9bad3e3f9948ecf0e9ab7356b3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_75ccfb5156a59ebd3b59de71bebace12(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 4, 26], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_398861261c8746d2881cedaba6979814(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8947a0ba9763d29f6df3181faa00e811(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 40, 40], dtype='float16'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_086aa9ecd36198eb2f410fb51ec5d3c2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 38, 38], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_557d967e46ba84e246ef28143474892e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 88, 25], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9e5b5017ef481ffe84d96a7e526c5fdc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_79a5d13472e01c086f5eba705a3482a5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 80, 80], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_03d2fb3dfca39e5be82fbabc33a13c2c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_892560361a7154478f5b2366eb05ea43(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_78ac0280d335b5d020d80c7278bc659f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 728, 19, 19], dtype='float32'),
            paddle.static.InputSpec(shape=[728], dtype='float32'),
            paddle.static.InputSpec(shape=[728], dtype='float32'),
            paddle.static.InputSpec(shape=[728], dtype='float32'),
            paddle.static.InputSpec(shape=[728], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a2e2e20ea35ca3f2eb4df3088dc67f39(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dbb0395720298544a11586fc9d5563b5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 416, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[416], dtype='float32'),
            paddle.static.InputSpec(shape=[416], dtype='float32'),
            paddle.static.InputSpec(shape=[416], dtype='float32'),
            paddle.static.InputSpec(shape=[416], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9ff4635bbd75d4d9fac9dcc46fc4bae0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 64, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_42b2574f34c443a4523c2ee045cbc4df(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c9114ff816b2f3e8d535e52e7d6050e7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 16, 256, 512], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8991372857f1063deb0639e3e0988529(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 18, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[18], dtype='float32'),
            paddle.static.InputSpec(shape=[18], dtype='float32'),
            paddle.static.InputSpec(shape=[18], dtype='float32'),
            paddle.static.InputSpec(shape=[18], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bb6c2f0038982adc38c6daa844fa9d46(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6dfff219623c2d00fe823701d0bb2d6b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, 128], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_88d2ee044265eec9b2d13f287f9e5975(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cf9747313572e8321f152fb443a5b43a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 4, 64, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_62213fc8161303858ca4ce0a0316e3dc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_433314088006787be77fb1fc69ad92d2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d733ad76e6def93a6431f82f0276bd67(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 138, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[138], dtype='float32'),
            paddle.static.InputSpec(shape=[138], dtype='float32'),
            paddle.static.InputSpec(shape=[138], dtype='float32'),
            paddle.static.InputSpec(shape=[138], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_653a2863d504e0a421142e940bd29d6e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e0c71bc41aeac4c359b0d5c6e94ea65c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 72, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9e0e872a60af4fbfabce465b69e0efeb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 61, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[61], dtype='float32'),
            paddle.static.InputSpec(shape=[61], dtype='float32'),
            paddle.static.InputSpec(shape=[61], dtype='float32'),
            paddle.static.InputSpec(shape=[61], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_025a876c127e7008591dd621431ad86e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 17, 17], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7be5f0cb427996b075b93f0c99ba8f58(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9fb4d2c35d0d8e55697ed2f596b3c41c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a6a02bcf79acd87c4a2d312c69458e8d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 88, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0126aeef335dbb2dd53ff29c62ab0a41(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_95752e262f48b523693ff1b90222dec0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 104, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_14fc0a24ced80903662f15f415902531(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3a14b87cec90c13cb6254bcbcfcb2843(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_60fafc0aedd1d0ef03e639ab59eed917(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 80, 80], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e68a28f7acc5e1fec128bfef6425f8d8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 17, 17], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3c414d695d1891198249a313f1480f9d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_95aba9563c2d8490e87f0ebea1f79ef9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 30, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e04a6d914746c47bf4f361eb509d1ae5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7b12fe585d6439f409f2d665b43cdde5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a674dc3c5ebc2981cf0bf4f72a31dacb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b9a89d6c5d649626b0ab5c3dc05a8ee8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 120, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_26198720caabf032d5962371af520b5a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 4, 3], dtype='float16'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9f150bcd6de150d77ac8d3cac5019fbc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 20, 20], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6672a4101d2c1bf5bf196ed91fc6113c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_306980a35aa819da3afc5c4d630f72be(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 36, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7bfe4867b2ee50af53c3d7bdc5f7f58f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 16, 256, 512], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_396e1479eb15e2e6034ce5d21e596c0c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 31, 31], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_72afea2bd83e26c58d662a0ddac1de3c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0f55a3fe994b3b3ea44afa5ed331d5a0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_011bb84273957ef1e8fcf8977296eb19(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_23598a8f265065965154dcdcde6d11d2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 2, None], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bde20d420d7966db0d5ad3bba41cdf3e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e4cab3560020f8e75167f6010b432790(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8908757f1e09639d6bcae97a8afe8842(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f1c461cb5f6f22728e44806e063a738c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3b283aa56dc1a8591160c9e27af049ee(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1b08cf49c3e0c08ba7759192916c262b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 906, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[906], dtype='float32'),
            paddle.static.InputSpec(shape=[906], dtype='float32'),
            paddle.static.InputSpec(shape=[906], dtype='float32'),
            paddle.static.InputSpec(shape=[906], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a977517bd9688cb9754372ab263ef171(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2048, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c4c699c445b062d30c0454b975ad4443(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 768, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9479e2f3222000ddf2ecb70587464185(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 336, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2d81571c6c5da8d099f4e8488504251f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 728, 19, 19], dtype='float16'),
            paddle.static.InputSpec(shape=[728], dtype='float32'),
            paddle.static.InputSpec(shape=[728], dtype='float32'),
            paddle.static.InputSpec(shape=[728], dtype='float32'),
            paddle.static.InputSpec(shape=[728], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8b69adeff2c71ee7b815f794bb461a6a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3890f5a700857b435490454b960a454b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bec5c67fc72cf90ddb9c224701278864(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_be7b23f378ba717a9d80dcf4b4dd4ee5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3ddfa948ffba0daaf2bab1fc3fe10349(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 60, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_40febeaac423e00552e9dbdc617b7213(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 640, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9765f38f093f55ef89226fee7f91366d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5bc89a0588a01a52fe89ef44e204e770(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2048, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4411473bb05eeca76341b499dfcf4c73(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 16, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0e05ff9dec9e57dda66ca079eab7ed32(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 512, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9580c5aad3dc54b081d5076a4dec374f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 20, 32, 24], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_88c91ea1acff8ca87cb913f04c8fb8c2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 60, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_10a96d116bdf16eac747ffae2f89d57e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fd6c62a5b14788f5279b50233d0e159f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a15e488b34fc5df2fd08f2ab54403dfb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 12, 12], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6b466436acd8abbe30b6123b8d9f3da3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 128, 128], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9a1a7d5ba2e42c33e716ce604cfade5b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 96, 96], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f3652582614c52d5e020edd850bbf63a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 20, 32, 24], dtype='float16'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0a8f2e0b225571cbdd0378af5cd9cb3a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cf0dfdaf0c4824bd9e3f298c3ab3d32d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b198c962704a1b49b5b64a507202a445(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 8, 6], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c892c60e1007241eb02b17ef2d1c1ece(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1b1de1cb6185e0e58217db08ea6109f6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 32, 128, 256], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f15ba70e4e8d2e88b323c4a8eb5175e3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_22e747980adc24d3e691b98fe7f97177(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b3a9c28ba2de97e4bf731fe5fcce0b52(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1104, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1104], dtype='float32'),
            paddle.static.InputSpec(shape=[1104], dtype='float32'),
            paddle.static.InputSpec(shape=[1104], dtype='float32'),
            paddle.static.InputSpec(shape=[1104], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_759a950012c9a587d288dd903532d5ef(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2048, 8, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6072bbc3aa1bc3812bd0572c3fc3ee0b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dea0f47511d5ea514ec4b33b6dcf178e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4804a0d2e5fe69e735374909976ae0f7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1200, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_458b69cc397b881816c00af070304cd6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1824, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1824], dtype='float32'),
            paddle.static.InputSpec(shape=[1824], dtype='float32'),
            paddle.static.InputSpec(shape=[1824], dtype='float32'),
            paddle.static.InputSpec(shape=[1824], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_720e8ed98eeae62b50a77b14408c2b10(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c0e228c07c5e1b9d3db71607a0a58d57(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 64, 350, 25], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e1a2bde34d7432d425068fe9e4eca25b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 20, 20], dtype='float16'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c70905879475ab161d7af4f4c9614458(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 17, 17], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5f7667cbaf3d2a84b08da0bcbc1ff90d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a43b16ef5c6d1f041d67efa1df4ef913(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 672, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_09f774ffd15045b465da35e642040316(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 62, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[62], dtype='float32'),
            paddle.static.InputSpec(shape=[62], dtype='float32'),
            paddle.static.InputSpec(shape=[62], dtype='float32'),
            paddle.static.InputSpec(shape=[62], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d0af63b422c5434e9c5ad57c6b33b9d3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3f5da1d25591a17618f32d5f4cdb2abd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, 17, 17], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_35dbfde51f98a7f8586486c5e8c555a0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_152dac02533dcdd72854cac3edf3c57d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 12, 12], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9e6e8c696fb140f453fb8604690d225f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 480, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bf9ca9a52e7714e3b3f92960025f51ac(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7ea9f08a9b8f521acd61667dd33f31da(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 18, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[18], dtype='float32'),
            paddle.static.InputSpec(shape=[18], dtype='float32'),
            paddle.static.InputSpec(shape=[18], dtype='float32'),
            paddle.static.InputSpec(shape=[18], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d2f89acc798a3450b177fda556d9c640(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 64, 48], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_37ca527caca66fa6bd8b014b3f6521f5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1536, 10, 10], dtype='float16'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ff3939c6254965c12dafccc71f346fe4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_970c9da2f86e92b99b203dec83780b08(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a77b7cfb6a89a98f582848027cf0d454(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 20, 20], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_69d4d8d352c35b330e34d3483128f699(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1469e5829ad6dd174a4d4301d92df512(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 38, 38], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9bd488df21cd55f3ba5ac049a495705b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 16, 12], dtype='float16'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9b35cc9659988100db4c2855c4d8b2ac(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 35, 35], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a90ad7972227a94e7fe7981f2ebabe70(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 160, 160], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_87d1861b61a5421593d15b19010c587b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_504e7b0e17e361e8bbfbcfd1c468bd6d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cfea30a48e65df8701477c50ffaabbaf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ac25a9f438fbee8212f63b2d4fc03f5b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e3d577d4507d8cf60b0d71c3eade4704(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 73, 73], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ab836b831e4fd09aff2ca9c06012a5f7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 6, 40], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b2f70fc54e606b9ae002fdec0cfc7df6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 30, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3cedf31f95c555c6bdb8a2150ea8087b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_44420878aef6a461745211eca5545482(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 576, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ad8305bcdf7029aba50842d7fed98e6e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, 128], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_127ed08cbf7d19b8962ef5d19cc14847(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4607809436989505002a1958a7354e68(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 208, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_60ec4ec8f0d673e48eaae3861d000368(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3f59838f0a9cd17f718200550a2bdf3a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 102, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[102], dtype='float32'),
            paddle.static.InputSpec(shape=[102], dtype='float32'),
            paddle.static.InputSpec(shape=[102], dtype='float32'),
            paddle.static.InputSpec(shape=[102], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dd1b6274f2c1be924839d5d12c6206af(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2432, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[2432], dtype='float32'),
            paddle.static.InputSpec(shape=[2432], dtype='float32'),
            paddle.static.InputSpec(shape=[2432], dtype='float32'),
            paddle.static.InputSpec(shape=[2432], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_adfbf8b3643a0b20cce605267bdb05fc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 320, 4, 3], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e5fdb4037aea50a357085c04275bec90(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 17, 17], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9ff1465bf90f692f92fcdbb9f0609310(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 64, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_140fff394b9bab4c3eb8c208a8bd719b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 840, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[840], dtype='float32'),
            paddle.static.InputSpec(shape=[840], dtype='float32'),
            paddle.static.InputSpec(shape=[840], dtype='float32'),
            paddle.static.InputSpec(shape=[840], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_31b49c768d52ebc24a46e57240ade1a2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 30, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_22abb3e6174fe3d2c487d3c21111b5bc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 336, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5441acb8165a81688c6b7dc201fc8c0a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 20, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_24e7b3aaa2ad76692a7ac50c0a6fad88(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2048, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0a83b1b40261c8df6f7d4e11860b3fb7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 30, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_26add3720a47ff50de0328dcec26b585(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_58335340d600825583357bb7285fdaf2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 5, 5], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_06188808edfa48caa40aa1fad819536e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, 31, 31], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_aaf74873a9105f0af43d9af182072d3c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f0eda39cb0cd71973c376984841f0139(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 128, 128, 256], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c0eb1e58665b6937952e19232a64ffa1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 4, 8, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b60fcd0b6f26428bc5c7770bad7ffa1d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 30, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4ff6ea202881aecbdab6f3d61896bf38(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5294ebd8899a711fe2314802f5ecba45(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 20, 20], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bce7f856811e02f9600c9a863f63cc58(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 320, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d9d060b5e64903648e88dc3cc8cf97c0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 72, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cf3bea7e81eba2f8803fd04ad3ecbdef(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 12, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_98ee279dc273eba3588a15388601fa2c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1888, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1888], dtype='float32'),
            paddle.static.InputSpec(shape=[1888], dtype='float32'),
            paddle.static.InputSpec(shape=[1888], dtype='float32'),
            paddle.static.InputSpec(shape=[1888], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d39b6179e48b5737892563f03fbd1dd8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 64, 128, 128], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bf9ee670235dff52a659518f1e62b055(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 12, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b8cad70198a579ef46dd2d7f8f14031f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_86bbe428ad7cdf86b46d39572e0601ab(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 30, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_32edf953fce616597ad2b44f8310ea33(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 8, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ce43855413328140ee22b283997d87ec(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 232, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[232], dtype='float32'),
            paddle.static.InputSpec(shape=[232], dtype='float32'),
            paddle.static.InputSpec(shape=[232], dtype='float32'),
            paddle.static.InputSpec(shape=[232], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_950c1cc2a634fed8a379cd4dfa33615a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7c8aafac5a643847685bb567642debfc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 32, 128, 256], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_32f7e7e456ac4fa76328f4c94180176e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 82, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9aaffcd1723b899daa7ac8d9d877163f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 24, 80], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0f299a987c28e8264dec78dad28a831c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 28, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ba12f00683fce2176c748a4d1787d506(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 8, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_148a3483c0f7584668318beaeeb76022(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 270, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[270], dtype='float32'),
            paddle.static.InputSpec(shape=[270], dtype='float32'),
            paddle.static.InputSpec(shape=[270], dtype='float32'),
            paddle.static.InputSpec(shape=[270], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5ac548483b1eb78cd51c68228482ed73(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_758efcb799911c6f0c6d536cf461001a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cdef1e178dcf7458f57f9c3d0f14101c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c3c82948e8ceddbaefb14f1f6e0adfe2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_47a6a1a509697146a8e9e71dd351270d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 40, 40], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bd9785b1c71ce71addc5a0faecd777f4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 80, 80], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0b2e3d95f27029d02d70f310dffa5275(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 4, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2eb2104ff4f2d8c45a218e5646e56e39(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 40, 40], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d005e9235308a277e8637c9104600f9a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 704, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0e7108aa4d5e6ec1d2977be007d1d6e5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 104, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ad27f36a6a00300e6d2db1669fcbc05c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_365d00131505c7d39b3fc12f494811a5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 320, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dea9f847be393beab56b023670ca37aa(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 128, 128, 256], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f68ee94db56350745cead1204ae65147(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_49740e807080b999383343e1de877743(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 160, 160], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_448c0f99cf80e2fd7cb8976990d3f7f2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_76819a13296acb267a71536c7d3a5599(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_21a16052f5089a16cac936325c38f956(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9199062195c29090c0af0cba91381c55(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 64, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9132d1421c6776b558135871c20de887(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 120, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a40598a92049528676957c4113ab8b71(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 672, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f830538a49880f7faa7090ed914b6a21(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_aed2ef1f3b09eb7417781b46d6c304a7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 32, 24], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_45f7eefe39f0cdc7e697e422c7996796(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 366, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[366], dtype='float32'),
            paddle.static.InputSpec(shape=[366], dtype='float32'),
            paddle.static.InputSpec(shape=[366], dtype='float32'),
            paddle.static.InputSpec(shape=[366], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e298979dcd323edbf2ba8f2f311beba3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 480, 2, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b9857c98fc49adea29001e09fea3d68d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ce8fcec7c43c5e4a66ab86f6974b50ed(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_84ee48c3345df24cca6b87e894987d91(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 19, 19], dtype='float16'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a512991055d5115589257aad5b14b81d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 2, 2], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2aca463f726fc557a6eb6fffd721807b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7a26ecf10bc78b5487d0ed21b0a39662(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_614edde15238b37cab52731fd37f39b7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9a6c60662dbee1ec9a8016799a0067fc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 336, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_31f4a4025c50f577a0d40bf10d25fb87(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c5344cdfe72e997e5159f4cf3d17f3fa(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1344, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1344], dtype='float32'),
            paddle.static.InputSpec(shape=[1344], dtype='float32'),
            paddle.static.InputSpec(shape=[1344], dtype='float32'),
            paddle.static.InputSpec(shape=[1344], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f7304499082d0cdfd12079a9338d57ba(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 40, 40], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6894ddba73590d8743b835ce9fdde0b8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, True), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bde2707e5c148515f5b22de093b4d6fd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5c4c45fe10373a4887651be55e63693a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1184, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1184], dtype='float32'),
            paddle.static.InputSpec(shape=[1184], dtype='float32'),
            paddle.static.InputSpec(shape=[1184], dtype='float32'),
            paddle.static.InputSpec(shape=[1184], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_299dae75cca55f0032f06e8de76caac3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0531163f843401361762cdccecb950eb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d5d4acd8a1de3893d6b69b05d68ff56f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 480, 12, 12], dtype='float16'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d4400d0065e88fba0edb22f813447e7e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_189ca6ed72ffb5645192b7ab85ce4e09(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 40, 40], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0ffdf4b0113987366d210bdd01ca249a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4295435d2d0a847d799e7a323e990fc1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_35ff2db50ad7ca1bf33f0c22454b1841(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 120, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_04ec187a23fb298c7d6e878a60cd785c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 40, 40], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_966af676a02e7234598162cc353f5a0c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 120, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_beab501839800f2f2cd590f0c425f144(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 120, 4, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2d1cf063b27b9372e920c977c556bfdd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 4, 64, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b3f81b916e59f28b56d7c33903e8a638(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a36275c4e1ca7ee9b59d383a698ba621(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 40, 40], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3ce24945cf09613f0e8e068528b5b165(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 56, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_62c0c2ffe276d6c810cba03d6d5dadf7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e2c7a5f2b9b5f2b02274d479cab4c5e4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 70, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ea3d781e827ac74579346115b20b8002(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_03d990597e58999bf0a44ee1621b83d8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 112, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9e30fae60f3b8093db147f92de4ca394(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 8, 6], dtype='float16'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1b831d030b659453f9f36a2daaa0d61f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9fc8111224fa03c870923298a0763440(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 16, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_488b2e1de9d2ce6934a4a2197e547553(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 52, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[52], dtype='float32'),
            paddle.static.InputSpec(shape=[52], dtype='float32'),
            paddle.static.InputSpec(shape=[52], dtype='float32'),
            paddle.static.InputSpec(shape=[52], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d3a35937d9c5845f9df447ced66c3c0f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2048, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f49279ac3043eda0a82805a7a8ea9779(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 128, 64, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_390e91adf11e1069414d8e99c9dae65e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 175, 25], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_80e042b16153a6a13000fc69bfd64db1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 64, 48], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f3218f271032194f87b942bbcdc87929(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 320, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4834a71b097f80307b114c152580156b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d47d6ea8c05fac573da3b91bfaffb8ca(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 704, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1921d031b73340a7414904caf27a3935(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 104, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dd488a3ab73b95d9e1bdd5c6be0fe6a5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 32, 100], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f4cec7d657e259f4c2913c4230392b9f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ce3c5438c2af00accb1b7081761e25eb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_76ab480359ef928c8720cc2e4d4649b2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, 8, 6], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bccf32763057437a36894e2efc788dbe(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 480, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_22166f4a1b8e7b18c57ffc83278e54fa(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 40, 40], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e09aa6a49354526ead685eacadee9c30(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_499000feeabf66e84f457239b361acaa(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 208, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3046179a193fe152f8a0c8032c94fb97(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_645d76d5c9cb991a81f41522b47a9679(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_425450a29c03def74064017764b43a41(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6884bef74e02ae00c3bff219ebcfee46(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 32, 100], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_156d2b02725383e937bf08c7a755c7a7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 32, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4e066cb3d7c6f78d55cfbada4f260ec9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_46f1db9af75718b28443482c27151a5b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1cc6aad071abf8ea64d3fe3ef4684e56(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 4, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f8f826ad0801bba2f1aa15a16f0af589(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4ea47978706cc46255775a9741b672c9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 672, 12, 12], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_76c4233d2f19f89ed4488de2d0bad695(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dcda6b8c7486d0b9bc82aaf2f1b677d1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 4, 26], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0aa32758ff3449ee07ce9667f89e0a15(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 10, 10], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_11304b7dd07aac514dcf1769ed48cdf2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 448, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[448], dtype='float32'),
            paddle.static.InputSpec(shape=[448], dtype='float32'),
            paddle.static.InputSpec(shape=[448], dtype='float32'),
            paddle.static.InputSpec(shape=[448], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_29cce43550466adc735962c12bec1544(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b27edf12bc366327fa179e9bda6aa083(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 26, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[26], dtype='float32'),
            paddle.static.InputSpec(shape=[26], dtype='float32'),
            paddle.static.InputSpec(shape=[26], dtype='float32'),
            paddle.static.InputSpec(shape=[26], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1f43c1722e1112a30fd429a884406979(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 4, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c624d63392a8c8c6e4d759243edecf88(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2048, 20, 20], dtype='float16'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2c4b0279f4467672e4b0396db0c2bcdc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 512, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4f14c932d02b6507f9245815661bb29a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6a86473a8eb8714791b7ea8aceca3b47(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_82dd2e51bf82f64805cdeb9e77f57b2e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e82b94524c25643933cc36c58a30e558(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b2209e036a8d188c0ae50a19c154061e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9b1661a5cd671a8b70b1304b03ebf241(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f20b7d281ea20a4072d0f48c8a19c847(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4a1101b6ba9ad30c33ffb1728d76a0d3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 232, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[232], dtype='float32'),
            paddle.static.InputSpec(shape=[232], dtype='float32'),
            paddle.static.InputSpec(shape=[232], dtype='float32'),
            paddle.static.InputSpec(shape=[232], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d5a30160af175087d67221cd759bd282(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3d0a848e20544900d50d701a3eb9b131(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3dd51b2ec7b03b02c4aa857a43f3dffc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_19bbd1b0980169ab4c479175a10906d4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1200, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_16a62df0a4b2e940b226096560e7c1c5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1296, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1296], dtype='float32'),
            paddle.static.InputSpec(shape=[1296], dtype='float32'),
            paddle.static.InputSpec(shape=[1296], dtype='float32'),
            paddle.static.InputSpec(shape=[1296], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_41d527d62e185f65abb0ee3c1bfbf1f1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 4, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e1fa640922dffe9ebdf7932ff113bf51(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 32, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8cff36adecaaee29f89c236933757419(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 32, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_eb989c26d3976af75d654c8ead02f5b3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, 4, 3], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_81171a73171edbafffb791570671a2bc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 4, 4], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_70d4159fe200b677a52f2592dd80cc4e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1280, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_97114e27a4e86e0b0689cef0f1ac820d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 128, 128], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d8a247c0ec9ff71630a7b32ce2c9213d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 76, 76], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0f091825d71d4e1ffa027da64f5d4394(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 35, 35], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ba222da548968190858df4cf68bc622e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 160, 160], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_85e67324a9e48c7e743d2bd01f5caa73(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 8, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_acd0d714098c95881a5ad397bed31056(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_edf331ca5fa8f99449330bf1525d87ae(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0cfcadeda21a27d8481eb1381f9e54f7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 64, 256, 512], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cd1ad2d50aa07808a6d27b1f1abf8ef2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 80, 80], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_42db160513b08a34ac34c58fd9f4cdfe(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_acaafb69840e887faefad3ca97ae8bba(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 12, 12], dtype='float16'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7941a58505ea31eaa83ce3690d3d3db9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_016ed345c77e700944f86327d3e197d6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 4, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_91169ace885e4a24d300106550e4c6a1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 152, 152], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ec58cf47a015169c640dba668a5ac0d4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2048, 4, 8, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c7e92a9b76af9d3c0da8e14097506bd7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1152, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_512d64956c2da18527392987c6edf207(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e58f60cd2c596281532515892b76457b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 4, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0967bad01a11b325be2075c12bcf6e8d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 38, 38], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_736bd2913629b3dcf106e7ca0229d249(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 4, 3], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3a294fe275f6cd8d1f3c173f6f1207c2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b8cc8eed43baa042d1303635c404d3fb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6019f3d38c8be9d7dcead7fde144fc69(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3c866ab54cebbaed9f5d6da4974354d8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 42, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[42], dtype='float32'),
            paddle.static.InputSpec(shape=[42], dtype='float32'),
            paddle.static.InputSpec(shape=[42], dtype='float32'),
            paddle.static.InputSpec(shape=[42], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a8300eb81472fa39883d842ffa8e5226(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2048, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1ace33d8bf5c0141d2ececd56c48ff1f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 672, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ec88a9a1602c9efb410e4e001ac243c8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 10, 10], dtype='float16'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_56388e95fe8a66c4dec6933f7243c2be(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 8, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_59a74a1aa3538b67792395a662b46c96(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 160, 160], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6a5770448924111a18dd88356c399321(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 120, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7141b8d9126bf91af1d01faeff73cc6e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 72, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_75c37502a040ce81a897458d73328af1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1152, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_68a3a37a967d00e45a032a75e100cbbe(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 60, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_89218944f1439edc3cce4e16883d0d01(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 16, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a9f5f909b34ff4c21dedcc1fa91a151a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 150, 150], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_df6f636f40813b72b55d61f3fe6a52b4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1776, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1776], dtype='float32'),
            paddle.static.InputSpec(shape=[1776], dtype='float32'),
            paddle.static.InputSpec(shape=[1776], dtype='float32'),
            paddle.static.InputSpec(shape=[1776], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c3f0b2690804cceb3a998c4afc73fbc7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, 64, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9e245464a02f8d5c9be937d9791fa951(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 4, 256, 512], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_012624d96a179bea34276af76c94f5da(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 4, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1660dc9441cdb2a96059f89ab09eaa94(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d66bd28761544b0977a883bd31ab43f5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 4, 32, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2109b8194661f3138acf2c8b007d6b84(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 480, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4b14494f518840e1d8943d5a28f48e86(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 416, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[416], dtype='float32'),
            paddle.static.InputSpec(shape=[416], dtype='float32'),
            paddle.static.InputSpec(shape=[416], dtype='float32'),
            paddle.static.InputSpec(shape=[416], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_45037d8ea16b2d2e3487e5b2fae77fa2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 4, 64, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1805ee5f66925e2e2281b2c7beb70074(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2048, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_964f2c00730c0f617a2caab3ff97bb96(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 972, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[972], dtype='float32'),
            paddle.static.InputSpec(shape=[972], dtype='float32'),
            paddle.static.InputSpec(shape=[972], dtype='float32'),
            paddle.static.InputSpec(shape=[972], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d0eb36d2a7b10a53ddf6786bc2ba46c9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 31, 31], dtype='float16'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_73a66f5a4072a54571f8e6b41c24a31b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 102, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[102], dtype='float32'),
            paddle.static.InputSpec(shape=[102], dtype='float32'),
            paddle.static.InputSpec(shape=[102], dtype='float32'),
            paddle.static.InputSpec(shape=[102], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6ce9c0d4736e8154caf21aad5b0a853e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_20ed86f8dadd07a4674aa0e32b32eae3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_625ea443ece00e0ece297543991b2b7f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 112, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9f5b52cc907400eaaf11705256ec7039(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_89131f01c734536b69f798872c41bf81(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c47b8a59da9ba8b283ceaee01f4f4e4f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 7, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c892f8255afcce5721399d8970d0c97d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1000], dtype='float32'),
            paddle.static.InputSpec(shape=[1000], dtype='float32'),
            paddle.static.InputSpec(shape=[1000], dtype='float32'),
            paddle.static.InputSpec(shape=[1000], dtype='float32'),
            paddle.static.InputSpec(shape=[1000], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bc261f501c79d13332c58d7ddd9be9c6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-06'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 128, 64, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7c7e6f5bbda6509b41494959f924cae4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_31c2fa12be1f4c1fc369656d85ae0b8b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 20, 20], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cb59aa2c490dbef566260bca50a12cd0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 16, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_40574a8c62f9d3b897f69456cf690e39(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 35, 35], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_85ae8fa92315e18b68b8dc16979bcd5d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9d8cb553c35dbdf8a43d27661cc9fc0c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1280, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_86de788e6bb94bd4959b9b0ba3ac5b8d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, 31, 31], dtype='float16'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ab1ad85a256f22d05c0ac5a2c9d383eb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1152, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c411f6f33680b03a4a7622bf3bbfefdd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3b17e66f7026f48e8e8b7b11d641c130(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f18595d2024ae3d46e77191cf6bf85e6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 32, 128], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_08c45d993c57945b8989d36d0b06f306(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1504, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1504], dtype='float32'),
            paddle.static.InputSpec(shape=[1504], dtype='float32'),
            paddle.static.InputSpec(shape=[1504], dtype='float32'),
            paddle.static.InputSpec(shape=[1504], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8592516a8edd37f0f9798a35009b999b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a4da64c4daa8327aeab4500c67f4a858(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 48, 48], dtype='float16'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7f61c38bb31decba43362a7297fa1282(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 6, 40], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_31a7804c24cf00b89a64598a2aa3f789(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 720, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1dc0990adccbbca8c2f6291e6ea35d61(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1b68c0dd3defc98dbe90fd15975e97f8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 32, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b2dcd37d98e3adf79f9f85ea7f694dc4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_93dffa82bac9460712e64f88ed24475c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 80, 80], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_82cb6c9abe3eac1e80505b61c84166b2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e85b60b290717e59a466822d1e42eb3d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b081fb752a993ad40bd7ed7adfc6290f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 20, 20], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_59c4e0bc731681aacc4f2d01dffa1b58(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_72b552375f023e383d17aa69b279f370(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 4, 64, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f1a082db6ea56cc6f834213b72feb187(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 32, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b42e2b0e2baa229ef0a8f97466061c29(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f3ef53dd7ffd9dd955963e7797739157(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a8a7335ee76d483408684aa7a0d340cd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 16, 12], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1ef9a37669994e790928e31cdc463b1e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 4, 26], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3a84edadf5bb6023f4da6bbef1abdfe8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 16, 12], dtype='float16'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9d4af7c9a02c07253705daca1609ea05(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8de2a97e06d91e33f36e2fe3b70e55ea(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_677631133c87b517e069a44dc621d676(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2064, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[2064], dtype='float32'),
            paddle.static.InputSpec(shape=[2064], dtype='float32'),
            paddle.static.InputSpec(shape=[2064], dtype='float32'),
            paddle.static.InputSpec(shape=[2064], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_98e11c90b7a6bbcdbacaa094234728d8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 32, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8cabf75de230bd48aadcb8135ae89fee(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 12, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_148cd4160d3a517d70ebea43e8198ce1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7aa782216c8bde1b95745d01bda60367(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 27, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[27], dtype='float32'),
            paddle.static.InputSpec(shape=[27], dtype='float32'),
            paddle.static.InputSpec(shape=[27], dtype='float32'),
            paddle.static.InputSpec(shape=[27], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_408d245d25126df77149f7d08aa9340f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 40, 40], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_acfdc8595c1523b3ad35de36da9e8119(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c2dc45c893631346faf0c93366b70b07(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_04553fcf88c43fb8aadb6eb006c585a8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 64, 256, 512], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a9f563c62a91878b553c08e2cc15b4df(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 20, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f6f835efd8494d188b7d808e1bf6ed16(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 106, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[106], dtype='float32'),
            paddle.static.InputSpec(shape=[106], dtype='float32'),
            paddle.static.InputSpec(shape=[106], dtype='float32'),
            paddle.static.InputSpec(shape=[106], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4a21513bcc72ad90df1926e60f25981d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_31ca4dbd630951b82d9b5bd88e830717(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_903d1293b7e98fd85bcbb07e234cacbf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1152, 6, 6], dtype='float16'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d20b18026bf4113dfbc298bef48cc387(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 40, 40], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_476fda35bc4beeae5de4624189dfb962(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 32, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8e4a97bc06557d54f21f41f06e3995ea(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c99838ebd7cb34868e5934a9df014868(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3c644406c3cf29f49b29643c4b71b219(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5f67d1a33739513a1ccf71884ffd49a9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 28, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a37223a4fd84d051cac78ea33cd6f506(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5e07108a5fd9f5f4cdc70500da5a1d42(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 8, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b9f67ea0ac53699cff422db5b35f4e7f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 64, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_659275afe5a1f942293ad5b944b482d6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 160, 160], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_70478f5bc84b794c41b5b155e2f71fa1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_263b59190fe88b474344cef4ce31e75c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 56, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_aa2f69b8d2bb435db583b9490adcbad5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_41bdd673393c791b8d2e8d2405883fa4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 336, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_af0f41bdd5b07e59f95fbca1a3f3f7c7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 70, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_480a729c402cc1c9b50f2fa16dc802a3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_534bd8605a1bc9db4c21abd12de320b4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 4, 12], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_aca3f1beaff9d735bbb1a39a91103cc0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 60, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1f3c8740f8eb41d61ae8d759be2f100d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 19, 19], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1c27edec4a7eb3e614a35b687d2cc820(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_70aba72961631a21282cfb34425d9745(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 720, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e1010f71f59d34d315de368d1204f260(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 480, 2, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6e3f86add704cec9080165103b6b20f7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 480, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3b849076f67f776bce6a44e2f283a361(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 16, 512, 1024], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e2e08385fc1ffddf21545f9dd55ed8b5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cf01bb348290461e0b99bb3ee855ed13(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 49], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_424f752909aaaad1bfdd43dd820623a9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2048, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_be7a980d242db80da503d468c7b6bc00(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 8, 6], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c5860002cd4776fbf03869697a41cb9a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_419833893c17c3b5d1c376f82c06ab01(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 4, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6d9296fab21040021a8756f2db62936e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, 48, 48], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_40567c7d44448ac41e165254a7ef9497(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 96, 96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b460480ca04be6d2344634c87163ebc1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 40, 40], dtype='float16'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_88907b574d986afac9f3889d9e2f029f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_00b7f414cd5d1873df88c7b75f6f611f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 16, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3b2dcaad09e2aad7cfc7870b395b86b4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 2, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_842b6ae9eb48422206b3058acc979f12(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 152, 152], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2ae2ff6147168e6bdf1ebed988f5fba8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 40, 40], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ff9a641fa2df9e7c95e9352624577d70(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_68d9ca62f121598cf5fce845fe4e3830(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 224, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[224], dtype='float32'),
            paddle.static.InputSpec(shape=[224], dtype='float32'),
            paddle.static.InputSpec(shape=[224], dtype='float32'),
            paddle.static.InputSpec(shape=[224], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b6eae07d54d9ef5dea95b59f6544ff89(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 480, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_46d4a4697665df3cdd855c4e6229b962(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 8, 25], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bf44cd9e9ae104c61b72ccf5ce9bf251(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_eba84ae045edc580a5322d6110ee9b6c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 75, 75], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_354d921a3759c2556cc892a56f9f4f21(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 224, 224], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_264a1ceeb5ec3adbfe3fddce6195166a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_73abd023f1b54ec5f4d5e00139e06a6e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, 8, 6], dtype='float16'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3eba81a3bc8b921c66b1715a61894576(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f0e9367ac584b9476f9ca96ce73f7233(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 4, 32, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1bd086ea33b1b0f960e73d3baae21f66(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 56, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_876d3325e90e8844fa413da7c1376370(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 80, 80], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3163f9128c785d514dbf57aba22c95fd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bd66dcd297483cb71457afafd752e6f1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 10, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[10], dtype='float32'),
            paddle.static.InputSpec(shape=[10], dtype='float32'),
            paddle.static.InputSpec(shape=[10], dtype='float32'),
            paddle.static.InputSpec(shape=[10], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_15ee5177d1b96df05331b81cd2a8b8d0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3473e493d0e249ac682f4c69cbbe0a25(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 52, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[52], dtype='float32'),
            paddle.static.InputSpec(shape=[52], dtype='float32'),
            paddle.static.InputSpec(shape=[52], dtype='float32'),
            paddle.static.InputSpec(shape=[52], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ba50b5cc13932f74cac90d833003cee7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 56, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f2b51560d87b84e91350c0e6b6f086d2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 4, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_238f535c78e73773dc86e6d27b960b9c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, 8, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1e9860a676272a81feaf9cf26dc4e4db(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 112, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f6e78da54ca3a40268fa01c537e98ffc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6f8d2811b8281ef9cc6d7e408dbcf600(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1536, 10, 10], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0924953f854a529bce7b5983b306f26a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_43c844305a33618763210ee186f27102(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-06'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 512, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_eefcbc750dc9b0e65d40ff7907f1312b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 16, None], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_51bab3307d196b0917f4c943a4dae42e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2423ebd1101a36394ab1b611cdd61dc9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 4, 64, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_206d8378e199fbf2586f83ad79a8b719(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 82, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ed693f230fbb7e9cdd184fea7edf650d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_37e9aada9535d6dfb765f3e078755a94(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_53dd2771c402a2715696773cf64d4723(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_db3075fe5be00cbb4c1ed9b2bee0f102(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 61, 61], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_59325918902e52b53ad2792238317f38(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 112, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2c4c3cc386cf13265b1440a9178fc404(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 8, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4313045e50e143d0b04f4beab2d1ffe6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, 17, 17], dtype='float16'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f0e3c2be8a5f1a675a0f7eb63f584142(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 20, 20], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e161b0e414bcce57157ab486158e34a5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, 32, 64, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4011c567561a3bf8cf22c9e03146685a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 52, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[52], dtype='float32'),
            paddle.static.InputSpec(shape=[52], dtype='float32'),
            paddle.static.InputSpec(shape=[52], dtype='float32'),
            paddle.static.InputSpec(shape=[52], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_809a14e7883394d5783c5ecea2723f94(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_307ff8dfcbd3f3dc711b97745f106617(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 64, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f924a2e0c96ef51562722729350c13a3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 8, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e790ff09b57e113e0ae53401cbebfed7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1152, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7757654bfe9b9d46a4cd48c8a31d3306(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 36, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_65710063276558e901a5a7cc7b2c59de(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 576, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_092cc30aab8f74baa2505d9006fd7209(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 4, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_814f4928eb7842b35504dbba4522c3f2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 128, 128], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ecc600a71fc16850cf7a760f447c83f3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 40, 40], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ab7a00ca20552836652977003ef332fe(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b8f3f640971c1628767991bd97777cc4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[1], dtype='float32'),
            paddle.static.InputSpec(shape=[1], dtype='float32'),
            paddle.static.InputSpec(shape=[1], dtype='float32'),
            paddle.static.InputSpec(shape=[1], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d60be19e367192a55056eb8a81936ab8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 768, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_01f1d79d398e2cfc61a4d9da75324daf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 336, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b4b14c7ab1450b3a7d537c3eba9ce1d5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 104, 4, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6fd75af07736e9d01ce333ebaf269a6d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f6d9f98a7283b104ec057a546d2c2fe1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 19, 19], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_25903a5acdd264202599b2e994bbd6cc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d7b7f624e1be1fb0d2bd47d575068cac(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 4, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a4d9198f3e3167489c2e0fb8793391b3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 576, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2b3dcda2a21fd69aef4c26fca2eaccad(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 104, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7af1867cf062e8e55bd074c997d4b02c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 4, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dea80a9adc2725dd7126b65cfeff8f4a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 480, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_de5634a75203069b0dbf3fbeeff796df(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 128, 128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_90ba953ddfc60bc98ceb008c571446d0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_99be4f5ea99d94116b6830a47a351b9e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 20, 20], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3fb28a18176d8047772168f049a5f096(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5fc991c75e15990436c661ad4f31bc2c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 76, 76], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_04f7d080fabba449b0b04ad4c7ae09a7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 768, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bdad8bf219e6ee27eb339ed3428412a6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, True), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1be5aad69366d175e7b219fd7a176437(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 4, 12], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6cd78aa28625ecef49649dd900f1df0a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 720, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a0459a49b41db6a39edb87eaa20aa89c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 176, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[176], dtype='float32'),
            paddle.static.InputSpec(shape=[176], dtype='float32'),
            paddle.static.InputSpec(shape=[176], dtype='float32'),
            paddle.static.InputSpec(shape=[176], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dbba9c6fdc045c77d11e5ce051888a27(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 32, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7bd3f8b6a0fa274a9e326618eee146d4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 160, 160], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2eae31498e57b686bccc36b149fb23bc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 56, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_782b1acaa87910c071667b87c8ca249c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_91d3aedc21153cdc01dfdc591767df6d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1901bf125bb1a77e4198b2c97ca6df6b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a9301b445d7a6ea9af96c2efe8af0364(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f38ab147038e4d8fcfdba47ca449d86c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 480, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ad543cd8907ef994e0ba07c7d7732da5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 2, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0003daa770852f0bc16a1add0a2c4e28(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3a69be4e44c5f356df59147f61e8e82e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 4, 128, 128], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_458ff5cc93c2853052d9f3baad6cac72(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1664, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1664], dtype='float32'),
            paddle.static.InputSpec(shape=[1664], dtype='float32'),
            paddle.static.InputSpec(shape=[1664], dtype='float32'),
            paddle.static.InputSpec(shape=[1664], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bc6609182c1039eafc255f808c90bb0a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 112, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cc105e3dcf83ddef145c899763feb1e9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c1772c848ed84edbc3c4009987e7f26a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1680, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1680], dtype='float32'),
            paddle.static.InputSpec(shape=[1680], dtype='float32'),
            paddle.static.InputSpec(shape=[1680], dtype='float32'),
            paddle.static.InputSpec(shape=[1680], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_935880d6a6dcada2471b24b5bbf2820c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 10, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[10], dtype='float32'),
            paddle.static.InputSpec(shape=[10], dtype='float32'),
            paddle.static.InputSpec(shape=[10], dtype='float32'),
            paddle.static.InputSpec(shape=[10], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a26556ca24a31378aa5d2b7a3043ef68(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 80, 80], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3cb3ebb4edf0758fe4ae9d28bb2db0fb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bc4581fc47f74a192165f23667354af1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_21c434ef269a633bfadd6adaf8a4a98d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_86243190853738b96a63e9b23bce77ba(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 32, 8, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f445cb80822c337d55dac39a368586d8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 672, 12, 12], dtype='float16'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c7b2e018e3bc863a28446ac27d7cdf15(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 416, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[416], dtype='float32'),
            paddle.static.InputSpec(shape=[416], dtype='float32'),
            paddle.static.InputSpec(shape=[416], dtype='float32'),
            paddle.static.InputSpec(shape=[416], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_acd1fa80420f1a6d3d273063bc1956d7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 36, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4e1aa685230bf4542d74141d26c0a6c8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 120, 4, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c0cfa8e556c17c27752ea924bef83b51(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 2, 2], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8cc70e99c5eb0598dfe49f580581078e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 256, 32, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9d82ba9551361ce0e94ae8e857ddafeb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 8, 25], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b182fcdc516e36875fcde1c5e04c6870(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 4, 3], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e60b25b6ed143be25e842d03da8550fd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 304, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[304], dtype='float32'),
            paddle.static.InputSpec(shape=[304], dtype='float32'),
            paddle.static.InputSpec(shape=[304], dtype='float32'),
            paddle.static.InputSpec(shape=[304], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e73dbcb3579ad48eae5a945696942094(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_25b4477b7535e9e0765139032b8778a7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 128, 175, 25], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_14c847de69b40a056a624183fdd9b27f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 72, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_58210f89e6aa313e3e1f81e847ac0802(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 28, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_48202d5ecfe341ee2cc8a38321d39991(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_df260ae3a5b7b61ca1667b85607c9a67(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 480, 12, 12], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_346cb3eb7a1be8bbb2058808f9fe9edf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 4, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fafb45dd41fc17f3a0094974515819a2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 32, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7b169e5997ed90c117631737166d16c2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3d69c22bbe12a85cc0b2687fa479e150(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 75, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[75], dtype='float32'),
            paddle.static.InputSpec(shape=[75], dtype='float32'),
            paddle.static.InputSpec(shape=[75], dtype='float32'),
            paddle.static.InputSpec(shape=[75], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_056c5654ab2a0e7ab0fd6aaeeb9464ae(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 185, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[185], dtype='float32'),
            paddle.static.InputSpec(shape=[185], dtype='float32'),
            paddle.static.InputSpec(shape=[185], dtype='float32'),
            paddle.static.InputSpec(shape=[185], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_45e50a0de819dadc779430c6014fe33b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 288, 31, 31], dtype='float16'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8d462e81eee1dce8cb3768f466d00faf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1536, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_77f52497a6de9a22ebc3d5a809e41670(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 10, 10], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f235e37fe90c6c3445d82bf4fa701baa(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 256, 64, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7eb422b6e701c3e8483c93d320397fb9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bec13ebeff44409d3eef899327206ea0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_29630784f95f784bcc017bebd7e94415(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 608, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[608], dtype='float32'),
            paddle.static.InputSpec(shape=[608], dtype='float32'),
            paddle.static.InputSpec(shape=[608], dtype='float32'),
            paddle.static.InputSpec(shape=[608], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d0f61ef67fec7264cd1f3036a26ed6ea(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 20, 20], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_474d841a3d7fd81c6525c72f0f53949e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 76, 76], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4fe064b7d072a933fc3df6cde2593246(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 160, 160], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c12e0a77c8c7a291cb84244dc2e185b1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1776, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1776], dtype='float32'),
            paddle.static.InputSpec(shape=[1776], dtype='float32'),
            paddle.static.InputSpec(shape=[1776], dtype='float32'),
            paddle.static.InputSpec(shape=[1776], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_aaee242e34764c026caea234b9e37583(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 48, 160], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_03076558df529649942a6626db697bc8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1200, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1242730d4df1050dfbe00528d15112b9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_21ef9c97a89964f4ef1bc3e4fd9758f8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3f81715057e631b27e8769c1ecebc423(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 82, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ad0d3f301065e9667c07af51436d8926(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 320, 4, 3], dtype='float16'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f01e32f14cd76fc4e89468d4c13603c2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 16, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_80eaa0c7bfdbd2b1e435a1a302663c21(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 10, 10], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3039e15c3bff93f37bc753a08dedbe9e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 2, None], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_df574d33db59afb378e0feda4f1d4f51(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 576, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_37c9be2f2073a048ee4868212a97123c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 175, 25], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9ea90a6b5fb2e8b263179eabec9990ff(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 32, 32, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ea0e8d966d9ebe90e2e6cf8e9a0f999a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, True), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f96500f1380943bc67b1f56886e95fb3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 672, 6, 6], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_04d13aaf40278fc81288610b41085998(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_578658953af6c5334a305c1d756ab95f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 256, 88, 25], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0727011728786dc7406d169be3df53e4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 528, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[528], dtype='float32'),
            paddle.static.InputSpec(shape=[528], dtype='float32'),
            paddle.static.InputSpec(shape=[528], dtype='float32'),
            paddle.static.InputSpec(shape=[528], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b87e946dbb1b9643a1a946af8e4fc59e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 32, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_90d324a1f7a05e6c93e4c5157ae1ba91(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ee746a5623fda017a4125de9688f8805(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 10, 10], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3ca06fdf72c37490a4acc661f47fdc1c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 480, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_322b006503f083c81d7cbf2702b856ce(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 4, None], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e58ed3e90c06992381b31f7b32cf3514(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 8, None], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b96bb580d591d5c3b46ed9cd4fc63c4d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 24, 80], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ee6a11d23b5e90b971cddd7859cb3f18(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 350, 25], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1908cae05be05cf11f8e07dc0f2507d0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 56, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b34827b700b57c2f272776970c550fa0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 256, 64, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c085bbf49f874f3d448e2e9fa624208b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 32, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_96c65253665b360fd1a7b9c2f0fbf56f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_56d19fb8aa02332835fbe6c0ab8ab7f2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 432, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_13672e74ed748a972307260bf7e23292(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 152, 152], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fe2cbcb53d2ab5f6bbc138deb132c192(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_08a77721120b46e1693801de3678d379(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 117, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[117], dtype='float32'),
            paddle.static.InputSpec(shape=[117], dtype='float32'),
            paddle.static.InputSpec(shape=[117], dtype='float32'),
            paddle.static.InputSpec(shape=[117], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f6df65240ac8925a89299b808655ce54(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 16, None], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d7ef8646f6fce33d8636affb4ac762fb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 38, 38], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cdeac6b9a7d9d6d44b3055add64d33e3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 304, 304], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e68404a4033630df4d02cbe3c9aa4a92(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 480, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_13c08fa84333383f04cb6725995513f2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_99af99761d0b8f2f67a9a4bce2b0398d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 40, 40], dtype='float16'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_462e07d96819e4c7122493866f197baa(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1536, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e0ef5e6f2b6f232144c21a5064e862d1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 320, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_deaa4bea46982ee0218f6e86772f7fed(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 160, 160], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f8c2b56849a92c7300392b789d801624(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_52fb4c7709facf72bb75957b3bc13cea(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 52, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[52], dtype='float32'),
            paddle.static.InputSpec(shape=[52], dtype='float32'),
            paddle.static.InputSpec(shape=[52], dtype='float32'),
            paddle.static.InputSpec(shape=[52], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5cf4bac0db298058a7de96a85d35606e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 168, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[168], dtype='float32'),
            paddle.static.InputSpec(shape=[168], dtype='float32'),
            paddle.static.InputSpec(shape=[168], dtype='float32'),
            paddle.static.InputSpec(shape=[168], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4fd88af2ada1b12474fa31f895c5a800(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_eea460abdf980da44cc6e72122d0288b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 768, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_75b1be54a495016f7a339c8c158ba5c3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7ebeea026c6e9dc157fb83b21e461b0b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6547322fd2cf406a2d8f4d79f0d12efa(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 128, 64, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c4d19e8d910b39d7d8899b665bb1f6a7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 702, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[702], dtype='float32'),
            paddle.static.InputSpec(shape=[702], dtype='float32'),
            paddle.static.InputSpec(shape=[702], dtype='float32'),
            paddle.static.InputSpec(shape=[702], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ae5b13e0e939198ab229111bdbeea01b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 304, 304], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bafe13e1cc66d01a31631a32bf01915c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a918669557d320cf0b2c025803eb766b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, 100], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0f48c60a5de0a31df82507672ae23681(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 112, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c9b3f21cd8e4297e6ac1d8630524b493(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2048, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8676f53cab2101556a3aa705d3f615f5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 64, 350, 25], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e14c44bb7fb8421d2f6ff9de06993154(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 480, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2e6e13cef89b90578915c4c6583801aa(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_92c7cf44ca9e699763bcfc4145325e45(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 32, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_73ad91d23619060c1394e490a731523b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 26], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2e9afbd276ea8658a969c5b6afad74cc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 672, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_def4008dc88fc8e348ea94877e806f89(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2048, 20, 20], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a571297f364fbba0735c6f470addef2a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c48808d6c4a1774361a66dc6663beaaa(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 4, 32, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3cc17f3071d80ab3eb49c5addce341bf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 50, 350], dtype='float16'),
            paddle.static.InputSpec(shape=[50], dtype='float32'),
            paddle.static.InputSpec(shape=[50], dtype='float32'),
            paddle.static.InputSpec(shape=[50], dtype='float32'),
            paddle.static.InputSpec(shape=[50], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_75f5ed0c9a39ff313160be27327a8985(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 224, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[224], dtype='float32'),
            paddle.static.InputSpec(shape=[224], dtype='float32'),
            paddle.static.InputSpec(shape=[224], dtype='float32'),
            paddle.static.InputSpec(shape=[224], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9f7e315e1209de59d7c3d8832718ef93(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 32, 64, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f546035d8aac286bca7b3ebe029dd18e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 26, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[26], dtype='float32'),
            paddle.static.InputSpec(shape=[26], dtype='float32'),
            paddle.static.InputSpec(shape=[26], dtype='float32'),
            paddle.static.InputSpec(shape=[26], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9ab99cbf2437e4d5fd27a05ac0ad88a9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 636, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[636], dtype='float32'),
            paddle.static.InputSpec(shape=[636], dtype='float32'),
            paddle.static.InputSpec(shape=[636], dtype='float32'),
            paddle.static.InputSpec(shape=[636], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_11d9df73c6718586e31773eefe4f9f88(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ddf56b24aa755252d82b107dabd2bb60(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 10, 10], dtype='float16'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f350b4c2e9ee3864e1625b6b59db562b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 64, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_07277bba8546a096d2aa0bb67f8b406d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 132, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[132], dtype='float32'),
            paddle.static.InputSpec(shape=[132], dtype='float32'),
            paddle.static.InputSpec(shape=[132], dtype='float32'),
            paddle.static.InputSpec(shape=[132], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_068236b963f20ae3772d95db2836bcc6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bb603ef7e789ce3a3c734fe1a71e5586(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 16, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_98f6af822451e6d8c0c9ff8cce770453(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fef8fe758bca37ef2d38ea6431493539(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 118, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[118], dtype='float32'),
            paddle.static.InputSpec(shape=[118], dtype='float32'),
            paddle.static.InputSpec(shape=[118], dtype='float32'),
            paddle.static.InputSpec(shape=[118], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b9c38b4e2b00f747c31761d68272255f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, 32, 64, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_80088b0ee55d4d921d63acab21fdb431(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1952, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1952], dtype='float32'),
            paddle.static.InputSpec(shape=[1952], dtype='float32'),
            paddle.static.InputSpec(shape=[1952], dtype='float32'),
            paddle.static.InputSpec(shape=[1952], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_506dcb579ce2bd0471091f30183046ce(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 160, 160], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_92c0ba69f1d3da47805e4633b3a97d29(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 112, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a0a46aa0a89a0eb975d77352fd88c9ed(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 60, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_79abae8720848df294fc716c787f83ee(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 128, 128], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3d0dedb55bd15e3225ad133db883e64e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, None], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6e3ca2d981870e25e3d4d5e6d1a4bf5e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_32afe5eddedac1a313481c9673ebea20(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5b297cbdbc20fd574274afcfdf60015f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 8, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9d9a59cc1149d5482d170ab28e98dede(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-06'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 512, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d746ac404df2cb95d7525b307d9e10ec(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b54f59a7929886bfe02b395062ae82a0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 8, 25], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f7cfab02d7eee8ea96de3e3e634ec8cf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 672, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_90033cb242d6a196769e55d5671fa6a7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 80, 80], dtype='float16'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6da9a98efad0ff7ae24f0aae4d7d96b4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1392, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1392], dtype='float32'),
            paddle.static.InputSpec(shape=[1392], dtype='float32'),
            paddle.static.InputSpec(shape=[1392], dtype='float32'),
            paddle.static.InputSpec(shape=[1392], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_378be52d1342297d3b33c6f3d55b3bee(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1152, 6, 6], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2f3de459085cd99a8d7aa22a10da2843(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 504, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[504], dtype='float32'),
            paddle.static.InputSpec(shape=[504], dtype='float32'),
            paddle.static.InputSpec(shape=[504], dtype='float32'),
            paddle.static.InputSpec(shape=[504], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_31ac0e1776ae9a4a2dc523abd9dcaa95(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 36, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3e2f62cc2e8f268471a9d1db21d2dad5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 32, 64, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_eb54a6198d0d098b5df559ad16cf3d3e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1872, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1872], dtype='float32'),
            paddle.static.InputSpec(shape=[1872], dtype='float32'),
            paddle.static.InputSpec(shape=[1872], dtype='float32'),
            paddle.static.InputSpec(shape=[1872], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1e650a105736cfce972de59b79c9de2e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 162, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[162], dtype='float32'),
            paddle.static.InputSpec(shape=[162], dtype='float32'),
            paddle.static.InputSpec(shape=[162], dtype='float32'),
            paddle.static.InputSpec(shape=[162], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_01be06e6f549cb4256fd5fbcd5391ac1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2048, 4, 8, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f636fa30a0565d79092b2b22dd6d384f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e14df009148bd5c0fa59a070b1ebf16e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 160, 160], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_842b58828ce8b0b3fea279b71a56f3f1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cfb0a2c14de50deca6e43948761b253b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 32, 24], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_25dec4d33c0318100e2218ef5aa8a01b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 1, 2], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2780c15474f5780d697bcab8c75e8283(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 184, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[184], dtype='float32'),
            paddle.static.InputSpec(shape=[184], dtype='float32'),
            paddle.static.InputSpec(shape=[184], dtype='float32'),
            paddle.static.InputSpec(shape=[184], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_08670a12885360769ac09c15fed4cd71(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 640, 8, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dc971ad76fd1055194fa611ddca13446(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 64, 256], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dfc6beecf20948b2d428320a8ff22420(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 10, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[10], dtype='float32'),
            paddle.static.InputSpec(shape=[10], dtype='float32'),
            paddle.static.InputSpec(shape=[10], dtype='float32'),
            paddle.static.InputSpec(shape=[10], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c22e309c5201e4b0852ec0fa242bab74(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 672, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d25e7427dd6d57504ddd98f54354eff3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 8, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c5f8394951e89737c107542c164c5fc4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 8, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4443cb96dab2b94b7b32f581252245ca(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 8, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3547c3c3d5ba9688e3cca445783999de(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 768, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7900aa0afd93247c2471f0ee5a44e050(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1104, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1104], dtype='float32'),
            paddle.static.InputSpec(shape=[1104], dtype='float32'),
            paddle.static.InputSpec(shape=[1104], dtype='float32'),
            paddle.static.InputSpec(shape=[1104], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6673197adf120ee098622608c376aa51(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 80, 80], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d5c09a718bde4f4f9b2728061f047e4a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f0d93ff64af4d85361149e262f90dc83(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, 24], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5f9d1da8241b988564b20c5f52acfb12(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 128, 128], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dc77783f4f885beba79336191518261a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 64, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0483512e4e012119dfa48bef933ef5bf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_20ebcae51176c55d4c98ad38f10d90a9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1152, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b606cf513c8f64479a9b2e2cbb83813c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1b872f8572aae335a7c38cb1cc0c210f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 640, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1718dfd92c51788ed97747a2f0e216bb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dc07229b50b6f866a2bf90a74fd7609d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 27, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[27], dtype='float32'),
            paddle.static.InputSpec(shape=[27], dtype='float32'),
            paddle.static.InputSpec(shape=[27], dtype='float32'),
            paddle.static.InputSpec(shape=[27], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_72b5909584deb756eebecd96ec2248ba(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 64, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_04c277ea268a9056176872ab5d1a5d4a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 19, 19], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9b2c2d5669307fc6f4b29b51d01f95fd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 128, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7281c73301669007031a5922202d9dc6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 4, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8861c9bac0c52fa7c8171476d14c50da(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ef0fffaeee2b34a7f8447f0125ee61ab(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_24cb734c54a26d6072a990b24082626f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2413fec6a4581a7335100a8ec156542a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 132, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[132], dtype='float32'),
            paddle.static.InputSpec(shape=[132], dtype='float32'),
            paddle.static.InputSpec(shape=[132], dtype='float32'),
            paddle.static.InputSpec(shape=[132], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_275d357f63fd1ce0ac4dca3ad5d569df(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 410, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[410], dtype='float32'),
            paddle.static.InputSpec(shape=[410], dtype='float32'),
            paddle.static.InputSpec(shape=[410], dtype='float32'),
            paddle.static.InputSpec(shape=[410], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_22236779444c94f02ab1a29f69c66e9a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 576, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_038d2626f2f1364e422ac477d1de94c8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 128, 350, 25], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d611060c19d2b5cb27db87cc3b170ddf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 120, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7dc14e15b0cf7a31536b29ecaebbc806(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 6, 6], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e12387775509b6aa385a0c26a8955671(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_92831198afc76b390f9884e9aaa79e85(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1920, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1920], dtype='float32'),
            paddle.static.InputSpec(shape=[1920], dtype='float32'),
            paddle.static.InputSpec(shape=[1920], dtype='float32'),
            paddle.static.InputSpec(shape=[1920], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1e09db09cad87ecfbedcedd6cddcd4d3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 75, 75], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a7f7776619c52925ca8dffec3fc3eba0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 128, 128], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_867f6b2e4842a55cae6550a09fcf98b1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 232, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[232], dtype='float32'),
            paddle.static.InputSpec(shape=[232], dtype='float32'),
            paddle.static.InputSpec(shape=[232], dtype='float32'),
            paddle.static.InputSpec(shape=[232], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_25a90f7243b0e1836ddf4f434eac60d0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cc5107c08852c64c8bad5e05415fdd87(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 2, 4], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4a3488f8258fad01c6ae4ec834ce7cce(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 92, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[92], dtype='float32'),
            paddle.static.InputSpec(shape=[92], dtype='float32'),
            paddle.static.InputSpec(shape=[92], dtype='float32'),
            paddle.static.InputSpec(shape=[92], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_07b63dcebc8a727bf977076bbf0b3bc0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 200, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f8b6f0326bb25d20671547dea2ea67b8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 16, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cc3eda264da511fb1672e23fe0d95a53(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 96, 96], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0acfc461b9347a995634a111ab651e07(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2560, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[2560], dtype='float32'),
            paddle.static.InputSpec(shape=[2560], dtype='float32'),
            paddle.static.InputSpec(shape=[2560], dtype='float32'),
            paddle.static.InputSpec(shape=[2560], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e6bbce1dac03ffdcfed09c99e5d56cb5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1312, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1312], dtype='float32'),
            paddle.static.InputSpec(shape=[1312], dtype='float32'),
            paddle.static.InputSpec(shape=[1312], dtype='float32'),
            paddle.static.InputSpec(shape=[1312], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_742a892484ff8f554a435063c08680e4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 7, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_08b46924600e40a736dc15f1f0c34c6d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 4, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ca44f8eeb684e8da550a314b2e180d9f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f268700c4da013d6a83d4a923b88f716(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-06'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 128, 64, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2529de372a4867bc4899bd15cd785d5d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 20, 20], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4cec5627b665aa86e36d6bb0743d780c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 720, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5f89ec4b18ab354fc7c6cf31b335ec0d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 35, 35], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5fa00bf5118cba4b4368cd3a35cf721a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 2, 4], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1d4c603928c952a4be367cbf82cfad9d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384], dtype='float16'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9a851f4c322cbd327bf4b5a9c7d23882(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 16, 128, 256], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5e3f12584c8ba623b5a6686df679b43a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 95, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[95], dtype='float32'),
            paddle.static.InputSpec(shape=[95], dtype='float32'),
            paddle.static.InputSpec(shape=[95], dtype='float32'),
            paddle.static.InputSpec(shape=[95], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d90e79439c86f700f2883ab09af3ca8b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_54e8bf0baf56a0e079ef4d3c0a37f1bf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6a135dd1b999265b811e0cba22b585f9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 88, 25], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0657ef22db0d18caaee4c49eefa3e86c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f93a4f741072d1bd0bddce02bc3e8f18(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2688, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[2688], dtype='float32'),
            paddle.static.InputSpec(shape=[2688], dtype='float32'),
            paddle.static.InputSpec(shape=[2688], dtype='float32'),
            paddle.static.InputSpec(shape=[2688], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3066684247f0145c25f963b02c805430(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 40, 40], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a65a037fa38df36bc9206503c5bc49ed(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 64, 256, 256], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_90e8782fd11c47935d861c75e6382c54(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9d30283f0e3a737119e83c43c06fb560(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c3d447776905d934c8118eef09940f2f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 100, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[100], dtype='float32'),
            paddle.static.InputSpec(shape=[100], dtype='float32'),
            paddle.static.InputSpec(shape=[100], dtype='float32'),
            paddle.static.InputSpec(shape=[100], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1f9f28d79bb8fc25e95db546ff0ad50b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 200, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7ceff3051ce8ffe6a3af6962933f1156(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 24, 24], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6d6ef9509cacb40b8f843af35ad1060a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0dabc235b01a7023f14e74ac18c8cdbc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 350, 25], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2f6636f0103abee5641ac26ee4dd8bcc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 544, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[544], dtype='float32'),
            paddle.static.InputSpec(shape=[544], dtype='float32'),
            paddle.static.InputSpec(shape=[544], dtype='float32'),
            paddle.static.InputSpec(shape=[544], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3b3f6ba1cde7cd5208a94f870259742a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 53, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[53], dtype='float32'),
            paddle.static.InputSpec(shape=[53], dtype='float32'),
            paddle.static.InputSpec(shape=[53], dtype='float32'),
            paddle.static.InputSpec(shape=[53], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_22d7cb24c63d8d8d9eb4cb0b9a2aa78d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 816, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[816], dtype='float32'),
            paddle.static.InputSpec(shape=[816], dtype='float32'),
            paddle.static.InputSpec(shape=[816], dtype='float32'),
            paddle.static.InputSpec(shape=[816], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e61071eff7204eee14630444e919506c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c6baed37cb404a4869b3e13de7f6e733(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 736, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[736], dtype='float32'),
            paddle.static.InputSpec(shape=[736], dtype='float32'),
            paddle.static.InputSpec(shape=[736], dtype='float32'),
            paddle.static.InputSpec(shape=[736], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_680b0435e1e42580f10b70824ad2c173(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 32, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bfb83320a839d9c940a6fea84319cafc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 636, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[636], dtype='float32'),
            paddle.static.InputSpec(shape=[636], dtype='float32'),
            paddle.static.InputSpec(shape=[636], dtype='float32'),
            paddle.static.InputSpec(shape=[636], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cd3b91bc227b5b65664dfc2688a3b672(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 32, 32, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4805c3dc30ed25876b62bc7c6d061d59(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 832, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bb18f953b4e05538bdade2c7611d012d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 768, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_459f781711e3a8ec07a566bed8fc9870(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 38, 38], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_74aeeefd61bfad3687975db378e50ec8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 42, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[42], dtype='float32'),
            paddle.static.InputSpec(shape=[42], dtype='float32'),
            paddle.static.InputSpec(shape=[42], dtype='float32'),
            paddle.static.InputSpec(shape=[42], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_68d42fd40adb5d3b982284414f88d27f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 64, 128, 256], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_96a1b2b0a9e248c70d83dee2efbe4820(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 4, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7efaa2655c87330018e1ae7b3862dde2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 2, 27], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_20c4c08b1310eb3f64a0bced48ef615d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fd5b2180e31ff9f1858fb00966299ec3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 336, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_74f7178073080c772e3857dd3fff3ea6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, True), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e78aae5a583b14bbbcf1ce11fd4a7365(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_26858e4bcc071dbb21f5b46f3b0410fd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, 24, 24], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b755acb08dfb84146f7673f238a48da2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 12, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_20e3e436c0a2a41e09dca057c88978d3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 320, 6, 6], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7a41d4c7f9022be3baa88964b959dfb3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 224, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[224], dtype='float32'),
            paddle.static.InputSpec(shape=[224], dtype='float32'),
            paddle.static.InputSpec(shape=[224], dtype='float32'),
            paddle.static.InputSpec(shape=[224], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4f79fd4b24bb5012a8cdfb4b3210f826(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_82424129111531e256cacea5e01fc53a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 300, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[300], dtype='float32'),
            paddle.static.InputSpec(shape=[300], dtype='float32'),
            paddle.static.InputSpec(shape=[300], dtype='float32'),
            paddle.static.InputSpec(shape=[300], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7e952e52418ffc8a014ade38876a7a85(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 352, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[352], dtype='float32'),
            paddle.static.InputSpec(shape=[352], dtype='float32'),
            paddle.static.InputSpec(shape=[352], dtype='float32'),
            paddle.static.InputSpec(shape=[352], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d4c980e711c4e8864589d6bba9225820(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a5510d82817ba9fa3db54559e25e015f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 120, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_12e434c0aba4bdedd142af1d54405487(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 6, 6], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4d4fb26491e469e694653a760f51e107(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 480, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c879500ef43c9583e591cf88058373fe(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_46b849d24e80432a68cb996293a9a1cf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 4, 64, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_89fc545d308c9959a47926c571a761f1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4998577f2715fb1654cdd7b9ad85a960(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 32, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a35dfd75f4a39aec9d44b367a36edb83(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 128, 128], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9bab6ffa8f76d6e5d8dd3c9144454c5c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 288, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_87f7d7b8718088814721bc24351b1ad7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 138, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[138], dtype='float32'),
            paddle.static.InputSpec(shape=[138], dtype='float32'),
            paddle.static.InputSpec(shape=[138], dtype='float32'),
            paddle.static.InputSpec(shape=[138], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6f601b8e703e0031cc4699e666abcfeb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 8, None], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5881edd31c99b5b3ac47d160a72576b5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 448, 8, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[448], dtype='float32'),
            paddle.static.InputSpec(shape=[448], dtype='float32'),
            paddle.static.InputSpec(shape=[448], dtype='float32'),
            paddle.static.InputSpec(shape=[448], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e84b9c88555246fdc19a330a2dc95ed7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 72, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_71b303d09dce0592b239a8d2f459e33c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 120, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8eb5dbf52e9523fb2b2f2fcfe9efaf89(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 10, 10], dtype='float16'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c2219a45e63d1019663f2e68962d68dc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 8, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a5874bd6cd476b15d682e91765085065(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 96, 96], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8a2a6ead45870594f2d0f3f9c27bc82c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2064, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[2064], dtype='float32'),
            paddle.static.InputSpec(shape=[2064], dtype='float32'),
            paddle.static.InputSpec(shape=[2064], dtype='float32'),
            paddle.static.InputSpec(shape=[2064], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_04e521ec6933d773533dce64e3d45124(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 36, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_467e0b54e8b3ce1fe94e1c2291ee10b1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_45d43e72ade43b488209dfb3f3e161b1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 2, 27], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c5edb2f18c8e47a8bf1b6d78f784956f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 150, 150], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_43cbbb0e3509424a4b9cc93860dc6aaf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2048, 10, 10], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_73b2de52b3a25edbe06de17c3b6ab65a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 4, None], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5144b12e6f28571ed44049cf5b11694a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 120, 31, 31], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5544828a7f181373d095ef45852af1a0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2112, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[2112], dtype='float32'),
            paddle.static.InputSpec(shape=[2112], dtype='float32'),
            paddle.static.InputSpec(shape=[2112], dtype='float32'),
            paddle.static.InputSpec(shape=[2112], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d9207fb6e42df320687e1abddc8cfd8e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 138, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[138], dtype='float32'),
            paddle.static.InputSpec(shape=[138], dtype='float32'),
            paddle.static.InputSpec(shape=[138], dtype='float32'),
            paddle.static.InputSpec(shape=[138], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a5af6cbfbe32b6178579798aa12e5117(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 768, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d2856a2bb276ab11c3e63dd5174de92f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1642fa8b2e044a5d3f3bf262d50d5508(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 8, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ceb994a879f8829a5efe1033d526d81f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 350, 25], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b24b502d328b553d9ef8b792d72b4b61(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 80, 80], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_75364c40faf4b75602b17cd756a2fca0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_379943e81a6a4c6bf2afc9dd9ce81850(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 768, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5b03cf60275a890056ef00c1c753d978(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 12, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_70e9da8c86903b09f59bb5b27e4c9293(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ce24ad847c2f00a65300d4a5d0606410(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 768], dtype='float16'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bd62a60ad014fc4d8d3bbdad0dc812e6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 62, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[62], dtype='float32'),
            paddle.static.InputSpec(shape=[62], dtype='float32'),
            paddle.static.InputSpec(shape=[62], dtype='float32'),
            paddle.static.InputSpec(shape=[62], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9d0fff58868f807bae16be1c5473db29(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_06d57437dde5bc9c97fe8a1ae10cbb06(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ea5dbc47a611c52732e4b437c6ca230c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[1], dtype='float32'),
            paddle.static.InputSpec(shape=[1], dtype='float32'),
            paddle.static.InputSpec(shape=[1], dtype='float32'),
            paddle.static.InputSpec(shape=[1], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d54dd1a553b1016b2cb9efe5deb82d10(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4d0b99242f00e944a18b9bafd47ea522(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 32, 24], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9819e83c80fad20383c50c93b943ba45(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a001f631eb3482fb4092b22194a111fd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1280, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_815a31a17cf07eefa6f7f921cee037bd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 256, 175, 25], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6868ec7b78224230b1c6b5ca3874bf9b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 1, 3], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bec0e815d3128b0780b7221166cacd9c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 20, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_24286b574aace75bf578db8ff07e0e99(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 31, 31], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_95c2a976b83a4958b1be6cfc2b6c3597(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 360, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[360], dtype='float32'),
            paddle.static.InputSpec(shape=[360], dtype='float32'),
            paddle.static.InputSpec(shape=[360], dtype='float32'),
            paddle.static.InputSpec(shape=[360], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2a07ea5061d7ad2a0c11974d825fb43b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 360, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[360], dtype='float32'),
            paddle.static.InputSpec(shape=[360], dtype='float32'),
            paddle.static.InputSpec(shape=[360], dtype='float32'),
            paddle.static.InputSpec(shape=[360], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_216919a91b32dc576df34acf88a4bbd2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 150, 150], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_84021c0cf0b138b0d1fa628478787faa(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7ac957503cb9abd3088c2dcf6c9489dc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0384eb41ea893946136c8462fa41d819(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 576, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_480b42b318e09534977c677347792d04(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 8, 25], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6ed2f9a7a3b3c0ddc363ed2cf9c5621f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 5, 5], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9d25370aca2ae679810e076223d2c19d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 4, 512, 1024], dtype='float16'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4139c4405ffe0867954eac812c78a849(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 30, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f6b9111cef8062043c1cb3d4a1b73a7f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 104, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_745afe881745275c03bbd782f2897adb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 608, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[608], dtype='float32'),
            paddle.static.InputSpec(shape=[608], dtype='float32'),
            paddle.static.InputSpec(shape=[608], dtype='float32'),
            paddle.static.InputSpec(shape=[608], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c7756dc33bcfe086f44d82256691cc50(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bbec8392257533d466fa69c0a1ee8f14(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 104, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3669a9be9c0c3f4eccfeccb2035ed7b6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 8, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_318c00cd40030242db3f705a93b2891a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1584, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1584], dtype='float32'),
            paddle.static.InputSpec(shape=[1584], dtype='float32'),
            paddle.static.InputSpec(shape=[1584], dtype='float32'),
            paddle.static.InputSpec(shape=[1584], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_361174189e20d55ed399f9d9a385b3e5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 117, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[117], dtype='float32'),
            paddle.static.InputSpec(shape=[117], dtype='float32'),
            paddle.static.InputSpec(shape=[117], dtype='float32'),
            paddle.static.InputSpec(shape=[117], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_163c4646273b5e8bce82af6b7496fc22(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1280], dtype='float16'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_aa301bbd5a1e416e0b921d7e268a8e4a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, True), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_224a201907eed15b1de5c8ae3bed5fc2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 20, 20], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_90cd01123450907824b1892589e818bb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 75, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[75], dtype='float32'),
            paddle.static.InputSpec(shape=[75], dtype='float32'),
            paddle.static.InputSpec(shape=[75], dtype='float32'),
            paddle.static.InputSpec(shape=[75], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_127ee73f1be2c7092762a34c121d6b79(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 12, 12], dtype='float16'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_09560c7f59586714659b8574fbdbfc34(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 256, 32, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fa2b8ac4c3930469a8943e54c818190c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 128, 128], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f2e4983afb6846aad56ac221ef11a43b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 16, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_714efb99a8ff6570f582a86d8c671fbc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 122, 122], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ca645b29629be37054ef146b655740b8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 87, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[87], dtype='float32'),
            paddle.static.InputSpec(shape=[87], dtype='float32'),
            paddle.static.InputSpec(shape=[87], dtype='float32'),
            paddle.static.InputSpec(shape=[87], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f19d743a92f26dc8cbc59531e4a073fd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 64, 128, 128], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2efda438036f5a8927b64a011fdf01af(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a245c093e969413a000f5b4cdefe6057(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 32, 100], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e2466faae82137046237da50a7dbcf3f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 12, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2af563fefd23155cf2ef7a456335678a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0b1ae707c8b4ab10db8aec9c3823350f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 320, 8, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7e47f00b89a83b58f2b40e9c647ac925(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, 16, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4213f15def29e76c2417266d316d747c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d0b1a0fda2fcb56df040381307a1519b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 32, 8, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_237e94dd03dbb3d597731d100450d5a0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 112, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_02825ebecc52b0450c7e5430b04e81b6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, 16, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_777ac0525f84696e6440d3ec6ec78be2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 640, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2db559d88ad496e3ddf832b3c8e53914(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_926de857b9640246d3a861bb96dc7238(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c4bbb9b8e17842bbd03aecaca5e7438b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1344, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1344], dtype='float32'),
            paddle.static.InputSpec(shape=[1344], dtype='float32'),
            paddle.static.InputSpec(shape=[1344], dtype='float32'),
            paddle.static.InputSpec(shape=[1344], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c9c78c6e93a10811a79f7a80805b6d47(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 162, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[162], dtype='float32'),
            paddle.static.InputSpec(shape=[162], dtype='float32'),
            paddle.static.InputSpec(shape=[162], dtype='float32'),
            paddle.static.InputSpec(shape=[162], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1c44d044048a874d2e962980a95e10bb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1856, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1856], dtype='float32'),
            paddle.static.InputSpec(shape=[1856], dtype='float32'),
            paddle.static.InputSpec(shape=[1856], dtype='float32'),
            paddle.static.InputSpec(shape=[1856], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b09ad1b67d807cc3d7f03b035f0d8c90(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1728, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1728], dtype='float32'),
            paddle.static.InputSpec(shape=[1728], dtype='float32'),
            paddle.static.InputSpec(shape=[1728], dtype='float32'),
            paddle.static.InputSpec(shape=[1728], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_423b983ffdfc7e65b383ccfeafd45b4c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 768, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a0aff5902af7fa4414fbcb1026214c4c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 2, 4], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1f76191763e5b0e9c94d9a20705e3d44(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 570, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[570], dtype='float32'),
            paddle.static.InputSpec(shape=[570], dtype='float32'),
            paddle.static.InputSpec(shape=[570], dtype='float32'),
            paddle.static.InputSpec(shape=[570], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_552489f69ad39a25d3ae7d7767b1907d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 4, 256, 512], dtype='float16'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_affec5a07fd82d74e9335cf58367bd07(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 70, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0651a63728a76e450adae5c8c420ebc3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 728, 38, 38], dtype='float32'),
            paddle.static.InputSpec(shape=[728], dtype='float32'),
            paddle.static.InputSpec(shape=[728], dtype='float32'),
            paddle.static.InputSpec(shape=[728], dtype='float32'),
            paddle.static.InputSpec(shape=[728], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4c58552bb28ad7d4dac823baed3c6698(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1856, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1856], dtype='float32'),
            paddle.static.InputSpec(shape=[1856], dtype='float32'),
            paddle.static.InputSpec(shape=[1856], dtype='float32'),
            paddle.static.InputSpec(shape=[1856], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_81915bd6f97fa7cc8ad5815845b097e9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 336, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5cb185b3b482fcf1e63a6baf58c67717(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1872, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1872], dtype='float32'),
            paddle.static.InputSpec(shape=[1872], dtype='float32'),
            paddle.static.InputSpec(shape=[1872], dtype='float32'),
            paddle.static.InputSpec(shape=[1872], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4ecd3e332c82c379847f781922767736(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 31, 31], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cc82439e4f1db73811bcc5c3210c2e04(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 200, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_467937c1b73b0a6445e3ed6e1e91f50b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cfd61e99ed9054a30920fda1d94f7475(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 120, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_899d17da3b944425f83c54ff28b71924(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 906, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[906], dtype='float32'),
            paddle.static.InputSpec(shape=[906], dtype='float32'),
            paddle.static.InputSpec(shape=[906], dtype='float32'),
            paddle.static.InputSpec(shape=[906], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2763dff4561e1c891db3c3a2068e1058(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, 8, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f4a3dfadd1b3aaddc906e16f8f00c76d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 228, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[228], dtype='float32'),
            paddle.static.InputSpec(shape=[228], dtype='float32'),
            paddle.static.InputSpec(shape=[228], dtype='float32'),
            paddle.static.InputSpec(shape=[228], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d99a1335c90ba60b4aafbcd7298b901a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 82, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2c016dd7817b554333a16cd34b1a9c4d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 88, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8e8f0133986ffb4135513dfd7b141efb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 44, 40, 40], dtype='float32'),
            paddle.static.InputSpec(shape=[44], dtype='float32'),
            paddle.static.InputSpec(shape=[44], dtype='float32'),
            paddle.static.InputSpec(shape=[44], dtype='float32'),
            paddle.static.InputSpec(shape=[44], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_eb7c00b666aecb66afa5d87f4384b4de(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 48, 160], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cdb498724826f2d1d9074560e678eafe(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 28, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_16fd19de33b48b46334b998f8ea7abd2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 8, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b33da5cfb82e732a25322a60aa9f34e6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 244, 244], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5c24ba61461480d4c6d3eb014c3fc86d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 8, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ad7841ec75393e1bda8df1d78b1bb062(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 31, 31], dtype='float16'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4eeff78ab9f44a29f6819ea991ca46eb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 128, 350, 25], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9f78f5cb4cf7428f7add9adec1b269a8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dcc0d2be3e3ead42d6e2ee26a70970cb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_42c7fa3df3c9ea87887339f90ff4d1e3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1056, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1056], dtype='float32'),
            paddle.static.InputSpec(shape=[1056], dtype='float32'),
            paddle.static.InputSpec(shape=[1056], dtype='float32'),
            paddle.static.InputSpec(shape=[1056], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f04b1f723198053247940073cf961599(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 32, 100], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cb4fd531d3a3e93a949a1ba0d55bd5d1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_589998bbd81988b3e1d94e3b20397c1b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 4, 8, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f8f872561986ee45e1ad7aed84c9510f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 320, 320], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8db2c8ea7e3b8c1d864304caad894b96(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 1, 26], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a09f14130ee5b9fa144a8e7689b473f5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 832, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7e32ab27fd7a674e8bdf0c4fc681e7e1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, True), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e9100befd923b9387a58d311c9f1d4af(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 2, 2], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dd5a4aad19df3645493d32f6bec951d4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 32, 8, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c7ec793f5fa22ce0ec1b09f6e26f303d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 320, 320], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_47ca2c4ca14cb788357420b8389cdcbd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 25, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[25], dtype='float32'),
            paddle.static.InputSpec(shape=[25], dtype='float32'),
            paddle.static.InputSpec(shape=[25], dtype='float32'),
            paddle.static.InputSpec(shape=[25], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_827369307d64dcfbca3d462bf795cce0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1872, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1872], dtype='float32'),
            paddle.static.InputSpec(shape=[1872], dtype='float32'),
            paddle.static.InputSpec(shape=[1872], dtype='float32'),
            paddle.static.InputSpec(shape=[1872], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_470c513e8f7f2702c7307c0ef6a7242c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 128, 128], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_54a5f985cd6257bfcb5e076f9e76e1e3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 256, 88, 25], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_44202e3624021085343f21e059d11524(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 92, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[92], dtype='float32'),
            paddle.static.InputSpec(shape=[92], dtype='float32'),
            paddle.static.InputSpec(shape=[92], dtype='float32'),
            paddle.static.InputSpec(shape=[92], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_84a078b7f89b849f5e6f8da37b96b34c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 162, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[162], dtype='float32'),
            paddle.static.InputSpec(shape=[162], dtype='float32'),
            paddle.static.InputSpec(shape=[162], dtype='float32'),
            paddle.static.InputSpec(shape=[162], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1624ac6e972a6b37a9132e27deaa15ee(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 768, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_af48b53061b6c74277cd331b33b43783(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 10, 10], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dbd9633630d82736e0e78f1238847b14(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1488, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1488], dtype='float32'),
            paddle.static.InputSpec(shape=[1488], dtype='float32'),
            paddle.static.InputSpec(shape=[1488], dtype='float32'),
            paddle.static.InputSpec(shape=[1488], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4f7322eae16189652b7ed5899e4a5aa0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2064, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[2064], dtype='float32'),
            paddle.static.InputSpec(shape=[2064], dtype='float32'),
            paddle.static.InputSpec(shape=[2064], dtype='float32'),
            paddle.static.InputSpec(shape=[2064], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_14436615a0f1d12a9ec45300b59a5a48(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 32, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_138aa3c869fc81b18cd947421698b73a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 320, 320], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dcb2f2f8150f2b965cf23f091bee3fcc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 151, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[151], dtype='float32'),
            paddle.static.InputSpec(shape=[151], dtype='float32'),
            paddle.static.InputSpec(shape=[151], dtype='float32'),
            paddle.static.InputSpec(shape=[151], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_84035fb2f3979b4ba0f62b4fa8c68bf3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1344, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1344], dtype='float32'),
            paddle.static.InputSpec(shape=[1344], dtype='float32'),
            paddle.static.InputSpec(shape=[1344], dtype='float32'),
            paddle.static.InputSpec(shape=[1344], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_171e444d3443a35c9e028717a8c7c8bb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 352, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[352], dtype='float32'),
            paddle.static.InputSpec(shape=[352], dtype='float32'),
            paddle.static.InputSpec(shape=[352], dtype='float32'),
            paddle.static.InputSpec(shape=[352], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cea65a08f23b566a04190ff1bc59e5e3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 16, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1ad5322a25dffead414186b218aab2b4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1536, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8289b332e10e28c5a84932d651624d7d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 118, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[118], dtype='float32'),
            paddle.static.InputSpec(shape=[118], dtype='float32'),
            paddle.static.InputSpec(shape=[118], dtype='float32'),
            paddle.static.InputSpec(shape=[118], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fc855164fbc439186f3dc281de66f87c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 160, 160], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c96ef0ab05eec9f0e73cc37e470d0097(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 20, 20], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_54af904919118db57b8a0801eea6f455(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2518c43ed17c92ba61515166afdb2e58(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 60, 20, 20], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c316a32894e7c28555759959a17c9869(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 128, 128], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a66c9ea1116f694a0225c128cd0ce6a3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 320, 320], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7f6dbe06aa383090672c873099fd6dfa(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_609ec7bf2a07fd26a1254d491f9e4e1e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 262, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[262], dtype='float32'),
            paddle.static.InputSpec(shape=[262], dtype='float32'),
            paddle.static.InputSpec(shape=[262], dtype='float32'),
            paddle.static.InputSpec(shape=[262], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fcc6dde260acc7188025c4c6de7f0c74(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 61, 61], dtype='float16'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0cab8e769743e9b14df2ada13681fbb1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 12, 40], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_95f3a13b0cc6136f39a3708c17c63e9c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 288, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ed4c9c36390ae7eb199866066fe8c806(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1216, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1216], dtype='float32'),
            paddle.static.InputSpec(shape=[1216], dtype='float32'),
            paddle.static.InputSpec(shape=[1216], dtype='float32'),
            paddle.static.InputSpec(shape=[1216], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8176ec319d7b31f697db792d91833999(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 352, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[352], dtype='float32'),
            paddle.static.InputSpec(shape=[352], dtype='float32'),
            paddle.static.InputSpec(shape=[352], dtype='float32'),
            paddle.static.InputSpec(shape=[352], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c4bfbc0e06310a32dd8ec11ccf8397dd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c69868eec40a5ad5dc2d6eb4fedf8513(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, True), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f490811295f67042d6aa6ee33d9fc946(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 64, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0d6acac54a26f9a7b8028cc698d0c0d7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 147, 147], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6872978f377bda77c1b49c5e0bf899f5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a3e21fe52e3c2c2a118074fc71e41be9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_75ee84f7236948cee9ff459d8bebe011(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1680, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1680], dtype='float32'),
            paddle.static.InputSpec(shape=[1680], dtype='float32'),
            paddle.static.InputSpec(shape=[1680], dtype='float32'),
            paddle.static.InputSpec(shape=[1680], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0c2f69248d988e152e721152baaefc6f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 320, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dd271ad96c296143ba2bfc64860ac86a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4f5421ea7cc936dcbad01422103e791b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 960, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6ae3234e602e64832c0c8da733fef782(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1280, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_efc5e703d69a415622d3df8531a072f6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 6, 6], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dd120fd62325ef88316b34cd1cfc1f64(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 50, 350], dtype='float32'),
            paddle.static.InputSpec(shape=[50], dtype='float32'),
            paddle.static.InputSpec(shape=[50], dtype='float32'),
            paddle.static.InputSpec(shape=[50], dtype='float32'),
            paddle.static.InputSpec(shape=[50], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cf4191e91099a34af466b305863c956e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_50ffa5edbcfdf476d9ac4dce58622b7b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 80, 80], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_695ade52943d861cbb70a061e1e9a339(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 512, 2, 2], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a5e52bc3285cb8166af50a2ee45803c4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 81, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[81], dtype='float32'),
            paddle.static.InputSpec(shape=[81], dtype='float32'),
            paddle.static.InputSpec(shape=[81], dtype='float32'),
            paddle.static.InputSpec(shape=[81], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bfc693cb64eafdcc84b0e80cbfb15276(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 24, 24], dtype='float16'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6acbf0634458293ad1d55909d3372c67(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, 100], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e0186a6004e5dbd78c27b1d461e918a9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 87, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[87], dtype='float32'),
            paddle.static.InputSpec(shape=[87], dtype='float32'),
            paddle.static.InputSpec(shape=[87], dtype='float32'),
            paddle.static.InputSpec(shape=[87], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_43c10356140810fa3f3a87f08f368419(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a70b972739753d83ff305a95acc22b14(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 2, 2], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5be7e6bac60987a818142ffaeaa572e7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 4, 12], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1893a07ad07eac7754dc602161ea858e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 76, 76], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a92fd16fd9e3b064b244d6b28a1a5996(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 1, 26], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a89884487612d4804b14f00b0ed71e98(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 800, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[800], dtype='float32'),
            paddle.static.InputSpec(shape=[800], dtype='float32'),
            paddle.static.InputSpec(shape=[800], dtype='float32'),
            paddle.static.InputSpec(shape=[800], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_117f8489baa6e501d70eb4bff85cf1d9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 4, 12], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8a82e1bc607f551a13054d8cd7b874b5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 28, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_877a4855048705141aeb14c3e0cbd641(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_75fe4f0c8a04a0ae02b307b58d9938d7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 50, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[50], dtype='float32'),
            paddle.static.InputSpec(shape=[50], dtype='float32'),
            paddle.static.InputSpec(shape=[50], dtype='float32'),
            paddle.static.InputSpec(shape=[50], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ba70d8ff83ade7d49521c28915f714e3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 20, 20], dtype='float16'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5361572d976f685ab7247dd0d524f3b5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 100, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[100], dtype='float32'),
            paddle.static.InputSpec(shape=[100], dtype='float32'),
            paddle.static.InputSpec(shape=[100], dtype='float32'),
            paddle.static.InputSpec(shape=[100], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cc775f221a7a29440383ed934d6f629d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 336, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2bb8c3500ffce4b5f421ae1a79c4e03f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 138, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[138], dtype='float32'),
            paddle.static.InputSpec(shape=[138], dtype='float32'),
            paddle.static.InputSpec(shape=[138], dtype='float32'),
            paddle.static.InputSpec(shape=[138], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ba9b51392051e1265ee4a8d918f6e698(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1056, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1056], dtype='float32'),
            paddle.static.InputSpec(shape=[1056], dtype='float32'),
            paddle.static.InputSpec(shape=[1056], dtype='float32'),
            paddle.static.InputSpec(shape=[1056], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_10a6e0f122747cdb1636ff08d1dba959(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 640, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_08e61d2bb0735c24358435bd74a7ffb5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 300, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[300], dtype='float32'),
            paddle.static.InputSpec(shape=[300], dtype='float32'),
            paddle.static.InputSpec(shape=[300], dtype='float32'),
            paddle.static.InputSpec(shape=[300], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ca3414dfe7bb7b85fd0ac3ef4a8fd707(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 72, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c35518f3334794ee130ef13ab20e2243(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 960, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_644964eb153e3db1922193903e2afb1f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 10, 10], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_78c4f2c3f90cca266e143cbe6eb5de63(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 840, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[840], dtype='float32'),
            paddle.static.InputSpec(shape=[840], dtype='float32'),
            paddle.static.InputSpec(shape=[840], dtype='float32'),
            paddle.static.InputSpec(shape=[840], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f010ac89e95313785804b533a6f28f66(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, 56, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0fc88152d11556dead22b51cf8347e8a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 82, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0d4ca7005d2aaa8cc3db49df521f1e39(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1968, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1968], dtype='float32'),
            paddle.static.InputSpec(shape=[1968], dtype='float32'),
            paddle.static.InputSpec(shape=[1968], dtype='float32'),
            paddle.static.InputSpec(shape=[1968], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f9f6ec79ccff23b64aeae5231be51af2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 304, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[304], dtype='float32'),
            paddle.static.InputSpec(shape=[304], dtype='float32'),
            paddle.static.InputSpec(shape=[304], dtype='float32'),
            paddle.static.InputSpec(shape=[304], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0b1de604e126b1fa09ae4b5eb369b696(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 16, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7b82cf83ff45043ef66c84f20c5b0beb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 26, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[26], dtype='float32'),
            paddle.static.InputSpec(shape=[26], dtype='float32'),
            paddle.static.InputSpec(shape=[26], dtype='float32'),
            paddle.static.InputSpec(shape=[26], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_963e64b1c5f9da6455d21443ca2e14b1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 72, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6b4849f1d1bd3486ed197dbbd329b78d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 320, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_67484094503db28bd97a3dc60c1ca3e2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 608, 608], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2c8dbe695d486f315b97329be5e529ba(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1632, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1632], dtype='float32'),
            paddle.static.InputSpec(shape=[1632], dtype='float32'),
            paddle.static.InputSpec(shape=[1632], dtype='float32'),
            paddle.static.InputSpec(shape=[1632], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c3c5069945d6e6e01fc969e96a2de2bb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fff5f20a5fd67f2fd593178094d7ab30(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 2, 27], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_20e7dca2d21b620ebcea6442292033d5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 4, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_07c8ef96c5099df647626e5400c6e7b9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dcbac1292344de20ab0353af24353e0d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 6, 6], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e751baaa77fd63c21bb116e6d6b3bb51(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 70, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0b8f44bc97a356e5c42c3042b7bd1505(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c112c4673f3887b2d0c4814080b7d845(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 32, 32, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ff40ca29a1d7f7c404a184f6433a19a0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1440, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1440], dtype='float32'),
            paddle.static.InputSpec(shape=[1440], dtype='float32'),
            paddle.static.InputSpec(shape=[1440], dtype='float32'),
            paddle.static.InputSpec(shape=[1440], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_df0a64ffe0bd41c2fc472b41150c17d5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 56, 4, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b6c7a69c80a82f51877a679621124dc0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, 48, 48], dtype='float16'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2b078752569198765df0efc82d9b357e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 84, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[84], dtype='float32'),
            paddle.static.InputSpec(shape=[84], dtype='float32'),
            paddle.static.InputSpec(shape=[84], dtype='float32'),
            paddle.static.InputSpec(shape=[84], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_40bdf43a585f465081df6be985c57b1c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 152, 152], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_41bee5f0c71d054ae209ee5ad94286ec(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 106, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[106], dtype='float32'),
            paddle.static.InputSpec(shape=[106], dtype='float32'),
            paddle.static.InputSpec(shape=[106], dtype='float32'),
            paddle.static.InputSpec(shape=[106], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ac9c506333fd23e58f317f5b39f87cf8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, True), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_752486afaea0903136dae08f821d2b70(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7fc0ea8eca85f35aafbd5d37c26a8733(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 432, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_86effc94cc6dd89c0a0bf56d23b38aab(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 44, 80, 80], dtype='float32'),
            paddle.static.InputSpec(shape=[44], dtype='float32'),
            paddle.static.InputSpec(shape=[44], dtype='float32'),
            paddle.static.InputSpec(shape=[44], dtype='float32'),
            paddle.static.InputSpec(shape=[44], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_57546f95726f7922fd067f54dd9b651f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 2, 27], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5ebf70de1703320a6f0fdaa614413bda(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 288, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_454d1a24aa37124b5dc12d58d14943d6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 128, 128], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b82eb8b23f613db89ec89a2c78416e3c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 48, 48], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_acd83c9a1d85a8d1ea6e14e7c158d54e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 88, 61, 61], dtype='float16'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_99b9674fb106caee579db82db9d6fed3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 150, 150], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_672dd7aa42f3a021e6d00bf92e9b1d76(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c0bdbfe9de0855b84834472a3b76ddfc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1000], dtype='float16'),
            paddle.static.InputSpec(shape=[1000], dtype='float32'),
            paddle.static.InputSpec(shape=[1000], dtype='float32'),
            paddle.static.InputSpec(shape=[1000], dtype='float32'),
            paddle.static.InputSpec(shape=[1000], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d0ce1bc02a3fa4134e621d76919466ca(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 320, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c03fa0a42d9b996e79b755dcd7c02fac(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1632, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1632], dtype='float32'),
            paddle.static.InputSpec(shape=[1632], dtype='float32'),
            paddle.static.InputSpec(shape=[1632], dtype='float32'),
            paddle.static.InputSpec(shape=[1632], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_13163815a12d509c4db92414f03c4a8e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f79d05a5e1260dd3828d3c4e9cb9c6e2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 24, 24], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_78c34eb38582027a3bfe5a17bdc8a49d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_019561d773e1092e6a64ee236ce8da12(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 304, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[304], dtype='float32'),
            paddle.static.InputSpec(shape=[304], dtype='float32'),
            paddle.static.InputSpec(shape=[304], dtype='float32'),
            paddle.static.InputSpec(shape=[304], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_672672e9aaa178cbcfa2285d1b93598e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 16, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_504102c0c322ef1243f795b3deabb28e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_35aac71abde1014925ad154485383613(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 640, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f01a7f175598123e3bf7903757624d6d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 4, 512, 1024], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_72066883c5c8cf4c909ee2905df02c64(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 8, 25], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4410ed18d17cf56c7edb48d8d9487131(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 512, 4, 4], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a61a6fdaddd8ab64e00f90b107930627(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1536, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_31eec24c3fafd54628b30060097f3435(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e1c59463e9917d9924e85605f4931ef3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1216, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1216], dtype='float32'),
            paddle.static.InputSpec(shape=[1216], dtype='float32'),
            paddle.static.InputSpec(shape=[1216], dtype='float32'),
            paddle.static.InputSpec(shape=[1216], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b16fefd9c298c968f9c51950d5c477df(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1248, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1248], dtype='float32'),
            paddle.static.InputSpec(shape=[1248], dtype='float32'),
            paddle.static.InputSpec(shape=[1248], dtype='float32'),
            paddle.static.InputSpec(shape=[1248], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_49c30032d4e97e0f384c253072b5a30c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 12, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a78c095564d577f66b6b6211bede521c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 16, 128, 256], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_09c329535e4dfc090393750161f37a90(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1824, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1824], dtype='float32'),
            paddle.static.InputSpec(shape=[1824], dtype='float32'),
            paddle.static.InputSpec(shape=[1824], dtype='float32'),
            paddle.static.InputSpec(shape=[1824], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3f89882aa66b7d6e0e6cf03848a21bf5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 4, 32, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_24cad3ff89579e26f4b24bc3de0dc351(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 366, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[366], dtype='float32'),
            paddle.static.InputSpec(shape=[366], dtype='float32'),
            paddle.static.InputSpec(shape=[366], dtype='float32'),
            paddle.static.InputSpec(shape=[366], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_db9f06a06e5ec263aa27a6578d0add80(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 200, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_067694d2f6a1153403fd29036112d116(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 35, 35], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_05ccffff69f6576dc66e7c98f18af5b9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 26], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3336a67ab9f984ea2d3fdb4dcba03618(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 12, 40], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_73d715474df6ef0f0d2778246c30dbba(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1184, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1184], dtype='float32'),
            paddle.static.InputSpec(shape=[1184], dtype='float32'),
            paddle.static.InputSpec(shape=[1184], dtype='float32'),
            paddle.static.InputSpec(shape=[1184], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6ee54eaec46cead9dfed8ffc36bfc004(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 30, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2906dd3f3e0640042e9e677d7525ed36(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1152, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_895bf33b9928199543d8314581d69c80(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7134ccac61be0ea0fd0aefa3dabd9403(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 432, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_67829caec9fb9189d7dae66df909adaa(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 38, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[38], dtype='float32'),
            paddle.static.InputSpec(shape=[38], dtype='float32'),
            paddle.static.InputSpec(shape=[38], dtype='float32'),
            paddle.static.InputSpec(shape=[38], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ad432a6b8fec317a29146d7760c1bc85(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2112, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[2112], dtype='float32'),
            paddle.static.InputSpec(shape=[2112], dtype='float32'),
            paddle.static.InputSpec(shape=[2112], dtype='float32'),
            paddle.static.InputSpec(shape=[2112], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_db7396cc0f24295e5ab6004b01dde58a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 128, 128], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bdd537d775a796d7f54474406ed58350(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1008, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1008], dtype='float32'),
            paddle.static.InputSpec(shape=[1008], dtype='float32'),
            paddle.static.InputSpec(shape=[1008], dtype='float32'),
            paddle.static.InputSpec(shape=[1008], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_36df1ca41ea7c15a1af97a21087edcdd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 4, 128, 128], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fb22ed800caf18e500816b81cb5a7da6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 320, 320], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a9fc2bb0c0bc225642518d337cf4d8bf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e8b98632cfbae41426fc220f445e6486(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 40, 40], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7f76141c7f28531e80d93ce1df1964fc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 400, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[400], dtype='float32'),
            paddle.static.InputSpec(shape=[400], dtype='float32'),
            paddle.static.InputSpec(shape=[400], dtype='float32'),
            paddle.static.InputSpec(shape=[400], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_16ee940437a12012167b722195d2b167(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 740, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[740], dtype='float32'),
            paddle.static.InputSpec(shape=[740], dtype='float32'),
            paddle.static.InputSpec(shape=[740], dtype='float32'),
            paddle.static.InputSpec(shape=[740], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4847d91611badf5406733cc49e5f50e5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dd18401b50f195d1b69ba1a453845f02(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2560, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[2560], dtype='float32'),
            paddle.static.InputSpec(shape=[2560], dtype='float32'),
            paddle.static.InputSpec(shape=[2560], dtype='float32'),
            paddle.static.InputSpec(shape=[2560], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d15a7bc616b152ced00d29a8e4488a03(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_248b29acd43f66463b4bf3cd2bd304b2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 150, 150], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_407935324dee569608d3b6e8e126ddf7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 2, 6], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3caf0ad33a9071ab1f2803f58fa730cc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 149, 149], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0dbd9fe2e1e04bf2270b23b74c3d9df4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 112, 12, 12], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c2e753fdbe3567442fc1e03f5e7ae07e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1984, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1984], dtype='float32'),
            paddle.static.InputSpec(shape=[1984], dtype='float32'),
            paddle.static.InputSpec(shape=[1984], dtype='float32'),
            paddle.static.InputSpec(shape=[1984], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4cefbe2f0fcd0a3c7d12d04da6294ddd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 25, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[25], dtype='float32'),
            paddle.static.InputSpec(shape=[25], dtype='float32'),
            paddle.static.InputSpec(shape=[25], dtype='float32'),
            paddle.static.InputSpec(shape=[25], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4ef7505da06e846b325d100749737dc6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9c791f891b65821f9042c01087883c7a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1044, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1044], dtype='float32'),
            paddle.static.InputSpec(shape=[1044], dtype='float32'),
            paddle.static.InputSpec(shape=[1044], dtype='float32'),
            paddle.static.InputSpec(shape=[1044], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7d292400455f3928d3db699b9b9c3a07(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 410, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[410], dtype='float32'),
            paddle.static.InputSpec(shape=[410], dtype='float32'),
            paddle.static.InputSpec(shape=[410], dtype='float32'),
            paddle.static.InputSpec(shape=[410], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a9d9a15467cfdfd89cbb3d3fc8c28aaa(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1440, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1440], dtype='float32'),
            paddle.static.InputSpec(shape=[1440], dtype='float32'),
            paddle.static.InputSpec(shape=[1440], dtype='float32'),
            paddle.static.InputSpec(shape=[1440], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b5f3fe04440aef0974d3c167829a2095(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 816, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[816], dtype='float32'),
            paddle.static.InputSpec(shape=[816], dtype='float32'),
            paddle.static.InputSpec(shape=[816], dtype='float32'),
            paddle.static.InputSpec(shape=[816], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e418ce2e68184e8ffa573e948b40ac86(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 864, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[864], dtype='float32'),
            paddle.static.InputSpec(shape=[864], dtype='float32'),
            paddle.static.InputSpec(shape=[864], dtype='float32'),
            paddle.static.InputSpec(shape=[864], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0aeca0fb119a33c1a5a9a8bebf74ed03(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 32, None], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e910b95a8213a95415372c2921f6d8cb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1792, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1792], dtype='float32'),
            paddle.static.InputSpec(shape=[1792], dtype='float32'),
            paddle.static.InputSpec(shape=[1792], dtype='float32'),
            paddle.static.InputSpec(shape=[1792], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ab8d8cc91b147b04c1301f363ffac4e9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 60, 20, 20], dtype='float16'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3f42d5a9dbdc022e1c99000b5a383ec2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 436, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[436], dtype='float32'),
            paddle.static.InputSpec(shape=[436], dtype='float32'),
            paddle.static.InputSpec(shape=[436], dtype='float32'),
            paddle.static.InputSpec(shape=[436], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a5751d1bddadb2e498a869c2225f2f15(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 672, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e878490e40f031cbdc6c269e2f5bef62(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 16, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_432bc6d74660ffe150b7bb18a72891b2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 8, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_76d92055e5e8b62b69ae9755a09f3f70(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1296, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1296], dtype='float32'),
            paddle.static.InputSpec(shape=[1296], dtype='float32'),
            paddle.static.InputSpec(shape=[1296], dtype='float32'),
            paddle.static.InputSpec(shape=[1296], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_540325bcccbcf7dc27dcd00ff9f304dc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 35, 35], dtype='float16'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_faca234a3b0e486e94372fd901fbd331(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 128, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_358243a6c0028056dc5741d7b08c7e4c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 184, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[184], dtype='float32'),
            paddle.static.InputSpec(shape=[184], dtype='float32'),
            paddle.static.InputSpec(shape=[184], dtype='float32'),
            paddle.static.InputSpec(shape=[184], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_21cdbaf050444108fe284ba07c7eab75(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 75, 75], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7ffe1a6ab63c287d150f682527662f65(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 20, 20], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_624cb3d959ca392f1eb717c362b9df66(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1696, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1696], dtype='float32'),
            paddle.static.InputSpec(shape=[1696], dtype='float32'),
            paddle.static.InputSpec(shape=[1696], dtype='float32'),
            paddle.static.InputSpec(shape=[1696], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4c61f7bc0e45296f623737d3903f679f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 768, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f966f37784637dd7f934a44ee272a76d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1776, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1776], dtype='float32'),
            paddle.static.InputSpec(shape=[1776], dtype='float32'),
            paddle.static.InputSpec(shape=[1776], dtype='float32'),
            paddle.static.InputSpec(shape=[1776], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ee7c94107106bd22819111f42d41d7dd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 64, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8ba8b0345268f0dc8f4ea2e68bcb1214(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 576, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_976b91526a5dc2f3a9e97c4a16e0b14a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 80, 80], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6ef1b1d00a9af60b2d7ce60ffa060a36(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1584, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1584], dtype='float32'),
            paddle.static.InputSpec(shape=[1584], dtype='float32'),
            paddle.static.InputSpec(shape=[1584], dtype='float32'),
            paddle.static.InputSpec(shape=[1584], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b9e6dde078f05face473d58a826284c7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, 32, 128, 128], dtype='float16'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_96750a0f5d1bba3bce0b9b30195dd6a9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 31, 31], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_43a85f8db69f9254d3acc022ca1f59a2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, 32, 128, 128], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_adbe90a6483c6d7a731fd2e1e9398487(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1488, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1488], dtype='float32'),
            paddle.static.InputSpec(shape=[1488], dtype='float32'),
            paddle.static.InputSpec(shape=[1488], dtype='float32'),
            paddle.static.InputSpec(shape=[1488], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_aa7e0b68c846dc747dd1bd9131491ec0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 73, 73], dtype='float16'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ccfefb6a7a202d41a9529e931fb5938e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1408, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1408], dtype='float32'),
            paddle.static.InputSpec(shape=[1408], dtype='float32'),
            paddle.static.InputSpec(shape=[1408], dtype='float32'),
            paddle.static.InputSpec(shape=[1408], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f159a255d63dc15c3c3578e376ed7fba(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 36, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4d152c44fc2b0626740a6b54eee6520a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 234, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[234], dtype='float32'),
            paddle.static.InputSpec(shape=[234], dtype='float32'),
            paddle.static.InputSpec(shape=[234], dtype='float32'),
            paddle.static.InputSpec(shape=[234], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_19f30ddf3f63d31426e6ec22a02a60f8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1584, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1584], dtype='float32'),
            paddle.static.InputSpec(shape=[1584], dtype='float32'),
            paddle.static.InputSpec(shape=[1584], dtype='float32'),
            paddle.static.InputSpec(shape=[1584], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_83391cfece5cd7542dbfff9afea65200(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 864, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[864], dtype='float32'),
            paddle.static.InputSpec(shape=[864], dtype='float32'),
            paddle.static.InputSpec(shape=[864], dtype='float32'),
            paddle.static.InputSpec(shape=[864], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b22a4c2a1aad45f0f951cac899fb522d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 8, 25], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_defe38e4814345693cd22be6063cb31f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 147, 147], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7185d170951224540264a15c38f820bd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 162, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[162], dtype='float32'),
            paddle.static.InputSpec(shape=[162], dtype='float32'),
            paddle.static.InputSpec(shape=[162], dtype='float32'),
            paddle.static.InputSpec(shape=[162], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b1da824bbe8be5568b51258a56eb234e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1056, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1056], dtype='float32'),
            paddle.static.InputSpec(shape=[1056], dtype='float32'),
            paddle.static.InputSpec(shape=[1056], dtype='float32'),
            paddle.static.InputSpec(shape=[1056], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b4c0fe2fca612284865477be8e7df52a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 4, 12], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8756cd79d20184e2edd37a3b21596ae0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 61, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[61], dtype='float32'),
            paddle.static.InputSpec(shape=[61], dtype='float32'),
            paddle.static.InputSpec(shape=[61], dtype='float32'),
            paddle.static.InputSpec(shape=[61], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_897e67fbd5d9de8a56c074c35802522f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 570, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[570], dtype='float32'),
            paddle.static.InputSpec(shape=[570], dtype='float32'),
            paddle.static.InputSpec(shape=[570], dtype='float32'),
            paddle.static.InputSpec(shape=[570], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_475036204effe55e8c35258b4f43447a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1968, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1968], dtype='float32'),
            paddle.static.InputSpec(shape=[1968], dtype='float32'),
            paddle.static.InputSpec(shape=[1968], dtype='float32'),
            paddle.static.InputSpec(shape=[1968], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0552be49233cc4ef96bce93ad608f02d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 47, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[47], dtype='float32'),
            paddle.static.InputSpec(shape=[47], dtype='float32'),
            paddle.static.InputSpec(shape=[47], dtype='float32'),
            paddle.static.InputSpec(shape=[47], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_461425de9bf2a33cced22dafb4c617e0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a67d6452b2a2ceec8c222c4c76a8aaf1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1584, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1584], dtype='float32'),
            paddle.static.InputSpec(shape=[1584], dtype='float32'),
            paddle.static.InputSpec(shape=[1584], dtype='float32'),
            paddle.static.InputSpec(shape=[1584], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_636654b5ec89cb97d90c57cd6bdfefee(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1280, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a7c051871ccaee599d41bd3baf7a0dde(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 4, 3], dtype='float16'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d892ba2b1df01c09159a3b7835f549af(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e039cbf6ffd8b4b9597ef7e012ea5b2c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 480, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e30289a77f655e48c492be98ebf63d03(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, 64, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_92d6fd2dc40fd765a0c08202e2f8e538(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 728, 38, 38], dtype='float16'),
            paddle.static.InputSpec(shape=[728], dtype='float32'),
            paddle.static.InputSpec(shape=[728], dtype='float32'),
            paddle.static.InputSpec(shape=[728], dtype='float32'),
            paddle.static.InputSpec(shape=[728], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a64af683a2d6deaf7a7b974a51c6ca37(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1536, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_203d377c673ee4774c67da3b7df226d8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 702, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[702], dtype='float32'),
            paddle.static.InputSpec(shape=[702], dtype='float32'),
            paddle.static.InputSpec(shape=[702], dtype='float32'),
            paddle.static.InputSpec(shape=[702], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bb16a312934e2197793f27fc62de7457(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 96, 96], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8bd61de5b447285a1a98100e41b57a01(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 168, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[168], dtype='float32'),
            paddle.static.InputSpec(shape=[168], dtype='float32'),
            paddle.static.InputSpec(shape=[168], dtype='float32'),
            paddle.static.InputSpec(shape=[168], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_de25d736449bbdfaf418d14f121fd0ec(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2048, 10, 10], dtype='float16'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_723b30814165a2832e63237aa12019f6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 320, 320], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3bc7ae257a778d0de8722c35efd20e3a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 640, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_82af7e822b695e09d38e6ea79a3bf084(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 104, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7870053f6ad183e47186933ccabb9856(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1728, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1728], dtype='float32'),
            paddle.static.InputSpec(shape=[1728], dtype='float32'),
            paddle.static.InputSpec(shape=[1728], dtype='float32'),
            paddle.static.InputSpec(shape=[1728], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8ff360b784807127d8dbb229d9af68fe(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1280, 6, 6], dtype='float16'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cd851a3b357e92bf5843a71272052852(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1488, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1488], dtype='float32'),
            paddle.static.InputSpec(shape=[1488], dtype='float32'),
            paddle.static.InputSpec(shape=[1488], dtype='float32'),
            paddle.static.InputSpec(shape=[1488], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_28fc9786c8eb51637dc327186b33a0c2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 61, 61], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_aed7e060052c4499918ab45fb112be96(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2160, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[2160], dtype='float32'),
            paddle.static.InputSpec(shape=[2160], dtype='float32'),
            paddle.static.InputSpec(shape=[2160], dtype='float32'),
            paddle.static.InputSpec(shape=[2160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cf58e051a021fc0201a846c5b7961f9a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1248, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1248], dtype='float32'),
            paddle.static.InputSpec(shape=[1248], dtype='float32'),
            paddle.static.InputSpec(shape=[1248], dtype='float32'),
            paddle.static.InputSpec(shape=[1248], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b5932dbf13639be2f7d7c2f7e01d89fe(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 40, 40], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_68999480cca03708b0919edb32d1a515(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 64, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_00e024924948f9530c4c5682daa6652b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_57a31e312c2e0e2f69d658b889ce8c6e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 61, 61], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0988a78b8dc830e8cc8cd52428e02408(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 80, 80], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4e1519cd585b63180b56b6e0fc574dc2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 336, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_37e481209dfa75f478bb7af2b6546dc0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 608, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[608], dtype='float32'),
            paddle.static.InputSpec(shape=[608], dtype='float32'),
            paddle.static.InputSpec(shape=[608], dtype='float32'),
            paddle.static.InputSpec(shape=[608], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b4aace2371eeae83ccd23970301645e6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 72, 122, 122], dtype='float16'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_864143a2abfd0f3d4c024275d30b97e4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1056, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1056], dtype='float32'),
            paddle.static.InputSpec(shape=[1056], dtype='float32'),
            paddle.static.InputSpec(shape=[1056], dtype='float32'),
            paddle.static.InputSpec(shape=[1056], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_12c379cc113a403f53a4caf467d6d562(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 336, 4, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_27a114a1ea98a13a7bb957d7d400fa15(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 12, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f79b3849c0ff7b562d2f8d37623473b9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 8, 25], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b242da51ea0bf8a59aa10b12507e499e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_367efcfceb41a4eac561294e555d7a26(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 208, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_99613e5121b570118b8f970abbba7f46(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8de152baa28ab7e9da7e2432f6719840(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 38, 38], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6e7ff31e0b879dcba9b6ebba828ef520(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 972, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[972], dtype='float32'),
            paddle.static.InputSpec(shape=[972], dtype='float32'),
            paddle.static.InputSpec(shape=[972], dtype='float32'),
            paddle.static.InputSpec(shape=[972], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_441ab70336b8cf138794def7506deaa9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, 100], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c25a8b8f243c9cfd1cd6059f193a96d4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 35, 35], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ff41a1da4d2282704bf6a4f294a4e176(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 672, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9a4bbba9f675abeabe36ab9163defc76(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2, 112, 224], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_eb0cf3322ed5ca54d3ec6aba3fa1ed38(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 8, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_386d24c874bfe130a74dc32f974e8743(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_17467ab67ef840c508175cd1d2686d33(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 624, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[624], dtype='float32'),
            paddle.static.InputSpec(shape=[624], dtype='float32'),
            paddle.static.InputSpec(shape=[624], dtype='float32'),
            paddle.static.InputSpec(shape=[624], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7ff7fed0f17c0351313ecfc46dc48cb3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 48, 160], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d39e7200e9bf7d28cc6a48a96099dc48(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 832, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d610cbf6adb1ac86386c4dbe83923177(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 624, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[624], dtype='float32'),
            paddle.static.InputSpec(shape=[624], dtype='float32'),
            paddle.static.InputSpec(shape=[624], dtype='float32'),
            paddle.static.InputSpec(shape=[624], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_be909b13142b62385fa32c76b9f65ef9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_34ebfe9e21ba9c66a500d45fd7f22e41(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_29b7631c13ce6f45783542634b6f9809(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 400, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[400], dtype='float32'),
            paddle.static.InputSpec(shape=[400], dtype='float32'),
            paddle.static.InputSpec(shape=[400], dtype='float32'),
            paddle.static.InputSpec(shape=[400], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f0310f299a03f53c6ac5e812c44939f7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1440, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1440], dtype='float32'),
            paddle.static.InputSpec(shape=[1440], dtype='float32'),
            paddle.static.InputSpec(shape=[1440], dtype='float32'),
            paddle.static.InputSpec(shape=[1440], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_88d417846c710f6bcb097ebfa8ce0714(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 8, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e688e2824ed9df8542cecce3c58a2d73(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 32, 128], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ffba008e796ae2ad1f7d7fca90d15a0f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2dee71c3d502d6667acafa1bf8810326(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1488, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1488], dtype='float32'),
            paddle.static.InputSpec(shape=[1488], dtype='float32'),
            paddle.static.InputSpec(shape=[1488], dtype='float32'),
            paddle.static.InputSpec(shape=[1488], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3254ec6234d29c6b79ccc83ef1b64d71(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_39d2d19b7d303851549f0d5fdbc4f59a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1680, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1680], dtype='float32'),
            paddle.static.InputSpec(shape=[1680], dtype='float32'),
            paddle.static.InputSpec(shape=[1680], dtype='float32'),
            paddle.static.InputSpec(shape=[1680], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_99e42cd7f95916211bc300acf6ff6d97(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 56, 4, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5e737435984eade283e467402a97dbae(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 32, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0bd5599424b42dedca8b043b614148ef(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2112, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[2112], dtype='float32'),
            paddle.static.InputSpec(shape=[2112], dtype='float32'),
            paddle.static.InputSpec(shape=[2112], dtype='float32'),
            paddle.static.InputSpec(shape=[2112], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_579400ce34160557edeed1dea40b82a1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1472, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1472], dtype='float32'),
            paddle.static.InputSpec(shape=[1472], dtype='float32'),
            paddle.static.InputSpec(shape=[1472], dtype='float32'),
            paddle.static.InputSpec(shape=[1472], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5f8f7211854d3fd37264b6b4fa71ea8e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 71, 71], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8264cd0ed09d33c4efb6b54b04181ed2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 58, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[58], dtype='float32'),
            paddle.static.InputSpec(shape=[58], dtype='float32'),
            paddle.static.InputSpec(shape=[58], dtype='float32'),
            paddle.static.InputSpec(shape=[58], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_937f0b1ae193bd9da7f88fe70cc20312(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 320, 8, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7de4059ee7757ed73afab8a6a75785f0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1632, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1632], dtype='float32'),
            paddle.static.InputSpec(shape=[1632], dtype='float32'),
            paddle.static.InputSpec(shape=[1632], dtype='float32'),
            paddle.static.InputSpec(shape=[1632], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_02e3dc0424af03931aaf0480e7113fca(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 20, 20], dtype='float16'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_aadc1f2d06d73f2d1a2e0c0c53deebd9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 300, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[300], dtype='float32'),
            paddle.static.InputSpec(shape=[300], dtype='float32'),
            paddle.static.InputSpec(shape=[300], dtype='float32'),
            paddle.static.InputSpec(shape=[300], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ef23543cccc6d8990591be5a7ae61303(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 19, 19], dtype='float16'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a57c9796087bacba8e8c4e883f980882(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 164, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[164], dtype='float32'),
            paddle.static.InputSpec(shape=[164], dtype='float32'),
            paddle.static.InputSpec(shape=[164], dtype='float32'),
            paddle.static.InputSpec(shape=[164], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_54fcde5cf795d819b395aaf19da10d8f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1872, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1872], dtype='float32'),
            paddle.static.InputSpec(shape=[1872], dtype='float32'),
            paddle.static.InputSpec(shape=[1872], dtype='float32'),
            paddle.static.InputSpec(shape=[1872], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_856ad6cbb55e3d24b41245c4b9560a41(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b2696b10e83c1d3c2a92b52e01f50ed6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 64, 128, 256], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_801797befffbea1a2ce19d019107bf8f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 8, 25], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f8dc552edc4be255e46b47d76908b345(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 8, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_af156719f364b1bbcc99a25272c7d11e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 336, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e69a017b00574952acf834813ed3760c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1600, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1600], dtype='float32'),
            paddle.static.InputSpec(shape=[1600], dtype='float32'),
            paddle.static.InputSpec(shape=[1600], dtype='float32'),
            paddle.static.InputSpec(shape=[1600], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_eb0b82455812b1e069ad7fe596f1bf3d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 32, 24], dtype='float16'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c3b4a6d65f831f3e96d102564a9c4de6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 640, 8, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e2bf442264d3c3537af1dd050f480205(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1920, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1920], dtype='float32'),
            paddle.static.InputSpec(shape=[1920], dtype='float32'),
            paddle.static.InputSpec(shape=[1920], dtype='float32'),
            paddle.static.InputSpec(shape=[1920], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b7d925347799e3b4b1f0dcf60a0ba371(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 288, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ce70511b552491bd0868bf96927138c2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 72, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cbf87a3922b26ccb48d13f6f2f36adbb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_da28452afd87fa7ec6700e3a52e92bcc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_073057315402ed17b5b67313b7b57236(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 4, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_69864754ba1223a990da32a7ad8aec62(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 512, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5e1344a34fc3a134b70131e7016a7e01(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 75, 75], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_696637a7fa2dd68d58407021599fbc16(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 24, 24], dtype='float16'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_011b6c68b397738302a4af50e26ee9a9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5235f72a7f34d2f45209d273b1ca4ce3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1968, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1968], dtype='float32'),
            paddle.static.InputSpec(shape=[1968], dtype='float32'),
            paddle.static.InputSpec(shape=[1968], dtype='float32'),
            paddle.static.InputSpec(shape=[1968], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d655591997fa9244a08e06784e6c676f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 3, 3], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5b8fa27111695d91eadafafb1a520f56(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 4, 12], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_32f082526f4718640e33f8509d211cda(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 64, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fb39e8e283e78cd77af02a9e175f5e0f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 64, 256], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f881908c233cff0d518f7bbddbe95ff2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 234, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[234], dtype='float32'),
            paddle.static.InputSpec(shape=[234], dtype='float32'),
            paddle.static.InputSpec(shape=[234], dtype='float32'),
            paddle.static.InputSpec(shape=[234], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0a6fd276aa8102f6d3d02f53851ec6b3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 28, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_487c1fe21a345440b06740e79a052423(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 288, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_26aa9a3de6019caaec46f3a13bf74266(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 256, 256], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8691d18211a75347bf7b588c139f54f1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e799d33cdc34eab378e0e515e5630486(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1408, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1408], dtype='float32'),
            paddle.static.InputSpec(shape=[1408], dtype='float32'),
            paddle.static.InputSpec(shape=[1408], dtype='float32'),
            paddle.static.InputSpec(shape=[1408], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_14b082ceadee49af49a550bae9fc8c51(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 528, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[528], dtype='float32'),
            paddle.static.InputSpec(shape=[528], dtype='float32'),
            paddle.static.InputSpec(shape=[528], dtype='float32'),
            paddle.static.InputSpec(shape=[528], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bd8359a9fa2df57fc823821810233f7b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1984, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1984], dtype='float32'),
            paddle.static.InputSpec(shape=[1984], dtype='float32'),
            paddle.static.InputSpec(shape=[1984], dtype='float32'),
            paddle.static.InputSpec(shape=[1984], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ea0b2107e75d7da7dece16d2818e62dd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 49], dtype='float16'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cb83104a20b06dc4b6ef79a0d2e04cc1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1504, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1504], dtype='float32'),
            paddle.static.InputSpec(shape=[1504], dtype='float32'),
            paddle.static.InputSpec(shape=[1504], dtype='float32'),
            paddle.static.InputSpec(shape=[1504], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6a26da77a4e4fb14b1a41ff1868ad0c2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 720, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5a7448d0c9ed8b89a0a1ac8fbc10cf8a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 104, 4, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_07eeb3f0cd7e65e6c274d576a8cf26a8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2016, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[2016], dtype='float32'),
            paddle.static.InputSpec(shape=[2016], dtype='float32'),
            paddle.static.InputSpec(shape=[2016], dtype='float32'),
            paddle.static.InputSpec(shape=[2016], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a175dad4f9465d67843636e9bee397e5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 32, 256, 512], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0627566346fd8d16b36302f5776ac6dc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6431429537b11f1f7cafb96123d13ff8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1472, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1472], dtype='float32'),
            paddle.static.InputSpec(shape=[1472], dtype='float32'),
            paddle.static.InputSpec(shape=[1472], dtype='float32'),
            paddle.static.InputSpec(shape=[1472], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f674ecd4caee1f22c0094e96a755c269(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 112, 12, 12], dtype='float16'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c681fab815e93f1cbf1a2b6f35ed0f5b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2432, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[2432], dtype='float32'),
            paddle.static.InputSpec(shape=[2432], dtype='float32'),
            paddle.static.InputSpec(shape=[2432], dtype='float32'),
            paddle.static.InputSpec(shape=[2432], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_755455be86c40f73334f229d81cfb610(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c198ebb05e54a668041e8d96c745ecc4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1248, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1248], dtype='float32'),
            paddle.static.InputSpec(shape=[1248], dtype='float32'),
            paddle.static.InputSpec(shape=[1248], dtype='float32'),
            paddle.static.InputSpec(shape=[1248], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6fa389afc2ca633de225c3e7427fda16(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 208, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4e126e17b02f24fa0aeed65cc485aa65(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e85aaae890c2775c3c0da038a97fc088(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1104, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1104], dtype='float32'),
            paddle.static.InputSpec(shape=[1104], dtype='float32'),
            paddle.static.InputSpec(shape=[1104], dtype='float32'),
            paddle.static.InputSpec(shape=[1104], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8caf97a8480c55f50ffb5e8a321f1a14(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 304, 304], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ad28ea0fc0c8e43ba920a50c234fdee9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 20, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c85ba9321ecc7a325e149f98792a3fe7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 174, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[174], dtype='float32'),
            paddle.static.InputSpec(shape=[174], dtype='float32'),
            paddle.static.InputSpec(shape=[174], dtype='float32'),
            paddle.static.InputSpec(shape=[174], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d3b7ddd8e1f1dc602d6a5a567c3eaf5c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 416, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[416], dtype='float32'),
            paddle.static.InputSpec(shape=[416], dtype='float32'),
            paddle.static.InputSpec(shape=[416], dtype='float32'),
            paddle.static.InputSpec(shape=[416], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c1cd91d7c1f51f7aaf5ae5a61587df98(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8c5167bc8aa6796e64bc2519c4f3a09c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 31, 31], dtype='float16'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2fc9a891a92f336694b4fffc9b756281(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 736, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[736], dtype='float32'),
            paddle.static.InputSpec(shape=[736], dtype='float32'),
            paddle.static.InputSpec(shape=[736], dtype='float32'),
            paddle.static.InputSpec(shape=[736], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_635d8c6eee3d97d4ebe54fb73bcada55(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 147, 147], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_829e10c433363b93c63afda25e46f6be(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4ae532ac2d899558c963968af9e9c8cd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 304, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[304], dtype='float32'),
            paddle.static.InputSpec(shape=[304], dtype='float32'),
            paddle.static.InputSpec(shape=[304], dtype='float32'),
            paddle.static.InputSpec(shape=[304], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3a411859b7a06d507d70073643073f19(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 19, 19], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d45ea446e0bee54a250317f48b43b7e5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 262, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[262], dtype='float32'),
            paddle.static.InputSpec(shape=[262], dtype='float32'),
            paddle.static.InputSpec(shape=[262], dtype='float32'),
            paddle.static.InputSpec(shape=[262], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e54c9b446b4ef105808a049283955adf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 10, 10], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_22078a374c6d5d4d8c3827de97839e41(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 88, 61, 61], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cd2bb7faa7685a1a88ad514922fbdfbf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 176, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[176], dtype='float32'),
            paddle.static.InputSpec(shape=[176], dtype='float32'),
            paddle.static.InputSpec(shape=[176], dtype='float32'),
            paddle.static.InputSpec(shape=[176], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a00bb4e257ab0a36e29ebfeeff0d842b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 200, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1b6c71d21f96d7faf2a2db0f5a70d973(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cafb0986540bfd1a3adbb5480dabffdf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 70, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b9bb75184127fca2f1c57ca008a66b75(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 38, 38], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bb759b99bdfcb9011ade436d7f22edac(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 432, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8e8f73e3bbd4fb2d7bba6e6d36e22f91(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 2, 4], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5195ebe7af6da40d319d41f61bf2ac2c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 3, 3], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e7f539abfc4c607a5f18a9fdd1e81b06(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 336, 4, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1cf9fed88898a52836a6b9e7175f3073(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 48, 48], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fd9bb3eff78417dce39f1c4c17582da6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 151, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[151], dtype='float32'),
            paddle.static.InputSpec(shape=[151], dtype='float32'),
            paddle.static.InputSpec(shape=[151], dtype='float32'),
            paddle.static.InputSpec(shape=[151], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a688e009b68e15cd63b76c86ddc74bb7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 19, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[19], dtype='float32'),
            paddle.static.InputSpec(shape=[19], dtype='float32'),
            paddle.static.InputSpec(shape=[19], dtype='float32'),
            paddle.static.InputSpec(shape=[19], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c793602ede73f9274317587bec0f6eb4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 16, 12], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4cc0a0f67bde72d35892dd309ed5aa8a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 672, 6, 6], dtype='float16'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_045547d9fd3b70e80400ddb0704629b5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1280, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a4cf30a4d5b9ad84515e68b21657b155(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b260b97a7dd4463e7f763f97d23bbb0c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_32eaba6084a13cca4928f97bea910403(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, 24, 24], dtype='float16'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c181f9728f90d171e0887bcbb8afe44d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 720, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3ff10796688edddd66d59da0c3e0da2e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 228, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[228], dtype='float32'),
            paddle.static.InputSpec(shape=[228], dtype='float32'),
            paddle.static.InputSpec(shape=[228], dtype='float32'),
            paddle.static.InputSpec(shape=[228], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_376eb4e64c16e7dc177259bc5f5ae981(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 672, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bf10236c687704c2c9a551d17669e0f6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 160, 160], dtype='float16'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3ff41849c1f594b322c9c2499c6faf7a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 160, 160], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e24cbc95fb090ef9681d5e1937fd754f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 800, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[800], dtype='float32'),
            paddle.static.InputSpec(shape=[800], dtype='float32'),
            paddle.static.InputSpec(shape=[800], dtype='float32'),
            paddle.static.InputSpec(shape=[800], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5f5147e6dcf0a2f6457540422bd0a3b0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1824, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1824], dtype='float32'),
            paddle.static.InputSpec(shape=[1824], dtype='float32'),
            paddle.static.InputSpec(shape=[1824], dtype='float32'),
            paddle.static.InputSpec(shape=[1824], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d4c51ec0a78417bef59b68da57c0db62(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_16c5998ca4ababc65fb3747fe0c754f5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5ea9bcc6c5efaff0e969963c1fbd5d7f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b2a27438a910061f2374bc6ff571b104(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1952, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1952], dtype='float32'),
            paddle.static.InputSpec(shape=[1952], dtype='float32'),
            paddle.static.InputSpec(shape=[1952], dtype='float32'),
            paddle.static.InputSpec(shape=[1952], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d8efe7286b43a5f44d9b4957f89df325(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 512, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6c5ec2ea31615061027a4c86396cf6be(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 960, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9578368f12d97c0d445fae3606a50612(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1280, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9ddf65cc5820d79f0cf44cf133b6ab42(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 528, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[528], dtype='float32'),
            paddle.static.InputSpec(shape=[528], dtype='float32'),
            paddle.static.InputSpec(shape=[528], dtype='float32'),
            paddle.static.InputSpec(shape=[528], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2166fbdf0dc76caa9eaeb4e7f2a18035(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 164, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[164], dtype='float32'),
            paddle.static.InputSpec(shape=[164], dtype='float32'),
            paddle.static.InputSpec(shape=[164], dtype='float32'),
            paddle.static.InputSpec(shape=[164], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a31fbafeb6158af7b3bc553784331220(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_92329144f48a3e23c44fb1c13743ae94(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 228, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[228], dtype='float32'),
            paddle.static.InputSpec(shape=[228], dtype='float32'),
            paddle.static.InputSpec(shape=[228], dtype='float32'),
            paddle.static.InputSpec(shape=[228], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5044464335106b9f394e8b42a34c2418(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_02d589f4fd9bc2be1304315eb1756767(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2, 112, 224], dtype='float16'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8359ec07e6727424210642ef7753057d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 84, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[84], dtype='float32'),
            paddle.static.InputSpec(shape=[84], dtype='float32'),
            paddle.static.InputSpec(shape=[84], dtype='float32'),
            paddle.static.InputSpec(shape=[84], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3cf26023911c7eda97b16db699ae0ff0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 352, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[352], dtype='float32'),
            paddle.static.InputSpec(shape=[352], dtype='float32'),
            paddle.static.InputSpec(shape=[352], dtype='float32'),
            paddle.static.InputSpec(shape=[352], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5f987c0d4fff88db50576a915fbddc61(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2064, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[2064], dtype='float32'),
            paddle.static.InputSpec(shape=[2064], dtype='float32'),
            paddle.static.InputSpec(shape=[2064], dtype='float32'),
            paddle.static.InputSpec(shape=[2064], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b0a4b7c90da363fc3fd7250aa8d5641d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 64, 256, 256], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1442575d9d6d8d21262f054611450fa7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 256, 256], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a235d6f6c02c5b2da51e817edea4be49(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 624, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[624], dtype='float32'),
            paddle.static.InputSpec(shape=[624], dtype='float32'),
            paddle.static.InputSpec(shape=[624], dtype='float32'),
            paddle.static.InputSpec(shape=[624], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8c241c5d842a81b164e8cd441263071c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1248, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1248], dtype='float32'),
            paddle.static.InputSpec(shape=[1248], dtype='float32'),
            paddle.static.InputSpec(shape=[1248], dtype='float32'),
            paddle.static.InputSpec(shape=[1248], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9b2b4704b181faf4a2a3c3b95962d9fc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 704, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8014a0a060c0e4e6a65b292adf06a740(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 672, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ad8c8da3857e6bf3898e178beaea5391(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 228, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[228], dtype='float32'),
            paddle.static.InputSpec(shape=[228], dtype='float32'),
            paddle.static.InputSpec(shape=[228], dtype='float32'),
            paddle.static.InputSpec(shape=[228], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_658067f7e912b60287bb6ef6f7f62f00(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 432, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e8261252208644d1e6de9fe1fff3934f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1568, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1568], dtype='float32'),
            paddle.static.InputSpec(shape=[1568], dtype='float32'),
            paddle.static.InputSpec(shape=[1568], dtype='float32'),
            paddle.static.InputSpec(shape=[1568], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8bbf3293f35b054878bddb7e19320873(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 448, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[448], dtype='float32'),
            paddle.static.InputSpec(shape=[448], dtype='float32'),
            paddle.static.InputSpec(shape=[448], dtype='float32'),
            paddle.static.InputSpec(shape=[448], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a7f9e1287adef234b360f8c5ce7bf39b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 608, 608], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ec3b3a2a6bb81c72145d5e0bbcee4810(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1312, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1312], dtype='float32'),
            paddle.static.InputSpec(shape=[1312], dtype='float32'),
            paddle.static.InputSpec(shape=[1312], dtype='float32'),
            paddle.static.InputSpec(shape=[1312], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_26c1b854eb872ba56ebedf9793de3768(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 120, 31, 31], dtype='float16'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8272a9122063f5942bfd66068ee30970(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 704, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dfaf72fe729072533518bfce8d3ee0f6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 49], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_01c50f499900b120da6225e338d2aa5d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1920, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1920], dtype='float32'),
            paddle.static.InputSpec(shape=[1920], dtype='float32'),
            paddle.static.InputSpec(shape=[1920], dtype='float32'),
            paddle.static.InputSpec(shape=[1920], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5b3c8fc32ea41d5175a58db71a369fc3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_eb3355eb5b3a1d649b681e628ff1b3bc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 112, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_84568216994c42dc9a78a7638acfc9f2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 4, 32, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_11b14a1ae41b0a31593836f12c423b4d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 3, 3], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_11552d4e360ad473a97ca3997cbdb69b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 320, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_626f3382699b88b9e7045ebd7392ddf0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 832, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_195a3ff1f6a14a915f756aa330dc479e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 740, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[740], dtype='float32'),
            paddle.static.InputSpec(shape=[740], dtype='float32'),
            paddle.static.InputSpec(shape=[740], dtype='float32'),
            paddle.static.InputSpec(shape=[740], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3d76b968cb943f794da8117263870d04(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 1, 2], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ad4300dd0dd7c1655f8157362f7a3fce(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 1, 26], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cac9f33a1d94791f207118c5166b9344(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1696, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1696], dtype='float32'),
            paddle.static.InputSpec(shape=[1696], dtype='float32'),
            paddle.static.InputSpec(shape=[1696], dtype='float32'),
            paddle.static.InputSpec(shape=[1696], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_968feae99ca59e88c788c7ab6dddb91d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 72, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_09d7974ad6e02321106838df9e145c82(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5ed73f86ee2dc70f7d877855c00607d3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 4, 64, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d0216cd2c002adfdd03e4fb5ed82c99f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 16, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_282105b8a4b9005639370dd9fb1097c9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, None], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2a5d33304f33681fe1edc766fa8ea08d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 4, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9ea508b07f7b60324874cbe201e660d4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 185, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[185], dtype='float32'),
            paddle.static.InputSpec(shape=[185], dtype='float32'),
            paddle.static.InputSpec(shape=[185], dtype='float32'),
            paddle.static.InputSpec(shape=[185], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ef279ca824491cb465daea4379cfef4f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 112, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b0beaf0879818191f4dbac2d4c9de54e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 832, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4e0c57e49b7bb71ad7507c34d30b87c2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 236, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[236], dtype='float32'),
            paddle.static.InputSpec(shape=[236], dtype='float32'),
            paddle.static.InputSpec(shape=[236], dtype='float32'),
            paddle.static.InputSpec(shape=[236], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7e323d9cc4d140b84a43a2e2bc7c76b5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 256, 175, 25], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bc61aa3e884868c6d7ea2b51aefc111f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 47, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[47], dtype='float32'),
            paddle.static.InputSpec(shape=[47], dtype='float32'),
            paddle.static.InputSpec(shape=[47], dtype='float32'),
            paddle.static.InputSpec(shape=[47], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_33f3e60285f76e0d85b8df1938b63567(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 175, 25], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1976f70b33b0bd4b60b0ccc3db188442(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 832, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b79a03b9889171b06565fd33ddcec551(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_294c3e3754c446ce8ef00acc72e1fe52(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, 32, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_18499de8d9e0603581052a3660719c44(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 768, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4192dcd65a629e7e519714b3d3047b86(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 118, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[118], dtype='float32'),
            paddle.static.InputSpec(shape=[118], dtype='float32'),
            paddle.static.InputSpec(shape=[118], dtype='float32'),
            paddle.static.InputSpec(shape=[118], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_61a8af54b96b78bb67ab9a5783933e1e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, 24], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9864e859e39ce967be5cf9baf8b4d3e3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 366, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[366], dtype='float32'),
            paddle.static.InputSpec(shape=[366], dtype='float32'),
            paddle.static.InputSpec(shape=[366], dtype='float32'),
            paddle.static.InputSpec(shape=[366], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e8d0c135d4805cf056ec128baf6021b6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 504, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[504], dtype='float32'),
            paddle.static.InputSpec(shape=[504], dtype='float32'),
            paddle.static.InputSpec(shape=[504], dtype='float32'),
            paddle.static.InputSpec(shape=[504], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bb7bdcfc57af9f15c0c2fa0f429ffa87(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 30, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5e860e3a3156f27bbc563cb769888402(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1824, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1824], dtype='float32'),
            paddle.static.InputSpec(shape=[1824], dtype='float32'),
            paddle.static.InputSpec(shape=[1824], dtype='float32'),
            paddle.static.InputSpec(shape=[1824], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4461f75cf5c1fb8a609859991616732f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 19, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[19], dtype='float32'),
            paddle.static.InputSpec(shape=[19], dtype='float32'),
            paddle.static.InputSpec(shape=[19], dtype='float32'),
            paddle.static.InputSpec(shape=[19], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f70d253a8c85f173170351faade870e3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 8, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1f6596b2204ca91d19f0a77cd570c1cf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1296, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1296], dtype='float32'),
            paddle.static.InputSpec(shape=[1296], dtype='float32'),
            paddle.static.InputSpec(shape=[1296], dtype='float32'),
            paddle.static.InputSpec(shape=[1296], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d092b5f6fce32e2a8e87b36f36408ab1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8f9a9f3e7b40fc374b85b405c3aff53a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 71, 71], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4c20f1ab47ef6ecb4fdaaf7c4394d1e6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 6, 6], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_87fdaf18cdc8c15e26667373853ec63d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1728, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1728], dtype='float32'),
            paddle.static.InputSpec(shape=[1728], dtype='float32'),
            paddle.static.InputSpec(shape=[1728], dtype='float32'),
            paddle.static.InputSpec(shape=[1728], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3af3a6b6d2c5c0ae73a92daa06d7104c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 4, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_debd32127a4e41e44c2010993e6a022e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 4, 4], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_66686e954e0e07c03d3f70fb917de5e3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 88, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b163aa986bc09bfc647f09b83b07825e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 608, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[608], dtype='float32'),
            paddle.static.InputSpec(shape=[608], dtype='float32'),
            paddle.static.InputSpec(shape=[608], dtype='float32'),
            paddle.static.InputSpec(shape=[608], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d558e69ba7079f1bfbd6bffa22b02a32(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 512, 4, 4], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6ba1fd2d16db2400e955308f53f3c0e0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1344, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1344], dtype='float32'),
            paddle.static.InputSpec(shape=[1344], dtype='float32'),
            paddle.static.InputSpec(shape=[1344], dtype='float32'),
            paddle.static.InputSpec(shape=[1344], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cba8ac77abe105a6ebc0c0dc696f46ba(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 320, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_18a785bcdfcfcb78b66199742907e263(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 4, 64, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0243fce825de868f4efe321aa58a736b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fd4d8fd92633f0918a445794c425b03f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 448, 8, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[448], dtype='float32'),
            paddle.static.InputSpec(shape=[448], dtype='float32'),
            paddle.static.InputSpec(shape=[448], dtype='float32'),
            paddle.static.InputSpec(shape=[448], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f53ebda9bb5560de5f9349dfa1388398(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 17, 17], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a65cad9da910bfc21515dc93ae466f7c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 8, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2d4a06ca563163a1911037f377fd7b54(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4a3780e143c6593be6a429686d661bb2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 4, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5b62c1eb1eea18fc53875f8c7b26385c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 8, 25], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8bb03deac1cb8964dbb9828bfc08fab8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1200, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2ade1c8c123212d0b4da7fdef1129c38(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 912, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[912], dtype='float32'),
            paddle.static.InputSpec(shape=[912], dtype='float32'),
            paddle.static.InputSpec(shape=[912], dtype='float32'),
            paddle.static.InputSpec(shape=[912], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3ef1128a7f0385fe2e93b55d907ff2c7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 640], dtype='float16'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9ef2f6d566b5486e9623c562160de48a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1296, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1296], dtype='float32'),
            paddle.static.InputSpec(shape=[1296], dtype='float32'),
            paddle.static.InputSpec(shape=[1296], dtype='float32'),
            paddle.static.InputSpec(shape=[1296], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a0110df3ba1d7944208519d64614db38(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 640, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6fdd1bc8e0e47d357c9681e3afc3aa73(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2016, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[2016], dtype='float32'),
            paddle.static.InputSpec(shape=[2016], dtype='float32'),
            paddle.static.InputSpec(shape=[2016], dtype='float32'),
            paddle.static.InputSpec(shape=[2016], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_11b605be9747462b91e5f789e3a7c2ab(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 624, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[624], dtype='float32'),
            paddle.static.InputSpec(shape=[624], dtype='float32'),
            paddle.static.InputSpec(shape=[624], dtype='float32'),
            paddle.static.InputSpec(shape=[624], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c401d1176ab51469a816b7bc7ba19212(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 48, 48], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d115a16c90d0f6858881e4f05a9f4700(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 112, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2f3626e73caf1cf25b8b075ab4aeac6d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 244, 244], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f3a752b0548f4aad8400ccaaa7d67719(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1632, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1632], dtype='float32'),
            paddle.static.InputSpec(shape=[1632], dtype='float32'),
            paddle.static.InputSpec(shape=[1632], dtype='float32'),
            paddle.static.InputSpec(shape=[1632], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_954e05d384686e7f64edaf73adcd531e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 16, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_25f103fa3896da099d3cde17bf1dc78e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 224, 224], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7896920255674cc01e20a46599623ade(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0866e64952cf5cd57c2696e7aa5e52bf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 19, 19], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d529bf5776f65ea06d894b0b3e3899d7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1664, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1664], dtype='float32'),
            paddle.static.InputSpec(shape=[1664], dtype='float32'),
            paddle.static.InputSpec(shape=[1664], dtype='float32'),
            paddle.static.InputSpec(shape=[1664], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c347cb6185eb6b1a8768caffbd4c20b9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 436, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[436], dtype='float32'),
            paddle.static.InputSpec(shape=[436], dtype='float32'),
            paddle.static.InputSpec(shape=[436], dtype='float32'),
            paddle.static.InputSpec(shape=[436], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fedc7d2ba8af28a5f2c30f5b54772f7f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 72, 61, 61], dtype='float16'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a6de8498df15d65b6c1e88fdee42bb3c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 4, 32, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_17dc6585acd08b6f30f9c456f2d4d68f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 4, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_629b5c94a3254f00139132c6cfefec6c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8d97cda29ed22e28b86427ac2b826eac(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 544, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[544], dtype='float32'),
            paddle.static.InputSpec(shape=[544], dtype='float32'),
            paddle.static.InputSpec(shape=[544], dtype='float32'),
            paddle.static.InputSpec(shape=[544], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cfb69fd406c5282be428fef1cb855749(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1680, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1680], dtype='float32'),
            paddle.static.InputSpec(shape=[1680], dtype='float32'),
            paddle.static.InputSpec(shape=[1680], dtype='float32'),
            paddle.static.InputSpec(shape=[1680], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_68d960ab8f7b2fae2e15eee62fbd6493(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1376, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1376], dtype='float32'),
            paddle.static.InputSpec(shape=[1376], dtype='float32'),
            paddle.static.InputSpec(shape=[1376], dtype='float32'),
            paddle.static.InputSpec(shape=[1376], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_75e478ce1c9870e0491a349dfe73dce1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 49], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d7f6d683fd92f78f719a0cf8725cd79b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 200, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_541c5e8fce76898fc2f665d66f8d8d58(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2016, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[2016], dtype='float32'),
            paddle.static.InputSpec(shape=[2016], dtype='float32'),
            paddle.static.InputSpec(shape=[2016], dtype='float32'),
            paddle.static.InputSpec(shape=[2016], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2dd377f23ba6d8724508ee0503e7e7c0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1392, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1392], dtype='float32'),
            paddle.static.InputSpec(shape=[1392], dtype='float32'),
            paddle.static.InputSpec(shape=[1392], dtype='float32'),
            paddle.static.InputSpec(shape=[1392], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1fc9939ee612c9963c71c7c09e5c4ef8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 16, 12], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_963b499957681d183a90ee6fbcf07b4e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_76d55cfaa6d0232d25592b0177013e3a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, 32, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_441e62d2bdc69610d368e2d17dd3cadd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1760, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1760], dtype='float32'),
            paddle.static.InputSpec(shape=[1760], dtype='float32'),
            paddle.static.InputSpec(shape=[1760], dtype='float32'),
            paddle.static.InputSpec(shape=[1760], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_621e043c1b9c17d3e7dba8409765afa5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 912, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[912], dtype='float32'),
            paddle.static.InputSpec(shape=[912], dtype='float32'),
            paddle.static.InputSpec(shape=[912], dtype='float32'),
            paddle.static.InputSpec(shape=[912], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8a424fee43ea8eaf70c4010d7010b5e9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 672, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_001883c96c6f4644672acdea4d262848(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 32, 100], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f8079b1546954a0de68d082173b56fe1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 640, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d7805fdbd9ebac60aa3ec08d9e6224f9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1280, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b94beeb62600b02926793987f4e3ec6c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 320, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8cc3eb9723866708e4398e370127417a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 118, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[118], dtype='float32'),
            paddle.static.InputSpec(shape=[118], dtype='float32'),
            paddle.static.InputSpec(shape=[118], dtype='float32'),
            paddle.static.InputSpec(shape=[118], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_754f3b2eceb3c969cafb4bcbd526a9be(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 576, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f13d91383492b6e76763077e02aa2a30(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 14, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5f1ffb74d8fc89de617e0bc961f4e6da(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 232, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[232], dtype='float32'),
            paddle.static.InputSpec(shape=[232], dtype='float32'),
            paddle.static.InputSpec(shape=[232], dtype='float32'),
            paddle.static.InputSpec(shape=[232], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5d9f2c3bc38fb6027110b24fa6671d9f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 175, 25], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6e736b36f0f4b87a9f4b5897e00b971d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1280, 6, 6], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3a21a55bb7927674d19430ed6366df76(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 8, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_00c9d6d672498715a61d2ae2a6f98c7d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 88, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4662f7dbf1bfba37b6e568739b9af629(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 208, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c1d8828d7848d83703110219cca6a1f7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 8, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_344329caff1bca5104933b5553a8f640(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 38, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[38], dtype='float32'),
            paddle.static.InputSpec(shape=[38], dtype='float32'),
            paddle.static.InputSpec(shape=[38], dtype='float32'),
            paddle.static.InputSpec(shape=[38], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7f27863bb556262ce9ca842c1183b000(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 44, 40, 40], dtype='float16'),
            paddle.static.InputSpec(shape=[44], dtype='float32'),
            paddle.static.InputSpec(shape=[44], dtype='float32'),
            paddle.static.InputSpec(shape=[44], dtype='float32'),
            paddle.static.InputSpec(shape=[44], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5548a634096bdea06c688a80376ff53e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 35, 35], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3a06f1475253de64e3094a1f5e5fc06e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 10, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[10], dtype='float32'),
            paddle.static.InputSpec(shape=[10], dtype='float32'),
            paddle.static.InputSpec(shape=[10], dtype='float32'),
            paddle.static.InputSpec(shape=[10], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_214efea8c0e6c920f8a4b8780c5b7a4b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2e4bcdc5bafdc514906583519befe93c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 704, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_30ab71bcbd86487bbd826b41f391c7ee(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 28, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d1427fd56e8a37dcd84075fe241cdac1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 350, 25], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e0f6ac9b4958357415fc8b72ec5ef4ad(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 1, 3], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2df9b1c06f9256f8108b4a1143df35e1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 2, 6], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_92b620aef5acafad4c0d157ef777b660(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 8, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b192f3bed431296b739a7e68dd3cf5b3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 8, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b88bdd4822487797778819bf35905a09(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 16, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9e5a434e12da2696a9f9d868b9f45477(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 53, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[53], dtype='float32'),
            paddle.static.InputSpec(shape=[53], dtype='float32'),
            paddle.static.InputSpec(shape=[53], dtype='float32'),
            paddle.static.InputSpec(shape=[53], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_049f5fe343bd1b155b9420ba95b6c506(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_be4e7ef61b88705310d92d4ae3c098fe(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 640, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ec15b4f365883c06a3a1e295ec9f5f69(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c3df70fd4a718b503a21bca5f0fa4598(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 960, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f7a00d309a2128ebc111858770c69ce9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0449ac4c00d9aa171a40fc10cc3bfa5a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, 100], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_16a6edfa5b6409b415d09ad05808ddc5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_601deb1983c5cb746610281b55124c97(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_98d6280b617eb479b02b2ca845c2b604(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 300, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[300], dtype='float32'),
            paddle.static.InputSpec(shape=[300], dtype='float32'),
            paddle.static.InputSpec(shape=[300], dtype='float32'),
            paddle.static.InputSpec(shape=[300], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_aff0a1d31bbbdf620cbc6dfa93b5124e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_61880af5b4bf23d7514c17811f77f734(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 512, 8, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4e20ab64f4cf542c00878428292e57de(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 8, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_45cb315e383525e33a9df82784065670(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 26, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[26], dtype='float32'),
            paddle.static.InputSpec(shape=[26], dtype='float32'),
            paddle.static.InputSpec(shape=[26], dtype='float32'),
            paddle.static.InputSpec(shape=[26], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e6281efc4237d516bf62349b005066d3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 512, 8, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e2e914dd9ce790e1723068f6c824bad3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1568, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1568], dtype='float32'),
            paddle.static.InputSpec(shape=[1568], dtype='float32'),
            paddle.static.InputSpec(shape=[1568], dtype='float32'),
            paddle.static.InputSpec(shape=[1568], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dadc711cf95ce5c6825bd88d9a162838(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1120, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1120], dtype='float32'),
            paddle.static.InputSpec(shape=[1120], dtype='float32'),
            paddle.static.InputSpec(shape=[1120], dtype='float32'),
            paddle.static.InputSpec(shape=[1120], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c7d2d72869661ab929394d1ec970ae31(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 30, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_463237fde79324bae936444e62ab79a1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 70, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d03669759404c17eea5fbe0fd1605b35(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1104, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1104], dtype='float32'),
            paddle.static.InputSpec(shape=[1104], dtype='float32'),
            paddle.static.InputSpec(shape=[1104], dtype='float32'),
            paddle.static.InputSpec(shape=[1104], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3e2e5292c99d809a2b77c71c9a22725d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 288, 31, 31], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ee81696f9229deba291d9f8d23cd674d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 512, 2, 2], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5cf4d4b147717d00b0b75a61caac84ce(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 8, 25], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ba5d7d0fced3c68f4e0a7a428e5b7ba9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 8, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7f9778ea4534f3fc63e1557084e6046b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1120, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1120], dtype='float32'),
            paddle.static.InputSpec(shape=[1120], dtype='float32'),
            paddle.static.InputSpec(shape=[1120], dtype='float32'),
            paddle.static.InputSpec(shape=[1120], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a22ef7c70c72a5961e472030518977ff(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 31, 31], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bf4172b5e8d85da0334345b3af72fd78(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 48, 160], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ecff041ed4f2865598c13f6893c95147(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 320, 320], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3a77ec7dc329722caafa62532aed3870(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 304, 304], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0fc98f3322a6295ddc05c1843662f7d0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 1, 26], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b6a497dbe1ffae0c9659fed8feba17d2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 960, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_23134b2b217e82c7d766d0ca62787db4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 150, 150], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2f0e1860449b1b7ebd00d0c0044deec2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 320, 6, 6], dtype='float16'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d7596fea3bbd1ad27288e9ee160c14c4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1728, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1728], dtype='float32'),
            paddle.static.InputSpec(shape=[1728], dtype='float32'),
            paddle.static.InputSpec(shape=[1728], dtype='float32'),
            paddle.static.InputSpec(shape=[1728], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f9d88b6dda9e692e5c8155ab0c881f8c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 32, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ca1c2938c7fadc96b0dc8dfa9209c1c9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 82, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_70f635c2e3f70bc2b9b030d020760420(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 58, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[58], dtype='float32'),
            paddle.static.InputSpec(shape=[58], dtype='float32'),
            paddle.static.InputSpec(shape=[58], dtype='float32'),
            paddle.static.InputSpec(shape=[58], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d1eff69a71e3ffa184d13ff92f3df2ab(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 4, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f8cdcba81ed75913b22b320109803f41(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 704, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bf527dc1fcf1a6fd8a5f4eaf7405ac3e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 72, 122, 122], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6b99617946dbcbd795bd7875ad9ea3de(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 122, 122], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fac92df0b62c9b0932a8ac0aa1ab203e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 149, 149], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d817aba74df7b35ce1e36288368ef6b4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1440, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1440], dtype='float32'),
            paddle.static.InputSpec(shape=[1440], dtype='float32'),
            paddle.static.InputSpec(shape=[1440], dtype='float32'),
            paddle.static.InputSpec(shape=[1440], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_19eafe3a5d3f9df80de4387e50945935(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2112, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[2112], dtype='float32'),
            paddle.static.InputSpec(shape=[2112], dtype='float32'),
            paddle.static.InputSpec(shape=[2112], dtype='float32'),
            paddle.static.InputSpec(shape=[2112], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_903c56f67d56a4bf30536590ac5a0891(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_16756718fec4c09e92352396baa2db01(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1920, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1920], dtype='float32'),
            paddle.static.InputSpec(shape=[1920], dtype='float32'),
            paddle.static.InputSpec(shape=[1920], dtype='float32'),
            paddle.static.InputSpec(shape=[1920], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6cf4c086dbf6fa4fa6d2360431a57ca3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 432, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_316a5c1bf1092b7bd8bd6dd93656af77(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 270, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[270], dtype='float32'),
            paddle.static.InputSpec(shape=[270], dtype='float32'),
            paddle.static.InputSpec(shape=[270], dtype='float32'),
            paddle.static.InputSpec(shape=[270], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fa5e018a7eef8e787a8176b3d388e876(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_43f623d01e33e4dd3ada01998e069b88(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 17, 17], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_69ec786644c8aff47383c886f382f69b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 1, 2], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_11949321c9995b8a52b792790afc1e4b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 236, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[236], dtype='float32'),
            paddle.static.InputSpec(shape=[236], dtype='float32'),
            paddle.static.InputSpec(shape=[236], dtype='float32'),
            paddle.static.InputSpec(shape=[236], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_78b820bce8ae8b83dfbfd35bf290faf9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 38, 38], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dfd191ac4d5b9dbeaa0c4a7641e92bf5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1008, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1008], dtype='float32'),
            paddle.static.InputSpec(shape=[1008], dtype='float32'),
            paddle.static.InputSpec(shape=[1008], dtype='float32'),
            paddle.static.InputSpec(shape=[1008], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8b71e647fe0089f035928ef765b0f2b4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 147, 147], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0e0a708a01ea26658a7494dbfc156383(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 960, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_67a3ffd7441e4f21b97d12c4ee68c34a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2016, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[2016], dtype='float32'),
            paddle.static.InputSpec(shape=[2016], dtype='float32'),
            paddle.static.InputSpec(shape=[2016], dtype='float32'),
            paddle.static.InputSpec(shape=[2016], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dbd626ea8f6cd8bc2426556814b8ba10(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 720, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9bca70962b358c3795b4a1775aa1749e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 6, 6], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_445ddf41551001cd9d5b69548dc48794(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 32, 100], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_be55e6b7bc3740624a173e767643bfa0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 17, 17], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_66848f63380fe3caf878efaf5f970000(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, 56, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8662976d923fc316f42c68b348aac645(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2160, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[2160], dtype='float32'),
            paddle.static.InputSpec(shape=[2160], dtype='float32'),
            paddle.static.InputSpec(shape=[2160], dtype='float32'),
            paddle.static.InputSpec(shape=[2160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1864570f21287a00d91d55c94bd03f33(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 3, 3], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_43c0695f75d5141303c5c0768d82a893(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 4, 12], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a3bd1bafc24393f61078257ba8e253d7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 128, 128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0b78a4c3f6f276413f0a6ddcb7b8c12f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1376, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1376], dtype='float32'),
            paddle.static.InputSpec(shape=[1376], dtype='float32'),
            paddle.static.InputSpec(shape=[1376], dtype='float32'),
            paddle.static.InputSpec(shape=[1376], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_87b6cbb642368169a0a6a927e0de77b2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1600, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1600], dtype='float32'),
            paddle.static.InputSpec(shape=[1600], dtype='float32'),
            paddle.static.InputSpec(shape=[1600], dtype='float32'),
            paddle.static.InputSpec(shape=[1600], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cab09c7c4fa56e2488ea07ea43375fe3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1760, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1760], dtype='float32'),
            paddle.static.InputSpec(shape=[1760], dtype='float32'),
            paddle.static.InputSpec(shape=[1760], dtype='float32'),
            paddle.static.InputSpec(shape=[1760], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bafe67d17663c70b06eb0ebcf0421bd9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1968, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1968], dtype='float32'),
            paddle.static.InputSpec(shape=[1968], dtype='float32'),
            paddle.static.InputSpec(shape=[1968], dtype='float32'),
            paddle.static.InputSpec(shape=[1968], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e10ff6dc7d9b5018dce6730c897ea458(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 174, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[174], dtype='float32'),
            paddle.static.InputSpec(shape=[174], dtype='float32'),
            paddle.static.InputSpec(shape=[174], dtype='float32'),
            paddle.static.InputSpec(shape=[174], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_25ecfef120be3bf167579a0043641a4e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1536, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3ba232b3e2066a7ca07d9e9f577e151a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 528, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[528], dtype='float32'),
            paddle.static.InputSpec(shape=[528], dtype='float32'),
            paddle.static.InputSpec(shape=[528], dtype='float32'),
            paddle.static.InputSpec(shape=[528], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e4be4db5ae01ae41df990d1e11794aa8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 19, 19], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7bac455786c9c12769ec77051cc472e1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_51f2ac2cd50c5d1639b2f39112e37803(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 4, 12], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1f87c021f4db14462734097720e6b742(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 208, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6ec6c331c3423d959a4fcd62b6abf104(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1200, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6275e405292e7be448c4e45a93e79d9b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 224, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[224], dtype='float32'),
            paddle.static.InputSpec(shape=[224], dtype='float32'),
            paddle.static.InputSpec(shape=[224], dtype='float32'),
            paddle.static.InputSpec(shape=[224], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_edbe30540ec177cce554ce48aac27083(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1280, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1d1a1451341812987def2e844afb08fd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 336, 2, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_df0a557d314c037d833d2be6f20c411f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 50, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[50], dtype='float32'),
            paddle.static.InputSpec(shape=[50], dtype='float32'),
            paddle.static.InputSpec(shape=[50], dtype='float32'),
            paddle.static.InputSpec(shape=[50], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f1e8941418a2f8bb9b0465e9d839d2ef(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 366, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[366], dtype='float32'),
            paddle.static.InputSpec(shape=[366], dtype='float32'),
            paddle.static.InputSpec(shape=[366], dtype='float32'),
            paddle.static.InputSpec(shape=[366], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a39b08a6d0b8c8bb33c0aae736ac2e0f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1280, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_754aecdb21f90647937ecba1c265a35c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c1bd354b6b2279703032a8552648a54f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 4, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6aa048fb1689e5cd251f916ed4cda099(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 17, 17], dtype='float16'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_32887b43f5b58adac22b860128eae73c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 112, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_517c329316f36622f36ec2c73e504025(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 14, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1a961731f39b2fa32ac9da1944219062(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 95, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[95], dtype='float32'),
            paddle.static.InputSpec(shape=[95], dtype='float32'),
            paddle.static.InputSpec(shape=[95], dtype='float32'),
            paddle.static.InputSpec(shape=[95], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1006a06063d0d7187b8ed43c448efc37(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 16, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d2edb31d7ed5044dc395a2ce8ae41db4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4f90e3704bca0bff852ea5a203b4b63b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 32, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fcd0ef96fde17bfe36d316834ad81acf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7e483b644adfab6a13faad290fd32bc8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1392, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1392], dtype='float32'),
            paddle.static.InputSpec(shape=[1392], dtype='float32'),
            paddle.static.InputSpec(shape=[1392], dtype='float32'),
            paddle.static.InputSpec(shape=[1392], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b5f7017c2ff4851109f25ef4646d6508(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 81, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[81], dtype='float32'),
            paddle.static.InputSpec(shape=[81], dtype='float32'),
            paddle.static.InputSpec(shape=[81], dtype='float32'),
            paddle.static.InputSpec(shape=[81], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3307b0fc511040ca02006ee72fb33cd8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 720, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9e98ccad392217803a8c7c0fa7e146a9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2208, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[2208], dtype='float32'),
            paddle.static.InputSpec(shape=[2208], dtype='float32'),
            paddle.static.InputSpec(shape=[2208], dtype='float32'),
            paddle.static.InputSpec(shape=[2208], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5f6cb4dd98a786871b8712340ce32611(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2688, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[2688], dtype='float32'),
            paddle.static.InputSpec(shape=[2688], dtype='float32'),
            paddle.static.InputSpec(shape=[2688], dtype='float32'),
            paddle.static.InputSpec(shape=[2688], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6f5027e141674adef965d45913bf3e82(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 96, 96], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d49a26835e4478a1258aea1911ea27d4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 72, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_caa476a83ca48a09742ad8c422eaae6b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 336, 2, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bdfecce40297b641425a981e6aa610fb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1792, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1792], dtype='float32'),
            paddle.static.InputSpec(shape=[1792], dtype='float32'),
            paddle.static.InputSpec(shape=[1792], dtype='float32'),
            paddle.static.InputSpec(shape=[1792], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8f3d8a328339bec099dcaac963428d56(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1888, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1888], dtype='float32'),
            paddle.static.InputSpec(shape=[1888], dtype='float32'),
            paddle.static.InputSpec(shape=[1888], dtype='float32'),
            paddle.static.InputSpec(shape=[1888], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fc756a9190ee566e71ff2d981c920b58(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 672, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1716243e54b0b8be76f839f145a0844c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 16, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d740db72a5878b910e548c0335883331(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 1, 2], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1f323dba5e9c868990c2fd1d31f924e8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 32, 256, 512], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_024d3110644a58f47c54fe0c5e6f8dff(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ec29c855527613d9aa5cb61230da837f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 44, 80, 80], dtype='float16'),
            paddle.static.InputSpec(shape=[44], dtype='float32'),
            paddle.static.InputSpec(shape=[44], dtype='float32'),
            paddle.static.InputSpec(shape=[44], dtype='float32'),
            paddle.static.InputSpec(shape=[44], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_972b3a4697e46b3ca787702f45ce6c14(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 72, 61, 61], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_566aec332f1857bb88d57c37943246f9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1200, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8c13024078fbf85f169eb5a282e12cc1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 288, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e528e1003f083ad06760d210da39d461(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 8, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b7c85a042e09774cd876c6c793c9f2f9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1392, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1392], dtype='float32'),
            paddle.static.InputSpec(shape=[1392], dtype='float32'),
            paddle.static.InputSpec(shape=[1392], dtype='float32'),
            paddle.static.InputSpec(shape=[1392], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d84f6f9a89ad1fa3a5427852edccd2f5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 320, 320], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_263cd5f275bc1c540115cfb0253257d0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_484b9a7cf89e7e836bef6dfb32fc77f5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_faab88388cbfec387fbee6d3da415e38(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2208, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[2208], dtype='float32'),
            paddle.static.InputSpec(shape=[2208], dtype='float32'),
            paddle.static.InputSpec(shape=[2208], dtype='float32'),
            paddle.static.InputSpec(shape=[2208], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_825e97677a5e98b9abcd3a54e281a971(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 140, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[140], dtype='float32'),
            paddle.static.InputSpec(shape=[140], dtype='float32'),
            paddle.static.InputSpec(shape=[140], dtype='float32'),
            paddle.static.InputSpec(shape=[140], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_07db224a34ba25292369d1e9d0d61db2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 32, None], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None




if __name__ == '__main__':
    unittest.main()