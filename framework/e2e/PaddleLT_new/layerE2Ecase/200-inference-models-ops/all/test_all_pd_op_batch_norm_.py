import os
os.environ['FLAGS_cinn_new_group_scheduler'] = '1'
os.environ['FLAGS_group_schedule_tiling_first'] = '1'
os.environ['FLAGS_enable_pir_api'] = '1'
os.environ['FLAGS_cinn_bucket_compile'] = '1'
import sys
import unittest
import numpy as np
from dataclasses import dataclass
import typing as t
import itertools

@dataclass
class Stage:
    name: str
    env_vars: t.Dict[str, str]

cinn_stages = [
    Stage(
        name="dynamic_to_static",
        env_vars=dict(
            PADDLE_DEBUG_ENABLE_CINN=False,
            FLAGS_prim_all=False,
            FLAGS_prim_enable_dynamic=False,
        ),
    ),
    Stage(
        name="prim",
        env_vars=dict(
            PADDLE_DEBUG_ENABLE_CINN=False,
            FLAGS_prim_all=True,
            FLAGS_prim_enable_dynamic=True,
        ),
    ),
    Stage(
        name="infer_symbolic",
        env_vars=dict(
            PADDLE_DEBUG_ENABLE_CINN=False,
            FLAGS_prim_all=True,
            FLAGS_prim_enable_dynamic=True,
            FLAGS_use_cinn=False,
            FLAGS_check_infer_symbolic=True,
        ),
    ),
	Stage(
        name="frontend",
        env_vars=dict(
            PADDLE_DEBUG_ENABLE_CINN=True,
            FLAGS_prim_all=True,
            FLAGS_prim_enable_dynamic=True,
            FLAGS_use_cinn=True,
            FLAGS_check_infer_symbolic=False,
            FLAGS_enable_fusion_fallback=True,
        ), 
    ),
    Stage(
        name="backend",
        env_vars=dict(
            PADDLE_DEBUG_ENABLE_CINN=True,
            FLAGS_prim_all=True,
            FLAGS_prim_enable_dynamic=True,
            FLAGS_use_cinn=True,
            FLAGS_check_infer_symbolic=False,
            FLAGS_enable_fusion_fallback=False,
        ), 
    ),
]

def GetCinnStageByName(name):
    for stage in cinn_stages:
        if stage.name == name:
            return stage
    return None

def GetCurrentCinnStage():
    name = os.getenv('PADDLE_DEBUG_CINN_STAGE_NAME')
    if name is None:
        return None
    stage_names = [stage.name for stage in cinn_stages]
    assert name in stage_names, (
        f"PADDLE_DEBUG_CINN_STAGE_NAME should be in {stage_names}"
    )
    return GetCinnStageByName(name)

def GetPrevCinnStage(stage):
    for i in range(1, len(cinn_stages)):
        if stage is cinn_stages[i]:
            return cinn_stages[i - 1]
    return None

def IsCinnStageEnableDiff():
    value = os.getenv('PADDLE_DEBUG_CINN_STAGE_ENABLE_DIFF')
    enabled = value in {
        '1',
        'true',
        'True',
    }
    if enabled:
        assert GetCurrentCinnStage() is not None
    return enabled

def GetExitCodeAndStdErr(cmd, env):
    env = {
        k:v
        for k, v in env.items()
        if v is not None
    }
    import subprocess
    result = subprocess.run(
        cmd,
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        text=True,
        env=env,
    )
    return result.returncode, result.stderr

def GetStageExitCodeAndStdErr(stage):
    return GetExitCodeAndStdErr(
        [sys.executable, __file__],
        env=dict(
            PADDLE_DEBUG_CINN_STAGE_NAME=stage.name,
            PADDLE_DEBUG_CINN_STAGE_ENABLE_DIFF='0',
            PYTHONPATH=os.getenv('PYTHONPATH'),
            ATHENA_ENABLE_TRY_RUN="False",
        ),
    )

def AthenaTryRunEnabled():
    return os.getenv('ATHENA_ENABLE_TRY_RUN') not in {
        "0",
        "False",
        "false",
        "OFF"
    }

def GetNeedSkipAndSkipMessage():
    current_stage = GetCurrentCinnStage()
    assert current_stage is not None
    if not IsCinnStageEnableDiff():
        return False, ""
    last_stage = GetPrevCinnStage(current_stage)
    if last_stage is None:
        return False, ""
    exitcode, stderr = GetStageExitCodeAndStdErr(last_stage)
    if exitcode != 0:
        return True, "last stage failed."
    return False, ""

def GetCurrentStageTryRunExitCodeAndStdErr():
    if not AthenaTryRunEnabled():
        return False, ""
    current_stage = GetCurrentCinnStage()
    assert current_stage is not None
    return GetStageExitCodeAndStdErr(current_stage)

def SetDefaultEnv(**env_var2value):
    for env_var, value in env_var2value.items():
        if os.getenv(env_var) is None:
            os.environ[env_var] = str(value)

SetDefaultEnv(
    PADDLE_DEBUG_CINN_STAGE_NAME="backend",
    PADDLE_DEBUG_CINN_STAGE_ENABLE_DIFF=False,
    PADDLE_DEBUG_ENABLE_CINN=True,
    FLAGS_enable_pir_api=True,
    FLAGS_prim_all=True,
    FLAGS_prim_enable_dynamic=True,
    FLAGS_use_cinn=False,
    FLAGS_check_infer_symbolic=False,
    FLAGS_enable_fusion_fallback=False,
)

import paddle

def SetEnvVar(env_var2value):
    for env_var, value in env_var2value.items():
        os.environ[env_var] = str(value)
    paddle.set_flags({
        env_var:value
        for env_var, value in env_var2value.items()
        if env_var.startswith('FLAGS_')
    })

if GetCurrentCinnStage() is not None:
    SetEnvVar(GetCurrentCinnStage().env_vars)

def GetEnvVarEnableJit():
    enable_jit = os.getenv('PADDLE_DEBUG_ENABLE_JIT')
    return enable_jit not in {
        "0",
        "False",
        "false",
        "OFF",
    }

def GetEnvVarEnableCinn():
    enable_cinn = os.getenv('PADDLE_DEBUG_ENABLE_CINN')
    if enable_cinn is None:
        return True
    return enable_cinn not in {
        "0",
        "False",
        "false",
        "OFF",
    }


def GetTolerance(dtype):
    if dtype == np.float16:
        return GetFloat16Tolerance()
    if dtype == np.float32:
        return GetFloat32Tolerance()
    return 1e-6

def GetFloat16Tolerance():
    try:
        return float(os.getenv('PADDLE_DEBUG_FLOAT16_TOL'))
    except:
        return 1e-3

def GetFloat32Tolerance():
    try:
        return float(os.getenv('PADDLE_DEBUG_FLOAT32_TOL'))
    except:
        return 1e-6

def IsInteger(dtype):
    return np.dtype(dtype).char in np.typecodes['AllInteger']

def ApplyToStatic(net, use_cinn):
    build_strategy = paddle.static.BuildStrategy()
    build_strategy.build_cinn_pass = use_cinn
    return paddle.jit.to_static(
        net,
        input_spec=net.get_input_spec(),
        build_strategy=build_strategy,
        full_graph=True,
    )

class InstanceTrait:

    @classmethod
    def instance(cls):
        if cls.instance_ is None:
            cls.instance_ = cls()
        return cls.instance_

    @classmethod
    def static_instance_with_cinn(cls):
        if cls.static_instance_with_cinn_ is None:
            cls.static_instance_with_cinn_ = ApplyToStatic(
                cls.instance(),
                use_cinn=True
            )
        return cls.static_instance_with_cinn_

    @classmethod
    def static_instance_without_cinn(cls):
        if cls.static_instance_without_cinn_ is None:
            cls.static_instance_without_cinn_ = ApplyToStatic(
                cls.instance(),
                use_cinn=False
            )
        return cls.static_instance_without_cinn_


class CinnTestBase:

    def setUp(self):
        paddle.seed(2024)
        self.prepare_data()

    def _test_entry(self):
        dy_outs = self.train(use_cinn=False)
        cinn_outs = self.train(use_cinn=GetEnvVarEnableCinn())

        for cinn_out, dy_out in zip(cinn_outs, dy_outs):
          if type(cinn_out) is list and type(dy_out) is list:
            for x, y in zip(cinn_out, dy_out):
              self.assert_all_close(x, y)
          else:
            self.assert_all_close(cinn_out, dy_out)

    def train(self, use_cinn):
        if GetEnvVarEnableJit():
            net = self.prepare_static_net(use_cinn)
        else:
            net = self.prepare_net()
        paddle.seed(2024)
        out = net(*self.inputs)
        return out
    
    def prepare_data(self):
        self.inputs = self.get_inputs()
        for input in self.inputs:
            input.stop_gradient = True

    def prepare_net(self):
        return self.get_test_class().instance()

    def prepare_static_net(self, use_cinn):
        if use_cinn:
            return self.get_test_class().static_instance_with_cinn()
        else:
            return self.get_test_class().static_instance_without_cinn()

    def assert_all_close(self, x, y):
        if (hasattr(x, "numpy") and hasattr(y, "numpy")):
            x_numpy = x.numpy()
            y_numpy = y.numpy()
            assert x_numpy.dtype == y_numpy.dtype
            if IsInteger(x_numpy.dtype):
                np.testing.assert_equal(x_numpy, y_numpy)
            else:
                tol = GetTolerance(x_numpy.dtype)
                np.testing.assert_allclose(x_numpy, y_numpy, atol=tol, rtol=tol)
        else:
            assert x == y





need_skip, skip_message = GetNeedSkipAndSkipMessage()
try_run_exit_code, try_run_stderr = GetCurrentStageTryRunExitCodeAndStdErr()
class TestTryRun(unittest.TestCase):
    def test_panic(self):
        if not AthenaTryRunEnabled():
            return
        if try_run_exit_code == 0:
            # All unittest cases passed.
            return
        if try_run_exit_code > 0:
            # program failed but not panic.
            return
        # program panicked.
        kOutputLimit = 65536
        message = try_run_stderr[-kOutputLimit:]
        raise RuntimeError(f"panicked. last {kOutputLimit} characters of stderr: \n{message}")
class PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c72ca6b7bf6dfe647f72f954b138fe92(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 175, 25], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d3eeb39e5f446c318b345b5858d89ea0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 8, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bf87ae2f597cb157cf84607637714ea8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9169a6687477bfda78bb7973d02820e1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 6, 40], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_1a2b114b88ba56b95370f75ce469d256(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_85086989d63185813228fb58ce28c230(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 1152, 6, 6], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_20a4151a7d8d2f84eda8efbfc3dd3382(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 80, 80], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e59e5e3b4e3af3a1769b356c33d1f663(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4a1daf224cc9aa206c6a22947a5ffcb4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.33507856726646423, 0.06761960685253143, 0.29248911142349243, 0.4999416768550873, 0.1009838655591011, 0.16835057735443115, 0.4991321861743927, 0.3252480626106262, 0.4265548288822174, 0.04473154619336128, 0.27504172921180725, 0.19786570966243744, 0.27618280053138733, 0.3722383379936218, 0.3119148313999176, 0.11653654277324677, 0.3974457383155823, 0.061593007296323776, 0.3041267693042755, 0.46964389085769653], dtype='float32').reshape([20]),
            paddle.to_tensor([0.3443640470504761, 0.47980597615242004, 0.10106723755598068, 0.19131627678871155, 0.2563283145427704, 0.24880653619766235, 0.310609370470047, 0.330210417509079, 0.3167042136192322, 0.371894508600235, 0.1366625428199768, 0.16205915808677673, 0.23158250749111176, 0.4695725739002228, 0.09578944742679596, 0.023290475830435753, 0.39161214232444763, 0.49595993757247925, 0.11508858948945999, 0.31202301383018494], dtype='float32').reshape([20]),
            paddle.to_tensor([0.16003206372261047, 0.4263792335987091, 0.47606489062309265, 0.4929846525192261, 0.31584495306015015, 0.4079931974411011, 0.1931810826063156, 0.3218790292739868, 0.4934626519680023, 0.40331146121025085, 0.258558452129364, 0.3173674941062927, 0.23131507635116577, 0.08228909224271774, 0.46402788162231445, 0.4067714810371399, 0.3614209294319153, 0.11529466509819031, 0.035579077899456024, 0.39298003911972046], dtype='float32').reshape([20]),
            paddle.to_tensor([0.2029818445444107, 0.006888680160045624, 0.10161563754081726, 0.019735774025321007, 0.4317803382873535, 0.4939731955528259, 0.3655456304550171, 0.36449629068374634, 0.15391255915164948, 0.37670981884002686, 0.13894417881965637, 0.13018500804901123, 0.03314916044473648, 0.03405546024441719, 0.061866700649261475, 0.20049381256103516, 0.2470446676015854, 0.1445508599281311, 0.46814894676208496, 0.044746920466423035], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e2d33363d9afc16b2454312d896c2ae0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 640, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d4fbcd5b27dbea3c1968dde2513e766d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 120, 120], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_44f098eda57946d262b4a5cfb3757d4b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 972, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([972], dtype='float32', min=0, max=0.5),
            paddle.uniform([972], dtype='float32', min=0, max=0.5),
            paddle.uniform([972], dtype='float32', min=0, max=0.5),
            paddle.uniform([972], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cfad5db87791c62f91fd9d9604dfc71c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 104, 4, 50], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_4497fcdfb9639144ea558daa18214543(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ecb229fd8c426dbfbfa433c986e2b5da(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 26, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.32039302587509155, 0.21429094672203064, 0.29628852009773254, 0.3690859079360962, 0.3149014711380005, 0.4054926335811615, 0.12180899828672409, 0.2935817241668701, 0.19377544522285461, 0.03933677449822426, 0.2870637774467468, 0.2091011255979538, 0.08351987600326538, 0.4109583795070648, 0.36055177450180054, 0.1802029311656952, 0.29351744055747986, 0.31965556740760803, 0.4520357549190521, 0.32826995849609375, 0.4108860194683075, 0.38825222849845886, 0.25121617317199707, 0.3399200439453125, 0.2463887631893158, 0.21925656497478485], dtype='float32').reshape([26]),
            paddle.to_tensor([0.026467226445674896, 0.23443976044654846, 0.2041643261909485, 0.421880304813385, 0.4995454251766205, 0.056549686938524246, 0.19676810503005981, 0.12520265579223633, 0.36378172039985657, 0.3170706629753113, 0.3535187840461731, 0.4887053072452545, 0.44272086024284363, 0.16344527900218964, 0.10381761193275452, 0.005563311744481325, 0.3882574439048767, 0.19543297588825226, 0.4247145652770996, 0.4348045289516449, 0.11461826413869858, 0.27663731575012207, 0.4579735994338989, 0.425560861825943, 0.1339256912469864, 0.036733657121658325], dtype='float32').reshape([26]),
            paddle.to_tensor([0.3298141658306122, 0.155882328748703, 0.18098869919776917, 0.035560280084609985, 0.266539067029953, 0.25725990533828735, 0.4122242331504822, 0.07419359683990479, 0.28032761812210083, 0.08232825994491577, 0.3341257870197296, 0.04222169145941734, 0.34173616766929626, 0.39514896273612976, 0.06553198397159576, 0.22356872260570526, 0.2743796408176422, 0.1351151317358017, 0.48064538836479187, 0.478583961725235, 0.41790205240249634, 0.023083681240677834, 0.47293633222579956, 0.036609046161174774, 0.3696824014186859, 0.0964118093252182], dtype='float32').reshape([26]),
            paddle.to_tensor([0.10561100393533707, 0.022752685472369194, 0.11221975088119507, 0.2867611050605774, 0.3213505148887634, 0.1141863465309143, 0.305376261472702, 0.46367958188056946, 0.11461435258388519, 0.13007447123527527, 0.3512367010116577, 0.0689505934715271, 0.43737322092056274, 0.08174065500497818, 0.12831899523735046, 0.19058138132095337, 0.07769280672073364, 0.0246881190687418, 0.14110282063484192, 0.3999466001987457, 0.2737349271774292, 0.3663480877876282, 0.10010642558336258, 0.2821538746356964, 0.07565581053495407, 0.01178661733865738], dtype='float32').reshape([26]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5cd3ff384bbbf84383f00198f4a1d399(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7e58733432019e545cb4a40927925308(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 480, 20, 20], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2b470e817e801a61466d52708374cb2f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 384, 8, 8], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ad0d2524e0aedb228de6741698de9592(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 160, 17, 17], dtype='float16', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e2e55b2214eeff4b7df8046b485348f7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1ccb5464a2502be094df9404274e1449(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 104, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b9f2630dedb605cb6cc4d1e88a6fabd0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e955537f4e8f61f85f209244cfb2b955(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 160, 8, 6], dtype='float16', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b11522da1514d39c31ec9e60418e62d8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 60, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b46042fd4384181bc348ef81c010d449(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 80, 8, 6], dtype='float16', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_eaa680a4ae5cdb85f42a36941b55ea3b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_698bd6cc55718cd16a7339f89d66295e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 28, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.15596985816955566, 0.19923488795757294, 0.46940508484840393, 0.015779804438352585, 0.35426387190818787, 0.05669037997722626, 0.4634357988834381, 0.18751674890518188, 0.21598750352859497, 0.493657648563385, 0.10840488225221634, 0.24904395639896393, 0.05876282975077629, 0.2143968790769577, 0.3103044927120209, 0.1863747388124466, 0.19540707767009735, 0.14677082002162933, 0.12172212451696396, 0.24858875572681427, 0.09357002377510071, 0.44108644127845764, 0.06195606663823128, 0.4632985591888428, 0.24162130057811737, 0.15099230408668518, 0.4810006022453308, 0.20560869574546814], dtype='float32').reshape([28]),
            paddle.to_tensor([0.14118318259716034, 0.12091069668531418, 0.23424749076366425, 0.17631132900714874, 0.03622374311089516, 0.32301124930381775, 0.4345172941684723, 0.011474897153675556, 0.00048226845683529973, 0.1477353870868683, 0.23420380055904388, 0.04555302485823631, 0.1025773286819458, 0.4463709890842438, 0.2074495404958725, 0.2217244952917099, 0.34978964924812317, 0.44811558723449707, 0.32540252804756165, 0.49859777092933655, 0.44915443658828735, 0.16730044782161713, 0.4461135268211365, 0.25705137848854065, 0.3079332411289215, 0.49262920022010803, 0.3212755620479584, 0.4862320125102997], dtype='float32').reshape([28]),
            paddle.to_tensor([0.13297197222709656, 0.3922654390335083, 0.4748806953430176, 0.13068988919258118, 0.004981321282684803, 0.025578750297427177, 0.2187875658273697, 0.3692747950553894, 0.39848798513412476, 0.050921786576509476, 0.482249915599823, 0.3179645836353302, 0.21997855603694916, 0.23312446475028992, 0.25877612829208374, 0.36391252279281616, 0.0732031911611557, 0.44422268867492676, 0.21656276285648346, 0.37828320264816284, 0.03481181710958481, 0.19483420252799988, 0.2965160012245178, 0.03902783617377281, 0.09188985824584961, 0.318258672952652, 0.4911819100379944, 0.20803594589233398], dtype='float32').reshape([28]),
            paddle.to_tensor([0.09256651252508163, 0.40629902482032776, 0.07701848447322845, 0.32512369751930237, 0.05039185658097267, 0.18039454519748688, 0.030733885243535042, 0.34555110335350037, 0.24921271204948425, 0.23668210208415985, 0.1754574030637741, 0.22338129580020905, 0.28102585673332214, 0.07095135748386383, 0.4925655424594879, 0.03722509741783142, 0.14834938943386078, 0.43957018852233887, 0.3954264223575592, 0.4043861925601959, 0.2924308478832245, 0.20323075354099274, 0.4271925091743469, 0.28627681732177734, 0.35875383019447327, 0.46952417492866516, 0.18484745919704437, 0.4647041857242584], dtype='float32').reshape([28]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c1f33cf53455cb0ef3e1a8824fe888d0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7cf0e49397bc65bd9b722f9b03a00623(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 36, 64, 128], dtype='float16', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_1a8301b810bccba10e4dfdee0df73334(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a20578f63cd6d70174569ed4772d0682(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a8301b810bccba10e4dfdee0df73334
    def get_inputs(self):
        return [
            paddle.uniform([1, 1536, 10, 10], dtype='float32', min=0, max=0.5),
            paddle.uniform([1536], dtype='float32', min=0, max=0.5),
            paddle.uniform([1536], dtype='float32', min=0, max=0.5),
            paddle.uniform([1536], dtype='float32', min=0, max=0.5),
            paddle.uniform([1536], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_db302fc68674ced0050fd6a551693744(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c6bb5ff6fbadffe1bc7cbbdb76745b5a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 88, 25], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_aa8f3110f32d9b563fe6595090d52fbb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 104, 60, 60], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_dece02e7752889c830ec6c3a1475f093(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 72, 61, 61], dtype='float16', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_dbc805aaf9a3720fa287c2e5d6468b97(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 32, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9c681ed43e80a75a4ce3eac352fc99b4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 256, 512], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_f78910a24ab00d45797c31273495c7e0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-06'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0cfa176e260cfbf04d01e6196989c538(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_f78910a24ab00d45797c31273495c7e0
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 32, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7bcb910a9eadefbb5d2153b853a912fb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 256, 512], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.3906932473182678, 0.3419807553291321, 0.27170684933662415, 0.2754526436328888, 0.43299612402915955, 0.4208524823188782, 0.32953253388404846, 0.26408082246780396, 0.05438247695565224, 0.45711907744407654, 0.23833255469799042, 0.14778882265090942, 0.4345686435699463, 0.3736828565597534, 0.1423591524362564, 0.016046013683080673], dtype='float32').reshape([16]),
            paddle.to_tensor([0.21802233159542084, 0.106820248067379, 0.45645418763160706, 0.22929449379444122, 0.13495001196861267, 0.296045184135437, 0.037258099764585495, 0.46368470788002014, 0.31641241908073425, 0.34005269408226013, 0.2795400321483612, 0.005495348479598761, 0.2557440996170044, 0.2876608967781067, 0.09786196798086166, 0.03559981659054756], dtype='float32').reshape([16]),
            paddle.to_tensor([0.24329568445682526, 0.39307698607444763, 0.2918446958065033, 0.4841318428516388, 0.059247687458992004, 0.3584492802619934, 0.4031553864479065, 0.4226260781288147, 0.1677584946155548, 0.17162342369556427, 0.3342907726764679, 0.05631396919488907, 0.10959892719984055, 0.13314004242420197, 0.057220298796892166, 0.06030542775988579], dtype='float32').reshape([16]),
            paddle.to_tensor([0.4608815908432007, 0.03239161893725395, 0.4036647081375122, 0.2255781590938568, 0.2009294182062149, 0.24534764885902405, 0.1292496621608734, 0.358727365732193, 0.1052006259560585, 0.4289543330669403, 0.25805357098579407, 0.3682522475719452, 0.09616005420684814, 0.34238380193710327, 0.4371320605278015, 0.373757004737854], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b80ae4e260cda84a803e2503ecb4ef36(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 672, 12, 12], dtype='float16', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_af91295b59c5fca17f890a802deb001f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e7935fb5f6e1e17186487960492b3d7d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_af91295b59c5fca17f890a802deb001f
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 32, 128, 128], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.42913031578063965, 0.15458422899246216, 0.4732336699962616, 0.048330992460250854, 0.22981275618076324, 0.35763606429100037, 0.14884880185127258, 0.37787196040153503], dtype='float32').reshape([8]),
            paddle.to_tensor([0.33223116397857666, 0.3030010759830475, 0.47321927547454834, 0.3565017580986023, 0.1470089703798294, 0.25630712509155273, 0.01752488501369953, 0.0643274113535881], dtype='float32').reshape([8]),
            paddle.to_tensor([0.49487802386283875, 0.19221670925617218, 0.1621500700712204, 0.016387779265642166, 0.4046112895011902, 0.17211134731769562, 0.11205943673849106, 0.4975109100341797], dtype='float32').reshape([8]),
            paddle.to_tensor([0.30601802468299866, 0.39261355996131897, 0.15591444075107574, 0.3401355743408203, 0.2516828775405884, 0.1964005082845688, 0.49975350499153137, 0.06161962077021599], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c54541fac09d4234cd8280aeba8d76f0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 80, 80], dtype='float16', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_018c90d2406f056ca90ca40699938dc9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 480, 12, 12], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b7552288242c6de5dca0ca3d75b8ceb0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 96, 96], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0fa660e214c82bd2f39888752c00d5fe(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 384, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_08db05122c1b633a0530508462126b52(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 384, 8, 8], dtype='float16', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_14cd2430129ad120f4d011c17b1b697b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 320, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fe687a83c7395703e953e11f1301849f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_857de27eabe50121327e926513653d21(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.1561337560415268, 0.4198450446128845, 0.48557043075561523, 0.4480826258659363, 0.4598988890647888, 0.1379704475402832, 0.4435088336467743, 0.03503430262207985, 0.252540647983551, 0.23459658026695251, 0.35487455129623413, 0.4016493856906891, 0.07319872826337814, 0.16643555462360382, 0.45000895857810974, 0.12866328656673431, 0.3098887801170349, 0.037907086312770844, 0.032008204609155655, 0.3917423486709595, 0.14487963914871216, 0.34959906339645386, 0.3494601249694824, 0.19800005853176117, 0.0587458498775959, 0.33901846408843994, 0.3267306983470917, 0.2100602090358734, 0.41931474208831787, 0.2772577702999115], dtype='float32').reshape([30]),
            paddle.to_tensor([0.09835915267467499, 0.12697137892246246, 0.2947874665260315, 0.48698851466178894, 0.2739505469799042, 0.26396605372428894, 0.3653283417224884, 0.12488432973623276, 0.09979773312807083, 0.3368346691131592, 0.4599892199039459, 0.3345615267753601, 0.40380531549453735, 0.15486754477024078, 0.35910382866859436, 0.42672601342201233, 0.2241266965866089, 0.1535578817129135, 0.36801648139953613, 0.0939655676484108, 0.10730137676000595, 0.3066447973251343, 0.4974898397922516, 0.11469428241252899, 0.10265788435935974, 0.391178697347641, 0.40788784623146057, 0.3115549683570862, 0.1419544816017151, 0.297855019569397], dtype='float32').reshape([30]),
            paddle.to_tensor([0.05742528662085533, 0.4017871022224426, 0.23737670481204987, 0.32803818583488464, 0.3147636950016022, 0.4704972803592682, 0.205858051776886, 0.1904243528842926, 0.3871108591556549, 0.04155779257416725, 0.09185314923524857, 0.020202046260237694, 0.061236537992954254, 0.428046315908432, 0.20615950226783752, 0.4889623522758484, 0.009704817086458206, 0.05983889847993851, 0.10618270933628082, 0.15070191025733948, 0.08432566374540329, 0.48347795009613037, 0.34803783893585205, 0.11988383531570435, 0.25115567445755005, 0.270203560590744, 0.09187445789575577, 0.05702660605311394, 0.24160464107990265, 0.4536075294017792], dtype='float32').reshape([30]),
            paddle.to_tensor([0.2549937665462494, 0.39648595452308655, 0.38835856318473816, 0.2781898081302643, 0.39010271430015564, 0.31099939346313477, 0.4243471026420593, 0.15281930565834045, 0.14898869395256042, 0.24759851396083832, 0.40896591544151306, 0.2954632341861725, 0.3365277945995331, 0.40713462233543396, 0.3273932933807373, 0.3769010305404663, 0.43377891182899475, 0.08801788091659546, 0.13069556653499603, 0.32945531606674194, 0.18264521658420563, 0.1796504110097885, 0.25634127855300903, 0.1529497355222702, 0.13537292182445526, 0.271973192691803, 0.1297098994255066, 0.30886951088905334, 0.3737485706806183, 0.23250876367092133], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_3a47a120d62a3f3a422a747f511df8d9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a55e3b5f07afb933a82493b7f65d9409(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_3a47a120d62a3f3a422a747f511df8d9
    def get_inputs(self):
        return [
            paddle.uniform([1, 1152, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1d1109c16c316b9a965a4a94622f0c0c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 120, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e05b80f7b3a899460748f64ebe53c56d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9653a8c9658758f169e155e1ff65ac71(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 128, 128], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d785fee8ea8409b14dae99604fef72ae(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 80, 80], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8392266e22057d57d07b65bb0b93c6f8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cd9aae4fc180c83520e004d951968db9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_752a1e247095788c510518931c242ac8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 1, 1], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_98edef8dc64cc56432351b9e658c82c4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 672, 64, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4dbe1df24e24eacf815abd30731be171(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 64, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fde2b4dd08efa7bcbdf3d088fe09e9e3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 144, 16, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_41b6de854fd474036a513ea9a448d340(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 61, 61], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a90d1e665e9deb202f231cf0c3550e24(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 64, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c99fde2121ca9becd4ba587e9aaf98c4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_0edd578f48a07aa5d1d79f9f678b472e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d46c0342cca3aab8350dced2e0460753(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_0edd578f48a07aa5d1d79f9f678b472e
    def get_inputs(self):
        return [
            paddle.uniform([49, 512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_12de28554061a33e3c070c052e56e32b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b114937a1009f2830939d865db725b1d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 384, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8449c016b2c3f2928cc9dce74af05508(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 12, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.24434585869312286, 0.15415410697460175, 0.0809108316898346, 0.45760399103164673, 0.390928715467453, 0.28884366154670715, 0.10480910539627075, 0.017764940857887268, 0.32692256569862366, 0.04743872955441475, 0.23077437281608582, 0.3039674758911133], dtype='float32').reshape([12]),
            paddle.to_tensor([0.07073536515235901, 0.41643187403678894, 0.22931502759456635, 0.2951864004135132, 0.35266539454460144, 0.1936926692724228, 0.1046305000782013, 0.32652395963668823, 0.3577878177165985, 0.3319157660007477, 0.3546082377433777, 0.19087693095207214], dtype='float32').reshape([12]),
            paddle.to_tensor([0.26428812742233276, 0.07186543196439743, 0.460632860660553, 0.28434330224990845, 0.005369166377931833, 0.09471042454242706, 0.45403650403022766, 0.07558105140924454, 0.302859365940094, 0.25000467896461487, 0.1454872339963913, 0.06672661751508713], dtype='float32').reshape([12]),
            paddle.to_tensor([0.2678889036178589, 0.20965491235256195, 0.16276760399341583, 0.09053395688533783, 0.152940034866333, 0.02282233163714409, 0.18737930059432983, 0.17106913030147552, 0.23726606369018555, 0.4283615052700043, 0.4447217285633087, 0.2050315886735916], dtype='float32').reshape([12]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d023fc383f4c2160e6d1bd7a3050ed22(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0c845e0918402437383bcfa5d465828e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([8, 64, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d38c95607ad83ba40f2b6865866f7d4a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 8, 50], dtype='float16', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_d589af777f5a8ae699611515c41499fb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7287f1709dd1abc9833899d41f479673(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_d589af777f5a8ae699611515c41499fb
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 40, 40], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cedd74ee0f0ee9d73c1569accd1e80b0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 2560, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([2560], dtype='float32', min=0, max=0.5),
            paddle.uniform([2560], dtype='float32', min=0, max=0.5),
            paddle.uniform([2560], dtype='float32', min=0, max=0.5),
            paddle.uniform([2560], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4f81eb475f9ceb8fd6358ef73a947efb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 16, 16], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f1c904092ea8394ce9f25737cf8479ed(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 104, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a9d995cd107bf2c85f74ba83b519d991(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 40, 40], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cffed439796cf8a8e04bd98f3ff2c82e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_d589af777f5a8ae699611515c41499fb
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 80, 80], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_df250feca2d1420b00e1bbd3a374d953(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 128, 128], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_01430a4b63b01263f8245da7e761f74b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 410, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([410], dtype='float32', min=0, max=0.5),
            paddle.uniform([410], dtype='float32', min=0, max=0.5),
            paddle.uniform([410], dtype='float32', min=0, max=0.5),
            paddle.uniform([410], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7a51315da474cc39c1bd357703bd3a3d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.10662107914686203, 0.264543354511261, 0.44468092918395996, 0.44572117924690247, 0.05859605595469475, 0.393385648727417, 0.07747464627027512, 0.11854700744152069, 0.18319736421108246, 0.10087038576602936, 0.487946480512619, 0.13804619014263153, 0.23525355756282806, 0.36511337757110596, 0.2747363746166229, 0.2229483425617218, 0.06885569542646408, 0.40784895420074463, 0.23847857117652893, 0.3860172629356384], dtype='float32').reshape([20]),
            paddle.to_tensor([0.27390506863594055, 0.25274088978767395, 0.1651288866996765, 0.4154762923717499, 0.057929325848817825, 0.341245174407959, 0.07727394998073578, 0.47528567910194397, 0.16850338876247406, 0.38835543394088745, 0.31735190749168396, 0.0009855548851191998, 0.15577153861522675, 0.404583215713501, 0.012751128524541855, 0.04490390047430992, 0.07664418965578079, 0.46140822768211365, 0.09121991693973541, 0.3612888753414154], dtype='float32').reshape([20]),
            paddle.to_tensor([0.0782100036740303, 0.04648572579026222, 0.11137943714857101, 0.18711109459400177, 0.19068990647792816, 0.3411007225513458, 0.36043187975883484, 0.4501413404941559, 0.20001095533370972, 0.3910396993160248, 0.15989242494106293, 0.3454514741897583, 0.4019584059715271, 0.4816538095474243, 0.16535057127475739, 0.17478913068771362, 0.35374221205711365, 0.2640015482902527, 0.03460525721311569, 0.02041122131049633], dtype='float32').reshape([20]),
            paddle.to_tensor([0.17012809216976166, 0.2508763372898102, 0.4839549660682678, 0.1334260255098343, 0.2802211344242096, 0.28585246205329895, 0.1154172345995903, 0.2530350387096405, 0.12899088859558105, 0.38404861092567444, 0.11446736007928848, 0.4586215615272522, 0.27324944734573364, 0.004378646146506071, 0.08874005079269409, 0.27669915556907654, 0.46570849418640137, 0.39464887976646423, 0.3864634931087494, 0.36788028478622437], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e93468c7803886f2b0aca38e561760b7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 20, 20], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f69a2e73735b68ca9a3ae9e3efe0d08a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2a274714bbe1f0c866ce7d05a20668d4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 31, 31], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6390db9e941d4b2588b0f93d072cdc44(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 128, 128], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d093bb5b3176553b1ae4908ad48037c3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 40, 40], dtype='float16', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3e0d312b5bd4371fbc3343dada545a3a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 128, 128], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7a51618b2bb8f8ad94ac0a39323d25d5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 160, 8, 6], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_582b87eb12e5187687dce92893184a24(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 4, 50], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6c7085e4f7dbbbc469ad4da2dd49bbd3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 320, 320], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d5248ea0432185b0509d3a6cd0a4da00(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 36, 28, 50], dtype='float16', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bc264b637fc58a0b9dbbfca306e26b2e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.08073193579912186, 0.16553173959255219, 0.07641062140464783, 0.4830760657787323, 0.45927873253822327, 0.2714868485927582, 0.17562811076641083, 0.2607552707195282, 0.37346017360687256, 0.4331331253051758, 0.3162607252597809, 0.2521645128726959, 0.2849119007587433, 0.466093510389328, 0.4233824908733368, 0.09798587113618851, 0.12127551436424255, 0.22090956568717957, 0.4895242154598236, 0.23196189105510712], dtype='float32').reshape([20]),
            paddle.to_tensor([0.12902690470218658, 0.35531601309776306, 0.3219203054904938, 0.28560367226600647, 0.37734654545783997, 0.46701478958129883, 0.13147199153900146, 0.2540455758571625, 0.4999323785305023, 0.20477823913097382, 0.00021264137467369437, 0.2004813700914383, 0.46677833795547485, 0.2772543132305145, 0.008347694762051105, 0.367427796125412, 0.347514808177948, 0.4485090374946594, 0.21427424252033234, 0.4754555821418762], dtype='float32').reshape([20]),
            paddle.to_tensor([0.18766573071479797, 0.06935503333806992, 0.07781074196100235, 0.031207192689180374, 0.4871346652507782, 0.06470472365617752, 0.036226194351911545, 0.2869066894054413, 0.2631470561027527, 0.09704170376062393, 0.3823024332523346, 0.062092989683151245, 0.09096488356590271, 0.20845505595207214, 0.04671821743249893, 0.31844833493232727, 0.4536534249782562, 0.39261138439178467, 0.38682547211647034, 0.18596480786800385], dtype='float32').reshape([20]),
            paddle.to_tensor([0.39705586433410645, 0.3236680328845978, 0.021686013787984848, 0.1650451421737671, 0.28524890542030334, 0.053033456206321716, 0.48147323727607727, 0.0896286815404892, 0.3574107587337494, 0.2825826406478882, 0.457227885723114, 0.46172693371772766, 0.3887995779514313, 0.0493658185005188, 0.08913232386112213, 0.25105854868888855, 0.13578400015830994, 0.2954939305782318, 0.457913875579834, 0.28400829434394836], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7c768321aa2dd0365749c3c67257aa46(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f8a8dd35be62d5174a829c2d9a123d4a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6df589ffa9475fc30ca58a1888b54845(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.34596291184425354, 0.3605799078941345, 0.15827129781246185, 0.2738679349422455, 0.49555471539497375, 0.12239010632038116, 0.1765468716621399, 0.49577829241752625, 0.34197086095809937, 0.38626131415367126, 0.18678516149520874, 0.2765870690345764, 0.4054650664329529, 0.29438167810440063, 0.30353638529777527, 0.4166869819164276], dtype='float32').reshape([16]),
            paddle.to_tensor([0.41610220074653625, 0.37242695689201355, 0.33795469999313354, 0.1280706226825714, 0.22086091339588165, 0.3675064444541931, 0.12311642616987228, 0.07005402445793152, 0.22372853755950928, 0.19980569183826447, 0.4267515540122986, 0.43426617980003357, 0.3618895709514618, 0.21566122770309448, 0.34249672293663025, 0.2563948631286621], dtype='float32').reshape([16]),
            paddle.to_tensor([0.08316248655319214, 0.2647395431995392, 0.1275911033153534, 0.015601404011249542, 0.19853347539901733, 0.40381646156311035, 0.24634793400764465, 0.21449729800224304, 0.4713570177555084, 0.4175795614719391, 0.12317956984043121, 0.06734642386436462, 0.36550047993659973, 0.09956810623407364, 0.2402939349412918, 0.03157610818743706], dtype='float32').reshape([16]),
            paddle.to_tensor([0.3757416903972626, 0.24847151339054108, 0.05761212483048439, 0.25650882720947266, 0.4260549247264862, 0.3978753983974457, 0.4452779293060303, 0.12178203463554382, 0.021073393523693085, 0.1919686645269394, 0.38092803955078125, 0.12907299399375916, 0.36935555934906006, 0.29743441939353943, 0.4716189205646515, 0.21264274418354034], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7f659e3dd436681a70f8535347c4f34d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 320, 320], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.17474313080310822, 0.12158620357513428, 0.13883326947689056, 0.4189715087413788, 0.43404170870780945, 0.05564383789896965, 0.08938619494438171, 0.23544199764728546, 0.025126634165644646, 0.23889248073101044, 0.2111179679632187, 0.3774566650390625, 0.10612328350543976, 0.3746371567249298, 0.1919274926185608, 0.3979851007461548], dtype='float32').reshape([16]),
            paddle.to_tensor([0.1394030600786209, 0.38611844182014465, 0.2334824800491333, 0.12676386535167694, 0.3264469802379608, 0.19352737069129944, 0.016597865149378777, 0.300331175327301, 0.4941093921661377, 0.07575323432683945, 0.4788273572921753, 0.18150337040424347, 0.40930911898612976, 0.16368381679058075, 0.08503960072994232, 0.1148504763841629], dtype='float32').reshape([16]),
            paddle.to_tensor([0.15328234434127808, 0.27689865231513977, 0.0657559484243393, 0.16696405410766602, 0.19935651123523712, 0.20971232652664185, 0.20196311175823212, 0.08225025981664658, 0.3910372257232666, 0.19002187252044678, 0.1753712147474289, 0.4816964864730835, 0.09077177196741104, 0.1073472872376442, 0.035163700580596924, 0.4926859736442566], dtype='float32').reshape([16]),
            paddle.to_tensor([0.30774635076522827, 0.10852959007024765, 0.4249836206436157, 0.046842969954013824, 0.29405561089515686, 0.01888219453394413, 0.3672284781932831, 0.029602015390992165, 0.1800508350133896, 0.4298548400402069, 0.008628605864942074, 0.40285900235176086, 0.04537059739232063, 0.15167947113513947, 0.058748919516801834, 0.49316832423210144], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4bfda9cd51f564a6caac57ee6d5345eb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 144, 48, 48], dtype='float16', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6f1bc158519854c96ce4c18119f95ff1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fce65fb236704f8df35b21778860d531(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 160, 17, 17], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_760ed7d32ac47ad32d6055eaae31c64b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 160, 160], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_89222663a9eec7bd6a21d63c2aa8e50f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 1280, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([1280], dtype='float32', min=0, max=0.5),
            paddle.uniform([1280], dtype='float32', min=0, max=0.5),
            paddle.uniform([1280], dtype='float32', min=0, max=0.5),
            paddle.uniform([1280], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b4cbf0ff7c11547773d343b4755780ce(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 26, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.008682666346430779, 0.07742586731910706, 0.20446762442588806, 0.40040454268455505, 0.12542970478534698, 0.43668535351753235, 0.24541695415973663, 0.33078327775001526, 0.18576329946517944, 0.06379961222410202, 0.14999718964099884, 0.15716829895973206, 0.23823459446430206, 0.08840649574995041, 0.4214136600494385, 0.3904346823692322, 0.06736749410629272, 0.3211595416069031, 0.1596764773130417, 0.15976284444332123, 0.48467594385147095, 0.010123244486749172, 0.1207241341471672, 0.35010281205177307, 0.21164728701114655, 0.4128369688987732], dtype='float32').reshape([26]),
            paddle.to_tensor([0.07730677723884583, 0.2933571934700012, 0.39149174094200134, 0.373500257730484, 0.1684592366218567, 0.049001432955265045, 0.2318839728832245, 0.07775977998971939, 0.3578716218471527, 0.0676223412156105, 0.27932339906692505, 0.2512679398059845, 0.44520118832588196, 0.4972549080848694, 0.38223153352737427, 0.2017194777727127, 0.20245708525180817, 0.03786349669098854, 0.4327920079231262, 0.480347216129303, 0.10313534736633301, 0.37627264857292175, 0.12557940185070038, 0.0032888981513679028, 0.3624570667743683, 0.4357312023639679], dtype='float32').reshape([26]),
            paddle.to_tensor([0.06531158834695816, 0.26113492250442505, 0.024364048615098, 0.20747911930084229, 0.4556407928466797, 0.09498320519924164, 0.1691916286945343, 0.3227861821651459, 0.1418522745370865, 0.29845210909843445, 0.08784361183643341, 0.17583417892456055, 0.0623609721660614, 0.03585701435804367, 0.03497587516903877, 0.31210869550704956, 0.34130969643592834, 0.4641765356063843, 0.3503555953502655, 0.4607350528240204, 0.410419762134552, 0.10456491261720657, 0.47725680470466614, 0.17760561406612396, 0.3903712332248688, 0.35443705320358276], dtype='float32').reshape([26]),
            paddle.to_tensor([0.31286653876304626, 0.05360940843820572, 0.40597131848335266, 0.3193321228027344, 0.44482871890068054, 0.15332329273223877, 0.06981624662876129, 0.14665600657463074, 0.16617590188980103, 0.06799530237913132, 0.48879340291023254, 0.08208722621202469, 0.2321900576353073, 0.017290471121668816, 0.08517389744520187, 0.4393863081932068, 0.4240933656692505, 0.48599544167518616, 0.4818030297756195, 0.35085734724998474, 0.486652672290802, 0.49704161286354065, 0.19093862175941467, 0.0036611813120543957, 0.2398311048746109, 0.4590197801589966], dtype='float32').reshape([26]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0725043ab09ccf9e1775d3b0afe76c08(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 80, 80], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d158c362eda895472cbb7071b8e730d4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 320, 8, 8], dtype='float16', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4c0de217e84174d5e697640e3737f85b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 200, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([200], dtype='float32', min=0, max=0.5),
            paddle.uniform([200], dtype='float32', min=0, max=0.5),
            paddle.uniform([200], dtype='float32', min=0, max=0.5),
            paddle.uniform([200], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_b4f4da3a8d44d1b5b3a2903a097df186(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3876249f6b63dac34902e628ca392b86(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_b4f4da3a8d44d1b5b3a2903a097df186
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 256, 256], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_618426c1ee0a4782a9c8da27fe24310d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f96290e1355076775534b2d7875b0c1a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([8, 32, 112, 112], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1bd65f8b66a9a27a5a6e440cb030323f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 16, 12], dtype='float16', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c4bee5819c3d00a59574ced23ae964ae(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 80, 12, 12], dtype='float16', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_faa48a48d846cdbe9b992a121a0a9ee4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_138b55270ac412ccefc62ec5f0e5a6ac(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 12, 960, 960], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.37705370783805847, 0.16765064001083374, 0.19111530482769012, 0.32777565717697144, 0.09806336462497711, 0.10420479625463486, 0.228814959526062, 0.012254424393177032, 0.14564450085163116, 0.029283028095960617, 0.1829545795917511, 0.3276773691177368], dtype='float32').reshape([12]),
            paddle.to_tensor([0.4859113097190857, 0.06003812327980995, 0.3574596047401428, 0.44217824935913086, 0.36243578791618347, 0.3556690812110901, 0.38163262605667114, 0.39550691843032837, 0.3194003403186798, 0.3805111050605774, 0.15356656908988953, 0.1621331423521042], dtype='float32').reshape([12]),
            paddle.to_tensor([0.09672968834638596, 0.23519155383110046, 0.2872145175933838, 0.3564112186431885, 0.14329828321933746, 0.4715942144393921, 0.46488460898399353, 0.43155959248542786, 0.3870643675327301, 0.18193300068378448, 0.3548692464828491, 0.029379669576883316], dtype='float32').reshape([12]),
            paddle.to_tensor([0.016813980415463448, 0.45801064372062683, 0.11635599285364151, 0.3859371244907379, 0.2335502952337265, 0.019704153761267662, 0.4418640434741974, 0.3522834777832031, 0.25090694427490234, 0.26961252093315125, 0.051347628235816956, 0.4088355302810669], dtype='float32').reshape([12]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4a81b38ecb5cf95d75bb8e36ec109d70(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 36, 64, 128], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_63fc04f477d2504d96b054d5959f6b0e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.10832411795854568, 0.22957690060138702, 0.4081784188747406, 0.4703223407268524, 0.34252291917800903, 0.23146124184131622, 0.1409875601530075, 0.3547273874282837, 0.42891329526901245, 0.1597321331501007, 0.22409717738628387, 0.4872796833515167, 0.46199774742126465, 0.16602280735969543, 0.49637752771377563, 0.17214539647102356, 0.3788946270942688, 0.042121272534132004, 0.1579367220401764, 0.0025303373113274574], dtype='float32').reshape([20]),
            paddle.to_tensor([0.12208598852157593, 0.009593741036951542, 0.14529503881931305, 0.404198557138443, 0.41258367896080017, 0.22035053372383118, 0.42421314120292664, 0.11933945119380951, 0.46761366724967957, 0.01565542072057724, 0.03559922054409981, 0.2002333402633667, 0.174664705991745, 0.23835636675357819, 0.49831461906433105, 0.22027862071990967, 0.31663164496421814, 0.11948101222515106, 0.04653676226735115, 0.02207028679549694], dtype='float32').reshape([20]),
            paddle.to_tensor([0.29498857259750366, 0.3036075830459595, 0.22633428871631622, 0.06256362795829773, 0.4275349974632263, 0.156571164727211, 0.23170213401317596, 0.053962498903274536, 0.03664134815335274, 0.3529442846775055, 0.3803720772266388, 0.34841176867485046, 0.2896499037742615, 0.056177567690610886, 0.3584928512573242, 0.024533770978450775, 0.10141115635633469, 0.4602433443069458, 0.05331259220838547, 0.4369911551475525], dtype='float32').reshape([20]),
            paddle.to_tensor([0.17434750497341156, 0.11073053628206253, 0.49044498801231384, 0.4360230565071106, 0.30742257833480835, 0.262793630361557, 0.30647939443588257, 0.029409967362880707, 0.17955128848552704, 0.40023332834243774, 0.391825407743454, 0.3158022165298462, 0.21447831392288208, 0.2864912152290344, 0.36503735184669495, 0.3223249614238739, 0.46853238344192505, 0.17434468865394592, 0.34529897570610046, 0.010838789865374565], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_62e5a96212b0fcc6919dad2ef21cf47e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 4, 3], dtype='float16', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_165ed706feb1399b6287dc7e160f3839(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 64, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_925515d29d60fc29be68dc0c3b464edd(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 40, 40], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5421538e781890ee61cf6601fa2192e9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 384, 64, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_17ae718d53546d0de454fbd0f64b08bb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 120, 128, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cc38afff86ec89e97f64f2ddf5ef8bef(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 30, 30], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6a9b6334f75cb517fd3b5aff823dace1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c50c1c92563e15641752ced1e35ed47b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_34519629d2b01e9728f353054bfb1426(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 60, 60], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6145d805b9d2025bec8ae85e89b783ed(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 16, 12], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_41dde5624aaa94a6e01c001574e93b08(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 240, 240], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1520116557362c5f17951722a1649b58(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 512, 1024], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b2286ecc58aae8c27c5dc5ab2b12f31a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 36, 28, 50], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d31a86b4694fd55f5c32dc43f8aed96a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8c6dccf1c702ebe105911afd50e22556(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 160, 4, 3], dtype='float16', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2e988b5b8a03199ad0812b67d2bd1daf(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.1522848904132843, 0.44862809777259827, 0.08253410458564758, 0.2779274582862854, 0.48370277881622314, 0.006392086856067181, 0.16081346571445465, 0.27009284496307373, 0.44630205631256104, 0.40076640248298645, 0.021454988047480583, 0.2083764523267746, 0.18717806041240692, 0.3501777648925781, 0.4399738013744354, 0.39403969049453735, 0.09644468128681183, 0.09264886379241943, 0.10050640255212784, 0.22903960943222046, 0.4096629321575165, 0.055614031851291656, 0.14381426572799683, 0.16327618062496185, 0.04343046620488167, 0.3827477991580963, 0.09195219725370407, 0.2741642892360687, 0.21621933579444885, 0.04539554938673973], dtype='float32').reshape([30]),
            paddle.to_tensor([0.16890296339988708, 0.05673185735940933, 0.3271726369857788, 0.24903631210327148, 0.463613897562027, 0.45923957228660583, 0.04673423245549202, 0.25723421573638916, 0.29808366298675537, 0.33920830488204956, 0.23567695915699005, 0.12396220862865448, 0.29779431223869324, 0.3319670557975769, 0.3291337490081787, 0.4956483542919159, 0.15144316852092743, 0.3635004758834839, 0.08554914593696594, 0.3160989284515381, 0.0996456891298294, 0.03917666897177696, 0.16836851835250854, 0.1545175164937973, 0.042410288006067276, 0.11135159432888031, 0.2047257423400879, 0.10227294266223907, 0.39137858152389526, 0.19707493484020233], dtype='float32').reshape([30]),
            paddle.to_tensor([0.438779354095459, 0.053267620503902435, 0.3459594249725342, 0.23503288626670837, 0.33579495549201965, 0.45214149355888367, 0.1357591450214386, 0.12630778551101685, 0.12667018175125122, 0.01251298002898693, 0.07500185817480087, 0.31898239254951477, 0.03736976906657219, 0.09497997909784317, 0.26055577397346497, 0.0872301459312439, 0.43045490980148315, 0.39500826597213745, 0.4519956111907959, 0.018326187506318092, 0.3682207763195038, 0.3709438145160675, 0.34521302580833435, 0.36597180366516113, 0.21863706409931183, 0.08258727937936783, 0.09337040036916733, 0.2248114049434662, 0.11159028112888336, 0.41479331254959106], dtype='float32').reshape([30]),
            paddle.to_tensor([0.21534131467342377, 0.10913644731044769, 0.4610022008419037, 0.24989444017410278, 0.4988919496536255, 0.1932317316532135, 0.08500836789608002, 0.31803736090660095, 0.3887564539909363, 0.4577285349369049, 0.32838523387908936, 0.24957631528377533, 0.3980032503604889, 0.132064089179039, 0.33769190311431885, 0.35160115361213684, 0.4406687617301941, 0.35891973972320557, 0.47063544392585754, 0.40030547976493835, 0.2707085907459259, 0.32018399238586426, 0.21964102983474731, 0.3191429376602173, 0.48548123240470886, 0.08091679215431213, 0.44869354367256165, 0.11607024073600769, 0.2925715446472168, 0.09237895905971527], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c03958cf605d342bba33c8d5bf10fca2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 512, 256], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c3c05934f94b06a50f94056a33c74bb2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 480, 480], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.30172446370124817, 0.38164573907852173, 0.40083083510398865, 0.23597609996795654, 0.36543017625808716, 0.1879006326198578, 0.38321542739868164, 0.29518741369247437], dtype='float32').reshape([8]),
            paddle.to_tensor([0.42444372177124023, 0.37563303112983704, 0.32983341813087463, 0.2938113212585449, 0.2265654355287552, 0.4767524302005768, 0.05787137523293495, 0.20064260065555573], dtype='float32').reshape([8]),
            paddle.to_tensor([0.19783030450344086, 0.09353290498256683, 0.12587840855121613, 0.21147091686725616, 0.05295928567647934, 0.19891299307346344, 0.1820230484008789, 0.11526894569396973], dtype='float32').reshape([8]),
            paddle.to_tensor([0.2097044289112091, 0.18118907511234283, 0.47356852889060974, 0.18380561470985413, 0.02758520096540451, 0.01522652618587017, 0.4358050525188446, 0.4336892068386078], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_da92cfe51222301c1b9780d8f9b1d1dc(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 36, 16, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_313cac7372f7a45a3800a21e81914d64(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 20, 20], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2f97faff04290bcd8986acb0e20c4a03(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_af91295b59c5fca17f890a802deb001f
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 32, 64, 64], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.4276238977909088, 0.3402268886566162, 0.08876517415046692, 0.469637930393219, 0.4241864085197449, 0.2357938140630722, 0.25508299469947815, 0.46479958295822144], dtype='float32').reshape([8]),
            paddle.to_tensor([0.022054404020309448, 0.21905258297920227, 0.05280282348394394, 0.23618361353874207, 0.15707048773765564, 0.21992671489715576, 0.4842948019504547, 0.3393869996070862], dtype='float32').reshape([8]),
            paddle.to_tensor([0.18322092294692993, 0.20581816136837006, 0.32598719000816345, 0.3696417808532715, 0.32822340726852417, 0.2831205725669861, 0.46974343061447144, 0.040279414504766464], dtype='float32').reshape([8]),
            paddle.to_tensor([0.47803768515586853, 0.18431814014911652, 0.006784247700124979, 0.219992995262146, 0.12704578042030334, 0.21928349137306213, 0.34999480843544006, 0.02490760013461113], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_09fc9828c12b993db35f07e515bd45a7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.41803261637687683, 0.41402122378349304, 0.04787979647517204, 0.47711777687072754, 0.101714588701725, 0.4864788353443146, 0.03947985917329788, 0.33252573013305664, 0.007113320287317038, 0.4618329405784607, 0.4306660294532776, 0.340067058801651, 0.38455578684806824, 0.08797561377286911, 0.27674600481987, 0.3683278560638428, 0.1259741336107254, 0.4560236930847168, 0.452872097492218, 0.11372347921133041, 0.23956085741519928, 0.4247048497200012, 0.32986533641815186, 0.21296809613704681, 0.43435683846473694, 0.2341325283050537, 0.10965750366449356, 0.37072131037712097, 0.4249871075153351, 0.358481228351593], dtype='float32').reshape([30]),
            paddle.to_tensor([0.32509395480155945, 0.31509268283843994, 0.3533545136451721, 0.3169051706790924, 0.36625921726226807, 0.1399015486240387, 0.24587437510490417, 0.11261843144893646, 0.45382729172706604, 0.08306103944778442, 0.4994165301322937, 0.11790303140878677, 0.29823797941207886, 0.22108641266822815, 0.41432157158851624, 0.2585843503475189, 0.1899416148662567, 0.015371235087513924, 0.11780045181512833, 0.30893734097480774, 0.3648150861263275, 0.1420447826385498, 0.33820879459381104, 0.02140338532626629, 0.20559808611869812, 0.13304416835308075, 0.055864471942186356, 0.45491284132003784, 0.318745881319046, 0.07248502224683762], dtype='float32').reshape([30]),
            paddle.to_tensor([0.006928340531885624, 0.4900380074977875, 0.20332635939121246, 0.04957558959722519, 0.029924701899290085, 0.3583701252937317, 0.37951838970184326, 0.0877910777926445, 0.14060819149017334, 0.3084448575973511, 0.28205111622810364, 0.10527759045362473, 0.40341904759407043, 0.13098208606243134, 0.3453892171382904, 0.4109606444835663, 0.43825432658195496, 0.3998722434043884, 0.36317840218544006, 0.057749319821596146, 0.4759015142917633, 0.14931108057498932, 0.1642659306526184, 0.12326344847679138, 0.31695982813835144, 0.4205192029476166, 0.2367590367794037, 0.30620160698890686, 0.428134560585022, 0.4915296733379364], dtype='float32').reshape([30]),
            paddle.to_tensor([0.2757815420627594, 0.4439810514450073, 0.4358019232749939, 0.2323462814092636, 0.14010964334011078, 0.12180730700492859, 0.18546397984027863, 0.2955663204193115, 0.08907774090766907, 0.025036202743649483, 0.15007776021957397, 0.4538847506046295, 0.1138942688703537, 0.11831458657979965, 0.10549252480268478, 0.020106781274080276, 0.2367790937423706, 0.11779158562421799, 0.10487262159585953, 0.3194362223148346, 0.045736122876405716, 0.2679264545440674, 0.055998433381319046, 0.3666035532951355, 0.22697900235652924, 0.25158777832984924, 0.4908917248249054, 0.45129096508026123, 0.3796835243701935, 0.24447698891162872], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8121ff122a866ad7f46e7f3d482e3e91(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1287766f5c24569daf2f86fcaec33208(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([8, 256, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_94d2b1ef02f3e4b69f79c2b0b4ac2723(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.14728586375713348, 0.3414001762866974, 0.04139332473278046, 0.30700764060020447, 0.386129766702652, 0.06821857392787933, 0.36377933621406555, 0.15444964170455933, 0.1415313184261322, 0.47541654109954834, 0.3473075330257416, 0.01352121215313673, 0.2978605628013611, 0.0605318583548069, 0.12956804037094116, 0.4606412649154663, 0.39865773916244507, 0.39783984422683716, 0.34143704175949097, 0.4768171012401581, 0.4120984673500061, 0.0017334397416561842, 0.22118456661701202, 0.22908654808998108, 0.34567391872406006, 0.09835159033536911, 0.11541694402694702, 0.37911728024482727, 0.32072633504867554, 0.1676488071680069], dtype='float32').reshape([30]),
            paddle.to_tensor([0.39727163314819336, 0.07209828495979309, 0.12574762105941772, 0.029664836823940277, 0.34941452741622925, 0.07231928408145905, 0.2620967626571655, 0.2782939672470093, 0.10591062158346176, 0.06335276365280151, 0.4482811391353607, 0.06718264520168304, 0.26657283306121826, 0.3213929533958435, 0.0015593769494444132, 0.050494782626628876, 0.255669504404068, 0.4284959137439728, 0.002090382855385542, 0.025958223268389702, 0.08367361128330231, 0.14579178392887115, 0.21605455875396729, 0.2198597937822342, 0.1520092487335205, 0.4756194055080414, 0.029144860804080963, 0.12394315004348755, 0.3619467616081238, 0.039176665246486664], dtype='float32').reshape([30]),
            paddle.to_tensor([0.4889386296272278, 0.13305728137493134, 0.06247160956263542, 0.08240775763988495, 0.18935687839984894, 0.18754561245441437, 0.3259337842464447, 0.10347437858581543, 0.1121356338262558, 0.3323129117488861, 0.35507339239120483, 0.36440134048461914, 0.2521071434020996, 0.1397167593240738, 0.47156447172164917, 0.0704771876335144, 0.48657548427581787, 0.25647974014282227, 0.4997507333755493, 0.4778321087360382, 0.020720448344945908, 0.08193299174308777, 0.02118874341249466, 0.3000895380973816, 0.4622310996055603, 0.12646953761577606, 0.4360828995704651, 0.1947329342365265, 0.3481901288032532, 0.45873090624809265], dtype='float32').reshape([30]),
            paddle.to_tensor([0.23899321258068085, 0.4454304277896881, 0.3434208333492279, 0.3865661323070526, 0.3661952316761017, 0.2238459289073944, 0.42204755544662476, 0.4075549840927124, 0.34784239530563354, 0.15175023674964905, 0.23286007344722748, 0.49711519479751587, 0.429912269115448, 0.22993643581867218, 0.04243601858615875, 0.03829970583319664, 0.17360113561153412, 0.01671617664396763, 0.497138649225235, 0.36929070949554443, 0.43725162744522095, 0.24407042562961578, 0.1322590857744217, 0.4481506645679474, 0.20610582828521729, 0.1502876728773117, 0.08380569517612457, 0.02966013178229332, 0.271088182926178, 0.04178566485643387], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e974c50b77aea63b55b5d8f8715227b0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 80, 30, 30], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d76dc775e3dff6308121c6b6063db1f8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d60342d62641ba53e36849c52cc5a9fe(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 384, 20, 20], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f2d9bdf1d2964eadc3bf99e0b1aa186a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 80, 80], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_79f3e50656ca7816c992e9ceaa18c473(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 1120, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([1120], dtype='float32', min=0, max=0.5),
            paddle.uniform([1120], dtype='float32', min=0, max=0.5),
            paddle.uniform([1120], dtype='float32', min=0, max=0.5),
            paddle.uniform([1120], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_da4d85bc213f70819a72e68f21214cdb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9fa679bc1dd184b4cc747400a78fb632(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_da4d85bc213f70819a72e68f21214cdb
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 32, 8, 8], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e0c41a2ad15b7ec06ac6579e8f7d6dd1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.39804282784461975, 0.27116668224334717, 0.4578166604042053, 0.16101877391338348, 0.49097925424575806, 0.2404414266347885, 0.358816921710968, 0.36365216970443726, 0.34797853231430054, 0.2612827718257904, 0.3428969383239746, 0.021941551938652992, 0.14823752641677856, 0.39554333686828613, 0.0049746367149055, 0.3125059902667999, 0.177294060587883, 0.2805713415145874, 0.044468529522418976, 0.2776702642440796, 0.1311364471912384, 0.3060095012187958, 0.40331462025642395, 0.3654356598854065], dtype='float32').reshape([24]),
            paddle.to_tensor([0.23356454074382782, 0.047932859510183334, 0.32120609283447266, 0.2775775194168091, 0.16299816966056824, 0.22079584002494812, 0.41612327098846436, 0.0756048709154129, 0.10984168946743011, 0.3255261778831482, 0.4074952006340027, 0.03546576946973801, 0.2149333506822586, 0.11642900854349136, 0.1369996964931488, 0.39534831047058105, 0.14761584997177124, 0.4218871295452118, 0.03304591774940491, 0.37543848156929016, 0.17592710256576538, 0.04657762497663498, 0.22784514725208282, 0.12422391772270203], dtype='float32').reshape([24]),
            paddle.to_tensor([0.05957365408539772, 0.4145265221595764, 0.3672828674316406, 0.2956692576408386, 0.3762301206588745, 0.19036351144313812, 0.36932435631752014, 0.2706626057624817, 0.4393993318080902, 0.26020029187202454, 0.0968787744641304, 0.48613399267196655, 0.06185359135270119, 0.06414993107318878, 0.4276765286922455, 0.3076213002204895, 0.17015469074249268, 0.3949149250984192, 0.42609885334968567, 0.05638517439365387, 0.004763609729707241, 0.4879893660545349, 0.350826233625412, 0.4853783845901489], dtype='float32').reshape([24]),
            paddle.to_tensor([0.46129441261291504, 0.34792131185531616, 0.07238025218248367, 0.3571946918964386, 0.4076196849346161, 0.4126727283000946, 0.41088083386421204, 0.18230651319026947, 0.16378533840179443, 0.403646320104599, 0.08382555842399597, 0.4746594727039337, 0.08875194936990738, 0.27091968059539795, 0.15272416174411774, 0.3179561197757721, 0.13141460716724396, 0.05726373940706253, 0.3598400950431824, 0.34174129366874695, 0.12100692838430405, 0.12096145749092102, 0.25225698947906494, 0.18917454779148102], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a593d881d032a6940c64a01880e682a2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 1632, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([1632], dtype='float32', min=0, max=0.5),
            paddle.uniform([1632], dtype='float32', min=0, max=0.5),
            paddle.uniform([1632], dtype='float32', min=0, max=0.5),
            paddle.uniform([1632], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_23eb6c11ae7ffebf5efbb1b8545f47ef(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4c8f498d599140d3aab75b866f0251ce(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.4532107710838318, 0.20674824714660645, 0.39593252539634705, 0.17843462526798248, 0.07028307020664215, 0.14368794858455658, 0.04630361124873161, 0.24611075222492218, 0.11431584507226944, 0.08089757710695267, 0.2329295575618744, 0.13888691365718842, 0.21223315596580505, 0.2726699709892273, 0.4406675398349762, 0.36554378271102905, 0.008237546309828758, 0.14471891522407532, 0.41980376839637756, 0.18792150914669037], dtype='float32').reshape([20]),
            paddle.to_tensor([0.1974020004272461, 0.45864611864089966, 0.1935604214668274, 0.07851482927799225, 0.4809388518333435, 0.2428283542394638, 0.24559083580970764, 0.15689800679683685, 0.3344629406929016, 0.2770606577396393, 0.39475148916244507, 0.3782365322113037, 0.28585630655288696, 0.037073954939842224, 0.34575286507606506, 0.29142555594444275, 0.28206905722618103, 0.42273256182670593, 0.028744803741574287, 0.05618155375123024], dtype='float32').reshape([20]),
            paddle.to_tensor([0.3759831190109253, 0.17245294153690338, 0.21996523439884186, 0.06684647500514984, 0.05809483677148819, 0.327384352684021, 0.45824941992759705, 0.18657881021499634, 0.05081522837281227, 0.1556323915719986, 0.083693727850914, 0.35101518034935, 0.04801327362656593, 0.20123954117298126, 0.13422328233718872, 0.0854772999882698, 0.09893997013568878, 0.24439749121665955, 0.4273565113544464, 0.3166423439979553], dtype='float32').reshape([20]),
            paddle.to_tensor([0.08211477100849152, 0.03661003336310387, 0.4959814250469208, 0.3545704185962677, 0.12957672774791718, 0.0728939101099968, 0.31439781188964844, 0.16915100812911987, 0.46254757046699524, 0.48616698384284973, 0.3126605749130249, 0.39561551809310913, 0.10526550561189651, 0.07136858999729156, 0.1806260496377945, 0.12113495171070099, 0.19631053507328033, 0.0392637625336647, 0.18303292989730835, 0.4508019983768463], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9e60825b03f828fb83877bb718213267(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ff35f3bd6a28e50b9177394162c0957a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 64, 128], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.2492724508047104, 0.04044491425156593, 0.17258134484291077, 0.129984512925148, 0.4848192632198334, 0.3926643431186676, 0.2772189974784851, 0.08157596737146378, 0.34677577018737793, 0.09221868962049484, 0.24118609726428986, 0.20133383572101593, 0.014820912852883339, 0.05375825986266136, 0.257407546043396, 0.4579992890357971, 0.23692908883094788, 0.029930567368865013], dtype='float32').reshape([18]),
            paddle.to_tensor([0.38577330112457275, 0.2795851230621338, 0.40521690249443054, 0.4688466191291809, 0.34417587518692017, 0.053289979696273804, 0.062486425042152405, 0.2908543050289154, 0.4944775402545929, 0.463208943605423, 0.08488006144762039, 0.4426557421684265, 0.08781173080205917, 0.4647093415260315, 0.1416270136833191, 0.03500841557979584, 0.23974719643592834, 0.3061506748199463], dtype='float32').reshape([18]),
            paddle.to_tensor([0.3712441027164459, 0.4861455261707306, 0.24962446093559265, 0.34248894453048706, 0.4660094082355499, 0.47616150975227356, 0.07558578252792358, 0.44781842827796936, 0.1509404480457306, 0.1124330684542656, 0.2394355982542038, 0.38318634033203125, 0.21671628952026367, 0.3569905757904053, 0.3224546015262604, 0.10976819694042206, 0.45428451895713806, 0.2981863021850586], dtype='float32').reshape([18]),
            paddle.to_tensor([0.09549971669912338, 0.18243573606014252, 0.39712005853652954, 0.20846134424209595, 0.43275633454322815, 0.1483422964811325, 0.4500959515571594, 0.4469130337238312, 0.3601011037826538, 0.28505298495292664, 0.0976787731051445, 0.09339140355587006, 0.24866452813148499, 0.15433622896671295, 0.4588134288787842, 0.003751905169337988, 0.07742753624916077, 0.14785607159137726], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cf4ea245311f72133754ecf7b8bc356e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 128, 128], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_10a24584a5f7e55354f16fed78ffd185(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 28, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.3141411244869232, 0.07118411362171173, 0.487453430891037, 0.48043093085289, 0.41012150049209595, 0.2692323625087738, 0.254991739988327, 0.11249478906393051, 0.16885541379451752, 0.2879337668418884, 0.1080489233136177, 0.44225266575813293, 0.2996901869773865, 0.14645835757255554, 0.48567354679107666, 0.009154902771115303, 0.27158963680267334, 0.17963771522045135, 0.47845757007598877, 0.43751195073127747, 0.44224920868873596, 0.08208513259887695, 0.09094338119029999, 0.008557128719985485, 0.08737808465957642, 0.4220463037490845, 0.21936751902103424, 0.31420832872390747], dtype='float32').reshape([28]),
            paddle.to_tensor([0.1866239309310913, 0.4772223234176636, 0.2738676965236664, 0.3392305076122284, 0.013099076226353645, 0.08228810131549835, 0.10237343609333038, 0.004600563086569309, 0.314486563205719, 0.11723776906728745, 0.4298044443130493, 0.1329580843448639, 0.0744650587439537, 0.33317163586616516, 0.2444217950105667, 0.45923110842704773, 0.44291260838508606, 0.1821025013923645, 0.0749482735991478, 0.46184104681015015, 0.07496176660060883, 0.11205439269542694, 0.38003426790237427, 0.09884475916624069, 0.3604939877986908, 0.4411430358886719, 0.3107360899448395, 0.08210466057062149], dtype='float32').reshape([28]),
            paddle.to_tensor([0.014934300445020199, 0.3484126329421997, 0.39549922943115234, 0.012328140437602997, 0.4341781437397003, 0.1448570191860199, 0.0544109009206295, 0.1513589471578598, 0.056469738483428955, 0.06985599547624588, 0.1867857426404953, 0.05267990380525589, 0.43286827206611633, 0.3213706910610199, 0.2513667941093445, 0.43931636214256287, 0.08436243236064911, 0.3297131061553955, 0.29536759853363037, 0.34540411829948425, 0.08397000283002853, 0.48472851514816284, 0.2916344106197357, 0.4045809507369995, 0.28168410062789917, 0.3400319516658783, 0.08657781034708023, 0.05808281525969505], dtype='float32').reshape([28]),
            paddle.to_tensor([0.07696674764156342, 0.02756514959037304, 0.060134775936603546, 0.16596989333629608, 0.44040167331695557, 0.013968444429337978, 0.3959023356437683, 0.09172262996435165, 0.49060192704200745, 0.302975058555603, 0.14843887090682983, 0.4627458155155182, 0.42972809076309204, 0.10800500214099884, 0.4779834449291229, 0.2392885982990265, 0.13432323932647705, 0.3107399642467499, 0.33123353123664856, 0.038208961486816406, 0.3877621293067932, 0.4967806339263916, 0.49829477071762085, 0.22749704122543335, 0.2943156063556671, 0.14248736202716827, 0.10011431574821472, 0.40602388978004456], dtype='float32').reshape([28]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1cca9b05a96a5910e0f54cac541a1bcb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_af91295b59c5fca17f890a802deb001f
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 4, 64, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b02b6536db8881cec7b19486c094276a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 147, 147], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fe1797688acc54ad18730fa2adcedda9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 120, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ed165700b5329da03cb6b81fd0321eb2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 122, 122], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_49009cfa015795b743d46190f06971fd(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 702, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([702], dtype='float32', min=0, max=0.5),
            paddle.uniform([702], dtype='float32', min=0, max=0.5),
            paddle.uniform([702], dtype='float32', min=0, max=0.5),
            paddle.uniform([702], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_16e7429502b5c4113b5eab55631bebae(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 144, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_14e50c883123088ef8290b0610445f20(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 320, 320], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_af7555471a5fa47dd9d2d25adeb36ed1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 480, 30, 30], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0d4eab4bf84c0adde027a36d3dd0fc01(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.19290995597839355, 0.22375106811523438, 0.215490460395813, 0.009548865258693695, 0.21796803176403046, 0.15518401563167572, 0.2367093414068222, 0.2425651252269745, 0.15403778851032257, 0.028490616008639336, 0.0941045880317688, 0.08225146681070328, 0.0015016032848507166, 0.09681206941604614, 0.11580712348222733, 0.4385620355606079, 0.2961457669734955, 0.10734105855226517, 0.17257650196552277, 0.055393822491168976, 0.49845781922340393, 0.47510266304016113, 0.09695926308631897, 0.07025036960840225, 0.48792126774787903, 0.026538584381341934, 0.2970225512981415, 0.36502572894096375, 0.39871251583099365, 0.27677083015441895], dtype='float32').reshape([30]),
            paddle.to_tensor([0.3460787236690521, 0.44200870394706726, 0.21973103284835815, 0.30538439750671387, 0.4636460542678833, 0.09047640860080719, 0.389238178730011, 0.3446405827999115, 0.41540345549583435, 0.41294461488723755, 0.39583778381347656, 0.41324812173843384, 0.33210891485214233, 0.4172556400299072, 0.4032793641090393, 0.32314199209213257, 0.08656580746173859, 0.4211755394935608, 0.3883002698421478, 0.38589712977409363, 0.31327855587005615, 0.12544402480125427, 0.2942081093788147, 0.14060592651367188, 0.4235610365867615, 0.027826957404613495, 0.43721961975097656, 0.3189155161380768, 0.1976928412914276, 0.2402050942182541], dtype='float32').reshape([30]),
            paddle.to_tensor([0.1363193243741989, 0.06537190079689026, 0.23927782475948334, 0.4983054995536804, 0.4810781478881836, 0.00018574186833575368, 0.3073505163192749, 0.43987682461738586, 0.2777775228023529, 0.1536940038204193, 0.05884025990962982, 0.060400571674108505, 0.40440550446510315, 0.014526458457112312, 0.19770711660385132, 0.12075463682413101, 0.15117307007312775, 0.22045494616031647, 0.14684157073497772, 0.08113140612840652, 0.4090661406517029, 0.035631876438856125, 0.3717552721500397, 0.37668055295944214, 0.3448239862918854, 0.48003795742988586, 0.053784433752298355, 0.19108575582504272, 0.21184059977531433, 0.11878865212202072], dtype='float32').reshape([30]),
            paddle.to_tensor([0.005463451612740755, 0.38651999831199646, 0.32105815410614014, 0.47207221388816833, 0.38869068026542664, 0.023851579055190086, 0.149016335606575, 0.36519330739974976, 0.4582411050796509, 0.059630297124385834, 0.1409219354391098, 0.0856667309999466, 0.4415678381919861, 0.001053343527019024, 0.056907977908849716, 0.22871269285678864, 0.0062204767018556595, 0.41302230954170227, 0.11223326623439789, 0.10924393683671951, 0.23379956185817719, 0.2281922549009323, 0.40996432304382324, 0.09686969965696335, 0.4585306644439697, 0.17171356081962585, 0.310493141412735, 0.21892224252223969, 0.34389033913612366, 0.30320894718170166], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_430952fe988d651b72c9e9c7ec89096b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([8, 256, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_121285eab43d364c3b8eaad8ceccca90(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 2, 112, 199], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.14988777041435242, 0.0660378634929657], dtype='float32').reshape([2]),
            paddle.to_tensor([0.4690491557121277, 0.43139365315437317], dtype='float32').reshape([2]),
            paddle.to_tensor([0.046419933438301086, 0.3336877226829529], dtype='float32').reshape([2]),
            paddle.to_tensor([0.4804142117500305, 0.47639885544776917], dtype='float32').reshape([2]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_64aec9621ba7b2e43e0f3a0bd25a6207(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 160, 160], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8cf7316702ee2c8c243bc7088e656a3e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 16, 16], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2f7708197cbf2a878490f5319b673eaa(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 144, 16, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b7f4ac1739e33ec250212f0e7ff03838(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 38, 38], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c9e24031fa87c6968e4c41876914940e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 8, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6a2c419721a2543439b56a8916a5a1d6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_3a47a120d62a3f3a422a747f511df8d9
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 150, 150], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f0ee75dbd23e2c0d57701a454c5ff2ae(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 432, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([432], dtype='float32', min=0, max=0.5),
            paddle.uniform([432], dtype='float32', min=0, max=0.5),
            paddle.uniform([432], dtype='float32', min=0, max=0.5),
            paddle.uniform([432], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_be54e93bdd5284ccf261683f4fa893c4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7f9c8e9b6313283ba3cbb7bf7dad5a18(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.033801089972257614, 0.0997665748000145, 0.43158549070358276, 0.025789806619286537, 0.03947991877794266, 0.3312632441520691, 0.05046870559453964, 0.23928113281726837, 0.13176082074642181, 0.309985876083374, 0.24719077348709106, 0.11815328896045685, 0.2434738427400589, 0.3141988217830658, 0.3564942479133606, 0.3695310652256012, 0.0020387552212923765, 0.3364078998565674, 0.16280387341976166, 0.3641359806060791], dtype='float32').reshape([20]),
            paddle.to_tensor([0.27329209446907043, 0.42545202374458313, 0.4377445876598358, 0.0034533371217548847, 0.37074777483940125, 0.42145630717277527, 0.10150715708732605, 0.018026210367679596, 0.1525866538286209, 0.0949864536523819, 0.12831035256385803, 0.29414087533950806, 0.3103164732456207, 0.33700206875801086, 0.009985125623643398, 0.3387131690979004, 0.07550692558288574, 0.2789250314235687, 0.27647775411605835, 0.27544155716896057], dtype='float32').reshape([20]),
            paddle.to_tensor([0.46891313791275024, 0.3098979592323303, 0.4480680525302887, 0.0736025795340538, 0.38459229469299316, 0.2360886186361313, 0.07536476105451584, 0.377002477645874, 0.42692914605140686, 0.32809150218963623, 0.020460380241274834, 0.07707490026950836, 0.3606189787387848, 0.34378287196159363, 0.2423757016658783, 0.26425307989120483, 0.09154105931520462, 0.3075561225414276, 0.13314276933670044, 0.0022357814013957977], dtype='float32').reshape([20]),
            paddle.to_tensor([0.1218063235282898, 0.3038828670978546, 0.03895460069179535, 0.2946072518825531, 0.40880391001701355, 0.41322067379951477, 0.2793310582637787, 0.1412394940853119, 0.052051447331905365, 0.36140790581703186, 0.45248284935951233, 0.07809560000896454, 0.11434502899646759, 0.18378272652626038, 0.191138356924057, 0.1635466068983078, 0.45946204662323, 0.05901912972331047, 0.4202323257923126, 0.1543794870376587], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_176ec3a270238334358328d07a041566(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 240, 240], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9a5e4fd533f3ef908a0a0957f57690ca(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_da4d85bc213f70819a72e68f21214cdb
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 32, 8, 8], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_34f538d80160ab024017022b51ab9f3a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 12, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.30863142013549805, 0.4815136194229126, 0.30531370639801025, 0.44595974683761597, 0.40097370743751526, 0.29665282368659973, 0.257607638835907, 0.45845237374305725, 0.08108267188072205, 0.10981456190347672, 0.3674909472465515, 0.2842317819595337], dtype='float32').reshape([12]),
            paddle.to_tensor([0.027620550245046616, 0.4584393799304962, 0.23212164640426636, 0.13546150922775269, 0.1582944095134735, 0.2853291630744934, 0.3332447409629822, 0.39805173873901367, 0.41712549328804016, 0.1962161660194397, 0.3029310405254364, 0.31220266222953796], dtype='float32').reshape([12]),
            paddle.to_tensor([0.24067682027816772, 0.47653964161872864, 0.49944573640823364, 0.38124099373817444, 0.14804337918758392, 0.3583293557167053, 0.11550416052341461, 0.2763727009296417, 0.21279260516166687, 0.3365224003791809, 0.01237911731004715, 0.2194843590259552], dtype='float32').reshape([12]),
            paddle.to_tensor([0.3739148676395416, 0.10829869657754898, 0.22321271896362305, 0.3563970625400543, 0.007409342098981142, 0.1703394651412964, 0.16411468386650085, 0.38102924823760986, 0.07667437195777893, 0.24305970966815948, 0.4527365267276764, 0.10954023152589798], dtype='float32').reshape([12]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_09203dcc29a15f88b9ae20bea9b1cfa6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ef8383315e9fd5b39a2fc2ff184c0d51(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 480, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b2df2ce156ba2a71e0dc28492905ca9d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5f35f5ad20a4138972922db14005ece2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_da4d85bc213f70819a72e68f21214cdb
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 4, 16, 16], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2ed4c31fcd8c6db6a7350e137957e302(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 640, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e9e03ce9eb9db3725cf5453210511178(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 2048, 64, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_89eade693c25932d04c67f3c7ba6845d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 50, 84], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_db3307622c8de0068b4d8e56e0f4fe68(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 60, 60], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_11bea1026744e3807e4c3ecdfbc2d62e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 256, 256], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f4ff3f0add67c83e386aa08f43a06e0b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([8, 64, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1ec578d50802117f24d62a45ca63a41d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 228, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([228], dtype='float32', min=0, max=0.5),
            paddle.uniform([228], dtype='float32', min=0, max=0.5),
            paddle.uniform([228], dtype='float32', min=0, max=0.5),
            paddle.uniform([228], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e54d5841d053102fe6c9b571c065550d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 4, 26], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_dc43dd0015eecaab463b62a64d5fa105(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 19, 19], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_78526eb339c022cc1f78a69eec0682f9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 2, 2], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_08ecfbca71b5233f6357b289e88ae3f0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6159b4ea3940844fcc323f50c946d485(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_3a47a120d62a3f3a422a747f511df8d9
    def get_inputs(self):
        return [
            paddle.uniform([1, 728, 19, 19], dtype='float16', min=0, max=0.5),
            paddle.uniform([728], dtype='float32', min=0, max=0.5),
            paddle.uniform([728], dtype='float32', min=0, max=0.5),
            paddle.uniform([728], dtype='float32', min=0, max=0.5),
            paddle.uniform([728], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6f83ab72978ada6c496ed4a598104d59(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 256, 256], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_eac502bad7ee4bc47a14963db719cbb0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 12, 40], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b7e67a9ae86fbbe874ea4b5585b0a952(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 10, 10], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_69d37a6b9a57c1a6cca5145294985547(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 640, 640], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_99fe90696970908a799e2b36ff07c60f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_41c4069d4ffdc77ecbca165e7912fd61(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_fc6b6917e04b7b7cf9fdb39dde97ec40(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_218d9f0dcbfe880db124463f56d7db14(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_fc6b6917e04b7b7cf9fdb39dde97ec40
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 80, 80], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_53c03582fdbe946c81856c99c10397e5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_696ac6583a0c789762ac239e1e0c7431(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 76, 76], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6ef8c5c6fd3cacb543ec996455da8a55(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 480, 2, 50], dtype='float16', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2afed2188fc5658e23815e9de49cc7fd(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1012792595c8e203b63c6e4e0f975c52(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a8301b810bccba10e4dfdee0df73334
    def get_inputs(self):
        return [
            paddle.uniform([1, 728, 19, 19], dtype='float32', min=0, max=0.5),
            paddle.uniform([728], dtype='float32', min=0, max=0.5),
            paddle.uniform([728], dtype='float32', min=0, max=0.5),
            paddle.uniform([728], dtype='float32', min=0, max=0.5),
            paddle.uniform([728], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a33430326591022362acec9607a09fce(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 80, 8, 6], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d9169045ab642af435891b0879aadaa9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 480, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0dc18fb58b442e885e791aef4f565826(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 56, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_183fecd1ba0631fd4f29b967c2b6012d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 350, 25], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b91fe106658097fb04407597f8f897da(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.0464291125535965, 0.04538453742861748, 0.15706738829612732, 0.4741125702857971, 0.4203767478466034, 0.45527875423431396, 0.17469677329063416, 0.460328608751297, 0.04749887064099312, 0.16155773401260376, 0.48319417238235474, 0.4897460639476776, 0.0946086049079895, 0.16118124127388, 0.31458407640457153, 0.4075525104999542, 0.4819018840789795, 0.34870341420173645], dtype='float32').reshape([18]),
            paddle.to_tensor([0.47053226828575134, 0.01646428368985653, 0.24043209850788116, 0.48813655972480774, 0.14217568933963776, 0.2904984652996063, 0.08437768369913101, 0.08415398746728897, 0.30865946412086487, 0.15495645999908447, 0.4351688325405121, 0.12038283050060272, 0.4540969133377075, 0.4429810345172882, 0.2665213942527771, 0.008664777502417564, 0.3219486474990845, 0.2520763576030731], dtype='float32').reshape([18]),
            paddle.to_tensor([0.24276202917099, 0.08437146246433258, 0.43586426973342896, 0.15579253435134888, 0.46083927154541016, 0.26473844051361084, 0.026267610490322113, 0.41418635845184326, 0.42666006088256836, 0.05173168703913689, 0.36890044808387756, 0.2412777692079544, 0.4961250424385071, 0.4490858018398285, 0.015941524878144264, 0.2802334129810333, 0.34495970606803894, 0.13095377385616302], dtype='float32').reshape([18]),
            paddle.to_tensor([0.4677999019622803, 0.17810021340847015, 0.2611295282840729, 0.27723827958106995, 0.39248600602149963, 0.02772645838558674, 0.4297061264514923, 0.07298687845468521, 0.12631873786449432, 0.23185540735721588, 0.3539925217628479, 0.4463989734649658, 0.38595205545425415, 0.17790114879608154, 0.23368416726589203, 0.282478928565979, 0.024132803082466125, 0.03499725088477135], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e02bd9b223075dd2023e26edbe2e5a92(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_0edd578f48a07aa5d1d79f9f678b472e
    def get_inputs(self):
        return [
            paddle.uniform([16, 384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_58c3adacf0d2f4b7df3d8db3096515c4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d1900f2e229a8c61388c6b41417f6e76(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 416, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([416], dtype='float32', min=0, max=0.5),
            paddle.uniform([416], dtype='float32', min=0, max=0.5),
            paddle.uniform([416], dtype='float32', min=0, max=0.5),
            paddle.uniform([416], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d5b2d619ba50b4a4103af73bf9d676e5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 4, 240, 240], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.22957834601402283, 0.09308242797851562, 0.09856889396905899, 0.41694575548171997], dtype='float32').reshape([4]),
            paddle.to_tensor([0.12396826595067978, 0.48722875118255615, 0.088325634598732, 0.3658806085586548], dtype='float32').reshape([4]),
            paddle.to_tensor([0.12332462519407272, 0.2625463306903839, 0.25691384077072144, 0.4208780527114868], dtype='float32').reshape([4]),
            paddle.to_tensor([0.1986144483089447, 0.4546404480934143, 0.4292086660861969, 0.01755201816558838], dtype='float32').reshape([4]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fc69e580e1f5bf769ce9df1dbdadaf51(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 72, 32, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e9b2b0605f574dc9619ea28220e079c8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 410, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([410], dtype='float32', min=0, max=0.5),
            paddle.uniform([410], dtype='float32', min=0, max=0.5),
            paddle.uniform([410], dtype='float32', min=0, max=0.5),
            paddle.uniform([410], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0a001b2820be98ff44a03464682375ed(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c5c711dbb6b10d9958a6315063fbf816(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 12, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.38847285509109497, 0.15227504074573517, 0.11920929700136185, 0.01785268262028694, 0.1467314213514328, 0.22583582997322083, 0.10762465000152588, 0.17802785336971283, 0.03851570188999176, 0.3753472566604614, 0.210448756814003, 0.26698946952819824], dtype='float32').reshape([12]),
            paddle.to_tensor([0.10238496959209442, 0.13464535772800446, 0.49308130145072937, 0.18103747069835663, 0.039028119295835495, 0.1425386220216751, 0.36164483428001404, 0.16489052772521973, 0.28651177883148193, 0.07467258721590042, 0.07139033079147339, 0.4252541959285736], dtype='float32').reshape([12]),
            paddle.to_tensor([0.4010891616344452, 0.05312168225646019, 0.03130098432302475, 0.4616217613220215, 0.35646021366119385, 0.11140583455562592, 0.009689073078334332, 0.14086014032363892, 0.1543915569782257, 0.09702037274837494, 0.44668951630592346, 0.145575150847435], dtype='float32').reshape([12]),
            paddle.to_tensor([0.17919939756393433, 0.4317523241043091, 0.2919354736804962, 0.35695773363113403, 0.4566723108291626, 0.43633660674095154, 0.4266854524612427, 0.4557671844959259, 0.23806796967983246, 0.0052697379142045975, 0.1636722981929779, 0.3165011405944824], dtype='float32').reshape([12]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d87726b8dda81de6d415c90ccce7d116(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 26, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.12835198640823364, 0.321270614862442, 0.05706533417105675, 0.3059636056423187, 0.00465283403173089, 0.03937748447060585, 0.20930823683738708, 0.00457577733322978, 0.31151247024536133, 0.14858092367649078, 0.4549747705459595, 0.26163220405578613, 0.09530606120824814, 0.26084038615226746, 0.44563665986061096, 0.383786141872406, 0.16319550573825836, 0.4241468906402588, 0.3052743375301361, 0.48146459460258484, 0.17745782434940338, 0.2849389314651489, 0.07800326496362686, 0.21736465394496918, 0.15827128291130066, 0.2691965699195862], dtype='float32').reshape([26]),
            paddle.to_tensor([0.2245461493730545, 0.4161500930786133, 0.0697682574391365, 0.42352423071861267, 0.3135800361633301, 0.42535921931266785, 0.4182533025741577, 0.48438510298728943, 0.22201773524284363, 0.06777776777744293, 0.4411766827106476, 0.023696787655353546, 0.0406164675951004, 0.44391363859176636, 0.44139623641967773, 0.3945164382457733, 0.35667410492897034, 0.32572001218795776, 0.22883476316928864, 0.05299738422036171, 0.44400933384895325, 0.13903798162937164, 0.21021819114685059, 0.4711228907108307, 0.042294472455978394, 0.4102746248245239], dtype='float32').reshape([26]),
            paddle.to_tensor([0.21262529492378235, 0.251199871301651, 0.01478856336325407, 0.10764973610639572, 0.4387189447879791, 0.41298919916152954, 0.37906745076179504, 0.21285922825336456, 0.3935145139694214, 0.39177051186561584, 0.3334619402885437, 0.4110656976699829, 0.393359899520874, 0.25429868698120117, 0.30615556240081787, 0.30278724431991577, 0.4588088393211365, 0.1393267959356308, 0.3124525547027588, 0.029298758134245872, 0.459507018327713, 0.08204207569360733, 0.46965062618255615, 0.4873667061328888, 0.45912522077560425, 0.0750567689538002], dtype='float32').reshape([26]),
            paddle.to_tensor([0.25302374362945557, 0.07109671831130981, 0.08930433541536331, 0.26297351717948914, 0.21834062039852142, 0.20772719383239746, 0.05793776735663414, 0.3124752342700958, 0.23527368903160095, 0.026194440200924873, 0.18692654371261597, 0.07326053828001022, 0.15562282502651215, 0.11149740219116211, 0.3916027247905731, 0.1093013659119606, 0.09169333428144455, 0.12673325836658478, 0.2889346182346344, 0.14606614410877228, 0.11598560214042664, 0.10016698390245438, 0.33504146337509155, 0.47472137212753296, 0.4102489650249481, 0.1802489459514618], dtype='float32').reshape([26]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d3bb317e2490c6a31555be1ecc45448d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 72, 32, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_01990334c5da521be99a8d8e076c511e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 50, 84], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_07abc2fa4dee0778c0d7c2d77d374a24(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 208, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([208], dtype='float32', min=0, max=0.5),
            paddle.uniform([208], dtype='float32', min=0, max=0.5),
            paddle.uniform([208], dtype='float32', min=0, max=0.5),
            paddle.uniform([208], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6b861ac8b1a6bf8f6dd9ebfc678d4898(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.005852916277945042, 0.4101138114929199, 0.06714354455471039, 0.23012204468250275, 0.4086144268512726, 0.409587562084198, 0.2159123420715332, 0.12081624567508698, 0.44012296199798584, 0.022620076313614845, 0.0518144927918911, 0.36060255765914917, 0.470304012298584, 0.06542416661977768, 0.15781405568122864, 0.3061596751213074, 0.38913285732269287, 0.4410591423511505, 0.4452938735485077, 0.16157229244709015, 0.2129702866077423, 0.35645943880081177, 0.0289683248847723, 0.14199261367321014], dtype='float32').reshape([24]),
            paddle.to_tensor([0.3359677791595459, 0.12655174732208252, 0.1889328509569168, 0.3885960578918457, 0.1778113692998886, 0.2534346580505371, 0.16926436126232147, 0.0994793102145195, 0.24571940302848816, 0.1929623931646347, 0.1579686850309372, 0.4902787208557129, 0.15160657465457916, 0.11028923839330673, 0.04781148582696915, 0.13523142039775848, 0.07683313637971878, 0.1441914588212967, 0.0003793449723161757, 0.17266081273555756, 0.2045287936925888, 0.10532345622777939, 0.25742560625076294, 0.12463309615850449], dtype='float32').reshape([24]),
            paddle.to_tensor([0.1449688971042633, 0.165817990899086, 0.42564424872398376, 0.4636206030845642, 0.21225020289421082, 0.017398511990904808, 0.31627732515335083, 0.017855990678071976, 0.4895354211330414, 0.174386128783226, 0.46880242228507996, 0.24626214802265167, 0.2107965052127838, 0.16309836506843567, 0.11818424612283707, 0.42541244626045227, 0.45007169246673584, 0.3274771273136139, 0.22880102694034576, 0.4672875702381134, 0.22037076950073242, 0.07152537256479263, 0.2159518003463745, 0.2469971626996994], dtype='float32').reshape([24]),
            paddle.to_tensor([0.49986758828163147, 0.21717312932014465, 0.33331531286239624, 0.09137836843729019, 0.3000735640525818, 0.39824870228767395, 0.4061768651008606, 0.035919416695833206, 0.14004535973072052, 0.3669655919075012, 0.17661094665527344, 0.36575013399124146, 0.17646169662475586, 0.48604196310043335, 0.1376163214445114, 0.42146873474121094, 0.37178942561149597, 0.3106604516506195, 0.2925601005554199, 0.30342769622802734, 0.3117887079715729, 0.39034369587898254, 0.49724844098091125, 0.20250535011291504], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a7e0d149c519d3a422d50aa390ec5832(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 112, 112], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9896baa85c577d8626e158f27caf5d23(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 128, 128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0d0d368441f890e35e0b943273997d24(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 104, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2d925e2b9200fdba607c48049c44c0ad(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.07142568379640579, 0.44652894139289856, 0.20886482298374176, 0.4290983974933624, 0.0035959454253315926, 0.2760245203971863, 0.14500632882118225, 0.21491552889347076, 0.05526125058531761, 0.24003174901008606, 0.34073254466056824, 0.0981970876455307, 0.05532527342438698, 0.3648165762424469, 0.22687360644340515, 0.15151579678058624, 0.08753686398267746, 0.3551648259162903, 0.26645058393478394, 0.1329326331615448, 0.371937096118927, 0.28355714678764343, 0.48538312315940857, 0.30824434757232666, 0.23152559995651245, 0.17214815318584442, 0.2844855785369873, 0.04848790541291237, 0.04371509701013565, 0.11620986461639404], dtype='float32').reshape([30]),
            paddle.to_tensor([0.13202577829360962, 0.38121485710144043, 0.3779209554195404, 0.05059757083654404, 0.16808336973190308, 0.06390267610549927, 0.037401679903268814, 0.29808738827705383, 0.024770909920334816, 0.29522794485092163, 0.49681591987609863, 0.05409087985754013, 0.15761731564998627, 0.2229590117931366, 0.08585073053836823, 0.2670867443084717, 0.43751654028892517, 0.18601122498512268, 0.09839876741170883, 0.45334306359291077, 0.09146901965141296, 0.028283195570111275, 0.0703815445303917, 0.34062865376472473, 0.07213123142719269, 0.17866817116737366, 0.03841894865036011, 0.37776467204093933, 0.05812543258070946, 0.43923085927963257], dtype='float32').reshape([30]),
            paddle.to_tensor([0.41009196639060974, 0.020518936216831207, 0.08281155675649643, 0.4112356901168823, 0.2168174833059311, 0.24922850728034973, 0.346205472946167, 0.35281938314437866, 0.2637592554092407, 0.3680144250392914, 0.28433629870414734, 0.17264878749847412, 0.03140268847346306, 0.2858657240867615, 0.28790566325187683, 0.28433749079704285, 0.07243486493825912, 0.19792743027210236, 0.36068373918533325, 0.4437618851661682, 0.027325673028826714, 0.4617871344089508, 0.2469063401222229, 0.091925710439682, 0.47578373551368713, 0.43269461393356323, 0.3476276397705078, 0.45174697041511536, 0.42661938071250916, 0.3533342480659485], dtype='float32').reshape([30]),
            paddle.to_tensor([0.4453590512275696, 0.19762173295021057, 0.26609131693840027, 0.39616769552230835, 0.36885279417037964, 0.3382258713245392, 0.43402716517448425, 0.15941941738128662, 0.16395226120948792, 0.10555913299322128, 0.06306856870651245, 0.006840168964117765, 0.0701320543885231, 0.15823709964752197, 0.29758161306381226, 0.40363821387290955, 0.4663820266723633, 0.357583612203598, 0.377606064081192, 0.09753351658582687, 0.04312996193766594, 0.40655362606048584, 0.3014279901981354, 0.0827743411064148, 0.03841051459312439, 0.13013909757137299, 0.4062126576900482, 0.3874121904373169, 0.13588550686836243, 0.01785937510430813], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_88f594b94ed79c90a3762907063b8ca6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 50, 84], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_892737f434d58f89232a4f0e4c292837(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.014614918269217014, 0.16851314902305603, 0.1086573451757431, 0.2629682719707489, 0.26999586820602417, 0.33993032574653625, 0.12246030569076538, 0.35208773612976074, 0.2111542969942093, 0.3949815034866333, 0.013607379980385303, 0.24921740591526031, 0.18520070612430573, 0.09332581609487534, 0.3085099458694458, 0.1221303939819336, 0.05505518987774849, 0.2623565196990967, 0.18150781095027924, 0.3867487907409668, 0.11804252117872238, 0.22881723940372467, 0.03850516304373741, 0.11023402959108353, 0.02561766467988491, 0.31699052453041077, 0.2370890974998474, 0.4066220223903656, 0.2693047523498535, 0.1555619090795517], dtype='float32').reshape([30]),
            paddle.to_tensor([0.15109217166900635, 0.32819196581840515, 0.4194304645061493, 0.2604173719882965, 0.25132453441619873, 0.20878511667251587, 0.1891440600156784, 0.23973284661769867, 0.49473729729652405, 0.36500898003578186, 0.07020407170057297, 0.17480537295341492, 0.22520476579666138, 0.18732444941997528, 0.056422580033540726, 0.3648427128791809, 0.26220279932022095, 0.2678626477718353, 0.4447740614414215, 0.3156129717826843, 0.3204907178878784, 0.04508337751030922, 0.25467512011528015, 0.41528084874153137, 0.044741302728652954, 0.463013619184494, 0.29711222648620605, 0.07325465977191925, 0.39654210209846497, 0.47169309854507446], dtype='float32').reshape([30]),
            paddle.to_tensor([0.31584203243255615, 0.2813560664653778, 0.4127698540687561, 0.096098393201828, 0.40487203001976013, 0.22934162616729736, 0.3476542532444, 0.36142075061798096, 0.10173724591732025, 0.2954200506210327, 0.08153963088989258, 0.06670564413070679, 0.0395747646689415, 0.1271359771490097, 0.45796939730644226, 0.1278051882982254, 0.3919612467288971, 0.282863050699234, 0.15317940711975098, 0.22722956538200378, 0.4946589767932892, 0.007845577783882618, 0.22267507016658783, 0.47670653462409973, 0.4173136055469513, 0.26810041069984436, 0.42033156752586365, 0.23165971040725708, 0.0990230068564415, 0.2848545014858246], dtype='float32').reshape([30]),
            paddle.to_tensor([0.27836790680885315, 0.33460119366645813, 0.3622412085533142, 0.015983248129487038, 0.40713104605674744, 0.09240823984146118, 0.21010518074035645, 0.06622570753097534, 0.26016417145729065, 0.2562212646007538, 0.3725777864456177, 0.40073177218437195, 0.19326402246952057, 0.20650571584701538, 0.12444529682397842, 0.34508606791496277, 0.2747591733932495, 0.18450316786766052, 0.407730370759964, 0.25331270694732666, 0.06377914547920227, 0.24885109066963196, 0.03567684814333916, 0.25214532017707825, 0.4103116989135742, 0.2988484799861908, 0.30135446786880493, 0.0229006540030241, 0.23627567291259766, 0.013977335765957832], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_c8eca113b3029d88b25158f95c03e4fe(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-06'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8256765909154d5f83366d5bdeefe1ee(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_c8eca113b3029d88b25158f95c03e4fe
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 32, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0c8d09166fd095c7e783c5d46de99856(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.16210855543613434, 0.3272436559200287, 0.3161545395851135, 0.4237343668937683, 0.4912666380405426, 0.49556782841682434, 0.4559853672981262, 0.45815202593803406, 0.10423529893159866, 0.22972287237644196, 0.1107659861445427, 0.2818334698677063, 0.01380177028477192, 0.14779292047023773, 0.4742876887321472, 0.4483310580253601, 0.47831350564956665, 0.1220293790102005], dtype='float32').reshape([18]),
            paddle.to_tensor([0.10741604119539261, 0.19312213361263275, 0.22212539613246918, 0.09094449877738953, 0.14766983687877655, 0.03835412114858627, 0.02204497717320919, 0.2544904053211212, 0.30409473180770874, 0.3120570778846741, 0.38387638330459595, 0.033690061420202255, 0.1768893003463745, 0.12230508029460907, 0.45769354701042175, 0.17630347609519958, 0.433313250541687, 0.04398823156952858], dtype='float32').reshape([18]),
            paddle.to_tensor([0.44239214062690735, 0.1959053874015808, 0.013226475566625595, 0.4778142273426056, 0.13472214341163635, 0.2412610799074173, 0.4342725872993469, 0.3281586468219757, 0.2850133776664734, 0.2125752866268158, 0.013621415011584759, 0.23705309629440308, 0.11617307364940643, 0.11160940676927567, 0.29936864972114563, 0.12527360022068024, 0.475422739982605, 0.1187686026096344], dtype='float32').reshape([18]),
            paddle.to_tensor([0.13935793936252594, 0.11995834112167358, 0.02718246541917324, 0.10111235827207565, 0.17955563962459564, 0.24957294762134552, 0.08481715619564056, 0.460380494594574, 0.11210687458515167, 0.4772525727748871, 0.21118493378162384, 0.020342493429780006, 0.19548597931861877, 0.17723293602466583, 0.10575462132692337, 0.16787436604499817, 0.33811622858047485, 0.094424769282341], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_067890abfb83f8910a777be9c42a6152(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_da4d85bc213f70819a72e68f21214cdb
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 4, 32, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3b3a72aba639d468cca3f029698622fb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([8, 64, 112, 112], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_145ad20e7a3b3c8aa31f6b40d4fe3d45(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e5eeb78d95074cfe82c876ece07775ff(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_145ad20e7a3b3c8aa31f6b40d4fe3d45
    def get_inputs(self):
        return [
            paddle.uniform([16, 512], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_60f8b95aba499b85b6b22561e5c747f1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_22e16a2388ae9fc6ae9cde5950dab568(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 17, 17], dtype='float16', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e98ca2b3f7fbfd2040a237455a4a77a6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_94507acd165b07b6baa4d8d0246e1900(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 4, 26], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ea00d92eb13b92b6930b53cc3ccc47a0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a3216f17f3ebae06e93b357d6cf0c915(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 80, 20, 20], dtype='float16', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1e59e7224cfe32d84634d904a309a75b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 2048, 10, 10], dtype='float16', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8834139b6e81cbef21eb6c154545d04e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_40e137450f77ebeae95a0c35f96197d4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 118, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([118], dtype='float32', min=0, max=0.5),
            paddle.uniform([118], dtype='float32', min=0, max=0.5),
            paddle.uniform([118], dtype='float32', min=0, max=0.5),
            paddle.uniform([118], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_a02f1e2888bea69fc79b31a122ed24eb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4116765c902d49014f54dcfb8d783a83(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_a02f1e2888bea69fc79b31a122ed24eb
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 49], dtype='float16', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b9f6a474f5cb148d14fbd14dbc7d9622(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 160, 4, 3], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6122b1a0441921508d39c58afd6eb10e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_360d01f29b1b5ad42d003173d7575fa6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_af91295b59c5fca17f890a802deb001f
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 32, 32, 32], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.016351869329810143, 0.365032821893692, 0.4854353666305542, 0.4037371277809143, 0.4843007028102875, 0.4912204444408417, 0.2892220616340637, 0.25219663977622986, 0.13464410603046417, 0.17323538661003113, 0.17872555553913116, 0.059904150664806366, 0.43341606855392456, 0.11760947853326797, 0.18755558133125305, 0.0729646310210228], dtype='float32').reshape([16]),
            paddle.to_tensor([0.011227174662053585, 0.1097465455532074, 0.06905337423086166, 0.28070417046546936, 0.405910849571228, 0.15306974947452545, 0.07823700457811356, 0.3319196403026581, 0.4949124753475189, 0.0420643575489521, 0.36575913429260254, 0.14638853073120117, 0.30180543661117554, 0.4755239188671112, 0.42324256896972656, 0.4873114228248596], dtype='float32').reshape([16]),
            paddle.to_tensor([0.15056391060352325, 0.18465843796730042, 0.3467945456504822, 0.15493768453598022, 0.33309921622276306, 0.058962348848581314, 0.4371882975101471, 0.4674690067768097, 0.27794086933135986, 0.13305066525936127, 0.07023237645626068, 0.0030591688118875027, 0.27570387721061707, 0.44551461935043335, 0.31829842925071716, 0.08879201114177704], dtype='float32').reshape([16]),
            paddle.to_tensor([0.3867628574371338, 0.15565669536590576, 0.1337127834558487, 0.3492707908153534, 0.3508647680282593, 0.2981593906879425, 0.2942676544189453, 0.0940355584025383, 0.29978513717651367, 0.203981414437294, 0.20133469998836517, 0.15366169810295105, 0.3895914554595947, 0.26266148686408997, 0.31403660774230957, 0.2122689187526703], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b1f8c57a96ac87455883a1c03cde7874(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 20, 20], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1ad9879d9fcee3a1929dbc8acc726c49(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 1776, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([1776], dtype='float32', min=0, max=0.5),
            paddle.uniform([1776], dtype='float32', min=0, max=0.5),
            paddle.uniform([1776], dtype='float32', min=0, max=0.5),
            paddle.uniform([1776], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1557ec0894a2f8cfb08dfd5c77714928(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 4, 26], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7334d82914828d2e07ceb816ab037200(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 320, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_16b3f84f42f4a8263accbda1d3cf5793(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 40, 40], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a83c093fc4925508d4cbebbbe2a3e146(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_3a47a120d62a3f3a422a747f511df8d9
    def get_inputs(self):
        return [
            paddle.uniform([1, 480, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_45a61101c044f7bf6622cdd807c27f03(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 30, 30], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_738daedfb3a4fb3f6bf032c7d7f24af2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 56, 40, 40], dtype='float16', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6698206ccd58209134b1df29b48d85d0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_fc6b6917e04b7b7cf9fdb39dde97ec40
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 40, 40], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f27b73d4f01bbe03786b1a2c1cd9ca5b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 2048, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_aa92acc5daa3be34542850d1371b9f27(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e546b708569e8d38741e5ecf599d46fc(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 61, 61], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_641fa009ab43def83afabee0e551174b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_17c96b06f3e2721752f0199c364cb799(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 320, 320], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.10529342293739319, 0.36454296112060547, 0.2163313776254654, 0.21458959579467773, 0.08144788444042206, 0.12936446070671082, 0.3706201910972595, 0.30302560329437256], dtype='float32').reshape([8]),
            paddle.to_tensor([0.064081571996212, 0.44077175855636597, 0.182289719581604, 0.1524200141429901, 0.43694359064102173, 0.25358283519744873, 0.014107448980212212, 0.4623524844646454], dtype='float32').reshape([8]),
            paddle.to_tensor([0.03944256901741028, 0.06431685388088226, 0.4058558940887451, 0.1016252189874649, 0.4138205349445343, 0.2123074233531952, 0.006756257265806198, 0.424261212348938], dtype='float32').reshape([8]),
            paddle.to_tensor([0.3304344713687897, 0.41184914112091064, 0.38600531220436096, 0.06566286832094193, 0.12684769928455353, 0.16866742074489594, 0.04583623632788658, 0.21056821942329407], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ae2c3bc40500c3f091be3e026b6d6983(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 320, 320], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.14275719225406647, 0.30527475476264954, 0.17192158102989197, 0.3369510769844055, 0.09127496927976608, 0.20456981658935547, 0.35674336552619934, 0.14151448011398315, 0.19459891319274902, 0.24618877470493317, 0.13953132927417755, 0.4253520369529724, 0.39784109592437744, 0.3036534786224365, 0.39568307995796204, 0.21528303623199463], dtype='float32').reshape([16]),
            paddle.to_tensor([0.024591904133558273, 0.38532015681266785, 0.24695627391338348, 0.05723090097308159, 0.07996148616075516, 0.40486952662467957, 0.3729487955570221, 0.39677971601486206, 0.09081865102052689, 0.0645708441734314, 0.07784170657396317, 0.07415221631526947, 0.13176962733268738, 0.2496061474084854, 0.03586077317595482, 0.0445263609290123], dtype='float32').reshape([16]),
            paddle.to_tensor([0.21646302938461304, 0.4901997148990631, 0.4564302861690521, 0.4448430836200714, 0.17200607061386108, 0.08060300350189209, 0.29903820157051086, 0.18441098928451538, 0.1573895663022995, 0.3132758140563965, 0.48525768518447876, 0.2277773767709732, 0.4869825541973114, 0.16973355412483215, 0.070552758872509, 0.011551463976502419], dtype='float32').reshape([16]),
            paddle.to_tensor([0.34219902753829956, 0.1559128761291504, 0.404989093542099, 0.38003668189048767, 0.10967203229665756, 0.16046211123466492, 0.4675765633583069, 0.4010494649410248, 0.23250968754291534, 0.440714567899704, 0.45660141110420227, 0.22456872463226318, 0.3990720212459564, 0.4899826645851135, 0.30529239773750305, 0.015579302795231342], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_27ccec9ceda761ba03d4ed9915254f5c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.23140111565589905, 0.3627079427242279, 0.20972013473510742, 0.15150342881679535, 0.19996602833271027, 0.2704237103462219, 0.34612125158309937, 0.20504559576511383, 0.10622728615999222, 0.17649398744106293, 0.1721685230731964, 0.1731203943490982, 0.13238024711608887, 0.301096647977829, 0.012531868182122707, 0.016426345333456993, 0.24656109511852264, 0.38984695076942444, 0.16886483132839203, 0.07473324239253998, 0.42506417632102966, 0.18664297461509705, 0.07195865362882614, 0.21635182201862335], dtype='float32').reshape([24]),
            paddle.to_tensor([0.4027324318885803, 0.36551812291145325, 0.1891956776380539, 0.21164783835411072, 0.05094939097762108, 0.05851272866129875, 0.06769627332687378, 0.06251543760299683, 0.22945666313171387, 0.039898935705423355, 0.3337092399597168, 0.4073289930820465, 0.06195820868015289, 0.30449244379997253, 0.22189126908779144, 0.35612940788269043, 0.12275958061218262, 0.10689089447259903, 0.4394473731517792, 0.1045248731970787, 0.41991832852363586, 0.3155243992805481, 0.4594903588294983, 0.07029987126588821], dtype='float32').reshape([24]),
            paddle.to_tensor([0.11507131904363632, 0.22302518784999847, 0.0012552209664136171, 0.06815658509731293, 0.045700617134571075, 0.29135361313819885, 0.23828178644180298, 0.037488650530576706, 0.41910386085510254, 0.04369736462831497, 0.44088318943977356, 0.34186264872550964, 0.31222590804100037, 0.00277457432821393, 0.35791388154029846, 0.09213227033615112, 0.2845199704170227, 0.392488032579422, 0.4511697292327881, 0.414360910654068, 0.2406311184167862, 0.019689470529556274, 0.4149763584136963, 0.13664360344409943], dtype='float32').reshape([24]),
            paddle.to_tensor([0.013669109903275967, 0.11387849599123001, 0.1741631180047989, 0.2999260127544403, 0.19631807506084442, 0.07517296075820923, 0.17296266555786133, 0.4988290071487427, 0.14305542409420013, 0.13479216396808624, 0.3305836319923401, 0.27924057841300964, 0.4500753879547119, 0.2621114253997803, 0.18913984298706055, 0.3571982681751251, 0.35659241676330566, 0.07922801375389099, 0.19789691269397736, 0.1464020013809204, 0.19881534576416016, 0.3399718403816223, 0.08132041245698929, 0.15046806633472443], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_523959420b2303693f728d445dac6d32(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 1, 1], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ad1f3ec9740f5898d60480d344f97741(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 8, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9f48d70023c69288d00c6f0215424282(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 768, 32, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7a744866386aab0c7cba4745ce3875c8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 32, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_489c40e3a8afb4709c05af985dfb2909(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 26, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.3198933005332947, 0.02234453149139881, 0.01929904893040657, 0.3472166359424591, 0.009506573900580406, 0.2707366645336151, 0.14578862488269806, 0.44285255670547485, 0.4243105947971344, 0.32686859369277954, 0.13708972930908203, 0.35130855441093445, 0.2648393213748932, 0.11341173201799393, 0.13438813388347626, 0.3990105390548706, 0.14959315955638885, 0.2151990681886673, 0.4253913164138794, 0.23890872299671173, 0.23737189173698425, 0.49510350823402405, 0.37724918127059937, 0.15508411824703217, 0.35182106494903564, 0.10130380094051361], dtype='float32').reshape([26]),
            paddle.to_tensor([0.4740086793899536, 0.1280662566423416, 0.26796892285346985, 0.48946771025657654, 0.050805285573005676, 0.17781266570091248, 0.30329766869544983, 0.14995227754116058, 0.16811953485012054, 0.2234751284122467, 0.0573989562690258, 0.19735468924045563, 0.4437968134880066, 0.057426970452070236, 0.12663540244102478, 0.44353187084198, 0.06185057759284973, 0.0844699814915657, 0.30842486023902893, 0.43650728464126587, 0.4086412489414215, 0.26684072613716125, 0.42495667934417725, 0.2680937945842743, 0.09633366763591766, 0.013298680074512959], dtype='float32').reshape([26]),
            paddle.to_tensor([0.11682729423046112, 0.36976107954978943, 0.01414666697382927, 0.305046945810318, 0.3603782653808594, 0.3838249146938324, 0.14970214664936066, 0.3199736177921295, 0.2613620460033417, 0.4683108627796173, 0.0737762376666069, 0.010473725385963917, 0.45228469371795654, 0.008384184911847115, 0.1110643520951271, 0.10429253429174423, 0.17336373031139374, 0.03261588141322136, 0.168707937002182, 0.3908293545246124, 0.20125998556613922, 0.36144566535949707, 0.06006043031811714, 0.11040721833705902, 0.385945588350296, 0.18928712606430054], dtype='float32').reshape([26]),
            paddle.to_tensor([0.18075180053710938, 0.45601019263267517, 0.35903510451316833, 0.09233894944190979, 0.47370994091033936, 0.3002532720565796, 0.27430808544158936, 0.23390991985797882, 0.02330894209444523, 0.3349371552467346, 0.4732322096824646, 0.32313209772109985, 0.24721825122833252, 0.3526100516319275, 0.02705528773367405, 0.08221397548913956, 0.2621230483055115, 0.3233223855495453, 0.34345731139183044, 0.36335089802742004, 0.36595624685287476, 0.464568167924881, 0.11432016640901566, 0.2582537531852722, 0.3373422920703888, 0.33601388335227966], dtype='float32').reshape([26]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_07e65756e32cdacabbf7fdc73c01ecee(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cfca0abcb28d56c468721ebd0aeedf64(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4bdb90595ddf49643317659b0909588a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 52, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([52], dtype='float32', min=0, max=0.5),
            paddle.uniform([52], dtype='float32', min=0, max=0.5),
            paddle.uniform([52], dtype='float32', min=0, max=0.5),
            paddle.uniform([52], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5805a5040549b0a687ef969ddc109509(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 640, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_236d86d5f0638c40a3f1874a57050b35(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 64, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_88156497569287f5c01399c7e6489fbb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 10, 10], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_86da30f7829434ed595e9f5d588b0929(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 336, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_95fd195e8169ad49d592a985fe3c2f69(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.4644696116447449, 0.2917326092720032, 0.39824679493904114, 0.19331304728984833, 0.3754020929336548, 0.4048278033733368, 0.06308356672525406, 0.24341827630996704, 0.030840793624520302, 0.18888045847415924, 0.21156026422977448, 0.43236976861953735, 0.4695238173007965, 0.054886430501937866, 0.09557639062404633, 0.18039152026176453, 0.16817045211791992, 0.05271299555897713, 0.03856992721557617, 0.3867906630039215, 0.08395827561616898, 0.4945756793022156, 0.1231127381324768, 0.21099980175495148, 0.03385961055755615, 0.1254386156797409, 0.059845395386219025, 0.16648396849632263, 0.36345335841178894, 0.0012981905601918697], dtype='float32').reshape([30]),
            paddle.to_tensor([0.16793207824230194, 0.2768537998199463, 0.4913659691810608, 0.15081250667572021, 0.05232236534357071, 0.31925272941589355, 0.30097806453704834, 0.1462680846452713, 0.36677086353302, 0.029622239992022514, 0.4384588301181793, 0.482565313577652, 0.44864606857299805, 0.1401982605457306, 0.15267348289489746, 0.0972612053155899, 0.17755363881587982, 0.2596225440502167, 0.16486315429210663, 0.10578646510839462, 0.4099743664264679, 0.29597896337509155, 0.10103271156549454, 0.30516743659973145, 0.4363374710083008, 0.35791265964508057, 0.47252157330513, 0.07325106859207153, 0.30869635939598083, 0.4392431378364563], dtype='float32').reshape([30]),
            paddle.to_tensor([0.20060643553733826, 0.2644520699977875, 0.23350517451763153, 0.039930883795022964, 0.07500415295362473, 0.4983149766921997, 0.18751567602157593, 0.068590447306633, 0.11808640509843826, 0.19884251058101654, 0.2186327874660492, 0.10451831668615341, 0.3598409593105316, 0.4497893750667572, 0.12406349927186966, 0.20898473262786865, 0.2264319360256195, 0.4270266890525818, 0.44744667410850525, 0.18062365055084229, 0.46880024671554565, 0.45587942004203796, 0.11845612525939941, 0.10646175593137741, 0.39013978838920593, 0.4641786515712738, 0.24249960482120514, 0.1457795351743698, 0.023616624996066093, 0.4845764935016632], dtype='float32').reshape([30]),
            paddle.to_tensor([0.19477778673171997, 0.10478050261735916, 0.3808749318122864, 0.4724077880382538, 0.28746160864830017, 0.40107524394989014, 0.13695062696933746, 0.3636597692966461, 0.17119039595127106, 0.49470779299736023, 0.031835731118917465, 0.20996692776679993, 0.01228861790150404, 0.15804114937782288, 0.27123427391052246, 0.3259708285331726, 0.1873006820678711, 0.3746669292449951, 0.30237653851509094, 0.4338512718677521, 0.3786207139492035, 0.007105026859790087, 0.10956152528524399, 0.34701645374298096, 0.49854010343551636, 0.26711505651474, 0.33007916808128357, 0.36969128251075745, 0.06320041418075562, 0.18097972869873047], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2f5fc2b98d6821f038f60e03fd3443d4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bc70696477abe5858d665b60f3258817(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 2048, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_590b938bdacfcee33d0a4259d23a37a6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bff12101d6ca75ac9e9f485c538b85c6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.10730389505624771, 0.4913719594478607, 0.48717644810676575, 0.35383546352386475, 0.31762170791625977, 0.22880183160305023, 0.07819218933582306, 0.013328535482287407, 0.4507797658443451, 0.17974670231342316, 0.03389253094792366, 0.43014827370643616, 0.07796171307563782, 0.018260467797517776, 0.28339889645576477, 0.00018180272309109569, 0.01626656763255596, 0.23690323531627655, 0.3725563585758209, 0.4642467498779297, 0.43274927139282227, 0.296994686126709, 0.35319775342941284, 0.016549471765756607, 0.4461604654788971, 0.12758757174015045, 0.47011271119117737, 0.4360191226005554, 0.39205992221832275, 0.37471985816955566], dtype='float32').reshape([30]),
            paddle.to_tensor([0.17978543043136597, 0.4534783363342285, 0.3980244994163513, 0.15306082367897034, 0.07332079857587814, 0.4508615732192993, 0.33629441261291504, 0.30310481786727905, 0.3082018494606018, 0.41282346844673157, 0.09117098152637482, 0.4843784272670746, 0.3067784011363983, 0.4487273395061493, 0.4786401391029358, 0.11845039576292038, 0.10160675644874573, 0.25927484035491943, 0.4820187985897064, 0.0743587464094162, 0.19648458063602448, 0.048575472086668015, 0.004136678762733936, 0.3423076570034027, 0.46128416061401367, 0.23834477365016937, 0.3912467360496521, 0.03896746411919594, 0.24629244208335876, 0.04838455468416214], dtype='float32').reshape([30]),
            paddle.to_tensor([0.22273844480514526, 0.1404358148574829, 0.2568831145763397, 0.4765680134296417, 0.08946110308170319, 0.4148317575454712, 0.20478606224060059, 0.3337913751602173, 0.150562584400177, 0.16227777302265167, 0.2822679579257965, 0.46407002210617065, 0.46187740564346313, 0.1995859295129776, 0.29020947217941284, 0.16967254877090454, 0.42677804827690125, 0.3690774440765381, 0.395185261964798, 0.24528880417346954, 0.2997523546218872, 0.3888661861419678, 0.421076238155365, 0.1889326125383377, 0.3782711923122406, 0.2906864881515503, 0.29732561111450195, 0.1480730175971985, 0.06175537779927254, 0.09214065968990326], dtype='float32').reshape([30]),
            paddle.to_tensor([0.06954938918352127, 0.3052767515182495, 0.3280656933784485, 0.17445459961891174, 0.21191003918647766, 0.43597060441970825, 0.3704303801059723, 0.3601659834384918, 0.3286464810371399, 0.4592219889163971, 0.306363046169281, 0.09881959110498428, 0.03317280113697052, 0.40925121307373047, 0.37400004267692566, 0.07099532335996628, 0.06168726459145546, 0.023894177749753, 0.11463150382041931, 0.47835153341293335, 0.2706063985824585, 0.34380123019218445, 0.36568695306777954, 0.40126916766166687, 0.4748266637325287, 0.013414059765636921, 0.034299422055482864, 0.39213666319847107, 0.05650224909186363, 0.10644722729921341], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_934ad465a492909700d4b4e94475bb82(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 320, 320], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_81957a704c1947d4666f67228f9b85d1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_09e7061f89f8e6e5faa8aa2d08e3358e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 1, 1], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_03058d14e912fd6847449ad535bc356d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 88, 25], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1a1f2849bb8b420b4f1f901c960beb59(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 72, 14, 25], dtype='float16', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_86826f4d0da7dea9069fe2c17605c70c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 112, 112], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_df7d2390d0e707c2a5b526423cc5922b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.39239346981048584, 0.1712416112422943, 0.2272498458623886, 0.0019133917521685362, 0.22247712314128876, 0.16370512545108795, 0.18654923141002655, 0.3274519741535187, 0.3281332552433014, 0.056221019476652145, 0.22836637496948242, 0.06983742862939835, 0.24239353835582733, 0.04550820216536522, 0.18405967950820923, 0.2809116244316101, 0.21013036370277405, 0.24718625843524933], dtype='float32').reshape([18]),
            paddle.to_tensor([0.19797462224960327, 0.22405096888542175, 0.3559949994087219, 0.12237508594989777, 0.3700949251651764, 0.32387951016426086, 0.06023421883583069, 0.4371655285358429, 0.11671202629804611, 0.14191508293151855, 0.4061477780342102, 0.19008415937423706, 0.15018203854560852, 0.4440266788005829, 0.3818033039569855, 0.2536478340625763, 0.4951344132423401, 0.16780056059360504], dtype='float32').reshape([18]),
            paddle.to_tensor([0.3596769869327545, 0.3883587718009949, 0.47499534487724304, 0.2648419737815857, 0.3844150900840759, 0.05279470607638359, 0.4749813377857208, 0.07211942970752716, 0.18517321348190308, 0.302572101354599, 0.38196924328804016, 0.18692222237586975, 0.27022507786750793, 0.4336407482624054, 0.2197418212890625, 0.3475591242313385, 0.39510294795036316, 0.448661744594574], dtype='float32').reshape([18]),
            paddle.to_tensor([0.49580562114715576, 0.06887573003768921, 0.40672141313552856, 0.10764139145612717, 0.47492527961730957, 0.24897804856300354, 0.3900698721408844, 0.269178181886673, 0.24615398049354553, 0.39928027987480164, 0.14268174767494202, 0.37511754035949707, 0.12988312542438507, 0.09523991495370865, 0.3740653693675995, 0.04800939932465553, 0.21802757680416107, 0.013703662902116776], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_06f7ece2c713a225890ec77419caab4e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.38099387288093567, 0.2598389685153961, 0.030358964577317238, 0.1904681771993637, 0.32664141058921814, 0.20819257199764252, 0.06182502955198288, 0.3390035927295685, 0.06556116044521332, 0.49676865339279175, 0.2174406200647354, 0.22361212968826294, 0.23191982507705688, 0.36573514342308044, 0.4936729073524475, 0.37955084443092346, 0.3293893039226532, 0.47940847277641296, 0.055427536368370056, 0.42071980237960815, 0.3363887369632721, 0.25244924426078796, 0.16182547807693481, 0.2871597707271576, 0.4063695967197418, 0.3035818338394165, 0.0764760971069336, 0.0946182832121849, 0.06952658295631409, 0.4131245017051697], dtype='float32').reshape([30]),
            paddle.to_tensor([0.038660019636154175, 0.40965113043785095, 0.37740954756736755, 0.183845654129982, 0.19525106251239777, 0.23133499920368195, 0.4762241840362549, 0.3618011772632599, 0.2811190187931061, 0.10434540361166, 0.3398408889770508, 0.06480919569730759, 0.44712552428245544, 0.11024929583072662, 0.4341760575771332, 0.43532004952430725, 0.40690022706985474, 0.12086429446935654, 0.274139940738678, 0.2362212836742401, 0.02280663698911667, 0.2710232138633728, 0.19414182007312775, 0.0642748773097992, 0.37146204710006714, 0.18085141479969025, 0.24741758406162262, 0.2709581255912781, 0.3605109751224518, 0.11899205297231674], dtype='float32').reshape([30]),
            paddle.to_tensor([0.40189701318740845, 0.040611185133457184, 0.022275177761912346, 0.00031166657572612166, 0.21427276730537415, 0.07122137397527695, 0.008774460293352604, 0.14448420703411102, 0.41258755326271057, 0.3807358145713806, 0.285265177488327, 0.4873582720756531, 0.07770545035600662, 0.1942955106496811, 0.45563650131225586, 0.4493890106678009, 0.02340666763484478, 0.16226208209991455, 0.3498629331588745, 0.39142870903015137, 0.4729367196559906, 0.0462479405105114, 0.48702767491340637, 0.17173197865486145, 0.2050527036190033, 0.09560322016477585, 0.47849777340888977, 0.2854967415332794, 0.35138002038002014, 0.24209609627723694], dtype='float32').reshape([30]),
            paddle.to_tensor([0.04125318303704262, 0.1808382123708725, 0.39195775985717773, 0.26813042163848877, 0.4557271897792816, 0.12503866851329803, 0.21596968173980713, 0.05854743719100952, 0.4795738458633423, 0.43644240498542786, 0.39841845631599426, 0.4280644357204437, 0.17298679053783417, 0.031159017235040665, 0.1918870359659195, 0.37679022550582886, 0.28252142667770386, 0.05048539862036705, 0.1521012783050537, 0.10537661612033844, 0.413791298866272, 0.4733349680900574, 0.29306498169898987, 0.4002496600151062, 0.27017447352409363, 0.38588643074035645, 0.44829654693603516, 0.2284695953130722, 0.3604258596897125, 0.27260249853134155], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e4bfe680fe98b4715b413f0a652d4cc3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_98c5e1927bb0744faf1707694ffaab06(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 32, 64], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.4171174466609955, 0.37712037563323975, 0.4718839228153229, 0.13734683394432068, 0.11559593677520752, 0.44471481442451477, 0.05248028039932251, 0.356161504983902, 0.07575149089097977, 0.4141785204410553, 0.2623213231563568, 0.41414353251457214, 0.2916812598705292, 0.0994371622800827, 0.08799534291028976, 0.46437546610832214, 0.2508847415447235, 0.32734665274620056], dtype='float32').reshape([18]),
            paddle.to_tensor([0.14300599694252014, 0.48591554164886475, 0.20267750322818756, 0.03141756355762482, 0.20972967147827148, 0.20111827552318573, 0.4626443684101105, 0.37108519673347473, 0.05814206972718239, 0.06822226941585541, 0.32915350794792175, 0.1852419227361679, 0.28340721130371094, 0.06367505341768265, 0.428298681974411, 0.012931330129504204, 0.10229425132274628, 0.4993528723716736], dtype='float32').reshape([18]),
            paddle.to_tensor([0.49112650752067566, 0.49063149094581604, 0.3196392059326172, 0.01813175342977047, 0.055321212857961655, 0.06811254471540451, 0.44620946049690247, 0.07111307978630066, 0.060771048069000244, 0.4821041524410248, 0.24960936605930328, 0.17646852135658264, 0.4663733243942261, 0.4409782886505127, 0.28774723410606384, 0.18446050584316254, 0.45883652567863464, 0.22238674759864807], dtype='float32').reshape([18]),
            paddle.to_tensor([0.06369934976100922, 0.3282361626625061, 0.3718903958797455, 0.3733542263507843, 0.2833000719547272, 0.4303815960884094, 0.23874391615390778, 0.4816870391368866, 0.09456891566514969, 0.2848251760005951, 0.47875356674194336, 0.48275676369667053, 0.31218835711479187, 0.1129387617111206, 0.2657906711101532, 0.3926719129085541, 0.49956849217414856, 0.13147211074829102], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_759135d1a53afcf66f97008c893151f3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 576, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([576], dtype='float32', min=0, max=0.5),
            paddle.uniform([576], dtype='float32', min=0, max=0.5),
            paddle.uniform([576], dtype='float32', min=0, max=0.5),
            paddle.uniform([576], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_85b3ea24693c6539d08bc70f949fb8d1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 80, 20, 20], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f14a1f7fccd4a55476e2f3701b907f2a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 1, 1], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fd2797ab8d2bc520bf017fc3a14c3865(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_3a47a120d62a3f3a422a747f511df8d9
    def get_inputs(self):
        return [
            paddle.uniform([1, 2048, 10, 10], dtype='float16', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_95b5016f483150de6dd04d96375a0416(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 40, 40], dtype='float16', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_47025a9d4e87f018677db5b418c84c08(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 16, 50], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_16baf02b4527b73c17598177911ff1b4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 64, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fe6731386d00d0a9d38d8f05d579717d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 112, 64, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_959b2ca67282a9c7adeef590e1d96c15(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 2048, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_31c35f788635ef8457295daabc7fdd21(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 80, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b082f346cd2b4ebc03ecb8cc0c7a5419(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8469db2003f3c47ca9124c89d2abd9d9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 480, 12, 12], dtype='float16', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f67aaf10828d2d53d7e48405f98d5357(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 104, 40, 40], dtype='float16', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7e47ae4515a111d328cfb01c4d7c397b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 4, 112, 112], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.03838838264346123, 0.024841556325554848, 0.3822416067123413, 0.07918883860111237], dtype='float32').reshape([4]),
            paddle.to_tensor([0.05163639783859253, 0.3028740882873535, 0.2189003825187683, 0.42686727643013], dtype='float32').reshape([4]),
            paddle.to_tensor([0.16289867460727692, 0.39871346950531006, 0.21372702717781067, 0.15554079413414001], dtype='float32').reshape([4]),
            paddle.to_tensor([0.01742655783891678, 0.3900909423828125, 0.22608838975429535, 0.4730971157550812], dtype='float32').reshape([4]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_548e5b257d3e6edd65480a65972b2be6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3a2f02479652f72aa6fd2920bc5b11df(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 17, 17], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9d30bb3f33c55292521493f2b45da89e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 2048, 30, 30], dtype='float16', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_818cadfa02544e56a265a600fd3cd771(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 160, 160], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.47437596321105957, 0.17704012989997864, 0.41178789734840393, 0.4384671747684479, 0.25572630763053894, 0.38435712456703186, 0.19957245886325836, 0.03654579073190689, 0.13530205190181732, 0.37065234780311584, 0.08829957246780396, 0.401441752910614, 0.006479881703853607, 0.10469140112400055, 0.3622550964355469, 0.3445228040218353], dtype='float32').reshape([16]),
            paddle.to_tensor([0.19637437164783478, 0.40139609575271606, 0.11472669988870621, 0.4409504532814026, 0.25892969965934753, 0.16512510180473328, 0.11165627092123032, 0.4716339409351349, 0.2360800951719284, 0.12374929338693619, 0.18860779702663422, 0.3254632353782654, 0.24035942554473877, 0.31530216336250305, 0.3869863748550415, 0.23077914118766785], dtype='float32').reshape([16]),
            paddle.to_tensor([0.0007690736674703658, 0.17648328840732574, 0.03140160068869591, 0.4700351655483246, 0.2673127055168152, 0.24413536489009857, 0.18462134897708893, 0.40233197808265686, 0.1456931084394455, 0.24750778079032898, 0.38135263323783875, 0.11416785418987274, 0.14166244864463806, 0.09628163278102875, 0.02384335920214653, 0.05354538932442665], dtype='float32').reshape([16]),
            paddle.to_tensor([0.4168091416358948, 0.00759444385766983, 0.2663125991821289, 0.3311464488506317, 0.1250780075788498, 0.09699060022830963, 0.042501479387283325, 0.486371248960495, 0.12687931954860687, 0.22559913992881775, 0.3272249400615692, 0.04673364385962486, 0.04320966452360153, 0.3671658933162689, 0.08497841656208038, 0.1349138468503952], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c39274b51611c3487a46588573ee57c5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_06f5fa28038d4fd23982c16fe85519d2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 32, 128], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_95337b399d6321d6eafc62f110cd1d01(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 72, 28, 50], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_79ce24998f3e759d86ab3b960574ab50(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 20, 20], dtype='float16', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8a5c0dbd0d8b191f908d5d13e9414626(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 800, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([800], dtype='float32', min=0, max=0.5),
            paddle.uniform([800], dtype='float32', min=0, max=0.5),
            paddle.uniform([800], dtype='float32', min=0, max=0.5),
            paddle.uniform([800], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f2292afb0c54437f367224fa6ac5b5af(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 40, 40], dtype='float16', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c06d42ffe7dba0e6eb15f8ea4e425e26(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 288, 16, 16], dtype='float16', min=0, max=0.5),
            paddle.uniform([288], dtype='float32', min=0, max=0.5),
            paddle.uniform([288], dtype='float32', min=0, max=0.5),
            paddle.uniform([288], dtype='float32', min=0, max=0.5),
            paddle.uniform([288], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9bcf356110d246c8d7fb3a578628f16f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 112, 199], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b273708324a2715050b6fdb156600308(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 70, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([70], dtype='float32', min=0, max=0.5),
            paddle.uniform([70], dtype='float32', min=0, max=0.5),
            paddle.uniform([70], dtype='float32', min=0, max=0.5),
            paddle.uniform([70], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c0876997be363f1cdc0d5dbb3d88d4c2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_387a349aed40eeccccca131fa909c7fa(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 12, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.3182404637336731, 0.4824049770832062, 0.13408726453781128, 0.15039771795272827, 0.20420058071613312, 0.4589427411556244, 0.054076969623565674, 0.39415472745895386, 0.28585875034332275, 0.18482644855976105, 0.03992044925689697, 0.33122020959854126], dtype='float32').reshape([12]),
            paddle.to_tensor([0.16609922051429749, 0.33505555987358093, 0.20558224618434906, 0.4001505374908447, 0.27219948172569275, 0.4385591149330139, 0.16253668069839478, 0.28459224104881287, 0.49659034609794617, 0.31121626496315, 0.17065222561359406, 0.10521264374256134], dtype='float32').reshape([12]),
            paddle.to_tensor([0.04901302978396416, 0.46603459119796753, 0.4785325229167938, 0.39261800050735474, 0.049667779356241226, 0.37528955936431885, 0.3960350751876831, 0.25783291459083557, 0.06646353006362915, 0.4924823045730591, 0.31837543845176697, 0.08233889937400818], dtype='float32').reshape([12]),
            paddle.to_tensor([0.27821457386016846, 0.21231143176555634, 0.43294957280158997, 0.410251647233963, 0.2697656750679016, 0.14951594173908234, 0.42129093408584595, 0.37692883610725403, 0.3552786111831665, 0.04878968372941017, 0.349334180355072, 0.29395201802253723], dtype='float32').reshape([12]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e117494db308a5339041ca22fb156a22(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 40, 40], dtype='float16', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_33f0082a3f8e8a8e4d9d66a465ad2081(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 53, 1, 1], dtype='float16', min=0, max=0.5),
            paddle.uniform([53], dtype='float32', min=0, max=0.5),
            paddle.uniform([53], dtype='float32', min=0, max=0.5),
            paddle.uniform([53], dtype='float32', min=0, max=0.5),
            paddle.uniform([53], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_029e211ee234f2f980fc19624d7941ca(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 64, 128], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.2911342680454254, 0.388385534286499, 0.002143212826922536, 0.4096139073371887, 0.054795533418655396, 0.39386042952537537, 0.22042216360569, 0.04563022404909134, 0.2752688229084015, 0.1630949229001999, 0.07521362602710724, 0.42171478271484375, 0.09929129481315613, 0.016870450228452682, 0.2090751677751541, 0.1799100637435913, 0.4438398480415344, 0.294705867767334], dtype='float32').reshape([18]),
            paddle.to_tensor([0.37560707330703735, 0.12394219636917114, 0.2657374143600464, 0.186678946018219, 0.2465490847826004, 0.4236675500869751, 0.31063881516456604, 0.11241328716278076, 0.40009427070617676, 0.3734482526779175, 0.010987377725541592, 0.4689978063106537, 0.25531989336013794, 0.1406196802854538, 0.20823819935321808, 0.16348153352737427, 0.25389280915260315, 0.4350299835205078], dtype='float32').reshape([18]),
            paddle.to_tensor([0.43376651406288147, 0.22717499732971191, 0.0365467369556427, 0.3908494710922241, 0.025621552020311356, 0.4497206509113312, 0.07532474398612976, 0.1008172258734703, 0.23607051372528076, 0.30719295144081116, 0.06436994671821594, 0.31260544061660767, 0.09247990697622299, 0.1911328285932541, 0.17782920598983765, 0.44704827666282654, 0.4787452816963196, 0.45724958181381226], dtype='float32').reshape([18]),
            paddle.to_tensor([0.42054006457328796, 0.1648264229297638, 0.1194537878036499, 0.06700633466243744, 0.3535667359828949, 0.38364189863204956, 0.1432279795408249, 0.08711403608322144, 0.23719635605812073, 0.004567431751638651, 0.15370406210422516, 0.4195560812950134, 0.28023451566696167, 0.36177438497543335, 0.2906893491744995, 0.39363980293273926, 0.14650824666023254, 0.08388591557741165], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7f87325796c8b773ed8597e60ca11d5f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cb581f74c4127889a3da3402b1dd7167(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_da4d85bc213f70819a72e68f21214cdb
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 4, 64, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bdf39b87d742a27dc992d097a1f96540(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 768, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d3659f024956290ab910c58f1526246e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_465c0017c1a52c557472087970776538(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ea9f98a236307734504362036f512508(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 1200, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([1200], dtype='float32', min=0, max=0.5),
            paddle.uniform([1200], dtype='float32', min=0, max=0.5),
            paddle.uniform([1200], dtype='float32', min=0, max=0.5),
            paddle.uniform([1200], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e3ea92bb071d55144e266a6c8f999e16(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 32, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_026a1368cd4f20a985083c719213675a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 64, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_975b258f7fdc5512a8c22ad1eebf2c8a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 20, 20], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_38ee47d84c502f0ea2de34186f9e0654(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 16, 16], dtype='float16', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9fd32e34d5c30cbe1b04a7fbe57fa6f7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a8301b810bccba10e4dfdee0df73334
    def get_inputs(self):
        return [
            paddle.uniform([1, 672, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2afe8847fe333d0d2c78ba47be862446(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 35, 35], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c750cbc8bf5dc7e47fa2bd9214ec567a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 40, 40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_862256c62a1a22852c828c1826b16055(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 2048, 25, 42], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ccb74d89917a7d7aa1d44e31898b0325(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 120, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_54c914f65bc938403808273a798e2693(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.1884244978427887, 0.2817295789718628, 0.21624788641929626, 0.18891024589538574, 0.0716656893491745, 0.09850074350833893, 0.4092131555080414, 0.3302360475063324, 0.35363495349884033, 0.3614068627357483, 0.1828060895204544, 0.4847216308116913, 0.24300271272659302, 0.44470760226249695, 0.17584942281246185, 0.39068475365638733, 0.3136552572250366, 0.1702345907688141], dtype='float32').reshape([18]),
            paddle.to_tensor([0.11734297126531601, 0.19950832426548004, 0.09257994592189789, 0.4041624665260315, 0.07511897385120392, 0.04374818131327629, 0.4721452295780182, 0.2769933342933655, 0.19188182055950165, 0.23645740747451782, 0.17101344466209412, 0.4351694583892822, 0.423155277967453, 0.17464028298854828, 0.11644470691680908, 0.143608957529068, 0.12538772821426392, 0.02262909896671772], dtype='float32').reshape([18]),
            paddle.to_tensor([0.3040832281112671, 0.1295015662908554, 0.1344129592180252, 0.01623704470694065, 0.1410445272922516, 0.4488343894481659, 0.26118984818458557, 0.018709853291511536, 0.45084166526794434, 0.16960150003433228, 0.0494806244969368, 0.3533153235912323, 0.20874911546707153, 0.13853095471858978, 0.24414177238941193, 0.23306965827941895, 0.4482983648777008, 0.3951126039028168], dtype='float32').reshape([18]),
            paddle.to_tensor([0.48444288969039917, 0.11616060137748718, 0.36564570665359497, 0.324601411819458, 0.12908749282360077, 0.1424894481897354, 0.10882727801799774, 0.43764451146125793, 0.2526687979698181, 0.14768140017986298, 0.10627134144306183, 0.35505789518356323, 0.429361492395401, 0.11343599855899811, 0.35047927498817444, 0.14343824982643127, 0.30944085121154785, 0.33450987935066223], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_148e5e94a11833f351678c562ddc2b60(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 60, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_20e4f4e2dd759015e6548be6370d4d8c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.32428988814353943, 0.08539704233407974, 0.06311029195785522, 0.47276806831359863, 0.14293591678142548, 0.027210505679249763, 0.12531395256519318, 0.41981416940689087, 0.3046775162220001, 0.038828328251838684, 0.4714425802230835, 0.10668843239545822, 0.17546388506889343, 0.4564661979675293, 0.10572759062051773, 0.4420556128025055, 0.15742836892604828, 0.3352494537830353, 0.31069982051849365, 0.06853644549846649], dtype='float32').reshape([20]),
            paddle.to_tensor([0.28741690516471863, 0.4400753974914551, 0.24017855525016785, 0.26351481676101685, 0.002943377010524273, 0.07404528558254242, 0.1786002516746521, 0.3393275737762451, 0.15386618673801422, 0.05572815611958504, 0.27253636717796326, 0.010601174086332321, 0.11808039993047714, 0.45252135396003723, 0.2547234892845154, 0.197381854057312, 0.00561516173183918, 0.10944177210330963, 0.4314638376235962, 0.2929226756095886], dtype='float32').reshape([20]),
            paddle.to_tensor([0.07022594660520554, 0.030726494267582893, 0.2877291142940521, 0.33198797702789307, 0.4484334886074066, 0.39565831422805786, 0.4071311950683594, 0.44597873091697693, 0.03888192027807236, 0.2732986807823181, 0.13878101110458374, 0.31657159328460693, 0.18098706007003784, 0.23038846254348755, 0.46016252040863037, 0.08709319680929184, 0.05783424153923988, 0.2138383686542511, 0.4608304798603058, 0.4430026710033417], dtype='float32').reshape([20]),
            paddle.to_tensor([0.19786269962787628, 0.0958893820643425, 0.3842897117137909, 0.31726402044296265, 0.4113210141658783, 0.17913028597831726, 0.08049798011779785, 0.37101486325263977, 0.01619955524802208, 0.3292948305606842, 0.3498024642467499, 0.0012900775764137506, 0.23770414292812347, 0.20474864542484283, 0.43840309977531433, 0.4818520247936249, 0.13104572892189026, 0.1531200408935547, 0.35598087310791016, 0.4229438006877899], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_11f9f2f76dd37e62fdfb7a44bd3a4800(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 720, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([720], dtype='float32', min=0, max=0.5),
            paddle.uniform([720], dtype='float32', min=0, max=0.5),
            paddle.uniform([720], dtype='float32', min=0, max=0.5),
            paddle.uniform([720], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5f414939bf280b87e1fa443c0c619365(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fd88bc8e81856abf43ba8cc8abe030d1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 12, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.047251828014850616, 0.3274604082107544, 0.0797540545463562, 0.1075509786605835, 0.48266279697418213, 0.05841778218746185, 0.3709082007408142, 0.13709434866905212, 0.24368643760681152, 0.07236678898334503, 0.48463037610054016, 0.34059619903564453], dtype='float32').reshape([12]),
            paddle.to_tensor([0.24644170701503754, 0.03302835300564766, 0.09369664639234543, 0.4941996932029724, 0.09224434942007065, 0.20478706061840057, 0.047671835869550705, 0.38826045393943787, 0.43140846490859985, 0.28177785873413086, 0.11138902604579926, 0.22903631627559662], dtype='float32').reshape([12]),
            paddle.to_tensor([0.1569141149520874, 0.4949071407318115, 0.4126342833042145, 0.032699014991521835, 0.2332657128572464, 0.426125168800354, 0.03080553188920021, 0.3556792736053467, 0.4353005886077881, 0.3158334791660309, 0.12518344819545746, 0.12237011641263962], dtype='float32').reshape([12]),
            paddle.to_tensor([0.4342863857746124, 0.11612066626548767, 0.36299943923950195, 0.07385696470737457, 0.20971311628818512, 0.3191681206226349, 0.06380878388881683, 0.4822060763835907, 0.38329797983169556, 0.2840336859226227, 0.11687222868204117, 0.1806168556213379], dtype='float32').reshape([12]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_413cd0c7f6a7232225e7c39f7af0f02d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 232, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([232], dtype='float32', min=0, max=0.5),
            paddle.uniform([232], dtype='float32', min=0, max=0.5),
            paddle.uniform([232], dtype='float32', min=0, max=0.5),
            paddle.uniform([232], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_30877f4063a08621273c9c9a5be40654(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 120, 120], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bf6f7ab8ee7d6c9a0b394a29dd6f973e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a5c6754eb5985cc5fa827c1eaf0cccce(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 28, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.09274990111589432, 0.38109177350997925, 0.16506990790367126, 0.2895779013633728, 0.06005825102329254, 0.13301794230937958, 0.19756461679935455, 0.30745628476142883, 0.2770753800868988, 0.4675104320049286, 0.163603276014328, 0.4462466537952423, 0.1503944993019104, 0.45643091201782227, 0.2555798590183258, 0.28788650035858154, 0.3286333978176117, 0.4991670846939087, 0.1528589427471161, 0.31746241450309753, 0.06847625970840454, 0.47015225887298584, 0.03347121551632881, 0.020653652027249336, 0.33504948019981384, 0.4280741512775421, 0.2809911072254181, 0.2347719967365265], dtype='float32').reshape([28]),
            paddle.to_tensor([0.16573165357112885, 0.3051053583621979, 0.2987011969089508, 0.16920173168182373, 0.44159767031669617, 0.08301430940628052, 0.36755135655403137, 0.1571144461631775, 0.2766885757446289, 0.28561699390411377, 0.15710614621639252, 0.3807750642299652, 0.13822335004806519, 0.024018961936235428, 0.4452022910118103, 0.20727220177650452, 0.001086588017642498, 0.4720069468021393, 0.14653551578521729, 0.29028475284576416, 0.24350205063819885, 0.39613330364227295, 0.3588389456272125, 0.17505338788032532, 0.19586607813835144, 0.3874380588531494, 0.06657430529594421, 0.04384692758321762], dtype='float32').reshape([28]),
            paddle.to_tensor([0.3930893540382385, 0.28492650389671326, 0.34100833535194397, 0.2475479692220688, 0.24518227577209473, 0.34574615955352783, 0.23682472109794617, 0.058562714606523514, 0.2513732314109802, 0.3899410367012024, 0.3598742187023163, 0.04421026259660721, 0.09465574473142624, 0.30995380878448486, 0.08428159356117249, 0.4187903106212616, 0.38855838775634766, 0.0457097627222538, 0.24471941590309143, 0.3331959545612335, 0.45707613229751587, 0.3917030692100525, 0.292784184217453, 0.37781351804733276, 0.3180609941482544, 0.14490386843681335, 0.3782930374145508, 0.008073980920016766], dtype='float32').reshape([28]),
            paddle.to_tensor([0.014698469080030918, 0.3697652220726013, 0.49838346242904663, 0.0820867195725441, 0.2349102795124054, 0.06473786383867264, 0.3661233186721802, 0.15537403523921967, 0.3676671087741852, 0.018094275146722794, 0.44365042448043823, 0.008168159984052181, 0.4084828495979309, 0.4236786663532257, 0.32829034328460693, 0.015736175701022148, 0.29094231128692627, 0.2122255265712738, 0.4859554171562195, 0.03272995725274086, 0.3188782036304474, 0.17753039300441742, 0.07257968932390213, 0.07087089121341705, 0.092707559466362, 0.47532570362091064, 0.16949383914470673, 0.11332198232412338], dtype='float32').reshape([28]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c3f352fca7c4a9234b3a30630a15e146(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 20, 20], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_5e7cb23119f65b33bdbad71952f6dd12(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8de35c465e97b08aeece3300ff92ddfb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_5e7cb23119f65b33bdbad71952f6dd12
    def get_inputs(self):
        return [
            paddle.uniform([1, 640, 32, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_889f7e03798cab8767f6e2c1a0829b84(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 35, 35], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e54a6a35d70cac51a3d14aa2063b9f61(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 50, 84], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_91a9e2d82dac75a22a91fd1a7718312b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5cf8893654aa6ed0d25983cfd73a1818(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([8, 512, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ff95c53ca3ad4a71139a89f43eeb9f33(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 288, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.uniform([288], dtype='float32', min=0, max=0.5),
            paddle.uniform([288], dtype='float32', min=0, max=0.5),
            paddle.uniform([288], dtype='float32', min=0, max=0.5),
            paddle.uniform([288], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_adb00724980688edf26af5f5eb7530d3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.29617947340011597, 0.041939862072467804, 0.38342562317848206, 0.06132867932319641, 0.38451871275901794, 0.4842243790626526, 0.4983815848827362, 0.38477063179016113, 0.05367141589522362, 0.16220615804195404, 0.4169284999370575, 0.2067832201719284, 0.09887640178203583, 0.2596266567707062, 0.18073436617851257, 0.13563209772109985, 0.279325395822525, 0.1115802749991417, 0.21622484922409058, 0.02913144789636135, 0.09287016093730927, 0.47888311743736267, 0.4622100591659546, 0.49528950452804565], dtype='float32').reshape([24]),
            paddle.to_tensor([0.36025863885879517, 0.3425048887729645, 0.14025232195854187, 0.48405152559280396, 0.04802833870053291, 0.18262508511543274, 0.19664236903190613, 0.13646213710308075, 0.4709712862968445, 0.4052446782588959, 0.24536091089248657, 0.3472345471382141, 0.16903434693813324, 0.3180166184902191, 0.3019789159297943, 0.27604514360427856, 0.07417259365320206, 0.08283515274524689, 0.46300598978996277, 0.45068201422691345, 0.176274836063385, 0.0036294208839535713, 0.08643120527267456, 0.13847698271274567], dtype='float32').reshape([24]),
            paddle.to_tensor([0.1096685454249382, 0.42768287658691406, 0.45284515619277954, 0.47001567482948303, 0.14144572615623474, 0.36002057790756226, 0.47135573625564575, 0.36738261580467224, 0.25239497423171997, 0.18981240689754486, 0.2904691696166992, 0.15664328634738922, 0.2486545890569687, 0.32129862904548645, 0.2505989372730255, 0.4333098232746124, 0.4490918517112732, 0.02587577886879444, 0.3100758492946625, 0.2698642909526825, 0.3231586515903473, 0.28744402527809143, 0.09555388242006302, 0.4108585715293884], dtype='float32').reshape([24]),
            paddle.to_tensor([0.377741277217865, 0.24477431178092957, 0.11855985224246979, 0.19932658970355988, 0.2629129886627197, 0.1401236355304718, 0.45005860924720764, 0.12340771406888962, 0.4349646270275116, 0.4176362454891205, 0.21012909710407257, 0.33479416370391846, 0.42892372608184814, 0.0022541582584381104, 0.3118542432785034, 0.20430906116962433, 0.19946832954883575, 0.2159627377986908, 0.37125882506370544, 0.13636514544487, 0.025995830073952675, 0.42069873213768005, 0.31440287828445435, 0.2954567074775696], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_983f8863c3714e8a0f47512dec601297(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.14761462807655334, 0.4004630446434021, 0.045846544206142426, 0.2540605366230011, 0.35383081436157227, 0.49216294288635254, 0.00023764459183439612, 0.14641407132148743, 0.37999069690704346, 0.08499849587678909, 0.47262904047966003, 0.2187623232603073, 0.46693021059036255, 0.4186055362224579, 0.08843270689249039, 0.27853482961654663, 0.33329981565475464, 0.1957124024629593, 0.20129849016666412, 0.43949535489082336, 0.33392205834388733, 0.06356910616159439, 0.38088706135749817, 0.09321906417608261, 0.2918979525566101, 0.10593277961015701, 0.48212775588035583, 0.40142351388931274, 0.22651462256908417, 0.28091952204704285], dtype='float32').reshape([30]),
            paddle.to_tensor([0.2337159365415573, 0.045200783759355545, 0.3235580623149872, 0.4726213812828064, 0.15300266444683075, 0.2677922546863556, 0.05332597717642784, 0.1505298912525177, 0.08698848634958267, 0.24517877399921417, 0.4272260367870331, 0.0578092522919178, 0.480338990688324, 0.4233841300010681, 0.24129706621170044, 0.2041904181241989, 0.3918641209602356, 0.028706340119242668, 0.06708784401416779, 0.16626016795635223, 0.26835715770721436, 0.34152793884277344, 0.29116806387901306, 0.24342186748981476, 0.21433496475219727, 0.41652387380599976, 0.09752218425273895, 0.08482293039560318, 0.03164002299308777, 0.06865687668323517], dtype='float32').reshape([30]),
            paddle.to_tensor([0.1301324963569641, 0.3917480409145355, 0.3454878628253937, 0.025657720863819122, 0.00015723536489531398, 0.35141631960868835, 0.16338860988616943, 0.17208713293075562, 0.2520141899585724, 0.2513522207736969, 0.29997843503952026, 0.3331138789653778, 0.4903217554092407, 0.49512383341789246, 0.13294735550880432, 0.0808604434132576, 0.1441536396741867, 0.008977148681879044, 0.06512492895126343, 0.38509485125541687, 0.018234239891171455, 0.458508163690567, 0.08111678063869476, 0.2570372223854065, 0.37356698513031006, 0.17939768731594086, 0.4427756369113922, 0.2249307632446289, 0.29888030886650085, 0.3910328149795532], dtype='float32').reshape([30]),
            paddle.to_tensor([0.4579693675041199, 0.15953847765922546, 0.3401917517185211, 0.010707920417189598, 0.07412903755903244, 0.43437525629997253, 0.09234185516834259, 0.1544007956981659, 0.42083388566970825, 0.4118379056453705, 0.3040471374988556, 0.4625568985939026, 0.15500003099441528, 0.36901700496673584, 0.18664845824241638, 0.1896873265504837, 0.4294719398021698, 0.4970294237136841, 0.10363337397575378, 0.2773014307022095, 0.01123205479234457, 0.34795230627059937, 0.21161580085754395, 0.2709549069404602, 0.38219544291496277, 0.42477983236312866, 0.2367681860923767, 0.0005280258483253419, 0.38403207063674927, 0.10886003077030182], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c0eeefefc0b8b59802798f7eda3591e5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 8, 6], dtype='float16', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_063dc7e01f236c8604db4f7a20cdb29e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 61, 61], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4895b06e4cbd5b787836470329a05327(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 80, 12, 12], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_52df21e5233cc2ad555b39a7188978f0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.0201954897493124, 0.1440056562423706, 0.4632291793823242, 0.23541341722011566, 0.3254412114620209, 0.07889637351036072, 0.28785717487335205, 0.3413114547729492, 0.02906850539147854, 0.13037264347076416, 0.2783147990703583, 0.2751340866088867, 0.047241829335689545, 0.4581940174102783, 0.23498234152793884, 0.3546989858150482, 0.2787950336933136, 0.32889991998672485, 0.4145975410938263, 0.43906447291374207, 0.083371601998806, 0.09754350036382675, 0.28501978516578674, 0.357424259185791], dtype='float32').reshape([24]),
            paddle.to_tensor([0.05441472306847572, 0.38391178846359253, 0.2548842132091522, 0.14918120205402374, 0.21574723720550537, 0.4453590512275696, 0.09626143425703049, 0.3838903307914734, 0.2550533711910248, 0.12730169296264648, 0.20025162398815155, 0.1490219086408615, 0.08597298711538315, 0.28154072165489197, 0.48835092782974243, 0.12500707805156708, 0.2939296066761017, 0.4942862391471863, 0.09269052743911743, 0.026530520990490913, 0.22882892191410065, 0.22720086574554443, 0.2673191726207733, 0.051833223551511765], dtype='float32').reshape([24]),
            paddle.to_tensor([0.04234704747796059, 0.25444015860557556, 0.16023778915405273, 0.4833909869194031, 0.08633417636156082, 0.0342143252491951, 0.1357952356338501, 0.0430637001991272, 0.2920278310775757, 0.3579765856266022, 0.1948651373386383, 0.007175190839916468, 0.18966425955295563, 0.0018885491881519556, 0.2448374629020691, 0.14722149074077606, 0.06609075516462326, 0.2777304947376251, 0.2179602086544037, 0.32126671075820923, 0.1471952646970749, 0.2423110157251358, 0.15565888583660126, 0.32424116134643555], dtype='float32').reshape([24]),
            paddle.to_tensor([0.05425292253494263, 0.00025324017042294145, 0.05955449119210243, 0.1349911093711853, 0.3566631078720093, 0.2503870725631714, 0.16987162828445435, 0.181374654173851, 0.04546354338526726, 0.13565312325954437, 0.2300637811422348, 0.19410516321659088, 0.013830134645104408, 0.3983395993709564, 0.11284313350915909, 0.07722243666648865, 0.2549225091934204, 0.35801467299461365, 0.4577109217643738, 0.34073901176452637, 0.43506327271461487, 0.45612189173698425, 0.4494020342826843, 0.09296289831399918], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d3e2359ca4e3523a60d293f7adbf1408(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 20, 20], dtype='float16', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3bb059612ece0d7fb3eafe9c4788596f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 240, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_45d46ce2a32eadf6dca7bce71b7ba372(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_b4f4da3a8d44d1b5b3a2903a097df186
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 512, 512], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8b9b4a3cc08e30f84f507863d51d9db3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 60, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c16610e51a9cd431110fe538c41483b9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 320, 320], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.13400249183177948, 0.25769832730293274, 0.26789066195487976, 0.0205446295440197, 0.44790926575660706, 0.3519396483898163, 0.21899639070034027, 0.39916467666625977], dtype='float32').reshape([8]),
            paddle.to_tensor([0.329842746257782, 0.34718507528305054, 0.387525737285614, 0.01645328477025032, 0.4844106137752533, 0.2568824291229248, 0.08091242611408234, 0.40959760546684265], dtype='float32').reshape([8]),
            paddle.to_tensor([0.29934582114219666, 0.21684333682060242, 0.015974827110767365, 0.2984558641910553, 0.05765755474567413, 0.04838607460260391, 0.21460814774036407, 0.014654918573796749], dtype='float32').reshape([8]),
            paddle.to_tensor([0.3008466362953186, 0.08827909827232361, 0.06895095854997635, 0.4061901569366455, 0.43001073598861694, 0.06423264741897583, 0.32243406772613525, 0.19334068894386292], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7a7c96533bdfd334b76337018c68dfeb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a56f45f32136d8931f4bd1978a289cf6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 40, 40], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fe00a1d7ab429a3351ce33080f96d2cc(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_145ad20e7a3b3c8aa31f6b40d4fe3d45
    def get_inputs(self):
        return [
            paddle.uniform([1, 640], dtype='float16', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_71837863ffa605506eea0b59ef15ec62(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.27031752467155457, 0.4693679213523865, 0.40630435943603516, 0.367717444896698, 0.3921094238758087, 0.16066694259643555, 0.3631919324398041, 0.29177001118659973, 0.26002398133277893, 0.09243358671665192, 0.14975108206272125, 0.08684226125478745, 0.25549614429473877, 0.3673219382762909, 0.37475693225860596, 0.4182276427745819], dtype='float32').reshape([16]),
            paddle.to_tensor([0.19931204617023468, 0.40862616896629333, 0.03601923957467079, 0.0988200455904007, 0.08929353952407837, 0.13341419398784637, 0.09702704101800919, 0.0821758285164833, 0.3767090439796448, 0.205297589302063, 0.08762937039136887, 0.31198105216026306, 0.3513900339603424, 0.05923355743288994, 0.034721311181783676, 0.21696874499320984], dtype='float32').reshape([16]),
            paddle.to_tensor([0.18697941303253174, 0.031525298953056335, 0.010121812112629414, 0.044528983533382416, 0.08303475379943848, 0.03598744049668312, 0.291949987411499, 0.4175771474838257, 0.012816504575312138, 0.43193793296813965, 0.27252259850502014, 0.14917881786823273, 0.029790956526994705, 0.08463073521852493, 0.4130313992500305, 0.1706952452659607], dtype='float32').reshape([16]),
            paddle.to_tensor([0.15937820076942444, 0.35771235823631287, 0.10511624813079834, 0.09927936643362045, 0.009382897987961769, 0.45767438411712646, 0.3493531346321106, 0.2739502787590027, 0.09073641151189804, 0.32680946588516235, 0.1606934815645218, 0.12095377594232559, 0.25589025020599365, 0.16100969910621643, 0.046615712344646454, 0.07342028617858887], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9a5c8fd16e5e495fe7f909f999eb01a9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 4, 50], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e3998578c3662eed154471ae3513e67f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_fc6b6917e04b7b7cf9fdb39dde97ec40
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 20, 20], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_adbf939a1b5e3c3f5a447ec9ee78813f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 80, 80], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a3c50431b71ea0500ba3dbe69e81b3e7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.07354678958654404, 0.005703661125153303, 0.26827940344810486, 0.4588471055030823, 0.19792233407497406, 0.15439216792583466, 0.08203797787427902, 0.04960481822490692, 0.08700468391180038, 0.2889334261417389, 0.04564999043941498, 0.4599287509918213, 0.4540945887565613, 0.2967672348022461, 0.35405778884887695, 0.47850340604782104, 0.44782134890556335, 0.08259496837854385], dtype='float32').reshape([18]),
            paddle.to_tensor([0.49127840995788574, 0.07388763874769211, 0.4685654044151306, 0.26733481884002686, 0.2686975300312042, 0.16377148032188416, 0.48626255989074707, 0.38064199686050415, 0.3994898498058319, 0.09272756427526474, 0.40969258546829224, 0.17547336220741272, 0.3349330425262451, 0.18471533060073853, 0.4980790317058563, 0.01350654847919941, 0.09780151396989822, 0.43245404958724976], dtype='float32').reshape([18]),
            paddle.to_tensor([0.3348464071750641, 0.16235145926475525, 0.38284385204315186, 0.02656603790819645, 0.34386247396469116, 0.19074580073356628, 0.21574349701404572, 0.08797810971736908, 0.17973197996616364, 0.23789271712303162, 0.4783160388469696, 0.440083384513855, 0.4339495599269867, 0.1380508840084076, 0.21434473991394043, 0.2132241576910019, 0.25739380717277527, 0.1801326423883438], dtype='float32').reshape([18]),
            paddle.to_tensor([0.45632845163345337, 0.21943725645542145, 0.4332980811595917, 0.22828489542007446, 0.2760845422744751, 0.4862494468688965, 0.30362647771835327, 0.42648300528526306, 0.2409939467906952, 0.03720277175307274, 0.23944984376430511, 0.24429388344287872, 0.258273184299469, 0.30982375144958496, 0.3621712028980255, 0.10544048249721527, 0.24613770842552185, 0.32349491119384766], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_62bd5c2de69fc8ee2244f2f769a50bee(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 240, 240], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9b4ddf112b4c78e73e1831e99073c066(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a3be81157a22baa5c7a4d5b400b5598d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.004480787552893162, 0.4374077022075653, 0.2648806571960449, 0.4657256007194519, 0.36877405643463135, 0.08904403448104858, 0.29243361949920654, 0.32473084330558777, 0.045188143849372864, 0.18210090696811676, 0.3628999888896942, 0.3985285758972168, 0.3071202039718628, 0.13459748029708862, 0.3709738254547119, 0.13211898505687714, 0.4882756173610687, 0.49170351028442383, 0.02094465121626854, 0.2708699107170105], dtype='float32').reshape([20]),
            paddle.to_tensor([0.2594720721244812, 0.45075464248657227, 0.2727828621864319, 0.36470872163772583, 0.17841921746730804, 0.038778383284807205, 0.32820942997932434, 0.045239727944135666, 0.3891225755214691, 0.39608338475227356, 0.4430147707462311, 0.41581621766090393, 0.0854215919971466, 0.3571648597717285, 0.345007061958313, 0.33342593908309937, 0.41045650839805603, 0.16488270461559296, 0.18460297584533691, 0.1666732132434845], dtype='float32').reshape([20]),
            paddle.to_tensor([0.3235386610031128, 0.009049431420862675, 0.2170420140028, 0.2300693392753601, 0.13268442451953888, 0.4875941574573517, 0.3417074978351593, 0.4757992625236511, 0.2126559466123581, 0.34193286299705505, 0.4728110134601593, 0.0633959025144577, 0.09493538737297058, 0.41584455966949463, 0.045324765145778656, 0.42953094840049744, 0.18303562700748444, 0.10236373543739319, 0.44823411107063293, 0.2327718585729599], dtype='float32').reshape([20]),
            paddle.to_tensor([0.22867123782634735, 0.11697501689195633, 0.3392705023288727, 0.3473562002182007, 0.40862858295440674, 0.1857735961675644, 0.03768407925963402, 0.2201927900314331, 0.15966618061065674, 0.36383119225502014, 0.37864208221435547, 0.14757102727890015, 0.40190497040748596, 0.03350180387496948, 0.09388724714517593, 0.061174824833869934, 0.10302901268005371, 0.2979927062988281, 0.49423107504844666, 0.4253156781196594], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c262c6de55bf24ce174438ad7b76de0b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 336, 20, 20], dtype='float16', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8907ebe5b54236f34c2c366f97ba615f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 40, 40], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cd7eb1b0c25429421d48c29b50497aa9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 160, 160], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ef898b02c7d645fc360c1daf96998b13(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 60, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3ed55101396cef1aebedb1b2f7de279b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 80, 80], dtype='float16', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9f63452503bdbb595689c59bd9cfe8f6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.49542781710624695, 0.2207074910402298, 0.21562370657920837, 0.2669766843318939, 0.03668134659528732, 0.017482900992035866, 0.47443658113479614, 0.23287205398082733, 0.11099032312631607, 0.4435146152973175, 0.3975767195224762, 0.28723153471946716, 0.11327280849218369, 0.3525233864784241, 0.22581619024276733, 0.44753628969192505, 0.21260115504264832, 0.06856109946966171, 0.34916555881500244, 0.07210364937782288, 0.1376665085554123, 0.22965532541275024, 0.29216960072517395, 0.2460400015115738], dtype='float32').reshape([24]),
            paddle.to_tensor([0.48213115334510803, 0.07332324236631393, 0.27558740973472595, 0.2696426212787628, 0.22649313509464264, 0.2523610293865204, 0.3845956027507782, 0.2891612946987152, 0.4914711117744446, 0.28209543228149414, 0.2960892915725708, 0.20454207062721252, 0.4141189754009247, 0.1710883527994156, 0.002119674114510417, 0.30367550253868103, 0.15418098866939545, 0.22509156167507172, 0.04605048894882202, 0.38951751589775085, 0.19222591817378998, 0.3984101116657257, 0.48995441198349, 0.01734102889895439], dtype='float32').reshape([24]),
            paddle.to_tensor([0.42293044924736023, 0.3127935826778412, 0.4583391547203064, 0.321071594953537, 0.4013855457305908, 0.33579930663108826, 0.4820530116558075, 0.4515843391418457, 0.053713440895080566, 0.48579660058021545, 0.01389192882925272, 0.3145381212234497, 0.4157615005970001, 0.31751343607902527, 0.11142280697822571, 0.06252356618642807, 0.43304094672203064, 0.33593860268592834, 0.1608947068452835, 0.12351524829864502, 0.006318368948996067, 0.11675742268562317, 0.32816073298454285, 0.15911227464675903], dtype='float32').reshape([24]),
            paddle.to_tensor([0.38113588094711304, 0.24452905356884003, 0.3548641800880432, 0.16151663661003113, 0.4735472798347473, 0.06555886566638947, 0.29173728823661804, 0.11422287672758102, 0.42048531770706177, 0.37076154351234436, 0.44584816694259644, 0.3382967710494995, 0.13223668932914734, 0.06975211948156357, 0.4840949773788452, 0.17247839272022247, 0.3454347550868988, 0.21488696336746216, 0.11685875803232193, 0.14414912462234497, 0.45357242226600647, 0.4963279664516449, 0.056734148412942886, 0.28550294041633606], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_423c279f731e81dee91193a7e859cd54(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.12405712902545929, 0.49449941515922546, 0.49769648909568787, 0.3847946226596832, 0.4812221825122833, 0.054114121943712234, 0.32161271572113037, 0.48798105120658875, 0.16514809429645538, 0.19078682363033295, 0.32529130578041077, 0.031033098697662354, 0.09683798253536224, 0.19027362763881683, 0.2204533964395523, 0.2872740924358368, 0.36680546402931213, 0.28868600726127625, 0.3435609042644501, 0.15123417973518372, 0.39242568612098694, 0.3541621267795563, 0.15935222804546356, 0.2602987289428711], dtype='float32').reshape([24]),
            paddle.to_tensor([0.2619622051715851, 0.14714352786540985, 0.04067882522940636, 0.09312519431114197, 0.10538841784000397, 0.039506588131189346, 0.029512489214539528, 0.13180112838745117, 0.25043120980262756, 0.1998651772737503, 0.22183918952941895, 0.28758952021598816, 0.06068083643913269, 0.49662357568740845, 0.30871322751045227, 0.39527711272239685, 0.08914003521203995, 0.0820796936750412, 0.46625885367393494, 0.01365098636597395, 0.17221905291080475, 0.4834870398044586, 0.030736079439520836, 0.1890821009874344], dtype='float32').reshape([24]),
            paddle.to_tensor([0.439588725566864, 0.027730409055948257, 0.481120228767395, 0.2567239999771118, 0.45002245903015137, 0.4328385293483734, 0.06764964759349823, 0.12825380265712738, 0.30096638202667236, 0.14603489637374878, 0.41167151927948, 0.134811133146286, 0.08936348557472229, 0.4166645109653473, 0.18756845593452454, 0.3014339506626129, 0.0811900794506073, 0.06500909477472305, 0.4280647933483124, 0.4582047760486603, 0.19576124846935272, 0.45735037326812744, 0.4078825116157532, 0.31080004572868347], dtype='float32').reshape([24]),
            paddle.to_tensor([0.17699255049228668, 0.3705022931098938, 0.16485212743282318, 0.41454145312309265, 0.21258530020713806, 0.41522035002708435, 0.49434563517570496, 0.1773761659860611, 0.05391717329621315, 0.36654335260391235, 0.025392545387148857, 0.07504404336214066, 0.49895909428596497, 0.4705585837364197, 0.2524239122867584, 0.2328300029039383, 0.3713773488998413, 0.19902963936328888, 0.42895960807800293, 0.06663846969604492, 0.3791434168815613, 0.1614178717136383, 0.1965235471725464, 0.43606457114219666], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_61094b27b4102a51b384c57532d9fa00(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 3, 3], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_82e4eeaa6471083fb8d5e1b145b0d696(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 16, 16], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2f2ec5544e17c5110d7a56a24e7f5d9f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 640, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8368f0a01542e4c9534fd05080b8bb71(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 4, 50], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e0bd4191349cb6c56bf3604643455c6b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.31739509105682373, 0.49814078211784363, 0.4000697731971741, 0.057625096291303635, 0.11519329249858856, 0.24397988617420197, 0.499385267496109, 0.36150896549224854, 0.0047482955269515514, 0.031365253031253815, 0.49540796875953674, 0.3811548948287964, 0.055271707475185394, 0.11887204647064209, 0.3792564570903778, 0.4544576406478882, 0.16003699600696564, 0.17037653923034668, 0.3632076382637024, 0.3527572453022003, 0.4115201234817505, 0.1879505217075348, 0.17946641147136688, 0.40527355670928955], dtype='float32').reshape([24]),
            paddle.to_tensor([0.02970084547996521, 0.11141727864742279, 0.14313913881778717, 0.43017351627349854, 0.14064137637615204, 0.411604642868042, 0.17550161480903625, 0.11099456995725632, 0.45260879397392273, 0.15197983384132385, 0.445378839969635, 0.21664902567863464, 0.41432473063468933, 0.03107488341629505, 0.0054030041210353374, 0.12054401636123657, 0.4048914313316345, 0.259743332862854, 0.0656016394495964, 0.024514464661478996, 0.4933050572872162, 0.12712663412094116, 0.21133293211460114, 0.29214584827423096], dtype='float32').reshape([24]),
            paddle.to_tensor([0.1311095505952835, 0.3524218201637268, 0.42905378341674805, 0.4936378598213196, 0.401930570602417, 0.32190370559692383, 0.4335799515247345, 0.06657742708921432, 0.22233371436595917, 0.038421109318733215, 0.019503850489854813, 0.4870494604110718, 0.1332315355539322, 0.391720175743103, 0.12447673827409744, 0.31224149465560913, 0.05159388855099678, 0.34042418003082275, 0.22969859838485718, 0.16201508045196533, 0.09076909720897675, 0.42385679483413696, 0.39976441860198975, 0.12349890917539597], dtype='float32').reshape([24]),
            paddle.to_tensor([0.22411862015724182, 0.44808998703956604, 0.3227449357509613, 0.20167137682437897, 0.4358738660812378, 0.23411764204502106, 0.06410015374422073, 0.21663698554039001, 0.033767059445381165, 0.3092005252838135, 0.2429165244102478, 0.02205340564250946, 0.4189144968986511, 0.21978336572647095, 0.020164556801319122, 0.2546645402908325, 0.1182577833533287, 0.3355584144592285, 0.07443923503160477, 0.024782126769423485, 0.07447822391986847, 0.27990102767944336, 0.20755048096179962, 0.052423134446144104], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_524ed3569788c4f7144cecd30f44c32d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 40, 40], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_15a2ac3cc70c034ac17727ced0a86fb9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 64, 128], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.21383105218410492, 0.454142302274704, 0.3592258393764496, 0.22327785193920135, 0.21503013372421265, 0.028371697291731834, 0.018651049584150314, 0.44705718755722046, 0.12819454073905945, 0.37044864892959595, 0.0878080278635025, 0.4201677441596985, 0.31777167320251465, 0.4959659278392792, 0.3472476601600647, 0.2851141691207886, 0.3343379497528076, 0.04706849530339241], dtype='float32').reshape([18]),
            paddle.to_tensor([0.21582259237766266, 0.3604413568973541, 0.12369690835475922, 0.1142132431268692, 0.06627524644136429, 0.35186609625816345, 0.1641787439584732, 0.006545241456478834, 0.36223891377449036, 0.3760007619857788, 0.32062235474586487, 0.22745823860168457, 0.3256477117538452, 0.44805172085762024, 0.3896961808204651, 0.481475293636322, 0.2607192099094391, 0.3592735230922699], dtype='float32').reshape([18]),
            paddle.to_tensor([0.3375810980796814, 0.43279287219047546, 0.4440169632434845, 0.38545119762420654, 0.3629682660102844, 0.2814786434173584, 0.1353248953819275, 0.4741280972957611, 0.3468230962753296, 0.07392555475234985, 0.25979310274124146, 0.35939106345176697, 0.4679538309574127, 0.494550883769989, 0.4029318690299988, 0.2334802895784378, 0.37593555450439453, 0.13746151328086853], dtype='float32').reshape([18]),
            paddle.to_tensor([0.40351763367652893, 0.3073887228965759, 0.1675126552581787, 0.11773573607206345, 0.4648720622062683, 0.2645661532878876, 0.01736827939748764, 0.3437037169933319, 0.46328550577163696, 0.15517710149288177, 0.001839629840105772, 0.15052321553230286, 0.27041569352149963, 0.1837652623653412, 0.01668861135840416, 0.2038169503211975, 0.35959312319755554, 0.005293693393468857], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_78a3a656a23a3289999b8cd36546af40(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 82, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([82], dtype='float32', min=0, max=0.5),
            paddle.uniform([82], dtype='float32', min=0, max=0.5),
            paddle.uniform([82], dtype='float32', min=0, max=0.5),
            paddle.uniform([82], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1ff2d464029e6d8c4b7a9b4f03f2cd5d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 1, 1], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.28705012798309326, 0.12703575193881989, 0.3160569965839386, 0.1456950604915619, 0.4283524453639984, 0.16882050037384033, 0.4959544539451599, 0.31956571340560913, 0.16699665784835815, 0.4823164641857147, 0.011875005438923836, 0.09360656887292862, 0.3829880654811859, 0.12600773572921753, 0.258556991815567, 0.32465872168540955, 0.12094390392303467, 0.11173107475042343, 0.1555289924144745, 0.4534289240837097, 0.476195365190506, 0.22093592584133148, 0.057422444224357605, 0.03807556629180908, 0.2486436665058136, 0.12090319395065308, 0.38729456067085266, 0.326732873916626, 0.27133333683013916, 0.37958285212516785], dtype='float32').reshape([30]),
            paddle.to_tensor([0.29079774022102356, 0.1918662041425705, 0.058889880776405334, 0.4408647418022156, 0.49477267265319824, 0.23320075869560242, 0.1054089367389679, 0.3413098156452179, 0.1078418716788292, 0.47428181767463684, 0.3584599494934082, 0.08707311004400253, 0.32186824083328247, 0.05283413082361221, 0.23152132332324982, 0.005555451847612858, 0.1977439522743225, 0.43862390518188477, 0.01933661662042141, 0.37523093819618225, 0.24671737849712372, 0.47295114398002625, 0.3210114538669586, 0.2512870728969574, 0.4109521210193634, 0.08294833451509476, 0.4030126929283142, 0.12027367204427719, 0.4351300299167633, 0.40591609477996826], dtype='float32').reshape([30]),
            paddle.to_tensor([0.25901275873184204, 0.14723153412342072, 0.03883379325270653, 0.20244257152080536, 0.08276672661304474, 0.4391733705997467, 0.15965314209461212, 0.05454926937818527, 0.018006056547164917, 0.04052411764860153, 0.0884324237704277, 0.49100035429000854, 0.26361778378486633, 0.05072230473160744, 0.40687423944473267, 0.28125154972076416, 0.002749363426119089, 0.3808327615261078, 0.031454913318157196, 0.23803812265396118, 0.07901414483785629, 0.2545927166938782, 0.2779213786125183, 0.27939122915267944, 0.2667503356933594, 0.2400830239057541, 0.40646713972091675, 0.401003897190094, 0.494683176279068, 0.21500785648822784], dtype='float32').reshape([30]),
            paddle.to_tensor([0.05919145792722702, 0.3859530985355377, 0.2102421522140503, 0.43559643626213074, 0.3521166145801544, 0.4287157952785492, 0.31005150079727173, 0.002479586051777005, 0.22485759854316711, 0.09353434294462204, 0.46121591329574585, 0.0031164828687906265, 0.03317665308713913, 0.051873791962862015, 0.2664174735546112, 0.4884854555130005, 0.40741807222366333, 0.011885564774274826, 0.34559565782546997, 0.20711244642734528, 0.4434787333011627, 0.13765043020248413, 0.287404328584671, 0.42194586992263794, 0.28034985065460205, 0.23924432694911957, 0.2734980285167694, 0.4679870009422302, 0.02412314899265766, 0.4656870365142822], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c8eafa4e3ac0d4f5a7fefc5a23eedbf6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 300, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([300], dtype='float32', min=0, max=0.5),
            paddle.uniform([300], dtype='float32', min=0, max=0.5),
            paddle.uniform([300], dtype='float32', min=0, max=0.5),
            paddle.uniform([300], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ac449147d45b699904bc3c9fbe4095d0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 80, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6d278b76e5c2e4093de3244ebe201be0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 224, 224], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.35613328218460083, 0.3882788121700287, 0.34735795855522156, 0.07112134248018265, 0.14198477566242218, 0.21214239299297333, 0.3689907491207123, 0.40773874521255493, 0.4686528146266937, 0.2614491879940033, 0.3231898248195648, 0.13378238677978516, 0.015714185312390327, 0.09078001230955124, 0.47755110263824463, 0.3000921308994293], dtype='float32').reshape([16]),
            paddle.to_tensor([0.014920626766979694, 0.27912265062332153, 0.3107409179210663, 0.35242959856987, 0.021514233201742172, 0.4017656743526459, 0.25515565276145935, 0.024009928107261658, 0.14267688989639282, 0.3435390591621399, 0.13640105724334717, 0.36004963517189026, 0.07298462837934494, 0.4855673611164093, 0.16000519692897797, 0.2841704189777374], dtype='float32').reshape([16]),
            paddle.to_tensor([0.4329688549041748, 0.18064315617084503, 0.12489103525876999, 0.0052765910513699055, 0.1847742348909378, 0.04530923441052437, 0.14433661103248596, 0.3216142952442169, 0.21118681132793427, 0.1113126277923584, 0.03837953507900238, 0.41758960485458374, 0.1565150022506714, 0.14674784243106842, 0.1145719438791275, 0.08749078959226608], dtype='float32').reshape([16]),
            paddle.to_tensor([0.18457797169685364, 0.4376453161239624, 0.4673737585544586, 0.48281246423721313, 0.11337161809206009, 0.014948401600122452, 0.015797331929206848, 0.3389691412448883, 0.1047484278678894, 0.3705715239048004, 0.4725445508956909, 0.18188025057315826, 0.2578841745853424, 0.23794016242027283, 0.3202212154865265, 0.18310467898845673], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_58b4b74648ba171b249d7bef639c29fc(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_af91295b59c5fca17f890a802deb001f
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 32, 8, 8], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_277124e2c59d7fa72d1379c1ea24365a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 2048, 15, 15], dtype='float16', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_516912a9a3c4d9429ae562b758632c72(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([8, 128, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e15276b31c10e67b61121ff3648d8c8e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 384, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_356efd400ed99c0d9098594ffccac793(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 256, 256], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e7124c660170bbc9bf39d526aa359339(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 8, 25], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1007accdc9c57f04c702afa04c30e570(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 16, 16], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_33b5d91dde84005161fc8146b4e203c1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 60, 20, 20], dtype='float16', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d28ed095c9e7bfa24842b50df21517c3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 240, 240], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0366cd303febed6bbb667de44d70348e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_851cb4eebe53841cc73526feded6db07(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 60, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_69492281f4d8accefc2cac91ba9a9edf(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 28, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.11265784502029419, 0.44843995571136475, 0.11319050937891006, 0.13463352620601654, 0.14319363236427307, 0.05670805647969246, 0.37959811091423035, 0.3078058958053589, 0.3035266101360321, 0.40281230211257935, 0.49561652541160583, 0.19529667496681213, 0.30976593494415283, 0.030353905633091927, 0.48315009474754333, 0.4362875521183014, 0.19521501660346985, 0.468150794506073, 0.025587843731045723, 0.2540479600429535, 0.232634037733078, 0.35519421100616455, 0.2550926208496094, 0.026959247887134552, 0.4333953261375427, 0.2589579224586487, 0.16951872408390045, 0.4944480359554291], dtype='float32').reshape([28]),
            paddle.to_tensor([0.09469985961914062, 0.07074546068906784, 0.3129121959209442, 0.14339619874954224, 0.24635174870491028, 0.11128305643796921, 0.3401011526584625, 0.2667485475540161, 0.14481279253959656, 0.24451756477355957, 0.4598385691642761, 0.058750540018081665, 0.09931135922670364, 0.13222511112689972, 0.12462002038955688, 0.4590413272380829, 0.2720918655395508, 0.057729728519916534, 0.33762025833129883, 0.1727040857076645, 0.3499159514904022, 0.08139359951019287, 0.205112487077713, 0.17777322232723236, 0.21502268314361572, 0.3151882588863373, 0.2759018838405609, 0.46587488055229187], dtype='float32').reshape([28]),
            paddle.to_tensor([0.4962283670902252, 0.18861016631126404, 0.4968067705631256, 0.20321644842624664, 0.1449507772922516, 0.337962806224823, 0.21158745884895325, 0.2821817994117737, 0.1529255211353302, 0.4890257716178894, 0.0271555595099926, 0.4985813498497009, 0.26896384358406067, 0.43486565351486206, 0.4454871118068695, 0.13658855855464935, 0.0268947035074234, 0.3691726624965668, 0.24491919577121735, 0.2995729148387909, 0.24317385256290436, 0.3320311903953552, 0.1849984973669052, 0.07741592824459076, 0.08542138338088989, 0.23593750596046448, 0.1632091999053955, 0.2842300236225128], dtype='float32').reshape([28]),
            paddle.to_tensor([0.040645815432071686, 0.17170928418636322, 0.35845643281936646, 0.38287314772605896, 0.0667884573340416, 0.26606762409210205, 0.25289544463157654, 0.1698753833770752, 0.44537588953971863, 0.3923143446445465, 0.44701823592185974, 0.2935742437839508, 0.10842825472354889, 0.41725000739097595, 0.39322492480278015, 0.2760065495967865, 0.38751348853111267, 0.14128641784191132, 0.4890329837799072, 0.36856168508529663, 0.2699039578437805, 0.13579730689525604, 0.17417621612548828, 0.05943402275443077, 0.20235294103622437, 0.17746277153491974, 0.10757007449865341, 0.2453315258026123], dtype='float32').reshape([28]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2ee6a2b9461766311e0f43b4a53fbce5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 160, 160], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.23018957674503326, 0.3536209464073181, 0.0673595741391182, 0.3462156355381012, 0.08283020555973053, 0.39057081937789917, 0.4238322377204895, 0.008680240251123905, 0.05673326924443245, 0.2782447934150696, 0.27378228306770325, 0.08804119378328323, 0.4957517087459564, 0.44642189145088196, 0.24638552963733673, 0.4695628583431244], dtype='float32').reshape([16]),
            paddle.to_tensor([0.21011440455913544, 0.32105180621147156, 0.3056776821613312, 0.10067795217037201, 0.42115604877471924, 0.4596797823905945, 0.2338317632675171, 0.08418809622526169, 0.24890729784965515, 0.423714280128479, 0.3010501265525818, 0.38905635476112366, 0.1465209424495697, 0.1019187942147255, 0.43644052743911743, 0.3768905699253082], dtype='float32').reshape([16]),
            paddle.to_tensor([0.07085271924734116, 0.31929904222488403, 0.32800453901290894, 0.021368157118558884, 0.23522068560123444, 0.4157986342906952, 0.20462249219417572, 0.2725712060928345, 0.4515577256679535, 0.49977704882621765, 0.30961233377456665, 0.48126423358917236, 0.08770105242729187, 0.3972344696521759, 0.07658575475215912, 0.17952145636081696], dtype='float32').reshape([16]),
            paddle.to_tensor([0.22197706997394562, 0.20283129811286926, 0.1637107878923416, 0.4111141562461853, 0.1805742383003235, 0.2803463935852051, 0.04606311768293381, 0.22161099314689636, 0.11744044721126556, 0.04700377956032753, 0.14499740302562714, 0.05161489173769951, 0.246906116604805, 0.15810756385326385, 0.1434602588415146, 0.009144779294729233], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fe6396b8458678c74562c4decc7c44c0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 384, 32, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a2dbd73ba2468da1c6edbb591c830078(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.2441863864660263, 0.4822709858417511, 0.33039453625679016, 0.2867189943790436, 0.20908911526203156, 0.311258465051651, 0.47461065649986267, 0.2607760429382324, 0.23816516995429993, 0.4446108341217041, 0.18520581722259521, 0.25811922550201416, 0.47515979409217834, 0.15727372467517853, 0.41908249258995056, 0.11606547236442566, 0.23728422820568085, 0.04538067430257797], dtype='float32').reshape([18]),
            paddle.to_tensor([0.044432979077100754, 0.08858159184455872, 0.27599871158599854, 0.2532585561275482, 0.15954948961734772, 0.03828732296824455, 0.352498322725296, 0.23198705911636353, 0.12142479419708252, 0.16893571615219116, 0.4280944764614105, 0.012006064876914024, 0.46530377864837646, 0.21680854260921478, 0.15016943216323853, 0.3811963200569153, 0.09348190575838089, 0.4712531864643097], dtype='float32').reshape([18]),
            paddle.to_tensor([0.3043256103992462, 0.007785311434417963, 0.3246372640132904, 0.2834504246711731, 0.45322731137275696, 0.3124130666255951, 0.37952926754951477, 0.17647509276866913, 0.21995024383068085, 0.37116795778274536, 0.49854570627212524, 0.35192447900772095, 0.024652253836393356, 0.18476951122283936, 0.4821469485759735, 0.03078201785683632, 0.06321532279253006, 0.2770747244358063], dtype='float32').reshape([18]),
            paddle.to_tensor([0.21273568272590637, 0.16091997921466827, 0.288571298122406, 0.39160650968551636, 0.03898460045456886, 0.4170399010181427, 0.1169133111834526, 0.29723504185676575, 0.21174347400665283, 0.09492965042591095, 0.2628598213195801, 0.059057123959064484, 0.07686249911785126, 0.3731721341609955, 0.4838556945323944, 0.15761083364486694, 0.4674360454082489, 0.0014220420271158218], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_650cb3d5dac21c3a05ed2dba9346ea24(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 8, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d32d5087fd068eafe1179401dcef04f0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 20, 20], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_810283540631737518ee8d40fd1652a7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 1200, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([1200], dtype='float32', min=0, max=0.5),
            paddle.uniform([1200], dtype='float32', min=0, max=0.5),
            paddle.uniform([1200], dtype='float32', min=0, max=0.5),
            paddle.uniform([1200], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_230d2719f862e7eb2b0d6a05fd139f65(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 320, 4, 3], dtype='float16', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_392f345d39880094263f542fc25200c0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 160, 160], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_abc41fed2b404fcc6993a1a1a9cd35c2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 80, 64, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f4203a9efbf1a409a9ca884675d09210(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.1890898197889328, 0.004414980765432119, 0.39062902331352234, 0.18059378862380981, 0.44730034470558167, 0.3581342101097107, 0.26878613233566284, 0.13366945087909698, 0.43708834052085876, 0.4594593644142151, 0.3965569734573364, 0.2914973199367523, 0.35566219687461853, 0.1868973821401596, 0.0999140590429306, 0.4495280683040619, 0.11387994885444641, 0.04416044056415558], dtype='float32').reshape([18]),
            paddle.to_tensor([0.08577682822942734, 0.31554216146469116, 0.0036431706976145506, 0.2226363867521286, 0.44047483801841736, 0.061546143144369125, 0.05965682864189148, 0.371341347694397, 0.004515437409281731, 0.15258122980594635, 0.08855870366096497, 0.1818934828042984, 0.17983882129192352, 0.09708510339260101, 0.4749532639980316, 0.23379746079444885, 0.33105939626693726, 0.2389659583568573], dtype='float32').reshape([18]),
            paddle.to_tensor([0.4160102605819702, 0.4169829785823822, 0.1940927505493164, 0.3027828335762024, 0.49093541502952576, 0.28594884276390076, 0.47250956296920776, 0.3936423361301422, 0.44075503945350647, 0.30514878034591675, 0.1850455403327942, 0.22626399993896484, 0.20766779780387878, 0.1761360764503479, 0.2587886154651642, 0.3274661600589752, 0.4790506958961487, 0.39393579959869385], dtype='float32').reshape([18]),
            paddle.to_tensor([0.44360461831092834, 0.25019535422325134, 0.48199641704559326, 0.04171879217028618, 0.22645026445388794, 0.022800439968705177, 0.11631740629673004, 0.40158388018608093, 0.25304967164993286, 0.28499335050582886, 0.20000451803207397, 0.2914099395275116, 0.32930272817611694, 0.2434239685535431, 0.18317566812038422, 0.28331053256988525, 0.1216040775179863, 0.38924914598464966], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8475c464a7ea42b8ab132bd49c791dea(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 20, 20], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ca129182aed00ef71d03776da682e3dc(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.19881762564182281, 0.12240464985370636, 0.3464501202106476, 0.0990399420261383, 0.19099926948547363, 0.44013166427612305, 0.3900357186794281, 0.3613186180591583, 0.2070072740316391, 0.09527423977851868, 0.287126749753952, 0.30022040009498596, 0.10723672062158585, 0.3371325433254242, 0.4837842285633087, 0.44833433628082275, 0.2413976788520813, 0.33588895201683044, 0.2635834217071533, 0.4751093089580536], dtype='float32').reshape([20]),
            paddle.to_tensor([0.45224007964134216, 0.1718263179063797, 0.16902042925357819, 0.3983973562717438, 0.12500940263271332, 0.09138017147779465, 0.14729319512844086, 0.10117039084434509, 0.33024173974990845, 0.39150166511535645, 0.12573611736297607, 0.22710448503494263, 0.21196913719177246, 0.39264440536499023, 0.023190170526504517, 0.07944943010807037, 0.32452088594436646, 0.3388645350933075, 0.20458923280239105, 0.4305790364742279], dtype='float32').reshape([20]),
            paddle.to_tensor([0.22688166797161102, 0.16272157430648804, 0.22314010560512543, 0.23046129941940308, 0.3956059515476227, 0.31831732392311096, 0.4254790246486664, 0.0635562315583229, 0.159379243850708, 0.06743542104959488, 0.46528223156929016, 0.38819947838783264, 0.4627445638179779, 0.3854708969593048, 0.1258261799812317, 0.3723297715187073, 0.465596467256546, 0.3880915939807892, 0.3394927978515625, 0.22743862867355347], dtype='float32').reshape([20]),
            paddle.to_tensor([0.2509360909461975, 0.24716989696025848, 0.04392947256565094, 0.44792628288269043, 0.2432997077703476, 0.4371209144592285, 0.13845857977867126, 0.0011112676002085209, 0.05779218673706055, 0.2863255739212036, 0.3237183094024658, 0.04990821331739426, 0.13969220221042633, 0.08290267735719681, 0.26013246178627014, 0.404438853263855, 0.49960991740226746, 0.020541470497846603, 0.36233600974082947, 0.1379094421863556], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_565a3604b7f82964097be11ffb38d124(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 56, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_818fcec8a73a139b187129182d9957e8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 360, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([360], dtype='float32', min=0, max=0.5),
            paddle.uniform([360], dtype='float32', min=0, max=0.5),
            paddle.uniform([360], dtype='float32', min=0, max=0.5),
            paddle.uniform([360], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_381f0b5a6d42f83baba97e9057d16213(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_5e7cb23119f65b33bdbad71952f6dd12
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 32, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f9a5053820c2b63d865e0b1f293ac18e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 112, 112], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.02469608187675476, 0.4539085328578949, 0.27012747526168823, 0.2103131264448166, 0.19604384899139404, 0.4597483277320862, 0.024243729189038277, 0.24676255881786346], dtype='float32').reshape([8]),
            paddle.to_tensor([0.33170852065086365, 0.14816240966320038, 0.1324068009853363, 0.28993380069732666, 0.311940997838974, 0.07348418235778809, 0.37922337651252747, 0.18369773030281067], dtype='float32').reshape([8]),
            paddle.to_tensor([0.18814139068126678, 0.0391010157763958, 0.4901973009109497, 0.27046775817871094, 0.4607864320278168, 0.25264742970466614, 0.24118845164775848, 0.03053385205566883], dtype='float32').reshape([8]),
            paddle.to_tensor([0.3073616325855255, 0.07251950353384018, 0.49840307235717773, 0.13767284154891968, 0.4808613657951355, 0.15143565833568573, 0.32384514808654785, 0.3406546115875244], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_624121c63a437f3f5252c5632d2fdc1a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 384, 128, 128], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8dcca255b3462324145c2e20d8ac6aeb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 12, 640, 640], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.1624138206243515, 0.4879700541496277, 0.16241301596164703, 0.004719305783510208, 0.3074057698249817, 0.12243974208831787, 0.3643744885921478, 0.012692268006503582, 0.4506535232067108, 0.058348722755908966, 0.09699493646621704, 0.35443103313446045], dtype='float32').reshape([12]),
            paddle.to_tensor([0.17731806635856628, 0.15369443595409393, 0.27839604020118713, 0.05570102483034134, 0.10627221316099167, 0.3882680833339691, 0.37959951162338257, 0.4355158805847168, 0.18442897498607635, 0.4287989139556885, 0.3856273889541626, 0.0762423574924469], dtype='float32').reshape([12]),
            paddle.to_tensor([0.04144319146871567, 0.420606791973114, 0.03843867406249046, 0.3729359805583954, 0.28949418663978577, 0.2963391840457916, 0.4736315608024597, 0.3059214949607849, 0.3821931481361389, 0.37332087755203247, 0.07437518239021301, 0.0360228568315506], dtype='float32').reshape([12]),
            paddle.to_tensor([0.22930890321731567, 0.05085400864481926, 0.1024908572435379, 0.00825822725892067, 0.3903360962867737, 0.04475722461938858, 0.21141909062862396, 0.38172346353530884, 0.12409064173698425, 0.07770659029483795, 0.11772589385509491, 0.050616953521966934], dtype='float32').reshape([12]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_04d9f50998862810eaf29d37241fade8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 16, 16], dtype='float16', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_de7c472144d79ac2bf3fbd411df42334(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 512, 1024], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.341220498085022, 0.39452722668647766, 0.15643136203289032, 0.15455837547779083, 0.260311484336853, 0.4381959140300751, 0.13461634516716003, 0.47559311985969543, 0.12352646887302399, 0.47386619448661804, 0.44062340259552, 0.31506067514419556, 0.06940735876560211, 0.25191888213157654, 0.3508020341396332, 0.47595342993736267], dtype='float32').reshape([16]),
            paddle.to_tensor([0.17270421981811523, 0.49958935379981995, 0.052063506096601486, 0.04364623501896858, 0.2336636483669281, 0.30030763149261475, 0.47517627477645874, 0.45729002356529236, 0.3059588074684143, 0.32804837822914124, 0.2506759464740753, 0.009993943385779858, 0.1783113181591034, 0.05075615271925926, 0.3623808026313782, 0.008996766060590744], dtype='float32').reshape([16]),
            paddle.to_tensor([0.08251497894525528, 0.030732303857803345, 0.3194615840911865, 0.41646334528923035, 0.12115365266799927, 0.24331380426883698, 0.12287507206201553, 0.44913166761398315, 0.236088827252388, 0.27061429619789124, 0.20121003687381744, 0.0844147801399231, 0.3241751194000244, 0.490165650844574, 0.0025649727322161198, 0.4335258901119232], dtype='float32').reshape([16]),
            paddle.to_tensor([0.030159180983901024, 0.03008163720369339, 0.027567503973841667, 0.18604764342308044, 0.10693860054016113, 0.1315010040998459, 0.195927232503891, 0.33544957637786865, 0.4972749650478363, 0.30484071373939514, 0.2582189738750458, 0.26075324416160583, 0.473126083612442, 0.09034696221351624, 0.40722641348838806, 0.18858835101127625], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9806e63237171a6d0501b8d59308387f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 32, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_922461e79cc1140e909fd0d887e435cc(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 672, 64, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_68f2cd7a65d59005d894f319802fd6ab(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a81f8e974259482675dca744840ce209(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 62, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([62], dtype='float32', min=0, max=0.5),
            paddle.uniform([62], dtype='float32', min=0, max=0.5),
            paddle.uniform([62], dtype='float32', min=0, max=0.5),
            paddle.uniform([62], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a7200bd36a8eda9c9b78fe5d75166bdc(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 8, 25], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_be2634e3e0da946dca4cd0e651ae6105(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 240, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_00526e8d39b4e12afc9f95f0f6665000(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 56, 60, 60], dtype='float16', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_52e8507157f61858423dedd30a414fef(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 120, 120], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.193492129445076, 0.12110726535320282, 0.3122110962867737, 0.15495765209197998, 0.18118682503700256, 0.11949510127305984, 0.27636003494262695, 0.3588675856590271, 0.0819355696439743, 0.37279388308525085, 0.12566502392292023, 0.4443337023258209, 0.09540149569511414, 0.3497927188873291, 0.43582460284233093, 0.4463057518005371, 0.18170614540576935, 0.4951325058937073, 0.3839532136917114, 0.3748869299888611, 0.25188228487968445, 0.15951550006866455, 0.21025818586349487, 0.10145994275808334], dtype='float32').reshape([24]),
            paddle.to_tensor([0.30688735842704773, 0.28890201449394226, 0.2795565724372864, 0.3844309449195862, 0.14959801733493805, 0.4396171569824219, 0.46983644366264343, 0.02745957113802433, 0.021861489862203598, 0.4858379364013672, 0.020108122378587723, 0.26247653365135193, 0.39123213291168213, 0.4888368844985962, 0.43276000022888184, 0.28737103939056396, 0.09974220395088196, 0.29806554317474365, 0.06488979607820511, 0.25410911440849304, 0.3549721837043762, 0.263554185628891, 0.4078151285648346, 0.06968745589256287], dtype='float32').reshape([24]),
            paddle.to_tensor([0.32676467299461365, 0.3331221342086792, 0.22980071604251862, 0.01134595088660717, 0.0658397451043129, 0.39489927887916565, 0.37311699986457825, 0.35238519310951233, 0.25757840275764465, 0.11299813538789749, 0.14083315432071686, 0.38664674758911133, 0.15217667818069458, 0.07592600584030151, 0.17004580795764923, 0.2697610557079315, 0.05105048418045044, 0.1703871488571167, 0.43467652797698975, 0.21310138702392578, 0.25546345114707947, 0.2965690791606903, 0.05661768838763237, 0.35482341051101685], dtype='float32').reshape([24]),
            paddle.to_tensor([0.17268390953540802, 0.4140874147415161, 0.09924963861703873, 0.3472070097923279, 0.06489817053079605, 0.2837197184562683, 0.09680411219596863, 0.4098600149154663, 0.3900511562824249, 0.10257668793201447, 0.014966028742492199, 0.10643091052770615, 0.023334527388215065, 0.0386507473886013, 0.27187252044677734, 0.31800827383995056, 0.2980465590953827, 0.34183189272880554, 0.13708168268203735, 0.2822462022304535, 0.1847718209028244, 0.3462548851966858, 0.47637584805488586, 0.029227057471871376], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b6f51999d78b80bd5604ad310baf38f9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a8301b810bccba10e4dfdee0df73334
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 10, 10], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c0be56c9e55ad7c7068d1c27b836fdaa(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 19, 19], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_56ba344d63318f77354f22823329049c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 10, 10], dtype='float16', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_73bc17bd75d5f77e970b0b6758de3277(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 350, 25], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f458041cc897cb31e866812589ef6e82(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 104, 4, 50], dtype='float16', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_917cc8a4b10b86d5c2c65cf264ee37c2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 8, 8], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_31d0c137872e62333693de7cab0e3ead(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d67ebb77c8963001d70e9bedeb759bd8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_af91295b59c5fca17f890a802deb001f
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 32, 16, 16], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9a35ee208a6d0d28e7e95bea9ba909d4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_da4d85bc213f70819a72e68f21214cdb
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 32, 64, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8164af0929bcfe07c3ae25ad3472902f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 104, 40, 40], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b6c33a7e1e8c7a776a21c959e0bce6bf(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cf7b0bfeafb18585a6eb0c78aa8c0f14(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 1376, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([1376], dtype='float32', min=0, max=0.5),
            paddle.uniform([1376], dtype='float32', min=0, max=0.5),
            paddle.uniform([1376], dtype='float32', min=0, max=0.5),
            paddle.uniform([1376], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8b60bbfee1fd574c6ea0d4d654a72ce3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 160, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e5515205ff6279da181e30520651049c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 112, 112], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_dcd4d369d4222203377cf2c9b6580ea7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 10, 10], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_52e43cc241afdd5927c058d793724e42(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 480, 480], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2e213d3cdbcd171194337bc1fa9bfc64(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 480, 480], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_68c8d15dbdcd300204a66d505b5d7621(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 80, 4, 3], dtype='float16', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b7d51a628cc3729e80f9f661a9c28457(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 160, 160], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.3548542857170105, 0.48722612857818604, 0.20382043719291687, 0.1489211469888687, 0.09477199614048004, 0.3890915811061859, 0.24966131150722504, 0.2759881913661957, 0.47305431962013245, 0.016102144494652748, 0.28689974546432495, 0.3089919090270996, 0.01843806356191635, 0.49934232234954834, 0.07826647907495499, 0.07392734289169312], dtype='float32').reshape([16]),
            paddle.to_tensor([0.29112520813941956, 0.24465277791023254, 0.09049421548843384, 0.44476792216300964, 0.02213810198009014, 0.18779872357845306, 0.07959066331386566, 0.49452677369117737, 0.010831862688064575, 0.12956814467906952, 0.4843941330909729, 0.22254545986652374, 0.029037367552518845, 0.3181959390640259, 0.2693321406841278, 0.1760721057653427], dtype='float32').reshape([16]),
            paddle.to_tensor([0.0729503408074379, 0.030995938926935196, 0.1336705982685089, 0.19491049647331238, 0.15932464599609375, 0.1630733758211136, 0.3901349902153015, 0.06975726783275604, 0.18608570098876953, 0.2049202173948288, 0.37860503792762756, 0.14880114793777466, 0.10622408241033554, 0.33406832814216614, 0.23967130482196808, 0.1825040876865387], dtype='float32').reshape([16]),
            paddle.to_tensor([0.04894278198480606, 0.4004504680633545, 0.2171110063791275, 0.4891422986984253, 0.017045987769961357, 0.40799063444137573, 0.1722140908241272, 0.1314772069454193, 0.41532814502716064, 0.4469236135482788, 0.49432680010795593, 0.4576936960220337, 0.38112711906433105, 0.13422051072120667, 0.2859035134315491, 0.4266812205314636], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f138603ba592a3b5d13c31bb6e7e1d0b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 112, 199], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.015622063539922237, 0.04968584328889847, 0.11914436519145966, 0.48063063621520996, 0.29688695073127747, 0.052603766322135925, 0.37707918882369995, 0.48540976643562317, 0.17302419245243073, 0.3224872946739197, 0.044619131833314896, 0.014204440638422966, 0.13516899943351746, 0.4853706657886505, 0.43976452946662903, 0.04384492710232735, 0.13500918447971344, 0.45965099334716797], dtype='float32').reshape([18]),
            paddle.to_tensor([0.257597416639328, 0.1532183587551117, 0.06425394862890244, 0.1717538684606552, 0.18045839667320251, 0.10984775424003601, 0.39470669627189636, 0.05230215936899185, 0.10856102406978607, 0.3778151273727417, 0.4453130066394806, 0.31626206636428833, 0.22211484611034393, 0.3344639241695404, 0.26027539372444153, 0.402383953332901, 0.3976818323135376, 0.4917246401309967], dtype='float32').reshape([18]),
            paddle.to_tensor([0.3972148597240448, 0.46576857566833496, 0.015703778713941574, 0.11261492967605591, 0.3440927267074585, 0.25048261880874634, 0.33088091015815735, 0.27749955654144287, 0.1266438066959381, 0.17332006990909576, 0.1494680643081665, 0.3792121410369873, 0.26597970724105835, 0.14128118753433228, 0.014290916733443737, 0.06507722288370132, 0.17182622849941254, 0.27995991706848145], dtype='float32').reshape([18]),
            paddle.to_tensor([0.14131848514080048, 0.25632089376449585, 0.1814182847738266, 0.2910650372505188, 0.14644286036491394, 0.2165362536907196, 0.40218862891197205, 0.44585299491882324, 0.12589691579341888, 0.03632315993309021, 0.4465898275375366, 0.05129159614443779, 0.4004549980163574, 0.15167464315891266, 0.48731938004493713, 0.2884191870689392, 0.4617730677127838, 0.2537815272808075], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3b096f7743dfa54831b5c4d0127ec9f2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 16, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4f683497f258f869d3291d480f4d70ac(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 256, 512], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_abdaaab8c8060b378c49e64ad433ae82(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_da4d85bc213f70819a72e68f21214cdb
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 4, 8, 8], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0963df03aaf9f43e41471a9fd232d274(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e1c984bc7e4322cf21a1844397da5b8d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 224, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([224], dtype='float32', min=0, max=0.5),
            paddle.uniform([224], dtype='float32', min=0, max=0.5),
            paddle.uniform([224], dtype='float32', min=0, max=0.5),
            paddle.uniform([224], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ead4e9297cfea0c20f4b600cf2ae66d7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 160, 160], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7ba53cdbed8af5d97053baf1faa5d1c7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 31, 31], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b5f1b1300277d91b7787acd36c82b254(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 15, 15], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ae1efc4cd48d0c2d2c4596eaeef8fcfd(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_b4f4da3a8d44d1b5b3a2903a097df186
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 1, 1], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1ffa5384829b2f3d9f3a83be7d363910(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 31, 31], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7a38d344bc704910d461605b2cfcb3dd(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([8, 2048, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_59984a112d7c9fa1470b7f9be32bb7c5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 8, 8], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8dd4c317cf5758b55a9053f37b9833c1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cd190daaaf836f21385cf3a7fc435599(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_af91295b59c5fca17f890a802deb001f
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 32, 16, 16], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0584604a14aa4390f80c3c3b75710c9c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 2208, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([2208], dtype='float32', min=0, max=0.5),
            paddle.uniform([2208], dtype='float32', min=0, max=0.5),
            paddle.uniform([2208], dtype='float32', min=0, max=0.5),
            paddle.uniform([2208], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3b6b405319aa4a4a442639f83b33333b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 95, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([95], dtype='float32', min=0, max=0.5),
            paddle.uniform([95], dtype='float32', min=0, max=0.5),
            paddle.uniform([95], dtype='float32', min=0, max=0.5),
            paddle.uniform([95], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cec9bbcaa9659c0f6157fb587f3384bc(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 42, 1, 1], dtype='float16', min=0, max=0.5),
            paddle.uniform([42], dtype='float32', min=0, max=0.5),
            paddle.uniform([42], dtype='float32', min=0, max=0.5),
            paddle.uniform([42], dtype='float32', min=0, max=0.5),
            paddle.uniform([42], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_af7207b3d33a7671f7dfe86196555398(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 2064, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([2064], dtype='float32', min=0, max=0.5),
            paddle.uniform([2064], dtype='float32', min=0, max=0.5),
            paddle.uniform([2064], dtype='float32', min=0, max=0.5),
            paddle.uniform([2064], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ccccb9caa4787400b003a725155c463f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a8301b810bccba10e4dfdee0df73334
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 150, 150], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_420c34a900b204bc2025a0703f392c58(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 8, 8], dtype='float16', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fcd67f827b682c28853fe69d171d68b3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.29030123353004456, 0.1523994505405426, 0.19805806875228882, 0.46236130595207214, 0.26467519998550415, 0.056576069444417953, 0.0990041196346283, 0.13728125393390656, 0.09698512405157089, 0.27477777004241943, 0.4495559334754944, 0.21884994208812714, 0.22575584053993225, 0.30188074707984924, 0.0010164778213948011, 0.09795898199081421], dtype='float32').reshape([16]),
            paddle.to_tensor([0.43380627036094666, 0.2657621502876282, 0.19512079656124115, 0.15188173949718475, 0.35548534989356995, 0.16122736036777496, 0.4301440417766571, 0.030300915241241455, 0.09975272417068481, 0.460444837808609, 0.15907233953475952, 0.2787216901779175, 0.3074343502521515, 0.149922713637352, 0.012827683240175247, 0.3198317289352417], dtype='float32').reshape([16]),
            paddle.to_tensor([0.41991469264030457, 0.32182765007019043, 0.0023225233890116215, 0.33583322167396545, 0.23579922318458557, 0.3940393924713135, 0.427266925573349, 0.35653355717658997, 0.06652756035327911, 0.2769809067249298, 0.48397567868232727, 0.0687878280878067, 0.4034329354763031, 0.47756537795066833, 0.3642165958881378, 0.2007666677236557], dtype='float32').reshape([16]),
            paddle.to_tensor([0.1096213161945343, 0.12859506905078888, 0.15290343761444092, 0.1344282180070877, 0.039049942046403885, 0.28665685653686523, 0.4302787184715271, 0.2116684913635254, 0.4489498734474182, 0.42087647318840027, 0.39519229531288147, 0.4937325119972229, 0.4333305358886719, 0.0692823976278305, 0.3262569308280945, 0.4973905086517334], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_304d885984149afbda27f1e5e49a3b9d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 160, 160], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_11dc0f839239edc26db3ec13c87c3cb1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 36, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7e19e021a85ead28cdbbf7ec51804e44(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 128, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fbdb8ac77e97c0db51c073d4b42eb789(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 40, 40], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_dbcc2c47ef76280e1d10826731204cc9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a8301b810bccba10e4dfdee0df73334
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 75, 75], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_60d97c6ba1391dc0c0dc72235997d16e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8ca51cbf600a2490b7ca40b7d02a9741(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9f45d081c0b6f9e483d080e1b3c5115c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_da4d85bc213f70819a72e68f21214cdb
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 32, 16, 16], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bc334618214d7b9d122310851c15533c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.3280597925186157, 0.16351942718029022, 0.2429465800523758, 0.08221390098333359, 0.33575478196144104, 0.40891462564468384, 0.2693589925765991, 0.08856447786092758, 0.06810326129198074, 0.48343387246131897, 0.02101581171154976, 0.06803513318300247, 0.29343461990356445, 0.2751739025115967, 0.02173447236418724, 0.05357164144515991, 0.0010355047415941954, 0.03826012834906578], dtype='float32').reshape([18]),
            paddle.to_tensor([0.45482441782951355, 0.270505428314209, 0.3256758749485016, 0.338899701833725, 0.44718489050865173, 0.051496781408786774, 0.39169180393218994, 0.38872405886650085, 0.21397317945957184, 0.0725393071770668, 0.3889792859554291, 0.4291795492172241, 0.1661165952682495, 0.40845879912376404, 0.3650687038898468, 0.3193883001804352, 0.42660850286483765, 0.4631088078022003], dtype='float32').reshape([18]),
            paddle.to_tensor([0.0321369431912899, 0.2296483963727951, 0.04034654051065445, 0.4162503182888031, 0.3053376376628876, 0.4420311152935028, 0.28892120718955994, 0.30773553252220154, 0.16565336287021637, 0.17397339642047882, 0.40874001383781433, 0.1700284332036972, 0.33152687549591064, 0.022903524339199066, 0.17617450654506683, 0.025378501042723656, 0.03594242036342621, 0.42313647270202637], dtype='float32').reshape([18]),
            paddle.to_tensor([0.2609189450740814, 0.4199933409690857, 0.4290144443511963, 0.044082678854465485, 0.046903081238269806, 0.2294977307319641, 0.43600571155548096, 0.42250436544418335, 0.14721114933490753, 0.3062037527561188, 0.31333938241004944, 0.10341513901948929, 0.04375361278653145, 0.21206940710544586, 0.3197081685066223, 0.27069154381752014, 0.3599182069301605, 0.025929665192961693], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1ce0797f025666260d79f9199fb7d3e0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.06378702819347382, 0.22369536757469177, 0.3757559657096863, 0.4450957775115967, 0.2663664221763611, 0.2093127965927124, 0.04388590157032013, 0.3978787660598755, 0.18858647346496582, 0.3197808563709259, 0.3930795192718506, 0.2816912829875946, 0.26135802268981934, 0.3728935122489929, 0.06679785996675491, 0.19786010682582855, 0.15395964682102203, 0.24820470809936523, 0.05036701261997223, 0.43846774101257324], dtype='float32').reshape([20]),
            paddle.to_tensor([0.23256441950798035, 0.05247459560632706, 0.055779848247766495, 0.421730101108551, 0.09808029979467392, 0.3839775025844574, 0.40444350242614746, 0.3211657404899597, 0.36319655179977417, 0.1528148651123047, 0.1707681268453598, 0.3733150362968445, 0.3456060588359833, 0.33516913652420044, 0.08534110337495804, 0.4827495813369751, 0.32426515221595764, 0.042636483907699585, 0.0390702523291111, 0.07048016041517258], dtype='float32').reshape([20]),
            paddle.to_tensor([0.4400515854358673, 0.2253400683403015, 0.300324022769928, 0.02385726198554039, 0.21204836666584015, 0.020281506702303886, 0.11892886459827423, 0.1547423005104065, 0.009884912520647049, 0.20909832417964935, 0.28190314769744873, 0.1534305363893509, 0.2012602537870407, 0.16093847155570984, 0.4437854290008545, 0.2186819165945053, 0.4525526165962219, 0.21522338688373566, 0.13408054411411285, 0.4515649080276489], dtype='float32').reshape([20]),
            paddle.to_tensor([0.011773920617997646, 0.27413037419319153, 0.3122686743736267, 0.10485810041427612, 0.25574445724487305, 0.12560594081878662, 0.08705069124698639, 0.3441113233566284, 0.019656803458929062, 0.07779531180858612, 0.26032334566116333, 0.34613919258117676, 0.1700890064239502, 0.3309069275856018, 0.1345650851726532, 0.3559795916080475, 0.12120509892702103, 0.004864701069891453, 0.19724693894386292, 0.27924343943595886], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7664b7ad73a2e97b56493691ed3463d5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_0edd578f48a07aa5d1d79f9f678b472e
    def get_inputs(self):
        return [
            paddle.uniform([16, 512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_43b7fef28f60b921a302b4d5bd0c9f06(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_db794406a0a5ae660b2575a890734563(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.10916091501712799, 0.4991006851196289, 0.2718498408794403, 0.28050777316093445, 0.3098934292793274, 0.46578094363212585, 0.2716691792011261, 0.12341655045747757, 0.013855532743036747, 0.3683456480503082, 0.2530662715435028, 0.3185764253139496, 0.3572215437889099, 0.2155379205942154, 0.40815845131874084, 0.35914847254753113, 0.3448808789253235, 0.17471317946910858, 0.2889775037765503, 0.015179128386080265], dtype='float32').reshape([20]),
            paddle.to_tensor([0.3023254871368408, 0.3724800646305084, 0.0674484521150589, 0.4399208426475525, 0.0066094291396439075, 0.23703356087207794, 0.42210710048675537, 0.47518616914749146, 0.08721331506967545, 0.4742961823940277, 0.4657176733016968, 0.29515331983566284, 0.36972546577453613, 0.2311907410621643, 0.12947845458984375, 0.0384870283305645, 0.20402315258979797, 0.44661423563957214, 0.17951825261116028, 0.17553623020648956], dtype='float32').reshape([20]),
            paddle.to_tensor([0.20158398151397705, 0.27202314138412476, 0.265821635723114, 0.11764256656169891, 0.4471619129180908, 0.4168131351470947, 0.4229811728000641, 0.08614519238471985, 0.45901384949684143, 0.09974552690982819, 0.04061770811676979, 0.36406224966049194, 0.0640290305018425, 0.49108603596687317, 0.43278613686561584, 0.14003129303455353, 0.05787738412618637, 0.042254265397787094, 0.3492916226387024, 0.3809170126914978], dtype='float32').reshape([20]),
            paddle.to_tensor([0.11112501472234726, 0.003309553489089012, 0.2092687338590622, 0.3599853813648224, 0.20009906589984894, 0.4804884195327759, 0.3902062177658081, 0.02413422241806984, 0.1866530328989029, 0.1584213227033615, 0.09089800715446472, 0.39161914587020874, 0.45443880558013916, 0.11810612678527832, 0.4635617434978485, 0.1782350391149521, 0.1376800388097763, 0.18222670257091522, 0.2860458195209503, 0.4271989166736603], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1e8b578dca7ceea0107a1cfc7134e8c1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.4847070276737213, 0.47880178689956665, 0.06631212681531906, 0.37431973218917847, 0.12092525511980057, 0.13525065779685974, 0.38035139441490173, 0.017887849360704422, 0.4288974106311798, 0.49717554450035095, 0.4425256848335266, 0.22373874485492706, 0.15744851529598236, 0.117840975522995, 0.2863854765892029, 0.20078542828559875, 0.07570534199476242, 0.2704007923603058], dtype='float32').reshape([18]),
            paddle.to_tensor([0.12732376158237457, 0.052943453192710876, 0.4103866219520569, 0.4469243586063385, 0.3943897485733032, 0.3175790309906006, 0.09794282913208008, 0.4783807098865509, 0.25753942131996155, 0.3871973156929016, 0.26996761560440063, 0.46622270345687866, 0.0416547916829586, 0.29717162251472473, 0.32456085085868835, 0.38873952627182007, 0.2676355540752411, 0.2704756557941437], dtype='float32').reshape([18]),
            paddle.to_tensor([0.3330056071281433, 0.062145158648490906, 0.13407377898693085, 0.42096373438835144, 0.3671891689300537, 0.2808572053909302, 0.4842706024646759, 0.07718323916196823, 0.3013990521430969, 0.036836590617895126, 0.4287157952785492, 0.0418374128639698, 0.44142216444015503, 0.09385104477405548, 0.4685171842575073, 0.460303395986557, 0.3937438428401947, 0.03584447503089905], dtype='float32').reshape([18]),
            paddle.to_tensor([0.2741813659667969, 0.24629373848438263, 0.3261919915676117, 0.09534964710474014, 0.15167924761772156, 0.3390970230102539, 0.40825897455215454, 0.38232266902923584, 0.2269771546125412, 0.40554335713386536, 0.06349525600671768, 0.47207412123680115, 0.0463351272046566, 0.3419063091278076, 0.29502734541893005, 0.03298033028841019, 0.3937026262283325, 0.43553972244262695], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ca3a7656e8488b1984fcbb1bea745994(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 320, 320], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.0554816797375679, 0.07444960623979568, 0.1554459184408188, 0.4061252772808075, 0.1590098887681961, 0.11732961237430573, 0.03197304531931877, 0.26368993520736694], dtype='float32').reshape([8]),
            paddle.to_tensor([0.33846336603164673, 0.25935569405555725, 0.11623300611972809, 0.39900636672973633, 0.14832688868045807, 0.3653186559677124, 0.3247357904911041, 0.477679044008255], dtype='float32').reshape([8]),
            paddle.to_tensor([0.2960939407348633, 0.32966139912605286, 0.009307420812547207, 0.48420262336730957, 0.2849457859992981, 0.41350263357162476, 0.15509472787380219, 0.4349375069141388], dtype='float32').reshape([8]),
            paddle.to_tensor([0.16162486374378204, 0.13760878145694733, 0.2944747805595398, 0.3128020465373993, 0.2731131911277771, 0.07156509160995483, 0.17226439714431763, 0.3168611526489258], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ad62b448b74fca5df53ef3b866d62c87(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 120, 4, 50], dtype='float16', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_39f301f0b7726800c3f64144f3a058ec(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 4, 26], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b49657184e4f1b4bbac9f6af07e52839(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.39532405138015747, 0.3479638993740082, 0.4155016541481018, 0.36550644040107727, 0.33684083819389343, 0.058384839445352554, 0.06648614257574081, 0.4377972185611725, 0.4252893924713135, 0.2960854768753052, 0.4029817581176758, 0.17104490101337433, 0.4093383252620697, 0.07881410419940948, 0.4752179682254791, 0.46390649676322937, 0.12694616615772247, 0.371763676404953, 0.30439743399620056, 0.494537353515625], dtype='float32').reshape([20]),
            paddle.to_tensor([0.49368661642074585, 0.25960490107536316, 0.1247178465127945, 0.021319927647709846, 0.36628052592277527, 0.062154095619916916, 0.3611670732498169, 0.08319047838449478, 0.05766994506120682, 0.08680787682533264, 0.032950691878795624, 0.16343532502651215, 0.4320835471153259, 0.43949005007743835, 0.4497375190258026, 0.3954150378704071, 0.34748804569244385, 0.012420505285263062, 0.02492167428135872, 0.10985132306814194], dtype='float32').reshape([20]),
            paddle.to_tensor([0.3102065622806549, 0.3914021849632263, 0.2692628800868988, 0.08181203901767731, 0.07778448611497879, 0.3434234857559204, 0.12741009891033173, 0.17469948530197144, 0.24763689935207367, 0.25662025809288025, 0.49549731612205505, 0.4133935570716858, 0.025915775448083878, 0.25845029950141907, 0.20581161975860596, 0.26154395937919617, 0.39264196157455444, 0.06358791887760162, 0.047622524201869965, 0.06817523390054703], dtype='float32').reshape([20]),
            paddle.to_tensor([0.4334016442298889, 0.00837019830942154, 0.4141572117805481, 0.19606880843639374, 0.3212693929672241, 0.2590637505054474, 0.4078916013240814, 0.01837511546909809, 0.4004949927330017, 0.1384197473526001, 0.290481299161911, 0.0819787085056305, 0.4709794223308563, 0.4836758077144623, 0.45055294036865234, 0.045124351978302, 0.09698928147554398, 0.11243537813425064, 0.13725139200687408, 0.45053571462631226], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7a6efabd134873fa4424fdba205e36ed(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 234, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([234], dtype='float32', min=0, max=0.5),
            paddle.uniform([234], dtype='float32', min=0, max=0.5),
            paddle.uniform([234], dtype='float32', min=0, max=0.5),
            paddle.uniform([234], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1d81991695bc94eb9385bf77e708a107(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.14186307787895203, 0.2050037533044815, 0.03936951234936714, 0.06276378035545349, 0.2298075407743454, 0.10108774900436401, 0.16737356781959534, 0.2826591730117798, 0.07195648550987244, 0.10816007107496262, 0.39575743675231934, 0.3299412727355957, 0.17600573599338531, 0.08385104686021805, 0.3608946204185486, 0.25115472078323364, 0.3197973668575287, 0.48863014578819275, 0.1562122404575348, 0.09043067693710327, 0.1950160712003708, 0.3061552941799164, 0.4372185170650482, 0.406579852104187, 0.04794912785291672, 0.47263699769973755, 0.029376208782196045, 0.46051180362701416, 0.0861489549279213, 0.1213759332895279], dtype='float32').reshape([30]),
            paddle.to_tensor([0.011450575664639473, 0.1045265719294548, 0.02416926436126232, 0.15208739042282104, 0.3017747700214386, 0.01819021999835968, 0.26448920369148254, 0.26911550760269165, 0.1271105706691742, 0.3901064097881317, 0.28088679909706116, 0.11052889376878738, 0.3707631528377533, 0.2664516568183899, 0.26537927985191345, 0.4268438518047333, 0.46956396102905273, 0.12822097539901733, 0.07983455806970596, 0.011825439520180225, 0.15527480840682983, 0.19202978909015656, 0.2720447778701782, 0.15559802949428558, 0.006027208641171455, 0.3205644190311432, 0.41645461320877075, 0.3996157646179199, 0.009777537547051907, 0.3659511208534241], dtype='float32').reshape([30]),
            paddle.to_tensor([0.23381558060646057, 0.10637206584215164, 0.4675585627555847, 0.2176744043827057, 0.37690865993499756, 0.45060449838638306, 0.3187820613384247, 0.46914345026016235, 0.031821154057979584, 0.2579173743724823, 0.3097599744796753, 0.27455613017082214, 0.4262045621871948, 0.31541529297828674, 0.19351650774478912, 0.40692925453186035, 0.07073221355676651, 0.10355169326066971, 0.219664067029953, 0.4804803431034088, 0.1668987274169922, 0.17129682004451752, 0.15276193618774414, 0.21539942920207977, 0.3502965569496155, 0.3900856375694275, 0.3000222444534302, 0.0261935293674469, 0.3191494047641754, 0.21857567131519318], dtype='float32').reshape([30]),
            paddle.to_tensor([0.42318078875541687, 0.4343198239803314, 0.4500608742237091, 0.025703467428684235, 0.11105413734912872, 0.4339958429336548, 0.00757261598482728, 0.19714510440826416, 0.23845036327838898, 0.08847295492887497, 0.15122345089912415, 0.38529589772224426, 0.2344755232334137, 0.1580663025379181, 0.49300095438957214, 0.2674793303012848, 0.29512444138526917, 0.129608154296875, 0.08358722925186157, 0.08543037623167038, 0.17409400641918182, 0.4150595963001251, 0.20683641731739044, 0.11370521783828735, 0.007413784042000771, 0.14228300750255585, 0.05212157219648361, 0.39146149158477783, 0.06778929382562637, 0.19630222022533417], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8ab29aa41f40d83dd1f017e1d79c2e74(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 25, 42], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_34f6b4a6020b5ad9847b83a777b6cb11(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_145ad20e7a3b3c8aa31f6b40d4fe3d45
    def get_inputs(self):
        return [
            paddle.uniform([49, 384], dtype='float16', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cd3973153ec39db7366eee02f4ef9872(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.31536203622817993, 0.10602741688489914, 0.0025629401206970215, 0.021801961585879326, 0.17630362510681152, 0.10743461549282074, 0.028559911996126175, 0.4652615487575531, 0.4048433005809784, 0.2995584309101105, 0.16492600739002228, 0.006556926295161247, 0.45702287554740906, 0.3153637945652008, 0.48755785822868347, 0.01722385175526142, 0.45109817385673523, 0.025482287630438805, 0.42779770493507385, 0.3998357951641083, 0.49428996443748474, 0.15871503949165344, 0.39939701557159424, 0.08871753513813019, 0.4344464838504791, 0.13779322803020477, 0.35998979210853577, 0.17531262338161469, 0.21002675592899323, 0.08549993485212326], dtype='float32').reshape([30]),
            paddle.to_tensor([0.2703680992126465, 0.03931223973631859, 0.3607461452484131, 0.11608278006315231, 0.13375960290431976, 0.34591275453567505, 0.4901735782623291, 0.0969456136226654, 0.4354389011859894, 0.04139498621225357, 0.08938415348529816, 0.1916615515947342, 0.2999681234359741, 0.005782286170870066, 0.16647273302078247, 0.013740125112235546, 0.09684890508651733, 0.29554954171180725, 0.4171275496482849, 0.4141368865966797, 0.03114384599030018, 0.42378899455070496, 0.32233932614326477, 0.4158047139644623, 0.48233166337013245, 0.23420894145965576, 0.0917392149567604, 0.39547866582870483, 0.1069779172539711, 0.17578983306884766], dtype='float32').reshape([30]),
            paddle.to_tensor([0.28221026062965393, 0.08432897925376892, 0.38893604278564453, 0.14701181650161743, 0.29986807703971863, 0.04671565443277359, 0.4990983009338379, 0.47325199842453003, 0.19780896604061127, 0.47009631991386414, 0.3166256546974182, 0.0040963804349303246, 0.1289445459842682, 0.49795493483543396, 0.11001358926296234, 0.30769363045692444, 0.2256513088941574, 0.16563504934310913, 0.31133514642715454, 0.3323601186275482, 0.4291432201862335, 0.27840858697891235, 0.3980552554130554, 0.1182200089097023, 0.08346181362867355, 0.499969482421875, 0.14168310165405273, 0.3839194178581238, 0.06133212149143219, 0.2684599757194519], dtype='float32').reshape([30]),
            paddle.to_tensor([0.12580792605876923, 0.3317759037017822, 0.05388481914997101, 0.035139862447977066, 0.4710080921649933, 0.1253051906824112, 0.4756215512752533, 0.29245123267173767, 0.49778446555137634, 0.40233713388442993, 0.3378664553165436, 0.33964625000953674, 0.21641913056373596, 0.42992088198661804, 0.3464225232601166, 0.36695292592048645, 0.33971166610717773, 0.48224976658821106, 0.47115540504455566, 0.18991097807884216, 0.26231011748313904, 0.43820589780807495, 0.4852324426174164, 0.3908706605434418, 0.3489646017551422, 0.3399140238761902, 0.4875586926937103, 0.15387070178985596, 0.13190147280693054, 0.13557712733745575], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1c378d64abd3f591c3ee2440e9e33f76(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 4, 8], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4233af795a92b4c97303357fd3bb10ea(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 240, 240], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_eeeeca2dda4d38b2f54c2bbdefdbb1f2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_573560859f1cae620315c9c7e14dd690(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.2659814655780792, 0.002657168311998248, 0.2270190715789795, 0.0759405791759491, 0.11712660640478134, 0.07746041566133499, 0.1619434952735901, 0.3925284147262573, 0.48809200525283813, 0.05144122615456581, 0.2663700580596924, 0.38437148928642273, 0.20299293100833893, 0.11748071759939194, 0.04963930323719978, 0.3724851906299591, 0.22383372485637665, 0.30742770433425903], dtype='float32').reshape([18]),
            paddle.to_tensor([0.3932938575744629, 0.391023188829422, 0.14824749529361725, 0.1543058156967163, 0.3413558006286621, 0.33838117122650146, 0.22990106046199799, 0.4026665985584259, 0.04192278906702995, 0.14274601638317108, 0.4524766802787781, 0.448688268661499, 0.43039771914482117, 0.4030398726463318, 0.08628416806459427, 0.26904699206352234, 0.1688535511493683, 0.15944568812847137], dtype='float32').reshape([18]),
            paddle.to_tensor([0.42479613423347473, 0.44833362102508545, 0.4951989948749542, 0.39889001846313477, 0.05320030078291893, 0.20365917682647705, 0.037367332726716995, 0.007786780595779419, 0.3185725808143616, 0.11365332454442978, 0.18443362414836884, 0.36541348695755005, 0.13160695135593414, 0.08560813963413239, 0.165353924036026, 0.22997845709323883, 0.24802829325199127, 0.46988558769226074], dtype='float32').reshape([18]),
            paddle.to_tensor([0.04821481183171272, 0.47000306844711304, 0.3356737494468689, 0.17885124683380127, 0.3666639029979706, 0.04879019781947136, 0.15440450608730316, 0.3152787387371063, 0.45995157957077026, 0.06961211562156677, 0.1215716004371643, 0.16830645501613617, 0.26091837882995605, 0.061193421483039856, 0.2418118417263031, 0.27590885758399963, 0.11012336611747742, 0.33110252022743225], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_89ff2cd406d34d720d4ca2b66914247f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 8, 6], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a145b94d20563d9cc7e605270e2b1385(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 88, 61, 61], dtype='float32', min=0, max=0.5),
            paddle.uniform([88], dtype='float32', min=0, max=0.5),
            paddle.uniform([88], dtype='float32', min=0, max=0.5),
            paddle.uniform([88], dtype='float32', min=0, max=0.5),
            paddle.uniform([88], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3c22e99a19551864e4c2c53dc9e25197(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_b4f4da3a8d44d1b5b3a2903a097df186
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 128, 128], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_320ae143d522d861680ef88ff832abef(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.39409327507019043, 0.09301836043596268, 0.33814430236816406, 0.341106116771698, 0.07921631634235382, 0.17731280624866486, 0.009455463849008083, 0.024451326578855515, 0.42760568857192993, 0.4299861490726471, 0.2453950047492981, 0.37921440601348877, 0.17172017693519592, 0.2173081338405609, 0.05372117459774017, 0.1438140869140625, 0.04092089459300041, 0.22993203997612, 0.05924447253346443, 0.14522747695446014], dtype='float32').reshape([20]),
            paddle.to_tensor([0.31343144178390503, 0.31012606620788574, 0.3532686233520508, 0.43076372146606445, 0.33863574266433716, 0.3383444845676422, 0.46738308668136597, 0.009122535586357117, 0.11383825540542603, 0.08094610273838043, 0.03048214688897133, 0.4932081699371338, 0.02425379492342472, 0.07939434796571732, 0.05377604812383652, 0.06517151743173599, 0.36446309089660645, 0.12193566560745239, 0.10138878971338272, 0.07135378569364548], dtype='float32').reshape([20]),
            paddle.to_tensor([0.3437943160533905, 0.41655656695365906, 0.15278050303459167, 0.25465068221092224, 0.3911225497722626, 0.04167701303958893, 0.32507506012916565, 0.47125932574272156, 0.47648900747299194, 0.13402876257896423, 0.13275650143623352, 0.28847113251686096, 0.4190431535243988, 0.4653981924057007, 0.2347593754529953, 0.24602632224559784, 0.206065833568573, 0.20094244182109833, 0.22685933113098145, 0.1683485060930252], dtype='float32').reshape([20]),
            paddle.to_tensor([0.12135547399520874, 0.34098637104034424, 0.1983112394809723, 0.17063473165035248, 0.02607206255197525, 0.10030476003885269, 0.06767422705888748, 0.32390668988227844, 0.26464125514030457, 0.12373364716768265, 0.06799374520778656, 0.21720053255558014, 0.41055694222450256, 0.26816269755363464, 0.3533453643321991, 0.2249242663383484, 0.08194894343614578, 0.4227794110774994, 0.4289209842681885, 0.04746317118406296], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_83bfcb5827bd4d1a24ef869734f0d5e7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.3161773085594177, 0.48637399077415466, 0.04072015359997749, 0.06095748394727707, 0.19490070641040802, 0.36932170391082764, 0.026001866906881332, 0.2444944828748703, 0.03009410761296749, 0.36925622820854187, 0.2630113661289215, 0.02619512751698494, 0.46636274456977844, 0.46336352825164795, 0.28954190015792847, 0.20177069306373596], dtype='float32').reshape([16]),
            paddle.to_tensor([0.41165676712989807, 0.32118871808052063, 0.06443444639444351, 0.4181230366230011, 0.0749845877289772, 0.1864529550075531, 0.3790665566921234, 0.42831292748451233, 0.06635201722383499, 0.12736566364765167, 0.3813301622867584, 0.25320160388946533, 0.3422386646270752, 0.276887446641922, 0.37180498242378235, 0.2937968075275421], dtype='float32').reshape([16]),
            paddle.to_tensor([0.20940148830413818, 0.16469545662403107, 0.37675774097442627, 0.3448629081249237, 0.4008786380290985, 0.11775869876146317, 0.35384294390678406, 0.49716347455978394, 0.19606684148311615, 0.3796042799949646, 0.24806156754493713, 0.09307096153497696, 0.2887641191482544, 0.2485598623752594, 0.18831861019134521, 0.3526265025138855], dtype='float32').reshape([16]),
            paddle.to_tensor([0.053045496344566345, 0.2729407846927643, 0.1785648763179779, 0.4460204541683197, 0.2981773018836975, 0.09404511749744415, 0.02158200740814209, 0.4139406681060791, 0.3567501902580261, 0.34164321422576904, 0.0011451058089733124, 0.3648822605609894, 0.4726121723651886, 0.20559611916542053, 0.16955770552158356, 0.30596596002578735], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_32d97dc4625dc16e30d6c81590385748(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 8, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fe2d6412a4949ed7695fbb7d210274d3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 128, 128], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b143c694a486b8d18e3e7e95456a53e9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.07950960099697113, 0.40766477584838867, 0.07116931676864624, 0.4170980155467987, 0.4232753813266754, 0.2649974822998047, 0.3564941883087158, 0.30365487933158875, 0.4752887189388275, 0.01668768934905529, 0.2330104261636734, 0.16091036796569824, 0.3286105990409851, 0.3246307075023651, 0.1513599008321762, 0.15300589799880981, 0.41245052218437195, 0.09795813262462616, 0.2562037706375122, 0.053201768547296524], dtype='float32').reshape([20]),
            paddle.to_tensor([0.38443896174430847, 0.4822688102722168, 0.3986814618110657, 0.188531756401062, 0.4958241283893585, 0.2589455544948578, 0.48519039154052734, 0.09737952053546906, 0.15019944310188293, 0.06264087557792664, 0.07179940491914749, 0.19954057037830353, 0.46863317489624023, 0.36437296867370605, 0.08878425508737564, 0.299624502658844, 0.28813761472702026, 0.2795884311199188, 0.2080017775297165, 0.36293619871139526], dtype='float32').reshape([20]),
            paddle.to_tensor([0.09344107657670975, 0.22643813490867615, 0.476220965385437, 0.10237397253513336, 0.3888767659664154, 0.03647984564304352, 0.023504886776208878, 0.1254798024892807, 0.07634967565536499, 0.2175179421901703, 0.19296854734420776, 0.05762570723891258, 0.49976545572280884, 0.3465266227722168, 0.46509167551994324, 0.29536861181259155, 0.3775186538696289, 0.1848486065864563, 0.00882788747549057, 0.22821331024169922], dtype='float32').reshape([20]),
            paddle.to_tensor([0.4707742929458618, 0.29523155093193054, 0.17988142371177673, 0.496794193983078, 0.13421466946601868, 0.3956516981124878, 0.35164785385131836, 0.46187305450439453, 0.40188708901405334, 0.0058210077695548534, 0.4935051500797272, 0.2554631233215332, 0.20955023169517517, 0.029323510825634003, 0.3857956826686859, 0.40746647119522095, 0.06879045814275742, 0.1335187405347824, 0.2811300754547119, 0.03361942246556282], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fe7b3ff6d0b1395d90c480317062d53e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 64, 256], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6af922ab14acc3c11d8287c3c65e3cce(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 102, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([102], dtype='float32', min=0, max=0.5),
            paddle.uniform([102], dtype='float32', min=0, max=0.5),
            paddle.uniform([102], dtype='float32', min=0, max=0.5),
            paddle.uniform([102], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4f51ace0e2ebe67d942c802bcff5f009(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 320, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a142c909e153d8fb714a0d19c8065431(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 80, 80], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3e23e100c6076656316df21831bcfd61(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 184, 64, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([184], dtype='float32', min=0, max=0.5),
            paddle.uniform([184], dtype='float32', min=0, max=0.5),
            paddle.uniform([184], dtype='float32', min=0, max=0.5),
            paddle.uniform([184], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c684ef3a7aac6d02f56312759c5d8cc3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 128, 128], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_34bd9fea540da17a48d9a3f426b1b296(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_5e7cb23119f65b33bdbad71952f6dd12
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 64, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2bba1baf959b6e4ac8a282c1b3494e9c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 122, 122], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cafeb6f7b4cac2346d4cba08981d90bf(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 480, 64, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8cffce9648203988a5f986e3ac4c4f15(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 320, 320], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a9ea46fef6b7b52b93ee4913f038c396(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 768, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e1b75669a79ba755159a560e8fa1e491(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 61, 61], dtype='float16', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c8523831babca1f5236ffc204c4c1f0f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 28, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.06447935104370117, 0.06687436252832413, 0.21010151505470276, 0.3967149555683136, 0.2410036325454712, 0.3016303479671478, 0.2484433650970459, 0.266423761844635, 0.13486938178539276, 0.20623916387557983, 0.2221951186656952, 0.39114317297935486, 0.24546125531196594, 0.2877524197101593, 0.07249145954847336, 0.3350079357624054, 0.19917477667331696, 5.9041951317340136e-05, 0.0640588104724884, 0.3143439292907715, 0.012925026006996632, 0.15239614248275757, 0.24589164555072784, 0.209670752286911, 0.2808023989200592, 0.14981141686439514, 0.22473222017288208, 0.06993722915649414], dtype='float32').reshape([28]),
            paddle.to_tensor([0.3793679475784302, 0.16091415286064148, 0.143375962972641, 0.10805012285709381, 0.16381993889808655, 0.4523352086544037, 0.4438258111476898, 0.15339204668998718, 0.39318257570266724, 0.48298007249832153, 0.2281257063150406, 0.033375080674886703, 0.4260673224925995, 0.4656526744365692, 0.43386226892471313, 0.3190149962902069, 0.2604638338088989, 0.21663980185985565, 0.30298352241516113, 0.44167858362197876, 0.12911918759346008, 0.4589299261569977, 0.2590218186378479, 0.05927042290568352, 0.2963124215602875, 0.36222603917121887, 0.12371961772441864, 0.43934786319732666], dtype='float32').reshape([28]),
            paddle.to_tensor([0.4807618260383606, 0.11573377996683121, 0.17792093753814697, 0.48976439237594604, 0.047542452812194824, 0.2041390985250473, 0.0895143672823906, 0.27191951870918274, 0.1457335501909256, 0.0008781678625382483, 0.27474600076675415, 0.16459068655967712, 0.23775887489318848, 0.46986085176467896, 0.41571271419525146, 0.25499382615089417, 0.3148913085460663, 0.3931301534175873, 0.24308845400810242, 0.3736855983734131, 0.025149742141366005, 0.2114022672176361, 0.23826779425144196, 0.04093274474143982, 0.3308742046356201, 0.0002265552175231278, 0.3133662939071655, 0.17161327600479126], dtype='float32').reshape([28]),
            paddle.to_tensor([0.1686706840991974, 0.18667618930339813, 0.04046430438756943, 0.49479424953460693, 0.02956838347017765, 0.43298089504241943, 0.3569069802761078, 0.44064798951148987, 0.26759183406829834, 0.22844471037387848, 0.4073353111743927, 0.37121742963790894, 0.3389713764190674, 0.32986724376678467, 0.09788711369037628, 0.23070435225963593, 0.29883861541748047, 0.1921658217906952, 0.3622291386127472, 0.39275577664375305, 0.2816886901855469, 0.35110563039779663, 0.29361751675605774, 0.24117149412631989, 0.2977941036224365, 0.3545040786266327, 0.2275536060333252, 0.37999874353408813], dtype='float32').reshape([28]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_932bdc22f3c5cf405ec0d4df0befe3f3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([8, 1024, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_268ed7fe830c2a9b0807c0851cd9624d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bbe9af8f06b58db52b978ea1089b990d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 64, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bde1c16c29a25806cede94c737d59361(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 1, 1], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3711e25ffedc5ef7da43f9f9233c0c06(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.06568524241447449, 0.046543773263692856, 0.4473097026348114, 0.4403280019760132, 0.021533409133553505, 0.4935939610004425, 0.0809769481420517, 0.3378923535346985, 0.05562876537442207, 0.16761714220046997, 0.43866291642189026, 0.0937153697013855, 0.06191664934158325, 0.4946346580982208, 0.012754848226904869, 0.4270765483379364, 0.09883398562669754, 0.36174580454826355, 0.25383085012435913, 0.404729425907135, 0.032275330275297165, 0.14690913259983063, 0.23172350227832794, 0.11955495923757553, 0.348985493183136, 0.0717359110713005, 0.20213672518730164, 0.4265163242816925, 0.028032660484313965, 0.0179620198905468], dtype='float32').reshape([30]),
            paddle.to_tensor([0.09798982739448547, 0.27902448177337646, 0.44241562485694885, 0.3095621168613434, 0.4886719882488251, 0.3225674331188202, 0.3206638991832733, 0.1524290293455124, 0.1328565925359726, 0.11926141381263733, 0.00207706680521369, 0.17214524745941162, 0.10436759144067764, 0.42076376080513, 0.08046910166740417, 0.4957595467567444, 0.46129927039146423, 0.4878893494606018, 0.04702922701835632, 0.21456734836101532, 0.2980474531650543, 0.04292978718876839, 0.12462148815393448, 0.03317940980195999, 0.06518088281154633, 0.20250746607780457, 0.1980203241109848, 0.29442211985588074, 0.46136415004730225, 0.14966434240341187], dtype='float32').reshape([30]),
            paddle.to_tensor([0.3745895028114319, 0.30691084265708923, 0.25381413102149963, 0.04468537122011185, 0.34118983149528503, 0.2694849371910095, 0.17831897735595703, 0.006222546566277742, 0.4785129427909851, 0.2821213901042938, 0.025120429694652557, 0.2344435304403305, 0.3066861629486084, 0.15614670515060425, 0.4607100486755371, 0.05147780105471611, 0.318676233291626, 0.0905795618891716, 0.3103542625904083, 0.22120876610279083, 0.13696850836277008, 0.3994508981704712, 0.1861991137266159, 0.24776168167591095, 0.25979524850845337, 0.4376906454563141, 0.2479126751422882, 0.35776105523109436, 0.40460172295570374, 0.016366250813007355], dtype='float32').reshape([30]),
            paddle.to_tensor([0.07316325604915619, 0.3552515506744385, 0.03461657837033272, 0.09299400448799133, 0.04660883545875549, 0.05602644011378288, 0.38142824172973633, 0.21950392425060272, 0.0990912988781929, 0.46368327736854553, 0.36438941955566406, 0.15395605564117432, 0.43120941519737244, 0.4464103877544403, 0.4965704679489136, 0.35465556383132935, 0.17799261212348938, 0.26973921060562134, 0.19356530904769897, 0.22857709228992462, 0.4078860878944397, 0.42530927062034607, 0.03142403066158295, 0.31020021438598633, 0.16382044553756714, 0.10885163396596909, 0.48411980271339417, 0.22202561795711517, 0.33580273389816284, 0.16730907559394836], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1f6a740214568bfffc91c14bcd016a71(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 6, 40], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5df9ec34282d229187056c10339a1c32(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 160, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fa4655390752720af92a7314e14ce621(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.316231369972229, 0.1941221058368683, 0.19421221315860748, 0.08768203854560852, 0.2762679159641266, 0.011525132693350315, 0.4214448034763336, 0.43435168266296387, 0.45646798610687256, 0.08476078510284424, 0.41317763924598694, 0.27022427320480347, 0.18939709663391113, 0.29328998923301697, 0.3169924020767212, 0.1409892439842224, 0.09477569907903671, 0.47062110900878906, 0.04763063043355942, 0.16789600253105164], dtype='float32').reshape([20]),
            paddle.to_tensor([0.23027513921260834, 0.3920740485191345, 0.28900769352912903, 0.0863126665353775, 0.025540556758642197, 0.1578047126531601, 0.21513445675373077, 0.3090534806251526, 0.4223214089870453, 0.2048635333776474, 0.39183059334754944, 0.07103168964385986, 0.4890194237232208, 0.49240055680274963, 0.17584647238254547, 0.40112221240997314, 0.18093158304691315, 0.14616142213344574, 0.07771116495132446, 0.039246778935194016], dtype='float32').reshape([20]),
            paddle.to_tensor([0.007012809161096811, 0.0030141640454530716, 0.15449248254299164, 0.45375964045524597, 0.3452339768409729, 0.43256622552871704, 0.2690724730491638, 0.06899643689393997, 0.24853961169719696, 0.2472793459892273, 0.39199283719062805, 0.47185397148132324, 0.04968200623989105, 0.33761683106422424, 0.4707692861557007, 0.31399601697921753, 0.47375014424324036, 0.33558759093284607, 0.45498126745224, 0.048744458705186844], dtype='float32').reshape([20]),
            paddle.to_tensor([0.047658488154411316, 0.48460859060287476, 0.09330632537603378, 0.19856612384319305, 0.3548623323440552, 0.07120946049690247, 0.4460725486278534, 0.32030928134918213, 0.021418623626232147, 0.34589883685112, 0.09280430525541306, 0.3562200963497162, 0.4247443675994873, 0.4623497426509857, 0.4454762637615204, 0.08267825841903687, 0.22708992660045624, 0.005523395258933306, 0.3635663092136383, 0.1514568030834198], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_385d768ff95cf75f94de6693ba2a2e51(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 4, 8], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f6f03fcaa1316f27821173938a7f7ef1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 200, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([200], dtype='float32', min=0, max=0.5),
            paddle.uniform([200], dtype='float32', min=0, max=0.5),
            paddle.uniform([200], dtype='float32', min=0, max=0.5),
            paddle.uniform([200], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f528870d4ae728bb6a1f656ed393b5e4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 32, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d03e09abd0c9c3fcf4a66ab90e09616e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 36, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d97e1437c3d4251789e3959e264d122b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 240, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ef18693e5435e2fb1f41ff480a539acf(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.031358618289232254, 0.0042498791590332985, 0.09526783972978592, 0.3347581624984741, 0.15453188121318817, 0.29614314436912537, 0.15915101766586304, 0.1842048317193985, 0.12526918947696686, 0.28880584239959717, 0.12610416114330292, 0.31231018900871277, 0.12237696349620819, 0.01317867636680603, 0.36054617166519165, 0.2560327649116516, 0.06276717782020569, 0.13846708834171295, 0.20066699385643005, 0.49650657176971436, 0.29449838399887085, 0.25914254784584045, 0.1137542799115181, 0.40457838773727417, 0.2807527482509613, 0.41419556736946106, 0.22310960292816162, 0.23472991585731506, 0.1340131163597107, 0.49243396520614624], dtype='float32').reshape([30]),
            paddle.to_tensor([0.24385347962379456, 0.42355450987815857, 0.38905900716781616, 0.27546435594558716, 0.350236713886261, 0.36536335945129395, 0.03570771589875221, 0.2504318654537201, 0.14565329253673553, 0.00318035832606256, 0.43294060230255127, 0.47455471754074097, 0.37617528438568115, 0.2905580997467041, 0.4891599714756012, 0.04378250986337662, 0.2756803333759308, 0.24128015339374542, 0.34535500407218933, 0.018420385196805, 0.3781883716583252, 0.1309327930212021, 0.1752634346485138, 0.441771924495697, 0.3438795208930969, 0.49311089515686035, 0.19359563291072845, 0.1172703430056572, 0.11731266975402832, 0.41655629873275757], dtype='float32').reshape([30]),
            paddle.to_tensor([0.06019333004951477, 0.1336389034986496, 0.03705423325300217, 0.485664039850235, 0.03091243840754032, 0.22048597037792206, 0.12794706225395203, 0.018085511401295662, 0.47965937852859497, 0.2792833149433136, 0.0681595578789711, 0.2940557599067688, 0.4055364727973938, 0.3015126585960388, 0.22679831087589264, 0.12033887952566147, 0.3151695728302002, 0.03515704721212387, 0.3116595447063446, 0.26200583577156067, 0.45520585775375366, 0.08441624045372009, 0.4891144931316376, 0.25750666856765747, 0.2917729914188385, 0.04196063056588173, 0.4079268276691437, 0.38875681161880493, 0.2841915786266327, 0.3889782130718231], dtype='float32').reshape([30]),
            paddle.to_tensor([0.3064166009426117, 0.4741303622722626, 0.08077952265739441, 0.22223998606204987, 0.22264008224010468, 0.1925913691520691, 0.10700865834951401, 0.19463083148002625, 0.1044706255197525, 0.15657909214496613, 0.3305682837963104, 0.23334535956382751, 0.3212564289569855, 0.07793982326984406, 0.48695996403694153, 0.13377061486244202, 0.1775909960269928, 0.17819197475910187, 0.04027421399950981, 0.050560515373945236, 0.319105327129364, 0.367276132106781, 0.003089998383074999, 0.10377678275108337, 0.06968895345926285, 0.18977420032024384, 0.12634089589118958, 0.4515724778175354, 0.2270948588848114, 0.48100289702415466], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b6579daf6a7ea8852537f1ebafd127f1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 17, 17], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2310390ab1cc780c8c9274491de3dfce(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 1568, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([1568], dtype='float32', min=0, max=0.5),
            paddle.uniform([1568], dtype='float32', min=0, max=0.5),
            paddle.uniform([1568], dtype='float32', min=0, max=0.5),
            paddle.uniform([1568], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_74119b0241e2560f861c162850b9c3b1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_3a47a120d62a3f3a422a747f511df8d9
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 112, 112], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.44059038162231445, 0.42072638869285583, 0.22581344842910767, 0.27305907011032104, 0.16826115548610687, 0.06817184388637543, 0.25871753692626953, 0.41038239002227783, 0.09172012656927109, 0.08806676417589188, 0.10460642725229263, 0.008895366452634335, 0.23043249547481537, 0.1697138398885727, 0.32949918508529663, 0.0014811083674430847], dtype='float32').reshape([16]),
            paddle.to_tensor([0.3082531690597534, 0.4424963891506195, 0.2293504774570465, 0.09905598312616348, 0.07920858263969421, 0.45710480213165283, 0.2906865179538727, 0.0857553631067276, 0.2844720184803009, 0.17196007072925568, 0.41332024335861206, 0.4449814260005951, 0.438772588968277, 0.24961425364017487, 0.3162781596183777, 0.07483665645122528], dtype='float32').reshape([16]),
            paddle.to_tensor([0.23848101496696472, 0.1564471572637558, 0.30327460169792175, 0.15581166744232178, 0.4002215266227722, 0.42753878235816956, 0.079869344830513, 0.4073653519153595, 0.30037957429885864, 0.34885165095329285, 0.48006749153137207, 0.3172839879989624, 0.2645427882671356, 0.25739261507987976, 0.0307505801320076, 0.13410067558288574], dtype='float32').reshape([16]),
            paddle.to_tensor([0.1750783920288086, 0.33501407504081726, 0.04778851196169853, 0.0846041664481163, 0.2973688244819641, 0.47872838377952576, 0.3366517424583435, 0.23952005803585052, 0.4879261553287506, 0.06574254482984543, 0.13666947185993195, 0.39737793803215027, 0.479941189289093, 0.49347829818725586, 0.14525344967842102, 0.15723630785942078], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f72d5df3d4061858bc188eb1bae45079(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 160, 160], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.09201503545045853, 0.21507121622562408, 0.19490249454975128, 0.3400993049144745, 0.1322820633649826, 0.15965120494365692, 0.4312058985233307, 0.31590986251831055, 0.3578491806983948, 0.4971369206905365, 0.17475080490112305, 0.09468333423137665, 0.14853397011756897, 0.222573921084404, 0.40174391865730286, 0.3531647324562073, 0.12320376932621002, 0.2582450807094574, 0.17320916056632996, 0.4642864465713501, 0.4014016389846802, 0.08087410777807236, 0.16636048257350922, 0.20824356377124786], dtype='float32').reshape([24]),
            paddle.to_tensor([0.04527496546506882, 0.14704234898090363, 0.1882198005914688, 0.3789229989051819, 0.23397700488567352, 0.2602750062942505, 0.15817785263061523, 0.24542830884456635, 0.05889881029725075, 0.01116346288472414, 0.4947739839553833, 0.1472311168909073, 0.06689224392175674, 0.2988821566104889, 0.04041755944490433, 0.042257655411958694, 0.05258724093437195, 0.1806795597076416, 0.017797475680708885, 0.4147655963897705, 0.3956994116306305, 0.26460355520248413, 0.042683783918619156, 0.11019694805145264], dtype='float32').reshape([24]),
            paddle.to_tensor([0.28743112087249756, 0.2208433300256729, 0.2663342356681824, 0.3329278528690338, 0.31373360753059387, 0.0663577988743782, 0.21539117395877838, 0.222202330827713, 0.3902963399887085, 0.25110387802124023, 0.33500418066978455, 0.19724999368190765, 0.4585006535053253, 0.398667573928833, 0.44170665740966797, 0.4147423803806305, 0.17917296290397644, 0.45754122734069824, 0.33306464552879333, 0.1657414585351944, 0.1775716096162796, 0.0071353064849972725, 0.04644716903567314, 0.1559315174818039], dtype='float32').reshape([24]),
            paddle.to_tensor([0.287620484828949, 0.3097628057003021, 0.06032354012131691, 0.4283043146133423, 0.46954792737960815, 0.4271683990955353, 0.4557281732559204, 0.21540354192256927, 0.005819621495902538, 0.17998529970645905, 0.43279215693473816, 0.41043707728385925, 0.3979704678058624, 0.14270123839378357, 0.2812314033508301, 0.026997243985533714, 0.35846495628356934, 0.49108126759529114, 0.1859232634305954, 0.17367778718471527, 0.2766924798488617, 0.43562746047973633, 0.42288580536842346, 0.4134945571422577], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_f5f2213be92f20fda5378beeed72688f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, True), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5b7edc0ccd81b717909287901b4505d0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_f5f2213be92f20fda5378beeed72688f
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 8, 8], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_04ee250d3f7b909faa7e4fb9b9bb33cc(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 160, 160], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_765095a358e3cb8a79ab260d3782d182(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 36, 32, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_49468dfc9f4bbc864d8b86ea99b9d12e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.09033919125795364, 0.07836975902318954, 0.04020725190639496, 0.2983887195587158, 0.3705920875072479, 0.3013762831687927, 0.01585891656577587, 0.3526743948459625, 0.2354043424129486, 0.19132235646247864, 0.07032905519008636, 0.45468592643737793, 0.32059574127197266, 0.3566904067993164, 0.2132992297410965, 0.49641916155815125, 0.3134949505329132, 0.27369898557662964, 0.22896352410316467, 0.3208143413066864, 0.3059121370315552, 0.12917183339595795, 0.41936132311820984, 0.36020103096961975, 0.3783975839614868, 0.3664313554763794, 0.4302803874015808, 0.11444701999425888, 0.16065050661563873, 0.08998315781354904], dtype='float32').reshape([30]),
            paddle.to_tensor([0.10717656463384628, 0.19691690802574158, 0.16466233134269714, 0.08267836272716522, 0.3691202700138092, 0.3824717104434967, 0.14713837206363678, 0.4220203757286072, 0.37874969840049744, 0.3193572461605072, 0.3447326421737671, 0.4328405559062958, 0.0687447190284729, 0.45998507738113403, 0.0987873300909996, 0.38755035400390625, 0.4463613033294678, 0.3846248388290405, 0.07150726765394211, 0.20425577461719513, 0.22973042726516724, 0.30288538336753845, 0.2488810271024704, 0.28408095240592957, 0.3681517243385315, 0.07199918478727341, 0.37543201446533203, 0.34466686844825745, 0.3128534257411957, 0.402769535779953], dtype='float32').reshape([30]),
            paddle.to_tensor([0.02147383987903595, 0.29751139879226685, 0.3512462079524994, 0.4191364049911499, 0.24761013686656952, 0.05099300295114517, 0.002912127412855625, 0.4702930152416229, 0.17751550674438477, 0.1322946846485138, 0.36281222105026245, 0.3569921553134918, 0.11846689879894257, 0.4382954239845276, 0.16855575144290924, 0.20116235315799713, 0.32930997014045715, 0.2991701364517212, 0.09230456501245499, 0.4484710395336151, 0.018285764381289482, 0.38655921816825867, 0.41597944498062134, 0.24602234363555908, 0.3214566111564636, 0.11770948022603989, 0.3600572347640991, 0.07667075097560883, 0.008479900658130646, 0.06632620841264725], dtype='float32').reshape([30]),
            paddle.to_tensor([0.4907977879047394, 0.06317147612571716, 0.22020027041435242, 0.17953088879585266, 0.18649713695049286, 0.19936703145503998, 0.3758070766925812, 0.48390889167785645, 0.18529900908470154, 0.4522795081138611, 0.48940280079841614, 0.059432439506053925, 0.10361529886722565, 0.25563621520996094, 0.17183732986450195, 0.483420193195343, 0.16665790975093842, 0.33535152673721313, 0.07255906611680984, 0.11527895927429199, 0.2466437965631485, 0.09244407713413239, 0.063902348279953, 0.2905175983905792, 0.31449633836746216, 0.22141921520233154, 0.08250615745782852, 0.4031270742416382, 0.05524100363254547, 0.17913098633289337], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5cbd2a5def56dcd3286531931b581b79(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 31, 31], dtype='float16', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9cde48c19eaae5df03ffe522c53c9c38(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 5, 5], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3c560d72834438ed3c0feda28291a02d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_da4d85bc213f70819a72e68f21214cdb
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 4, 64, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_03eca6fb312f8659782bb2f9472c638c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 20, 20], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1c89209b4004f62773bea4ea22bea530(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 240, 240], dtype='float16', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c36ba18e77462c0a827bc6c454f266dc(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 8, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e6b26d75b10141b67fd1e4d577de30da(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.25642818212509155, 0.43072065711021423, 0.061513807624578476, 0.10003042221069336, 0.2771868109703064, 0.21356260776519775, 0.01808599755167961, 0.2396092712879181, 0.18551571667194366, 0.11121974140405655, 0.08083615452051163, 0.23618094623088837, 0.13256831467151642, 0.3978639543056488, 0.44115880131721497, 0.23913109302520752, 0.319044828414917, 0.30602148175239563], dtype='float32').reshape([18]),
            paddle.to_tensor([0.4356195330619812, 0.3654853105545044, 0.3677677512168884, 0.14968819916248322, 0.15912042558193207, 0.08182749152183533, 0.16409865021705627, 0.42456525564193726, 0.47970259189605713, 0.014842256903648376, 0.09772548824548721, 0.12605878710746765, 0.4444737434387207, 0.3802841603755951, 0.03884654492139816, 0.17499297857284546, 0.41467514634132385, 0.19750025868415833], dtype='float32').reshape([18]),
            paddle.to_tensor([0.09294404089450836, 0.2761135399341583, 0.0681847333908081, 0.11475188285112381, 0.4064101576805115, 0.26704713702201843, 0.009180865250527859, 0.38619986176490784, 0.10095479339361191, 0.3431445360183716, 0.1943882554769516, 0.37395358085632324, 0.1952495574951172, 0.4303578734397888, 0.3554462790489197, 0.4066162705421448, 0.21653170883655548, 0.4101579189300537], dtype='float32').reshape([18]),
            paddle.to_tensor([0.006256279069930315, 0.45106714963912964, 0.28135067224502563, 0.04492902383208275, 0.16480425000190735, 0.23580487072467804, 0.06476115435361862, 0.10532964020967484, 0.23687690496444702, 0.13383232057094574, 0.3711032569408417, 0.29949381947517395, 0.28096434473991394, 0.19701027870178223, 0.36239832639694214, 0.2980748414993286, 0.2002270370721817, 0.07983487844467163], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e0f15764a58e7ecbce9119f3eafcd630(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 320, 4, 3], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9c49c3d4cdd1a3e7e6672d44cea385a3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_af91295b59c5fca17f890a802deb001f
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 4, 32, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7270d05b9878aa316cc03e10379371e4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 32, 24], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_373ffcc58bd8b22ebb92f12138d16ced(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 6, 6], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c8b47252ca30f4972dccf63c360cbe6f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.30801644921302795, 0.3763123154640198, 0.058948665857315063, 0.29179298877716064, 0.18090347945690155, 0.4841707944869995, 0.0693320706486702, 0.4179067611694336, 0.16918064653873444, 0.10454635322093964, 0.38542360067367554, 0.15726003050804138, 0.24752062559127808, 0.31145650148391724, 0.08346347510814667, 0.2837703227996826, 0.40289443731307983, 0.3145767152309418, 0.2955133616924286, 0.283377081155777, 0.3974252939224243, 0.2491854727268219, 0.11513766646385193, 0.21933837234973907, 0.49727684259414673, 0.36579686403274536, 0.313982218503952, 0.1655060201883316, 0.4124353229999542, 0.07054337859153748], dtype='float32').reshape([30]),
            paddle.to_tensor([0.001649175537750125, 0.45202159881591797, 0.181324765086174, 0.42612138390541077, 0.276523619890213, 0.2287958860397339, 0.13252581655979156, 0.40513673424720764, 0.29933738708496094, 0.23600813746452332, 0.24387796223163605, 0.21075241267681122, 0.41083285212516785, 0.4170151948928833, 0.21140815317630768, 0.2356189340353012, 0.22357222437858582, 0.3940555453300476, 0.37323975563049316, 0.17416591942310333, 0.20952804386615753, 0.250018835067749, 0.2513660192489624, 0.08151853829622269, 0.19701480865478516, 0.4544662833213806, 0.18567895889282227, 0.03890923783183098, 0.35925647616386414, 0.07331689447164536], dtype='float32').reshape([30]),
            paddle.to_tensor([0.3319120407104492, 0.36668893694877625, 0.3726322054862976, 0.45931538939476013, 0.4649363160133362, 0.28213533759117126, 0.37838613986968994, 0.40454092621803284, 0.24973171949386597, 0.4437313973903656, 0.4624112546443939, 0.152391716837883, 0.33444565534591675, 0.42733436822891235, 0.11204349249601364, 0.2814973294734955, 0.4815809428691864, 0.34207460284233093, 0.1878860592842102, 0.337114155292511, 0.22125351428985596, 0.2160046398639679, 0.04322313144803047, 0.15470702946186066, 0.12993782758712769, 0.3361392617225647, 0.07795248180627823, 0.3007495701313019, 0.041037026792764664, 0.23273293673992157], dtype='float32').reshape([30]),
            paddle.to_tensor([0.41450023651123047, 0.4460705518722534, 0.01083256397396326, 0.1359926164150238, 0.33948200941085815, 0.17292022705078125, 0.11867471039295197, 0.12783510982990265, 0.3614400327205658, 0.25707775354385376, 0.43234869837760925, 0.10659390687942505, 0.3018762469291687, 0.24719130992889404, 0.24069827795028687, 0.4429932236671448, 0.04288002848625183, 0.42320531606674194, 0.4802933931350708, 0.39849358797073364, 0.4972853362560272, 0.1282576322555542, 0.45923280715942383, 0.2712363302707672, 0.1573439985513687, 0.48454225063323975, 0.047803618013858795, 0.4772012531757355, 0.30126073956489563, 0.049908872693777084], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6be3a5d5032d77d89ea93f6cdfc9a5fa(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 50, 84], dtype='float16', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_97b47dc24d5154d982ef8cc701e9017d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_972534663799a28f3f84ce6eac5c4db2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 120, 60, 60], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_26985e6c103ecd2d7b0fed5a5be2d237(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 512, 1024], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.12416733801364899, 0.1407860666513443, 0.35741084814071655, 0.35854512453079224, 0.33333441615104675, 0.2887665331363678, 0.08355837315320969, 0.20229296386241913, 0.3739827275276184, 0.3446084260940552, 0.4232497811317444, 0.1371711939573288, 0.12101009488105774, 0.24833886325359344, 0.41762033104896545, 0.28125011920928955], dtype='float32').reshape([16]),
            paddle.to_tensor([0.03042028658092022, 0.21481755375862122, 0.21910342574119568, 0.17056874930858612, 0.045171208679676056, 0.19780108332633972, 0.3655908405780792, 0.47198617458343506, 0.23698841035366058, 0.13137303292751312, 0.23252269625663757, 0.07304660230875015, 0.18043439090251923, 0.3344789445400238, 0.3430621027946472, 0.25624173879623413], dtype='float32').reshape([16]),
            paddle.to_tensor([0.1902199238538742, 0.04742211848497391, 0.4816286265850067, 0.4994838237762451, 0.24950960278511047, 0.3072483539581299, 0.022525211796164513, 0.403931200504303, 0.4779376685619354, 0.4911869764328003, 0.41915225982666016, 0.3844073712825775, 0.157607302069664, 0.1279994398355484, 0.32987359166145325, 0.4816076159477234], dtype='float32').reshape([16]),
            paddle.to_tensor([0.012970300391316414, 0.23943278193473816, 0.08356773108243942, 0.12563356757164001, 0.26420673727989197, 0.001768580637872219, 0.13486890494823456, 0.43616724014282227, 0.19746358692646027, 0.4536236822605133, 0.30962449312210083, 0.09216401726007462, 0.49736079573631287, 0.12113672494888306, 0.379414439201355, 0.14678893983364105], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a62766a8b419dd50ff93483076c4c8ea(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 60, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
            paddle.uniform([60], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1bf66c7105f3099c0b458ae455682bf3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 208, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([208], dtype='float32', min=0, max=0.5),
            paddle.uniform([208], dtype='float32', min=0, max=0.5),
            paddle.uniform([208], dtype='float32', min=0, max=0.5),
            paddle.uniform([208], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c94aeeb24aa908019158afc87c7e1db2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_5e7cb23119f65b33bdbad71952f6dd12
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 1, 1], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_28c8dc626d83829195ccc4688a795b92(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 112, 112], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8171d7164b4887cfd5df2e4ad7e30cc4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 16, 50], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_18c85722b7f424e734508dd0a966147e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a5b4c971a6184820d2b34c8edbc4f7fb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_5e7cb23119f65b33bdbad71952f6dd12
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 32, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c2ed835ccb7cea31f6d8f17e874f319e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 2, 50], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_77138f08f0b14978e46436683db57d7a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 112, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cdca329a884c530c9e37a97c84353454(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 16, 16], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d2e0fc47bf68fd24771b08758fb50861(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 4, 50], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_69b56f2ac0645460c38a914a6947d654(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 1056, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([1056], dtype='float32', min=0, max=0.5),
            paddle.uniform([1056], dtype='float32', min=0, max=0.5),
            paddle.uniform([1056], dtype='float32', min=0, max=0.5),
            paddle.uniform([1056], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c8d28c65eb3b7522679776e65c02b3b3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_3a47a120d62a3f3a422a747f511df8d9
    def get_inputs(self):
        return [
            paddle.uniform([1, 240, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_afbad9c04633d36616438adf1dc22313(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 512, 1024], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.018377142027020454, 0.1395421326160431, 0.15150511264801025, 0.263047456741333, 0.23306334018707275, 0.49955350160598755, 0.1744810789823532, 0.45389997959136963, 0.401751309633255, 0.46731773018836975, 0.028211314231157303, 0.11839532852172852, 0.07825510948896408, 0.1922937035560608, 0.407084584236145, 0.3913995921611786], dtype='float32').reshape([16]),
            paddle.to_tensor([0.016486849635839462, 0.12020495533943176, 0.4079383313655853, 0.33161410689353943, 0.4339738190174103, 0.2178310602903366, 0.17648446559906006, 0.25289303064346313, 0.10712303966283798, 0.3173082172870636, 0.4266469180583954, 0.1547086238861084, 0.24764879047870636, 0.4496151804924011, 0.0887557789683342, 0.13740162551403046], dtype='float32').reshape([16]),
            paddle.to_tensor([0.3371370732784271, 0.19355453550815582, 0.2592906355857849, 0.23312707245349884, 0.3397027254104614, 0.4783564805984497, 0.030668072402477264, 0.3218688368797302, 0.205958291888237, 0.49423426389694214, 0.06733744591474533, 0.43570831418037415, 0.1837790161371231, 0.28864607214927673, 0.26531487703323364, 0.08694852143526077], dtype='float32').reshape([16]),
            paddle.to_tensor([0.020802510902285576, 0.013521376997232437, 0.009138924069702625, 0.31051796674728394, 0.322893351316452, 0.49662280082702637, 0.4892651438713074, 0.16893000900745392, 0.09849037230014801, 0.21055497229099274, 0.31682828068733215, 0.32878395915031433, 0.1646783947944641, 0.048623211681842804, 0.4580172300338745, 0.24047186970710754], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_281a5a7074e416a713532dba8ddbe32c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_145ad20e7a3b3c8aa31f6b40d4fe3d45
    def get_inputs(self):
        return [
            paddle.uniform([1, 512], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5c2b39c750f5f5e66855e1838f2a70cb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 640, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
            paddle.uniform([640], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6eea2e3edc6619ae1f1952ca255b5849(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.1889139711856842, 0.27075543999671936, 0.028581177815794945, 0.03159236162900925, 0.013682610355317593, 0.007290398236364126, 0.17892330884933472, 0.015813425183296204, 0.22933359444141388, 0.09024292975664139, 0.11147327721118927, 0.45574653148651123, 0.4375168979167938, 0.42469650506973267, 0.15378302335739136, 0.45759108662605286, 0.01676967553794384, 0.4340059757232666, 0.45255619287490845, 0.10211069136857986, 0.047000765800476074, 0.31280842423439026, 0.22450731694698334, 0.4355026185512543, 0.45331811904907227, 0.246412456035614, 0.3597434163093567, 0.13805538415908813, 0.19628946483135223, 0.34260499477386475], dtype='float32').reshape([30]),
            paddle.to_tensor([0.2813074290752411, 0.3700079917907715, 0.05083618313074112, 0.2725585103034973, 0.08114151656627655, 0.11052295565605164, 0.30239999294281006, 0.273695707321167, 0.1616557538509369, 0.27228865027427673, 0.2883189618587494, 0.3277687728404999, 0.15039312839508057, 0.24244484305381775, 0.16163356602191925, 0.2789318859577179, 0.1975000649690628, 0.39755979180336, 0.1378752440214157, 0.0548807755112648, 0.016578271985054016, 0.3380817472934723, 0.27189722657203674, 0.0449623242020607, 0.23108461499214172, 0.06764259934425354, 0.2735666334629059, 0.021678803488612175, 0.14080679416656494, 0.27653446793556213], dtype='float32').reshape([30]),
            paddle.to_tensor([0.44189292192459106, 0.4564138650894165, 0.16024509072303772, 0.08104787021875381, 0.2598187029361725, 0.41702800989151, 0.26039421558380127, 0.3835490942001343, 0.1825709342956543, 0.036437224596738815, 0.3305678367614746, 0.16168609261512756, 0.14899864792823792, 0.11346054822206497, 0.05634114146232605, 0.0771372839808464, 0.1572273224592209, 0.17324452102184296, 0.41578033566474915, 0.37907248735427856, 0.07260210067033768, 0.48974233865737915, 0.008351393975317478, 0.41797107458114624, 0.12408983707427979, 0.22459007799625397, 0.3885575830936432, 0.3950292468070984, 0.08900737017393112, 0.46343398094177246], dtype='float32').reshape([30]),
            paddle.to_tensor([0.43821075558662415, 0.20633825659751892, 0.025531981140375137, 0.1644669622182846, 0.24413518607616425, 0.263694167137146, 0.26040413975715637, 0.23052820563316345, 0.24359573423862457, 0.4052474796772003, 0.1436043083667755, 0.1689310222864151, 0.33039340376853943, 0.34619197249412537, 0.34375810623168945, 0.1990761160850525, 0.3635575473308563, 0.3492278456687927, 0.28564274311065674, 0.24473422765731812, 0.10791683942079544, 0.19409863650798798, 0.2952554225921631, 0.14241567254066467, 0.1352187991142273, 0.4664124548435211, 0.42194342613220215, 0.4359928071498871, 0.4125266671180725, 0.3265916407108307], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3fd8f490e5e74c814df209bb8611cfd2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.16911453008651733, 0.25277015566825867, 0.0426480770111084, 0.408068984746933, 0.4015157222747803, 0.44237005710601807, 0.25501489639282227, 0.1764434576034546, 0.34987178444862366, 0.06271930783987045, 0.1998930722475052, 0.39309850335121155, 0.11022961884737015, 0.28852543234825134, 0.09373443573713303, 0.35783258080482483, 0.0750114843249321, 0.06214475259184837], dtype='float32').reshape([18]),
            paddle.to_tensor([0.05758000537753105, 0.1343955546617508, 0.33985090255737305, 0.380034863948822, 0.07436125725507736, 0.02028593234717846, 0.4531385600566864, 0.26283133029937744, 0.3652889132499695, 0.35122135281562805, 0.4895707368850708, 0.3587232530117035, 0.4776524603366852, 0.006667608395218849, 0.25732943415641785, 0.37808462977409363, 0.10970304906368256, 0.49502214789390564], dtype='float32').reshape([18]),
            paddle.to_tensor([0.395114004611969, 0.3495413064956665, 0.4679558277130127, 0.3686012327671051, 0.4967314600944519, 0.2711819112300873, 0.12028059363365173, 0.1994623988866806, 0.415601909160614, 0.13196149468421936, 0.4559338688850403, 0.40710821747779846, 0.4767051041126251, 0.4828464388847351, 0.04805006459355354, 0.25971928238868713, 0.07725261151790619, 0.05955871567130089], dtype='float32').reshape([18]),
            paddle.to_tensor([0.1726457178592682, 0.09073508530855179, 0.26397377252578735, 0.46959954500198364, 0.3457556962966919, 0.2679222524166107, 0.12949000298976898, 0.20441652834415436, 0.07025863975286484, 0.15987129509449005, 0.13055045902729034, 0.059385985136032104, 0.027708224952220917, 0.4621638357639313, 0.4256209433078766, 0.10541801154613495, 0.47202378511428833, 0.012565551325678825], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_71411c9dd06d096716dc11378bfc6b39(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 28, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.447907030582428, 0.04740143567323685, 0.15137965977191925, 0.37981438636779785, 0.41592979431152344, 0.2351982295513153, 0.21825988590717316, 0.01836596243083477, 0.002433357760310173, 0.0691513940691948, 0.06005879119038582, 0.3505338132381439, 0.27104124426841736, 0.005719742272049189, 0.2029600739479065, 0.3366461396217346, 0.4465627670288086, 0.24472202360630035, 0.10174800455570221, 0.13713189959526062, 0.2565009593963623, 0.350388765335083, 0.45622843503952026, 0.303487092256546, 0.233985036611557, 0.09828692674636841, 0.043860528618097305, 0.18085895478725433], dtype='float32').reshape([28]),
            paddle.to_tensor([0.39819687604904175, 0.1217254176735878, 0.12952332198619843, 0.28018951416015625, 0.42755571007728577, 0.41292986273765564, 0.3156353235244751, 0.03379923477768898, 0.4521636664867401, 0.2619890570640564, 0.18815800547599792, 0.11771611124277115, 0.036964062601327896, 0.037188831716775894, 0.15644940733909607, 0.16575448215007782, 0.32567766308784485, 0.29992571473121643, 0.03516240045428276, 0.2597636282444, 0.003295734291896224, 0.36829087138175964, 0.230657696723938, 0.41013336181640625, 0.007655397057533264, 0.4927080273628235, 0.40306854248046875, 0.23498281836509705], dtype='float32').reshape([28]),
            paddle.to_tensor([0.2289329171180725, 0.4059429168701172, 0.09862722456455231, 0.06639979034662247, 0.12199067324399948, 0.16749198734760284, 0.13666564226150513, 0.13304100930690765, 0.07815045863389969, 0.11633336544036865, 0.2910126745700836, 0.16824181377887726, 0.22487035393714905, 0.1650666892528534, 0.1732647866010666, 0.4105662703514099, 0.3158104419708252, 0.4296441376209259, 0.44311782717704773, 0.29535990953445435, 0.24313494563102722, 0.017925987020134926, 0.35384416580200195, 0.22005562484264374, 0.42500194907188416, 0.23044635355472565, 0.2548272907733917, 0.03273520618677139], dtype='float32').reshape([28]),
            paddle.to_tensor([0.07899174839258194, 0.3177819848060608, 0.007238824851810932, 0.19243484735488892, 0.23420928418636322, 0.26712527871131897, 0.14341966807842255, 0.3045029044151306, 0.26254695653915405, 0.31483495235443115, 0.35954582691192627, 0.4130019247531891, 0.3645201027393341, 0.3837989270687103, 0.3952321410179138, 0.35935208201408386, 0.4921528398990631, 0.021759623661637306, 0.48110756278038025, 0.4058692753314972, 0.17629995942115784, 0.3288619816303253, 0.4253133237361908, 0.4861202836036682, 0.489982932806015, 0.08682868629693985, 0.44868946075439453, 0.41996365785598755], dtype='float32').reshape([28]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a5ab9e67816d12695fe668a698f0232e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a8301b810bccba10e4dfdee0df73334
    def get_inputs(self):
        return [
            paddle.uniform([1, 1152, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_97c4070257cbb4c2a6ef119fbc133e23(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 416, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([416], dtype='float32', min=0, max=0.5),
            paddle.uniform([416], dtype='float32', min=0, max=0.5),
            paddle.uniform([416], dtype='float32', min=0, max=0.5),
            paddle.uniform([416], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_def5816f64ebdf58c28a4d7cf66807fe(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 320, 320], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.21357165277004242, 0.014660271815955639, 0.4643087387084961, 0.2459571212530136, 0.11080914735794067, 0.22653742134571075, 0.41146230697631836, 0.3473755121231079], dtype='float32').reshape([8]),
            paddle.to_tensor([0.014800450764596462, 0.15035299956798553, 0.4529062807559967, 0.44501993060112, 0.10329513251781464, 0.006022248882800341, 0.333737313747406, 0.1469637006521225], dtype='float32').reshape([8]),
            paddle.to_tensor([0.04311221465468407, 0.33689892292022705, 0.11200093477964401, 0.1473720222711563, 0.059613700956106186, 0.4725561738014221, 0.3192014694213867, 0.014463917352259159], dtype='float32').reshape([8]),
            paddle.to_tensor([0.15439404547214508, 0.46696606278419495, 0.08220233023166656, 0.06443139165639877, 0.17376787960529327, 0.012240482494235039, 0.14393573999404907, 0.33160004019737244], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_017ec56b902df5417299af4e9fcf0fe1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 160, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_15e7e69637e823dded621f63f2ae6842(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 480, 480], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.05530841648578644, 0.032796792685985565, 0.48146557807922363, 0.3654020428657532, 0.23326881229877472, 0.3800043761730194, 0.3492427468299866, 0.3621598780155182], dtype='float32').reshape([8]),
            paddle.to_tensor([0.3991594612598419, 0.23615938425064087, 0.07917375862598419, 0.2618623375892639, 0.14694273471832275, 0.44650858640670776, 0.40365180373191833, 0.42205703258514404], dtype='float32').reshape([8]),
            paddle.to_tensor([0.06711722165346146, 0.23267428576946259, 0.4431309998035431, 0.28828734159469604, 0.1870393604040146, 0.4709409773349762, 0.18997955322265625, 0.24513298273086548], dtype='float32').reshape([8]),
            paddle.to_tensor([0.41370829939842224, 0.2844159007072449, 0.19884292781352997, 0.25408312678337097, 0.13452716171741486, 0.07245590537786484, 0.025612352415919304, 0.2749255895614624], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_65887cca0d5578f0c3fe5fb8291914c5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 12, 40], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6d2554ee8ed65d66dea15c3b791c97a1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_0edd578f48a07aa5d1d79f9f678b472e
    def get_inputs(self):
        return [
            paddle.uniform([16, 768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d1a836f6b4f6bdd2f959c62ba0ca3fde(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_da4d85bc213f70819a72e68f21214cdb
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 4, 32, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_01c289ac9d05072efc778c1a98e677cd(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2cb377ab69949213d43a18e2f100e598(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 256, 512], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.17476961016654968, 0.35497209429740906, 0.04005216807126999, 0.4189341962337494, 0.10834922641515732, 0.30476513504981995, 0.27481111884117126, 0.4297516345977783, 0.02393895946443081, 0.43757596611976624, 0.253230482339859, 0.46504661440849304, 0.04728183150291443, 0.2561868131160736, 0.45053017139434814, 0.1297730952501297], dtype='float32').reshape([16]),
            paddle.to_tensor([0.2026069015264511, 0.06863649189472198, 0.2986642122268677, 0.14978335797786713, 0.29900678992271423, 0.1092996746301651, 0.4464435577392578, 0.1777898073196411, 0.3554125130176544, 0.14833880960941315, 0.32334986329078674, 0.3894345760345459, 0.40659964084625244, 0.3712638020515442, 0.09781142324209213, 0.25344201922416687], dtype='float32').reshape([16]),
            paddle.to_tensor([0.24557830393314362, 0.39413055777549744, 0.1386386901140213, 0.06254477053880692, 0.2245970070362091, 0.12135647982358932, 0.09023517370223999, 0.36032378673553467, 0.22882606089115143, 0.22731991112232208, 0.07092566043138504, 0.4335445463657379, 0.4583882689476013, 0.4727879762649536, 0.3599429428577423, 0.22260360419750214], dtype='float32').reshape([16]),
            paddle.to_tensor([0.08731798082590103, 0.2966662347316742, 0.4923921823501587, 0.1350502222776413, 0.35803598165512085, 0.298601359128952, 0.0950208455324173, 0.1338592916727066, 0.4651249051094055, 0.4330146610736847, 0.3488473892211914, 0.19451668858528137, 0.3706069588661194, 0.05292576178908348, 0.3823983669281006, 0.06993187218904495], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4d52aca10b715f909e08597c6a8903db(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 480, 480], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.2192784696817398, 0.36061209440231323, 0.08330144733190536, 0.3258006274700165, 0.48591747879981995, 0.4683603346347809, 0.1350814253091812, 0.3172341585159302], dtype='float32').reshape([8]),
            paddle.to_tensor([0.05486398562788963, 0.4235367774963379, 0.4796181917190552, 0.23901407420635223, 0.2687256634235382, 0.33261632919311523, 0.08837419748306274, 0.28919634222984314], dtype='float32').reshape([8]),
            paddle.to_tensor([0.3645450174808502, 0.2841646671295166, 0.4566173851490021, 0.24795043468475342, 0.2581181228160858, 0.2827288508415222, 0.13161665201187134, 0.39545688033103943], dtype='float32').reshape([8]),
            paddle.to_tensor([0.21157848834991455, 0.17600984871387482, 0.2610033452510834, 0.08894413709640503, 0.26640093326568604, 0.06341060250997543, 0.4782079756259918, 0.042573314160108566], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_34200e6f0ee69419882d7a6ed01bee0f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 40, 40], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c44a22b74ee31fb7f1ce33abbdad5a27(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 336, 40, 40], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_664bde66ccab76f248f34e29f1d288ac(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.03746756166219711, 0.1863957792520523, 0.037590887397527695, 0.24573321640491486, 0.27977022528648376, 0.4848816692829132, 0.4406426250934601, 0.23115968704223633, 0.3282606601715088, 0.14162582159042358, 0.440115749835968, 0.42566895484924316, 0.022362561896443367, 0.0012337455991655588, 0.016235360875725746, 0.06514500826597214, 0.20109891891479492, 0.19946518540382385, 0.04998001456260681, 0.13436567783355713, 0.06994817405939102, 0.13780882954597473, 0.03035346418619156, 0.14337585866451263, 0.3220956027507782, 0.489493727684021, 0.47191351652145386, 0.003969279117882252, 0.3480239510536194, 0.2296450436115265], dtype='float32').reshape([30]),
            paddle.to_tensor([0.09537363052368164, 0.29685842990875244, 0.19484320282936096, 0.07750143855810165, 0.19551803171634674, 0.25445783138275146, 0.37328964471817017, 0.445608913898468, 0.4849274754524231, 0.02756684273481369, 0.01193379145115614, 0.2101326733827591, 0.17842069268226624, 0.11108274757862091, 0.15216326713562012, 0.00213025975972414, 0.029133358970284462, 0.12335410714149475, 0.4569787085056305, 0.0756206065416336, 0.23775245249271393, 0.1730746030807495, 0.052202656865119934, 0.14759817719459534, 0.2262726128101349, 0.3716219663619995, 0.3935610353946686, 0.2881900370121002, 0.1582556813955307, 0.1908692568540573], dtype='float32').reshape([30]),
            paddle.to_tensor([0.4574427008628845, 0.021402962505817413, 0.38638269901275635, 0.055747974663972855, 0.08425785601139069, 0.48134177923202515, 0.36251598596572876, 0.16531530022621155, 0.08275903761386871, 0.3818565011024475, 0.10939216613769531, 0.31361693143844604, 0.35450178384780884, 0.1401594877243042, 0.3516535758972168, 0.09933461248874664, 0.427426815032959, 0.47301262617111206, 0.17910176515579224, 0.38392677903175354, 0.42351654171943665, 0.26368457078933716, 0.4026780128479004, 0.08192626386880875, 0.45284271240234375, 0.014048452489078045, 0.347472220659256, 0.00646959338337183, 0.219376802444458, 0.3698251247406006], dtype='float32').reshape([30]),
            paddle.to_tensor([0.40889933705329895, 0.2524360418319702, 0.10988972336053848, 0.3640629053115845, 0.1762806922197342, 0.24002666771411896, 0.4209989905357361, 0.01752689853310585, 0.26354727149009705, 0.033718548715114594, 0.0062293969094753265, 0.19226735830307007, 0.18617337942123413, 0.11960338801145554, 0.3077905774116516, 0.2524649202823639, 0.05822215601801872, 0.2759156823158264, 0.11907073855400085, 0.27177268266677856, 0.4531469941139221, 0.20514324307441711, 0.4673909842967987, 0.020119190216064453, 0.1185062900185585, 0.43861836194992065, 0.453294575214386, 0.4047172963619232, 0.16617372632026672, 0.3556199073791504], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_696bd4e4e5cb5a173b3b1653904e9662(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 768, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_005ba2cee38eea640c561b5899a13a9a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 480, 480], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.03452927991747856, 0.32643190026283264, 0.27526968717575073, 0.4114666283130646, 0.2864353656768799, 0.1141350269317627, 0.09641847014427185, 0.3073626756668091, 0.08762791007757187, 0.10682404041290283, 0.449752539396286, 0.08208492398262024, 0.07095129787921906, 0.4254215955734253, 0.07998113334178925, 0.2639103829860687, 0.27382972836494446, 0.3952486217021942, 0.19585685431957245, 0.16284102201461792, 0.14390206336975098, 0.3934497833251953, 0.087894968688488, 0.057142142206430435], dtype='float32').reshape([24]),
            paddle.to_tensor([0.4808780550956726, 0.17342418432235718, 0.45862460136413574, 0.20280645787715912, 0.10891222208738327, 0.3716462552547455, 0.19112956523895264, 0.17292708158493042, 0.4068905711174011, 0.32359054684638977, 0.2600994110107422, 0.47249147295951843, 0.3529423475265503, 0.4270927906036377, 0.28562119603157043, 0.331590861082077, 0.21404801309108734, 0.3705367147922516, 0.4056006669998169, 0.2932572662830353, 0.1836729347705841, 0.1409774273633957, 0.33579516410827637, 0.0409688875079155], dtype='float32').reshape([24]),
            paddle.to_tensor([0.33164212107658386, 0.21395857632160187, 0.13548852503299713, 0.016971426084637642, 0.4954759478569031, 0.47467461228370667, 0.44266974925994873, 0.31055980920791626, 0.159849613904953, 0.08033183962106705, 0.43870648741722107, 0.499070405960083, 0.3073539435863495, 0.050189308822155, 0.4047929644584656, 0.2090522050857544, 0.09602737426757812, 0.2370087057352066, 0.4195132851600647, 0.2982034683227539, 0.4466729164123535, 0.2075517773628235, 0.4772450625896454, 0.3416072726249695], dtype='float32').reshape([24]),
            paddle.to_tensor([0.2895991802215576, 0.49014565348625183, 0.3966194689273834, 0.1735427975654602, 0.44172096252441406, 0.2520543336868286, 0.3203389048576355, 0.4787787199020386, 0.44647470116615295, 0.11991000920534134, 0.23796673119068146, 0.49204200506210327, 0.09120314568281174, 0.26932379603385925, 0.2906685173511505, 0.08966178447008133, 0.08198069781064987, 0.1160503402352333, 0.25305068492889404, 0.36149802803993225, 0.20402856171131134, 0.3826028108596802, 0.3170991539955139, 0.03938169777393341], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d3967ea7119843557b7e643485f28fff(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 16, 50], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.3803068697452545, 0.27109572291374207, 0.19215048849582672, 0.08591406792402267, 0.16163773834705353, 0.14901401102542877, 0.07998296618461609, 0.0010962530504912138], dtype='float32').reshape([8]),
            paddle.to_tensor([0.06537934392690659, 0.2601459324359894, 0.2358785718679428, 0.19503116607666016, 0.34638431668281555, 0.18062861263751984, 0.37817591428756714, 0.14079414308071136], dtype='float32').reshape([8]),
            paddle.to_tensor([0.08697938919067383, 0.3792218267917633, 0.13650524616241455, 0.016098421066999435, 0.4822410047054291, 0.42594048380851746, 0.40302446484565735, 0.022727318108081818], dtype='float32').reshape([8]),
            paddle.to_tensor([0.1654280126094818, 0.3223731517791748, 0.1800953447818756, 0.3568929135799408, 0.26496192812919617, 0.2957013249397278, 0.26901161670684814, 0.2441093772649765], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fb1a0769844180b462edf5fdd28213ad(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 149, 149], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e47bd296a1aa7690394e4a17e574a3b8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 38, 38], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_222f5e94d39eeaa66f0675930e7934f7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.013403921388089657, 0.12049631774425507, 0.386843204498291, 0.02944650687277317, 0.13687095046043396, 0.47467222809791565, 0.11233609169721603, 0.15071502327919006, 0.43643829226493835, 0.48966947197914124, 0.40051618218421936, 0.10850051045417786, 0.22243280708789825, 0.12112440168857574, 0.4376365542411804, 0.1377282291650772], dtype='float32').reshape([16]),
            paddle.to_tensor([0.0857725515961647, 0.26284560561180115, 0.24186265468597412, 0.3238699734210968, 0.15195100009441376, 0.17066584527492523, 0.19173942506313324, 0.14235243201255798, 0.39895105361938477, 0.44328686594963074, 0.3903599977493286, 0.22210310399532318, 0.435055136680603, 0.400387704372406, 0.1842203587293625, 0.20452377200126648], dtype='float32').reshape([16]),
            paddle.to_tensor([0.35719215869903564, 0.062338992953300476, 0.3638106882572174, 0.22931785881519318, 0.34068763256073, 0.26282626390457153, 0.36231258511543274, 0.3056061565876007, 0.051435958594083786, 0.4123377501964569, 0.30633410811424255, 0.006102360785007477, 0.08290620148181915, 0.02982626110315323, 0.1567906141281128, 0.4961795210838318], dtype='float32').reshape([16]),
            paddle.to_tensor([0.0920286625623703, 0.10616816580295563, 0.38624894618988037, 0.44511568546295166, 0.49080026149749756, 0.48842653632164, 0.1967698037624359, 0.2901870310306549, 0.42820605635643005, 0.06793008744716644, 0.480680376291275, 0.19923458993434906, 0.36904317140579224, 0.22393576800823212, 0.10802061855792999, 0.01525964867323637], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1fa25d8d2ba881f7c57fa471e85221a3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 32, 24], dtype='float16', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5c2bd0432229df0d2dadf03249f03669(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 120, 80, 80], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d4fb6690ccd311ba9976779a6c0b8f53(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 256, 512], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.26347801089286804, 0.08706808090209961, 0.43520843982696533, 0.46192964911460876, 0.07767940312623978, 0.4828043580055237, 0.25759994983673096, 0.2694047689437866, 0.22236564755439758, 0.32774922251701355, 0.25277677178382874, 0.2696114182472229, 0.024601325392723083, 0.08537555485963821, 0.09847895056009293, 0.22904451191425323], dtype='float32').reshape([16]),
            paddle.to_tensor([0.0002158666611649096, 0.3811846971511841, 0.34157323837280273, 0.1807752549648285, 0.37551814317703247, 0.25548380613327026, 0.25527188181877136, 0.24594232439994812, 0.3695155382156372, 0.3256456255912781, 0.1964290887117386, 0.14963990449905396, 0.04350009188055992, 0.023933785036206245, 0.10962939262390137, 0.010303624905645847], dtype='float32').reshape([16]),
            paddle.to_tensor([0.05866178497672081, 0.3531167209148407, 0.43099454045295715, 0.2444014996290207, 0.33628049492836, 0.11635053157806396, 0.45900484919548035, 0.4180324673652649, 0.00428282655775547, 0.40605583786964417, 0.4049803614616394, 0.17140749096870422, 0.3209185302257538, 0.47300225496292114, 0.14661283791065216, 0.03205859661102295], dtype='float32').reshape([16]),
            paddle.to_tensor([0.17226117849349976, 0.024035025388002396, 0.25856316089630127, 0.189191073179245, 0.1622437983751297, 0.03656475618481636, 0.2127065807580948, 0.3952476978302002, 0.265546053647995, 0.49998122453689575, 0.2329183667898178, 0.2823791205883026, 0.08515878021717072, 0.3775516748428345, 0.39391106367111206, 0.2741260230541229], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0ff408318a8f614c4a83c5b1470205da(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 200, 336], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_00bfc72f576336e551f9e8ed822c5e4f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 28, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.4167650043964386, 0.383291631937027, 0.39695417881011963, 0.39326682686805725, 0.07538419961929321, 0.3346165418624878, 0.17681984603405, 0.4175180196762085, 0.3947189152240753, 0.23432119190692902, 0.05586644634604454, 0.46222013235092163, 0.021059947088360786, 0.4249926209449768, 0.39065441489219666, 0.23876747488975525], dtype='float32').reshape([16]),
            paddle.to_tensor([0.27006006240844727, 0.3754368722438812, 0.21268771588802338, 0.10386519879102707, 0.4727344512939453, 0.3236342668533325, 0.10162381082773209, 0.42663148045539856, 0.22459760308265686, 0.431138277053833, 0.14476585388183594, 0.17136286199092865, 0.22766047716140747, 0.26136094331741333, 0.22218868136405945, 0.14972229301929474], dtype='float32').reshape([16]),
            paddle.to_tensor([0.027646906673908234, 0.053729210048913956, 0.1813054233789444, 0.1584167629480362, 0.05848286673426628, 0.0643557533621788, 0.4362315535545349, 0.23209702968597412, 0.3695153594017029, 0.06417765468358994, 0.07213177531957626, 0.012185420840978622, 0.17965483665466309, 0.2507178485393524, 0.2795872986316681, 0.02337278053164482], dtype='float32').reshape([16]),
            paddle.to_tensor([0.3679082691669464, 0.4137710630893707, 0.3445611298084259, 0.31417539715766907, 0.3334025740623474, 0.386936217546463, 0.11670371145009995, 0.304794043302536, 0.14736083149909973, 0.09370634704828262, 0.12146282196044922, 0.19285400211811066, 0.3807727098464966, 0.3741711378097534, 0.3938881754875183, 0.46784329414367676], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_efa044b8c49bc663d99ae2b96ab11e8d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_d589af777f5a8ae699611515c41499fb
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 80, 80], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c36a42869d60f34992b5389045efa76d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 2048, 64, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_26c63e8cdc316719fc0b123f1f056a03(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 80, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fa9f4e10bcd9d91d4068bc7189b67476(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 58, 1, 1], dtype='float16', min=0, max=0.5),
            paddle.uniform([58], dtype='float32', min=0, max=0.5),
            paddle.uniform([58], dtype='float32', min=0, max=0.5),
            paddle.uniform([58], dtype='float32', min=0, max=0.5),
            paddle.uniform([58], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_00cd46a92889af15bf78508a30ec915a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 8, 25], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d937ecd7c74bfacbba5e6a85e5c37917(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 240, 60, 60], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_47a54859993a2c95a5784d8025b22ff5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a8301b810bccba10e4dfdee0df73334
    def get_inputs(self):
        return [
            paddle.uniform([1, 144, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_eb777c0c87b0930ad8ac08a1fcc8297f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_89792713fe37a120ae7a473278cd0404(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 232, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([232], dtype='float32', min=0, max=0.5),
            paddle.uniform([232], dtype='float32', min=0, max=0.5),
            paddle.uniform([232], dtype='float32', min=0, max=0.5),
            paddle.uniform([232], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_cdc3b0a148b551f86897e4668e0bdb6b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, True), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_00fe46f5d0ab5a58990863f62abf9894(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cdc3b0a148b551f86897e4668e0bdb6b
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 64, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8515fbb702653dd24718abeacc5dd679(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 256, 256], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d8d6f16cdbf1f1096df1810058621e0f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7c06599f6493bcbbe7e79a56051bc716(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 32, 128], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3eda85712a385076092322428da7e7a6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 4, 256, 512], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.2692737579345703, 0.032683294266462326, 0.4067620635032654, 0.07188519090414047], dtype='float32').reshape([4]),
            paddle.to_tensor([0.09066152572631836, 0.1544688493013382, 0.12872739136219025, 0.12046650052070618], dtype='float32').reshape([4]),
            paddle.to_tensor([0.49009621143341064, 0.35971224308013916, 0.49833256006240845, 0.0427071750164032], dtype='float32').reshape([4]),
            paddle.to_tensor([0.2278638631105423, 0.08133730292320251, 0.04434063285589218, 0.18308255076408386], dtype='float32').reshape([4]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_04e184098d1665a7bb3a3d3b51d7d29a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 8, 8], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3a8f5211399a84f10a158bea8c6903b2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 576, 32, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([576], dtype='float32', min=0, max=0.5),
            paddle.uniform([576], dtype='float32', min=0, max=0.5),
            paddle.uniform([576], dtype='float32', min=0, max=0.5),
            paddle.uniform([576], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a2b2113ffee33c2baa59700466ca2b47(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 480, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e6b553aee25a14095fcaea1a114717a9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 60, 60], dtype='float16', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f04931d0e94d92ec95a8440ae91fe6f0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_0edd578f48a07aa5d1d79f9f678b472e
    def get_inputs(self):
        return [
            paddle.uniform([196, 128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_585f319f0a336dc373218775aa1e9029(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 72, 14, 25], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f79f31e9d410fcf6c51a2a736092a5db(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 480, 480], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.354194700717926, 0.043505504727363586, 0.46159979701042175, 0.3998764753341675, 0.15110169351100922, 0.30366799235343933, 0.31200331449508667, 0.04116097092628479], dtype='float32').reshape([8]),
            paddle.to_tensor([0.16995206475257874, 0.2990005910396576, 0.018542269244790077, 0.31090959906578064, 0.39062368869781494, 0.4022885262966156, 0.48989155888557434, 0.06041065976023674], dtype='float32').reshape([8]),
            paddle.to_tensor([0.07873714715242386, 0.1641119420528412, 0.0884157344698906, 0.0628172755241394, 0.12463943660259247, 0.26476725935935974, 0.24215123057365417, 0.04983748495578766], dtype='float32').reshape([8]),
            paddle.to_tensor([0.3417048156261444, 0.14500999450683594, 0.10787112265825272, 0.1750970333814621, 0.20717023313045502, 0.44037818908691406, 0.4906281530857086, 0.4321024715900421], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_75ac840bb5a2c5077b68bddcab1f1008(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 576, 32, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([576], dtype='float32', min=0, max=0.5),
            paddle.uniform([576], dtype='float32', min=0, max=0.5),
            paddle.uniform([576], dtype='float32', min=0, max=0.5),
            paddle.uniform([576], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f23763a3cace8ba81c8e2627ed031282(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_af91295b59c5fca17f890a802deb001f
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 4, 16, 16], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9519d736d1cb06178a76d775c683023d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 40, 40], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_721c9263245d990ef2bd0316cf6fd6a8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.10556434094905853, 0.43457069993019104, 0.19455605745315552, 0.2355814129114151, 0.4051324725151062, 0.29828545451164246, 0.4465015232563019, 0.04694854095578194, 0.3862263560295105, 0.36396703124046326, 0.04415758699178696, 0.25369468331336975, 0.42958930134773254, 0.40210720896720886, 0.3744564354419708, 0.44059523940086365, 0.40983864665031433, 0.2622098922729492, 0.34602174162864685, 0.33100080490112305, 0.09914540499448776, 0.4486658275127411, 0.2834817171096802, 0.14356295764446259, 0.1687726229429245, 0.12146291136741638, 0.26445549726486206, 0.4488707482814789, 0.47115376591682434, 0.1521652191877365], dtype='float32').reshape([30]),
            paddle.to_tensor([0.20488685369491577, 0.34872350096702576, 0.33541038632392883, 0.008884002454578876, 0.11537252366542816, 0.4161762595176697, 0.08624592423439026, 0.47641128301620483, 0.272045373916626, 0.40645086765289307, 0.1137494295835495, 0.2679968774318695, 0.1331098973751068, 0.43968501687049866, 0.042916662991046906, 0.0619969516992569, 0.2983931601047516, 0.37342914938926697, 0.20715419948101044, 0.47996553778648376, 0.006982084829360247, 0.07565980404615402, 0.248883917927742, 0.13365332782268524, 0.3289203345775604, 0.42975565791130066, 0.06075085327029228, 0.496250182390213, 0.050612498074769974, 0.34157609939575195], dtype='float32').reshape([30]),
            paddle.to_tensor([0.28674936294555664, 0.36438146233558655, 0.4161311089992523, 0.19745352864265442, 0.38358545303344727, 0.3998926877975464, 0.43113234639167786, 0.13925667107105255, 0.39673537015914917, 0.20900529623031616, 0.11641933023929596, 0.005760689731687307, 0.08770851790904999, 0.09484300762414932, 0.4683350622653961, 0.07334496080875397, 0.32596027851104736, 0.13349759578704834, 0.48455116152763367, 0.44225209951400757, 0.15498682856559753, 0.23633837699890137, 0.24866151809692383, 0.322671502828598, 0.36252421140670776, 0.3636664152145386, 0.26362788677215576, 0.33295321464538574, 0.23325926065444946, 0.27149781584739685], dtype='float32').reshape([30]),
            paddle.to_tensor([0.46353453397750854, 0.13688385486602783, 0.20010511577129364, 0.49668169021606445, 0.21709388494491577, 0.3988426625728607, 0.17940407991409302, 0.3404451310634613, 0.1792992651462555, 0.16961276531219482, 0.3457895517349243, 0.15811896324157715, 0.3716956377029419, 0.25248217582702637, 0.3239997327327728, 0.29059818387031555, 0.35421738028526306, 0.18318532407283783, 0.2651739716529846, 0.14486616849899292, 0.10260044038295746, 0.17016562819480896, 0.21304866671562195, 0.1884441375732422, 0.0628591775894165, 0.36855176091194153, 0.20699310302734375, 0.07992029935121536, 0.474145770072937, 0.4864099621772766], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_50d223a0100c42923eb0df076c8c3721(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 100, 168], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_df8e1f9ea8d21ab9f2cccdbe698593e8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 64, 48], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f1ae2705e8f3342365f27e3731a5fe4e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.010130367241799831, 0.25562936067581177, 0.28899091482162476, 0.2889437973499298, 0.41366657614707947, 0.09610605239868164, 0.1693526953458786, 0.09832525253295898, 0.035198990255594254, 0.20428331196308136, 0.0669892430305481, 0.43319830298423767, 0.36907461285591125, 0.1523737758398056, 0.1369066834449768, 0.3113517165184021, 0.3531736433506012, 0.19447623193264008], dtype='float32').reshape([18]),
            paddle.to_tensor([0.43540844321250916, 0.08926573395729065, 0.3993627727031708, 0.2551223039627075, 0.4080241620540619, 0.44538602232933044, 0.09888896346092224, 0.28765979409217834, 0.4618452489376068, 0.23605303466320038, 0.3473946154117584, 0.017405837774276733, 0.15585090219974518, 0.16299839317798615, 0.31154865026474, 0.2364783138036728, 0.12832388281822205, 0.3765844702720642], dtype='float32').reshape([18]),
            paddle.to_tensor([0.10140383243560791, 0.10112952440977097, 0.02692090906202793, 0.3088915944099426, 0.025738747790455818, 0.10160715878009796, 0.3927866816520691, 0.2063847780227661, 0.30003049969673157, 0.1211608424782753, 0.06651411950588226, 0.49074825644493103, 0.1724635362625122, 0.13876023888587952, 0.3312205970287323, 0.3629763722419739, 0.018834060057997704, 0.2023012936115265], dtype='float32').reshape([18]),
            paddle.to_tensor([0.055001452565193176, 0.405924528837204, 0.3748704493045807, 0.4942888021469116, 0.3272738456726074, 0.07668408751487732, 0.039712805300951004, 0.3479772210121155, 0.2510892450809479, 0.3527676463127136, 0.3542316257953644, 0.24772964417934418, 0.21571089327335358, 0.4582810699939728, 0.36059069633483887, 0.12482929974794388, 0.014622305519878864, 0.4897480010986328], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9507d2c9bd78a8c106f98786da18b846(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 32, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6fc7d95137750a36e1070801badefc10(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.4679213762283325, 0.3983072340488434, 0.21371816098690033, 0.08458060771226883, 0.3811655640602112, 0.2489589899778366, 0.34314852952957153, 0.18150755763053894, 0.376705527305603, 0.34775424003601074, 0.3714965879917145, 0.13616524636745453, 0.015313402749598026, 0.3660903871059418, 0.2532818913459778, 0.3146202862262726, 0.00480947969481349, 0.25117453932762146], dtype='float32').reshape([18]),
            paddle.to_tensor([0.03121953271329403, 0.4862905442714691, 0.17008525133132935, 0.3148394227027893, 0.178673654794693, 0.27235445380210876, 0.15405283868312836, 0.13448350131511688, 0.2589775621891022, 0.26312530040740967, 0.48613354563713074, 0.13799484074115753, 0.2904928922653198, 0.06716155260801315, 0.4990520775318146, 0.42691996693611145, 0.4837758541107178, 0.15934139490127563], dtype='float32').reshape([18]),
            paddle.to_tensor([0.00841057300567627, 0.09183941036462784, 0.011402353644371033, 0.21945005655288696, 0.27417969703674316, 0.36667338013648987, 0.46985724568367004, 0.281950443983078, 0.3611147999763489, 0.13283494114875793, 0.39039117097854614, 0.37081244587898254, 0.019071727991104126, 0.3608924448490143, 0.35761332511901855, 0.21906276047229767, 0.410103976726532, 0.44144517183303833], dtype='float32').reshape([18]),
            paddle.to_tensor([0.4707166254520416, 0.37248778343200684, 0.4704672396183014, 0.06460852921009064, 0.45600250363349915, 0.10592858493328094, 0.1697239726781845, 0.20791904628276825, 0.10872772336006165, 0.44191667437553406, 0.2287890464067459, 0.01430099830031395, 0.12997402250766754, 0.4236127436161041, 0.11748616397380829, 0.16649524867534637, 0.21753965318202972, 0.4638468325138092], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_42523e384eb13a180c8cb3c4a2fe5651(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 224, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([224], dtype='float32', min=0, max=0.5),
            paddle.uniform([224], dtype='float32', min=0, max=0.5),
            paddle.uniform([224], dtype='float32', min=0, max=0.5),
            paddle.uniform([224], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_06ab2cf341fd42ba288dde8ecda82e1a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.19220785796642303, 0.32495033740997314, 0.3915257453918457, 0.3599690794944763, 0.4301842153072357, 0.38294023275375366, 0.33068767189979553, 0.304830938577652, 0.25536125898361206, 0.44620028138160706, 0.31586650013923645, 0.029425987973809242, 0.47576257586479187, 0.21036361157894135, 0.13270911574363708, 0.3477281332015991, 0.09181108325719833, 0.06504064053297043, 0.3880213499069214, 0.03339119255542755], dtype='float32').reshape([20]),
            paddle.to_tensor([0.3781934976577759, 0.10631106048822403, 0.23366454243659973, 0.013181200250983238, 0.03833823651075363, 0.24136464297771454, 0.40626540780067444, 0.4254743456840515, 0.13618159294128418, 0.22390542924404144, 0.04910393804311752, 0.04285033419728279, 0.23895077407360077, 0.32939696311950684, 0.08903543651103973, 0.022015254944562912, 0.04273787885904312, 0.12493617832660675, 0.3534148931503296, 0.1364421397447586], dtype='float32').reshape([20]),
            paddle.to_tensor([0.3943265676498413, 0.33341699838638306, 0.3616289496421814, 0.10330761224031448, 0.04315199702978134, 0.04671262577176094, 0.49625715613365173, 0.29576486349105835, 0.2279100865125656, 0.09154587984085083, 0.20141436159610748, 0.12452729791402817, 0.36705121397972107, 0.04092053323984146, 0.36803528666496277, 0.4267020523548126, 0.27039337158203125, 0.22215543687343597, 0.10967754572629929, 0.4315495789051056], dtype='float32').reshape([20]),
            paddle.to_tensor([0.325244665145874, 0.3466102182865143, 0.2503001093864441, 0.34718194603919983, 0.2968346178531647, 0.25625815987586975, 0.4127688705921173, 0.31467053294181824, 0.14464369416236877, 0.2946220934391022, 0.3598153293132782, 0.20268622040748596, 0.04423777386546135, 0.35070955753326416, 0.43338918685913086, 0.1244012713432312, 0.43840205669403076, 0.019413623958826065, 0.17041847109794617, 0.2252107411623001], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_feb128c66d9cf3b7254f92f4573a584f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.23685216903686523, 0.2580718994140625, 0.10748308151960373, 0.4456171989440918, 0.08886612206697464, 0.48846524953842163, 0.41996800899505615, 0.15261688828468323, 0.34089499711990356, 0.08014644682407379, 0.404520183801651, 0.4608275890350342, 0.4134775698184967, 0.14103910326957703, 0.16640906035900116, 0.4277142584323883, 0.29425927996635437, 0.48987090587615967, 0.33120593428611755, 0.1846490353345871, 0.4638846218585968, 0.30308815836906433, 0.14832517504692078, 0.43848493695259094, 0.49354681372642517, 0.3951510787010193, 0.28197500109672546, 0.090325728058815, 0.3236946165561676, 0.41629141569137573], dtype='float32').reshape([30]),
            paddle.to_tensor([0.21821296215057373, 0.15400400757789612, 0.38203415274620056, 0.438302606344223, 0.13581843674182892, 0.4317638576030731, 0.18486852943897247, 0.3187115490436554, 0.17719241976737976, 0.20091046392917633, 0.48938411474227905, 0.4876904785633087, 0.3160233795642853, 0.11087828874588013, 0.12319063395261765, 0.06285957992076874, 0.18729524314403534, 0.05997827649116516, 0.4320869743824005, 0.4541662931442261, 0.29809585213661194, 0.4404076933860779, 0.438174843788147, 0.053681936115026474, 0.4264947175979614, 0.42189544439315796, 0.37001559138298035, 0.3790624737739563, 0.15756124258041382, 0.32130661606788635], dtype='float32').reshape([30]),
            paddle.to_tensor([0.48518890142440796, 0.0037843366153538227, 0.32875099778175354, 0.1092897281050682, 0.0035619025584310293, 0.018911942839622498, 0.13585911691188812, 0.10054381936788559, 0.11698060482740402, 0.38253238797187805, 0.3870813846588135, 0.039682723581790924, 0.1624661386013031, 0.3648931682109833, 0.10469643771648407, 0.49548035860061646, 0.4274672865867615, 0.2342650443315506, 0.1508987993001938, 0.4146081507205963, 0.09442712366580963, 0.22673089802265167, 0.24000315368175507, 0.38320013880729675, 0.2222348302602768, 0.15286797285079956, 0.2532275319099426, 0.12173950672149658, 0.024328840896487236, 0.11671887338161469], dtype='float32').reshape([30]),
            paddle.to_tensor([0.12662750482559204, 0.07401005923748016, 0.31270352005958557, 0.3733927607536316, 0.23572228848934174, 0.4196605384349823, 0.11182577908039093, 0.00678076408803463, 0.073341004550457, 0.041376449167728424, 0.10145903378725052, 0.1931174248456955, 0.4130977690219879, 0.3930968642234802, 0.46719086170196533, 0.3412784934043884, 0.03523603454232216, 0.44834139943122864, 0.47284919023513794, 0.2334081083536148, 0.2547220289707184, 0.18774454295635223, 0.024557027965784073, 0.2882760167121887, 0.033122468739748, 0.2946964502334595, 0.49410754442214966, 0.33868125081062317, 0.26749253273010254, 0.3992109000682831], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9df6fdcec9c76bec2ba37e4c38b4ba11(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 6, 6], dtype='float16', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d6c764b4c147ba990edebde59f3b55f4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d9498b0a34eedc60742cf270eea15ad2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.42127725481987, 0.46768996119499207, 0.38405656814575195, 0.2923717796802521, 0.3366645574569702, 0.13763101398944855, 0.11472037434577942, 0.43852412700653076, 0.41175758838653564, 0.2297428399324417, 0.34903883934020996, 0.4442347586154938, 0.39101216197013855, 0.07772137224674225, 0.2278663069009781, 0.15609467029571533, 0.3924008905887604, 0.18442460894584656], dtype='float32').reshape([18]),
            paddle.to_tensor([0.04806862771511078, 0.1609405279159546, 0.46606752276420593, 0.0035610722843557596, 0.396455854177475, 0.3491400182247162, 0.25564107298851013, 0.17161741852760315, 0.4958084225654602, 0.1396375149488449, 0.18289649486541748, 0.1311730593442917, 0.3421874940395355, 0.34807631373405457, 0.34422069787979126, 0.44495105743408203, 0.3268984258174896, 0.09798724204301834], dtype='float32').reshape([18]),
            paddle.to_tensor([0.13137727975845337, 0.14712859690189362, 0.43179208040237427, 0.23722338676452637, 0.25079259276390076, 0.420023649930954, 0.3593270480632782, 0.22533804178237915, 0.18586048483848572, 0.0490015372633934, 0.2758116126060486, 0.35343825817108154, 0.09894023090600967, 0.024774998426437378, 0.20539194345474243, 0.20802026987075806, 0.30328139662742615, 0.30044472217559814], dtype='float32').reshape([18]),
            paddle.to_tensor([0.2556198537349701, 0.3034186363220215, 0.17474265396595, 0.3707045316696167, 0.3160352110862732, 0.4952901303768158, 0.476264089345932, 0.1831832230091095, 0.4004672169685364, 0.33336111903190613, 0.06610298901796341, 0.23366884887218475, 0.45405441522598267, 0.05329703539609909, 0.28060054779052734, 0.3011595606803894, 0.3888190984725952, 0.44786229729652405], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b6699fc58002d2772e2745a9c3c08ffd(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 960, 960], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_def578a427cb3c6a5d6ba0b59092d5c9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 36, 32, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_22f9b9bea7c1506874f7de7c6dc4a213(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cbf35f312d2842be6c8a96c161582406(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 185, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([185], dtype='float32', min=0, max=0.5),
            paddle.uniform([185], dtype='float32', min=0, max=0.5),
            paddle.uniform([185], dtype='float32', min=0, max=0.5),
            paddle.uniform([185], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fb440921a2081dfef2c49c166e5ec6c8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 672, 12, 12], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_433c0397ab823465c5848d084a4a7858(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([8, 512, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bf41b311c937ca7f26e1042bd814576d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 2, 27], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_630a0e162ff03fca2afad19f97d6aa2b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 56, 40, 40], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b595bf894a8205c40f28878331e17792(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 60, 60], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6604ca43e007a966a0b18784d6699c6e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 10, 10], dtype='float16', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_13f40dc2c75f555d40eaccee03db5ecb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 1200, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([1200], dtype='float32', min=0, max=0.5),
            paddle.uniform([1200], dtype='float32', min=0, max=0.5),
            paddle.uniform([1200], dtype='float32', min=0, max=0.5),
            paddle.uniform([1200], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_05d7e494486f86c6f139c42881a2920f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 256, 256], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e9b487dbcf6a73881fa38188ff36b2b9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 48, 48], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.36349180340766907, 0.42308729887008667, 0.3085876703262329, 0.21123743057250977, 0.4824902415275574, 0.45557788014411926, 0.4275228977203369, 0.0525687001645565, 0.11315887421369553, 0.28247445821762085, 0.30356359481811523, 0.44311144948005676, 0.1422998458147049, 0.049057260155677795, 0.45778703689575195, 0.1540319174528122, 0.4864272475242615, 0.22463084757328033, 0.002698985394090414, 0.26250484585762024, 0.020541880279779434, 0.40558451414108276, 0.3210488557815552, 0.21983538568019867], dtype='float32').reshape([24]),
            paddle.to_tensor([0.3479381203651428, 0.32846590876579285, 0.42246606945991516, 0.20756755769252777, 0.1283976286649704, 0.3153877854347229, 0.013607999309897423, 0.07632100582122803, 0.20931077003479004, 0.2729138731956482, 0.3174930810928345, 0.43903225660324097, 0.4884064197540283, 0.29420095682144165, 0.14970827102661133, 0.08723221719264984, 0.0034481454640626907, 0.24867410957813263, 0.0993812158703804, 0.38095465302467346, 0.4815393090248108, 0.14865921437740326, 0.3321038484573364, 0.10128388553857803], dtype='float32').reshape([24]),
            paddle.to_tensor([0.1963355392217636, 0.3015138506889343, 0.2531355619430542, 0.4063326120376587, 0.3888736665248871, 0.040346547961235046, 0.1219472587108612, 0.4030798673629761, 0.4807160496711731, 0.23786431550979614, 0.060545891523361206, 0.1256965845823288, 0.1655772626399994, 0.439512699842453, 0.002669410314410925, 0.11705787479877472, 0.21492744982242584, 0.25972333550453186, 0.04573717340826988, 0.18415692448616028, 0.42952296137809753, 0.3715111017227173, 0.14399616420269012, 0.060037706047296524], dtype='float32').reshape([24]),
            paddle.to_tensor([0.36508357524871826, 0.006426273379474878, 0.36236536502838135, 0.4377431571483612, 0.4451858699321747, 0.29852527379989624, 0.21151308715343475, 0.19115987420082092, 0.4590838551521301, 0.247017502784729, 0.3113521635532379, 0.16274939477443695, 0.29808878898620605, 0.4429168701171875, 0.17406025528907776, 0.48795628547668457, 0.24032332003116608, 0.48082995414733887, 0.26500216126441956, 0.35485029220581055, 0.10382917523384094, 0.2424548864364624, 0.18256713449954987, 0.23055602610111237], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_79fd552d1a7561c7bde5bb8ae4d3b774(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 128, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_83a513c035ef0f9d35a3e2b69487e779(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 25, 42], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_021e4fc5070c3c3fe7fc4ef7f5dd47f9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.036792073398828506, 0.09563609957695007, 0.22692452371120453, 0.4584481418132782, 0.11776848137378693, 0.23246997594833374, 0.4586905241012573, 0.3567383885383606, 0.3424362540245056, 0.004062559921294451, 0.28504008054733276, 0.10172346979379654, 0.39051353931427, 0.06883794069290161, 0.4168645143508911, 0.3201988637447357, 0.3031618893146515, 0.07972730696201324, 0.05540429800748825, 0.21787509322166443], dtype='float32').reshape([20]),
            paddle.to_tensor([0.15609239041805267, 0.42894721031188965, 0.03274732455611229, 0.2228832244873047, 0.0985153466463089, 0.2459079921245575, 0.4235060214996338, 0.4392266273498535, 0.2132139950990677, 0.3949642479419708, 0.025229623541235924, 0.11119586229324341, 0.37254175543785095, 0.34213653206825256, 0.028303317725658417, 0.3196072280406952, 0.06963400542736053, 0.4119208753108978, 0.29288530349731445, 0.3176426291465759], dtype='float32').reshape([20]),
            paddle.to_tensor([0.11468270421028137, 0.05778799578547478, 0.10602398961782455, 0.4232615828514099, 0.44890448451042175, 0.35744771361351013, 0.17788738012313843, 0.10822974890470505, 0.25059768557548523, 0.3124375343322754, 0.49875888228416443, 0.286378413438797, 0.36299556493759155, 0.3093564808368683, 0.19757315516471863, 0.3931346833705902, 0.02095966972410679, 0.029870476573705673, 0.3881177008152008, 0.25684988498687744], dtype='float32').reshape([20]),
            paddle.to_tensor([0.43598321080207825, 0.15444853901863098, 0.036957599222660065, 0.14092031121253967, 0.25482818484306335, 0.4516276717185974, 0.28304290771484375, 0.0017506659496575594, 0.39844846725463867, 0.46826380491256714, 0.37313956022262573, 0.1142197996377945, 0.44266659021377563, 0.42977264523506165, 0.29097598791122437, 0.365619033575058, 0.3478888273239136, 0.4686542749404907, 0.39441704750061035, 0.38228386640548706], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1fa8320b5e3c67f9c47e5a1586a1dc75(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a2de474ff22cd1ba05d164d0b9d0a341(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 1824, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([1824], dtype='float32', min=0, max=0.5),
            paddle.uniform([1824], dtype='float32', min=0, max=0.5),
            paddle.uniform([1824], dtype='float32', min=0, max=0.5),
            paddle.uniform([1824], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_aa70fe7087f868e8c10fe2726085f3aa(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 864, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([864], dtype='float32', min=0, max=0.5),
            paddle.uniform([864], dtype='float32', min=0, max=0.5),
            paddle.uniform([864], dtype='float32', min=0, max=0.5),
            paddle.uniform([864], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bd78e080520db67791c9f82f48afc04c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6762576774a03824ffebb057fe365968(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 240, 240], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.0960831269621849, 0.4486542344093323, 0.2481568455696106, 0.41822874546051025, 0.11089622229337692, 0.42464369535446167, 0.3249639868736267, 0.32681095600128174, 0.21588389575481415, 0.43372946977615356, 0.12189717590808868, 0.46504858136177063, 0.4695775806903839, 0.1716781109571457, 0.1950746476650238, 0.19109715521335602, 0.16747599840164185, 0.30204498767852783, 0.24585337936878204, 0.3754575252532959, 0.3895961046218872, 0.4714418947696686, 0.20045003294944763, 0.4303033649921417], dtype='float32').reshape([24]),
            paddle.to_tensor([0.32367971539497375, 0.4832593500614166, 0.06718825548887253, 0.41186249256134033, 0.46174994111061096, 0.39733898639678955, 0.2597660720348358, 0.16553503274917603, 0.24868687987327576, 0.13213232159614563, 0.28266704082489014, 0.42293936014175415, 0.120882548391819, 0.05665960907936096, 0.06506378948688507, 0.2688784599304199, 0.08918807655572891, 0.48856744170188904, 0.3097764551639557, 0.2228422462940216, 0.3741193413734436, 0.16973894834518433, 0.1817609965801239, 0.4506334066390991], dtype='float32').reshape([24]),
            paddle.to_tensor([0.26088425517082214, 0.20236091315746307, 0.39992469549179077, 0.4772912859916687, 0.3973532021045685, 0.178345188498497, 0.35214006900787354, 0.006104560568928719, 0.332294762134552, 0.11425884068012238, 0.0768788680434227, 0.18602819740772247, 0.3338185250759125, 0.36417943239212036, 0.4848875105381012, 0.3249915540218353, 0.39281824231147766, 0.09421808272600174, 0.010934141464531422, 0.015751052647829056, 0.1966448873281479, 0.385698139667511, 0.1432640105485916, 0.15047095715999603], dtype='float32').reshape([24]),
            paddle.to_tensor([0.007074319291859865, 0.1425466537475586, 0.02198953367769718, 0.39345869421958923, 0.45762473344802856, 0.30224862694740295, 0.44403207302093506, 0.3929651081562042, 0.46495842933654785, 0.037340857088565826, 0.018726840615272522, 0.409798264503479, 0.4335956871509552, 0.21407563984394073, 0.2717125713825226, 0.3219931423664093, 0.4368762969970703, 0.16638652980327606, 0.43825793266296387, 0.3540289103984833, 0.09956780076026917, 0.08401960134506226, 0.12835851311683655, 0.33265864849090576], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f2b1aa3b9465437313283e091ce2d7ec(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a8301b810bccba10e4dfdee0df73334
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1aca5b2f7d5193b8c956405eaa029246(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 31, 31], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a8bbeeae114c69fd65e7ce9c4f278fe4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 256, 512], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.2738110423088074, 0.296219140291214, 0.10498381406068802, 0.07966380566358566, 0.34146785736083984, 0.30614355206489563, 0.0021608127281069756, 0.17786625027656555, 0.24666240811347961, 0.1998603790998459, 0.35764288902282715, 0.14023110270500183, 0.003154759295284748, 0.04187082499265671, 0.49174830317497253, 0.04409200698137283], dtype='float32').reshape([16]),
            paddle.to_tensor([0.45406582951545715, 0.4981190860271454, 0.20599737763404846, 0.45975106954574585, 0.40828070044517517, 0.45078110694885254, 0.24647758901119232, 0.2749686539173126, 0.295689195394516, 0.09695243835449219, 0.20864638686180115, 0.45421433448791504, 0.2526780962944031, 0.05812251940369606, 0.09456810355186462, 0.09216397255659103], dtype='float32').reshape([16]),
            paddle.to_tensor([0.43215519189834595, 0.22132273018360138, 0.033497147262096405, 0.2912103235721588, 0.07039725035429001, 0.08815600723028183, 0.08052824437618256, 0.3983486294746399, 0.4833194315433502, 0.3904094994068146, 0.36088550090789795, 0.17971079051494598, 0.27457207441329956, 0.03203953430056572, 0.18424053490161896, 0.09924159944057465], dtype='float32').reshape([16]),
            paddle.to_tensor([0.3593619763851166, 0.20785219967365265, 0.4454345405101776, 0.010894857347011566, 0.39262494444847107, 0.03147382289171219, 0.27518168091773987, 0.044413596391677856, 0.1192917749285698, 0.15895909070968628, 0.10790393501520157, 0.008186527527868748, 0.35090088844299316, 0.10180118680000305, 0.014716481789946556, 0.0028423084877431393], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_74bf5a1ce70e836f6d8c9fd928eaf4db(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.38302090764045715, 0.17694875597953796, 0.21192176640033722, 0.30648645758628845, 0.16971613466739655, 0.22300787270069122, 0.02852087840437889, 0.14112333953380585, 0.4362502098083496, 0.2636057138442993, 0.28476691246032715, 0.40809786319732666, 0.4091683626174927, 0.00042994803516194224, 0.09677817672491074, 0.3990978002548218, 0.2356831431388855, 0.22318585216999054, 0.2710151970386505, 0.03638416528701782, 0.406074583530426, 0.1870957762002945, 0.03381933644413948, 0.3138676583766937, 0.3901304006576538, 0.12213385850191116, 0.34637847542762756, 0.12476843595504761, 0.06368530541658401, 0.2835393249988556], dtype='float32').reshape([30]),
            paddle.to_tensor([0.22232592105865479, 0.12418482452630997, 0.1768098622560501, 0.14289258420467377, 0.1496831774711609, 0.4047800302505493, 0.3222956359386444, 0.07668721675872803, 0.3818069398403168, 0.17586295306682587, 0.4132121205329895, 0.07950860261917114, 0.03406846150755882, 0.3216911554336548, 0.39575761556625366, 0.2697213292121887, 0.20538702607154846, 0.29704368114471436, 0.004963351413607597, 0.14547137916088104, 0.043259963393211365, 0.08524266630411148, 0.04091290012001991, 0.362483948469162, 0.40572309494018555, 0.26011571288108826, 0.417002409696579, 0.305938720703125, 0.2910826802253723, 0.39373311400413513], dtype='float32').reshape([30]),
            paddle.to_tensor([0.16857534646987915, 0.05450805276632309, 0.40859097242355347, 0.4200719892978668, 0.26227879524230957, 0.38022324442863464, 0.24940581619739532, 0.13106603920459747, 0.34859761595726013, 0.2958499491214752, 0.18600189685821533, 0.40024545788764954, 0.22109737992286682, 0.38933464884757996, 0.4389590620994568, 0.3022560477256775, 0.1634381264448166, 0.4633655548095703, 0.414612740278244, 0.2304045706987381, 0.07354940474033356, 0.2664686143398285, 0.2990145981311798, 0.2767218053340912, 0.3863585889339447, 0.07556349784135818, 0.475750207901001, 0.06300897896289825, 0.04777674749493599, 0.2795635759830475], dtype='float32').reshape([30]),
            paddle.to_tensor([0.21741722524166107, 0.03582308441400528, 0.46487489342689514, 0.08194459974765778, 0.37787145376205444, 0.02515382319688797, 0.434249609708786, 0.4950769245624542, 0.3623485267162323, 0.4027737081050873, 0.44629162549972534, 0.08757840096950531, 0.10748866945505142, 0.4432947635650635, 0.26009491086006165, 0.010368303395807743, 0.27769145369529724, 0.409893274307251, 0.381710022687912, 0.22741346061229706, 0.37095361948013306, 0.3870723247528076, 0.38728711009025574, 0.4288281798362732, 0.27123457193374634, 0.10441704839468002, 0.4478566646575928, 0.08192669600248337, 0.08037500828504562, 0.4946631193161011], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_acf571488bdcd17813095bf8120098b1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 1, 1], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a05b7565f3f0b4965cd53bc39b36c0fa(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 147, 147], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5bdb8a0a25881c8086d05ae159c7d708(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.2883455753326416, 0.16044972836971283, 0.025039907544851303, 0.11338621377944946, 0.07269461452960968, 0.10770747810602188, 0.09378392994403839, 0.18573063611984253, 0.21429717540740967, 0.3364622890949249, 0.09150660783052444, 0.45513680577278137, 0.269797682762146, 0.22822092473506927, 0.217511847615242, 0.4126722812652588, 0.17926739156246185, 0.07298742234706879, 0.3643266558647156, 0.09341519325971603, 0.15412095189094543, 0.24356818199157715, 0.19614006578922272, 0.3559397757053375], dtype='float32').reshape([24]),
            paddle.to_tensor([0.29219263792037964, 0.22368203103542328, 0.030801406130194664, 0.24663536250591278, 0.28145599365234375, 0.4730997383594513, 0.08213987946510315, 0.3310387134552002, 0.24960918724536896, 0.21828202903270721, 0.33891937136650085, 0.4439001977443695, 0.1560160517692566, 0.23786157369613647, 0.43347862362861633, 0.3600817918777466, 0.07300763577222824, 0.4826394021511078, 0.15292899310588837, 0.21017616987228394, 0.13530738651752472, 0.08976346999406815, 0.435241162776947, 0.212165966629982], dtype='float32').reshape([24]),
            paddle.to_tensor([0.11116232722997665, 0.49710360169410706, 0.4194769263267517, 0.45584753155708313, 0.007977738976478577, 0.23160122334957123, 0.448161780834198, 0.43436387181282043, 0.34302958846092224, 0.3610193431377411, 0.4194239675998688, 0.2208189070224762, 0.2777397036552429, 0.37212666869163513, 0.08957875519990921, 0.039207518100738525, 0.332706093788147, 0.0597180537879467, 0.2037191540002823, 0.45183873176574707, 0.30721744894981384, 0.23960334062576294, 0.36326858401298523, 0.3857601583003998], dtype='float32').reshape([24]),
            paddle.to_tensor([0.30935266613960266, 0.44409769773483276, 0.2908612787723541, 0.25642088055610657, 0.01643502525985241, 0.028032056987285614, 0.4333247244358063, 0.24340584874153137, 0.06790846586227417, 0.15851137042045593, 0.13950999081134796, 0.4477216303348541, 0.2559426426887512, 0.42319902777671814, 0.11214779317378998, 0.14361441135406494, 0.497297465801239, 0.11999837309122086, 0.3549456298351288, 0.011029889807105064, 0.1930651068687439, 0.27239155769348145, 0.017194874584674835, 0.3221518397331238], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b7b8f50031b81075172b3259fab10871(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 672, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_743221de295aa387a6ff975f5b004959(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 1280, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([1280], dtype='float32', min=0, max=0.5),
            paddle.uniform([1280], dtype='float32', min=0, max=0.5),
            paddle.uniform([1280], dtype='float32', min=0, max=0.5),
            paddle.uniform([1280], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9668e41c5601fb0a6edfdeb3ed5cbbfb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_d589af777f5a8ae699611515c41499fb
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 20, 20], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_08bf57d1b77b8dba80b71ddecc2eaf1f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 50, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([50], dtype='float32', min=0, max=0.5),
            paddle.uniform([50], dtype='float32', min=0, max=0.5),
            paddle.uniform([50], dtype='float32', min=0, max=0.5),
            paddle.uniform([50], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a4072b0552fbcd6e712cc5c5992de113(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 4, 50], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6b11084edc0991183b65e9ff16daca93(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 1856, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([1856], dtype='float32', min=0, max=0.5),
            paddle.uniform([1856], dtype='float32', min=0, max=0.5),
            paddle.uniform([1856], dtype='float32', min=0, max=0.5),
            paddle.uniform([1856], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_be09309982aeec5c6fd1d6f151bdb941(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 64, 128], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.36733296513557434, 0.4210754632949829, 0.20643611252307892, 0.2740685045719147, 0.458688884973526, 0.002182651311159134, 0.13224337995052338, 0.22944101691246033, 0.2870001494884491, 0.4823206961154938, 0.04191935062408447, 0.45828959345817566, 0.42128241062164307, 0.4550018012523651, 0.09838711470365524, 0.4186434745788574, 0.08233243227005005, 0.2450522631406784], dtype='float32').reshape([18]),
            paddle.to_tensor([0.4327774941921234, 0.486635684967041, 0.18354864418506622, 0.08839191496372223, 0.37720027565956116, 0.06612461060285568, 0.08748764544725418, 0.33201462030410767, 0.42193087935447693, 0.11622254550457001, 0.4487384557723999, 0.4664972126483917, 0.19077663123607635, 0.10242907702922821, 0.36151114106178284, 0.34646767377853394, 0.3536575734615326, 0.05715426057577133], dtype='float32').reshape([18]),
            paddle.to_tensor([0.4244813919067383, 0.13710254430770874, 0.3789684474468231, 0.19806797802448273, 0.41981831192970276, 0.44627708196640015, 0.06601890921592712, 0.41457176208496094, 0.45417723059654236, 0.21034294366836548, 0.1954410970211029, 0.19503416121006012, 0.1901542842388153, 0.38677704334259033, 0.2597595751285553, 0.16928434371948242, 0.4635699391365051, 0.3708350360393524], dtype='float32').reshape([18]),
            paddle.to_tensor([0.40778473019599915, 0.3271121382713318, 0.164038747549057, 0.0129255261272192, 0.1556001603603363, 0.007648248691111803, 0.056242477148771286, 0.33929240703582764, 0.04300357773900032, 0.18781128525733948, 0.20652799308300018, 0.2099640667438507, 0.28421369194984436, 0.4474541246891022, 0.10805374383926392, 0.016172725707292557, 0.1697588860988617, 0.127902552485466], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_810b342bc9a430a7ac281d9de9ec33a4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 32, 64], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.10810364782810211, 0.16142751276493073, 0.23607365787029266, 0.233940988779068, 0.2679259181022644, 0.14219632744789124, 0.4934418201446533, 0.3515552580356598, 0.160443976521492, 0.14240318536758423, 0.4102935492992401, 0.009602699428796768, 0.04029231145977974, 0.11701025068759918, 0.0714564397931099, 0.18378819525241852, 0.26280635595321655, 0.13700664043426514], dtype='float32').reshape([18]),
            paddle.to_tensor([0.01750062219798565, 0.49749448895454407, 0.2411031424999237, 0.47535738348960876, 0.11892688274383545, 0.09255754202604294, 0.2991779148578644, 0.31997525691986084, 0.4694483280181885, 0.3778989315032959, 0.11396244168281555, 0.16258564591407776, 0.05195772647857666, 0.13451284170150757, 0.062453851103782654, 0.31346452236175537, 0.33531880378723145, 0.11449567973613739], dtype='float32').reshape([18]),
            paddle.to_tensor([0.42906454205513, 0.03882352635264397, 0.45372045040130615, 0.18661579489707947, 0.29156118631362915, 0.24773846566677094, 0.4819693863391876, 0.15431727468967438, 0.06407336890697479, 0.3509310781955719, 0.3184274137020111, 0.1040852814912796, 0.030599521473050117, 0.2473895102739334, 0.3497467041015625, 0.4653012156486511, 0.0890156701207161, 0.0677085816860199], dtype='float32').reshape([18]),
            paddle.to_tensor([0.37905585765838623, 0.10033101588487625, 0.03998889774084091, 0.10323619097471237, 0.23610608279705048, 0.4118909239768982, 0.13927839696407318, 0.3103291690349579, 0.32044553756713867, 0.3968808054924011, 0.38290610909461975, 0.41782745718955994, 0.18719813227653503, 0.03726613149046898, 0.22594547271728516, 0.32109472155570984, 0.48827436566352844, 0.30066898465156555], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4c34af641aa3289578d1eac7a962a61e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 52, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([52], dtype='float32', min=0, max=0.5),
            paddle.uniform([52], dtype='float32', min=0, max=0.5),
            paddle.uniform([52], dtype='float32', min=0, max=0.5),
            paddle.uniform([52], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ff11788e3b27bcb502be28b47b4d3c46(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 112, 112], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.30973440408706665, 0.2958256006240845, 0.19389769434928894, 0.09171169996261597, 0.011662730015814304, 0.024438802152872086, 0.3742188513278961, 0.16780781745910645], dtype='float32').reshape([8]),
            paddle.to_tensor([0.13471892476081848, 0.3168277442455292, 0.248243510723114, 0.33491578698158264, 0.4090222716331482, 0.04794131964445114, 0.04511001333594322, 0.09101468324661255], dtype='float32').reshape([8]),
            paddle.to_tensor([0.3091069757938385, 0.20441767573356628, 0.1799260973930359, 0.09917349368333817, 0.11079350858926773, 0.21653470396995544, 0.4864845871925354, 0.10992363095283508], dtype='float32').reshape([8]),
            paddle.to_tensor([0.2744200825691223, 0.4725802540779114, 0.32520800828933716, 0.34965845942497253, 0.31982123851776123, 0.10983871668577194, 0.3349230885505676, 0.2890755534172058], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9e8bd60d7f350e2e93dfbb61a18a215c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.4999679923057556, 0.07998345792293549, 0.10835390537977219, 0.43522927165031433, 0.22606709599494934, 0.4156269431114197, 0.48586755990982056, 0.32645511627197266, 0.12159334123134613, 0.37067511677742004, 0.1996658593416214, 0.15123140811920166, 0.32425644993782043, 0.009496888145804405, 0.05865951627492905, 0.40955299139022827], dtype='float32').reshape([16]),
            paddle.to_tensor([0.3682045340538025, 0.37117549777030945, 0.140777125954628, 0.40542829036712646, 0.33331069350242615, 0.04359709471464157, 0.08036261796951294, 0.2663789987564087, 0.26055559515953064, 0.1587456464767456, 0.4292372763156891, 0.30981016159057617, 0.38409367203712463, 0.09627985954284668, 0.3537690341472626, 0.011677183210849762], dtype='float32').reshape([16]),
            paddle.to_tensor([0.3708694577217102, 0.3573744297027588, 0.2692687213420868, 0.3602375090122223, 0.19711919128894806, 0.25359952449798584, 0.4201509654521942, 0.4553612172603607, 0.2438603788614273, 0.2904795706272125, 0.16131113469600677, 0.06582875549793243, 0.18714971840381622, 0.15256893634796143, 0.21364963054656982, 0.09424152225255966], dtype='float32').reshape([16]),
            paddle.to_tensor([0.19516220688819885, 0.20802564918994904, 0.07346981763839722, 0.298403799533844, 0.38461872935295105, 0.34361642599105835, 0.13101376593112946, 0.0267168115824461, 0.16106916964054108, 0.006733139045536518, 0.1057586595416069, 0.004205986857414246, 0.4137699604034424, 0.014733977615833282, 0.4504837691783905, 0.28357234597206116], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5c1930923ebe5f73f4c8cde8363e90f9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 95, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([95], dtype='float32', min=0, max=0.5),
            paddle.uniform([95], dtype='float32', min=0, max=0.5),
            paddle.uniform([95], dtype='float32', min=0, max=0.5),
            paddle.uniform([95], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_40044ac99dc23c0ea70783c59a1c58eb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 149, 149], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7f13eb13cc76d9cefc6d1135caf654a7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.to_tensor([[[[319697.03125]], [[332591.59375]], [[383012.3125]], [[291799.3125]], [[313455.15625]], [[398704.3125]], [[311517.625]], [[354651.4375]], [[380503.8125]], [[332279.21875]], [[333328.34375]], [[366048.71875]], [[381309.84375]], [[324116.78125]], [[268547.78125]], [[347021.65625]], [[343293.90625]], [[391112.75]], [[370377.46875]], [[372410.15625]], [[343438.125]], [[338387.375]], [[390757.8125]], [[270121.125]], [[301748.46875]], [[296884.65625]], [[421730.25]], [[347112.8125]], [[338615.0]], [[311616.75]]]], dtype='float32').reshape([1, 30, 1, 1]),
            paddle.to_tensor([0.42713016271591187, 0.20036765933036804, 0.28863710165023804, 0.04568522423505783, 0.24037238955497742, 0.3603256940841675, 0.25283417105674744, 0.04521032050251961, 0.28762781620025635, 0.16259025037288666, 0.11375322192907333, 0.36695003509521484, 0.36860203742980957, 0.36538439989089966, 0.10700493305921555, 0.10347983241081238, 0.24846051633358002, 0.25908201932907104, 0.12603998184204102, 0.09639326483011246, 0.33747008442878723, 0.2283627986907959, 0.44188252091407776, 0.18054042756557465, 0.009543070569634438, 0.23673862218856812, 0.394249826669693, 0.4958147406578064, 0.19170285761356354, 0.29997390508651733], dtype='float32').reshape([30]),
            paddle.to_tensor([0.3316817581653595, 0.1400393545627594, 0.09693347662687302, 0.1912415623664856, 0.43047022819519043, 0.46726587414741516, 0.47427061200141907, 0.4220074713230133, 0.35515448451042175, 0.31123825907707214, 0.12534989416599274, 0.09445095807313919, 0.03914542496204376, 0.2218446135520935, 0.46287235617637634, 0.25365573167800903, 0.20282267034053802, 0.3107331693172455, 0.2721121609210968, 0.06174226105213165, 0.1421450525522232, 0.477716863155365, 0.3905569314956665, 0.45617446303367615, 0.26923537254333496, 0.25534573197364807, 0.3133150339126587, 0.17214719951152802, 0.16380736231803894, 0.4880430996417999], dtype='float32').reshape([30]),
            paddle.to_tensor([0.3771612048149109, 0.2718251347541809, 0.4769939184188843, 0.156199648976326, 0.3660706579685211, 0.481036514043808, 0.49936920404434204, 0.34869664907455444, 0.27495330572128296, 0.19357730448246002, 0.24299709498882294, 0.13750168681144714, 0.2204006016254425, 0.21317364275455475, 0.17268909513950348, 0.2697044909000397, 0.33556661009788513, 0.25715258717536926, 0.4592314064502716, 0.2601824700832367, 0.35162490606307983, 0.35491010546684265, 0.4716690182685852, 0.41030409932136536, 0.028931470587849617, 0.2607344388961792, 0.4577219784259796, 0.4294196665287018, 0.14333149790763855, 0.06412267684936523], dtype='float32').reshape([30]),
            paddle.to_tensor([0.391030490398407, 0.11373794078826904, 0.23402826488018036, 0.043127018958330154, 0.46471479535102844, 0.4124040901660919, 0.2153969407081604, 0.06984207779169083, 0.1053338423371315, 0.025956744328141212, 0.47647297382354736, 0.31242722272872925, 0.13740453124046326, 0.01779978908598423, 0.3293261229991913, 0.15372473001480103, 0.14122097194194794, 0.08006773889064789, 0.21595241129398346, 0.1086123064160347, 0.22750627994537354, 0.38217368721961975, 0.06733054667711258, 0.44789111614227295, 0.11153823882341385, 0.16533926129341125, 0.3679802119731903, 0.23746202886104584, 0.03871583193540573, 0.36920878291130066], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4a472cd68d23a0b27c5ffe26fa39a909(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.23996254801750183, 0.028034454211592674, 0.21929678320884705, 0.1527319699525833, 0.1162010207772255, 0.41855478286743164, 0.13112320005893707, 0.15467636287212372, 0.3230155408382416, 0.45452937483787537, 0.4568418562412262, 0.023719042539596558, 0.09574740380048752, 0.4560919404029846, 0.3992064297199249, 0.17963147163391113, 0.29696786403656006, 0.1363591104745865, 0.032152868807315826, 0.3997971713542938, 0.22608977556228638, 0.3354165554046631, 0.12671512365341187, 0.4673048257827759], dtype='float32').reshape([24]),
            paddle.to_tensor([0.259675532579422, 0.42236045002937317, 0.3994949162006378, 0.16588616371154785, 0.11743985116481781, 0.14238226413726807, 0.40983253717422485, 0.24884048104286194, 0.30481866002082825, 0.293732225894928, 0.297392338514328, 0.49748799204826355, 0.10321886092424393, 0.4531717002391815, 0.27640292048454285, 0.3373067378997803, 0.44717323780059814, 0.307422935962677, 0.023775169625878334, 0.19954951107501984, 0.33979156613349915, 0.3093847632408142, 0.37555089592933655, 0.08156365901231766], dtype='float32').reshape([24]),
            paddle.to_tensor([0.37472236156463623, 0.05303610488772392, 0.4516527056694031, 0.04761334881186485, 0.08797156810760498, 0.3091147840023041, 0.09542091190814972, 0.2816798686981201, 0.01715659908950329, 0.3353802263736725, 0.32835349440574646, 0.29599300026893616, 0.2338458150625229, 0.2831166386604309, 0.0803370550274849, 0.23703110218048096, 0.4912365674972534, 0.28698742389678955, 0.3957304358482361, 0.28182142972946167, 0.20070968568325043, 0.40653181076049805, 0.3460298478603363, 0.288748174905777], dtype='float32').reshape([24]),
            paddle.to_tensor([0.2985590398311615, 0.3702733814716339, 0.19481892883777618, 0.10466635227203369, 0.32344722747802734, 0.21403741836547852, 0.46537983417510986, 0.06767010688781738, 0.4909738302230835, 0.11364839971065521, 0.45579251646995544, 0.3192811906337738, 0.12857095897197723, 0.436314195394516, 0.0858566015958786, 0.21226072311401367, 0.49535006284713745, 0.02148914895951748, 0.1540193408727646, 0.2705882489681244, 0.10006656497716904, 0.2967562675476074, 0.2274513840675354, 0.23067887127399445], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_570c7f44d46bc406d963f37b031fbd2e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 162, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.uniform([162], dtype='float32', min=0, max=0.5),
            paddle.uniform([162], dtype='float32', min=0, max=0.5),
            paddle.uniform([162], dtype='float32', min=0, max=0.5),
            paddle.uniform([162], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_30f9ef1024e731b3df840823301b8e7e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 112, 112], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.41511136293411255, 0.1018056571483612, 0.09703943878412247, 0.22814027965068817, 0.1353512555360794, 0.48619499802589417, 0.15787190198898315, 0.13499975204467773, 0.43666160106658936, 0.26767513155937195, 0.47225719690322876, 0.1980229616165161, 0.46826407313346863, 0.34841588139533997, 0.10881766676902771, 0.22449226677417755, 0.4397290349006653, 0.49520954489707947, 0.3210107982158661, 0.05981943756341934, 0.4909276068210602, 0.4085344076156616, 0.2768207788467407, 0.3436531126499176], dtype='float32').reshape([24]),
            paddle.to_tensor([0.42310860753059387, 0.39824897050857544, 0.06651149690151215, 0.26300230622291565, 0.41600218415260315, 0.005697163287550211, 0.22116784751415253, 0.10610805451869965, 0.15206563472747803, 0.056790925562381744, 0.4639257788658142, 0.05076827108860016, 0.24695958197116852, 0.16694039106369019, 0.43778717517852783, 0.38830363750457764, 0.3211832344532013, 0.34627828001976013, 0.24232453107833862, 0.012839547358453274, 0.15329892933368683, 0.4366258978843689, 0.08943687379360199, 0.054436780512332916], dtype='float32').reshape([24]),
            paddle.to_tensor([0.22919058799743652, 0.3000994920730591, 0.1409359872341156, 0.05095285549759865, 0.4623696804046631, 0.4277544319629669, 0.4288652539253235, 0.14896266162395477, 0.19814537465572357, 0.4539797604084015, 0.3281719386577606, 0.2189929038286209, 0.18863898515701294, 0.4747583866119385, 0.1808404177427292, 0.18497662246227264, 0.11394805461168289, 0.28509053587913513, 0.3649255633354187, 0.328986257314682, 0.09402112662792206, 0.08025681972503662, 0.18423813581466675, 0.05156111717224121], dtype='float32').reshape([24]),
            paddle.to_tensor([0.3925107419490814, 0.210453599691391, 0.31735873222351074, 0.01480215322226286, 0.3336741030216217, 0.057968296110630035, 0.4930800199508667, 0.06085412576794624, 0.302147775888443, 0.13344822824001312, 0.43549296259880066, 0.14804331958293915, 0.12430858612060547, 0.3103991746902466, 0.11289657652378082, 0.05539260059595108, 0.2930392324924469, 0.272765189409256, 0.016583820804953575, 0.3515886664390564, 0.45609423518180847, 0.15740074217319489, 0.26296791434288025, 0.48433971405029297], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3650ad2b4f4d06890e2b8beb13f20640(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 20, 20], dtype='float16', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0cb619b3c87fd20e766b170517f46a22(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 40, 40], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8a6a060c8f346231c0eb898a1e937ab3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 120, 120], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e6d4dbbbd6c950e4132b5fa7ff5622c4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.3595534563064575, 0.24666762351989746, 0.09822535514831543, 0.16525912284851074, 0.25561490654945374, 0.18887776136398315, 0.4359869062900543, 0.08647208660840988, 0.16284169256687164, 0.36993324756622314, 0.03901759162545204, 0.4301421046257019, 0.16074295341968536, 0.41173192858695984, 0.34824055433273315, 0.14980436861515045, 0.16249528527259827, 0.09256890416145325, 0.43193963170051575, 0.07425025850534439, 0.07025454193353653, 0.217275470495224, 0.3446362018585205, 0.48051679134368896, 0.10288456082344055, 0.4614458382129669, 0.38262003660202026, 0.14735150337219238, 0.05105216056108475, 0.2759479880332947], dtype='float32').reshape([30]),
            paddle.to_tensor([0.2943437695503235, 0.3843139708042145, 0.2541607618331909, 0.09066142141819, 0.19736185669898987, 0.2163921594619751, 0.4609872102737427, 0.41835272312164307, 0.2766030430793762, 0.36671575903892517, 0.1887180656194687, 0.4025826156139374, 0.34738466143608093, 0.42264649271965027, 0.13681375980377197, 0.3744238317012787, 0.49282822012901306, 0.41471654176712036, 0.08170311897993088, 0.006876812316477299, 0.4498974084854126, 0.46043023467063904, 0.014924063347280025, 0.27230992913246155, 0.0021410686895251274, 0.014003160409629345, 0.011151909828186035, 0.45652151107788086, 0.4097024202346802, 0.36433282494544983], dtype='float32').reshape([30]),
            paddle.to_tensor([0.10373412072658539, 0.3990916311740875, 0.19083744287490845, 0.1441487818956375, 0.19924890995025635, 0.4381980299949646, 0.12419478595256805, 0.20055776834487915, 0.40100181102752686, 0.4876578748226166, 0.15591414272785187, 0.30262988805770874, 0.3382847011089325, 0.03083515167236328, 0.14607492089271545, 0.27782464027404785, 0.22608938813209534, 0.34749066829681396, 0.4510718882083893, 0.36642614006996155, 0.37886783480644226, 0.45107680559158325, 0.41470271348953247, 0.47678524255752563, 0.2933667302131653, 0.08085356652736664, 0.1585734337568283, 0.1256045401096344, 0.36221182346343994, 0.16124899685382843], dtype='float32').reshape([30]),
            paddle.to_tensor([0.08937013894319534, 0.007921692915260792, 0.032623231410980225, 0.3643587827682495, 0.32351961731910706, 0.03647111356258392, 0.28954043984413147, 0.0011628170032054186, 0.48725664615631104, 0.2931087017059326, 0.009793750010430813, 0.3042824864387512, 0.22403621673583984, 0.3980180621147156, 0.4052867591381073, 0.10766678303480148, 0.4490649700164795, 0.4902901351451874, 0.08571743965148926, 0.43181073665618896, 0.044121596962213516, 0.21631880104541779, 0.4182208776473999, 0.1695639044046402, 0.29059603810310364, 0.09381026029586792, 0.011606423184275627, 0.1015525534749031, 0.0021935829427093267, 0.2207718938589096], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c1beb666bd488d3b99674091e7b9783d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 4, 50], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.35900408029556274, 0.3530247211456299, 0.11969753354787827, 0.36069396138191223, 0.3180259168148041, 0.3584717810153961, 0.11278951913118362, 0.3917384147644043, 0.36582088470458984, 0.1568869650363922, 0.4407356083393097, 0.3463113307952881, 0.09394104033708572, 0.44387224316596985, 0.21247711777687073, 0.05797165632247925, 0.4981553256511688, 0.2743687331676483, 0.246768981218338, 0.40041616559028625, 0.20971347391605377, 0.17063069343566895, 0.05387216806411743, 0.11744514852762222], dtype='float32').reshape([24]),
            paddle.to_tensor([0.36836254596710205, 0.2536773681640625, 0.17588010430335999, 0.04341486841440201, 0.08755195885896683, 0.09455695003271103, 0.3974589407444, 0.49681803584098816, 0.32174375653266907, 0.2663393020629883, 0.48491424322128296, 0.11166588962078094, 0.1855144202709198, 0.2441788911819458, 0.041992608457803726, 0.16530336439609528, 0.40119990706443787, 0.2798812687397003, 0.0868084579706192, 0.4314044117927551, 0.09392495453357697, 0.055206604301929474, 0.18497951328754425, 0.1243332028388977], dtype='float32').reshape([24]),
            paddle.to_tensor([0.4455339312553406, 0.41440123319625854, 0.25731196999549866, 0.013979203067719936, 0.2567659616470337, 0.4353007376194, 0.4480942487716675, 0.15440033376216888, 0.16363686323165894, 0.03913050517439842, 0.4990069270133972, 0.11031652987003326, 0.20781908929347992, 0.19091397523880005, 0.34451016783714294, 0.17395849525928497, 0.045604415237903595, 0.17779971659183502, 0.40219542384147644, 0.32565954327583313, 0.31545087695121765, 0.3450634479522705, 0.3541927933692932, 0.1352934092283249], dtype='float32').reshape([24]),
            paddle.to_tensor([0.38619473576545715, 0.27709996700286865, 0.0702449157834053, 0.2657777965068817, 0.44542282819747925, 0.46366870403289795, 0.4790883958339691, 0.06923693418502808, 0.47012796998023987, 0.48327574133872986, 0.003631279803812504, 0.009263652376830578, 0.3874892592430115, 0.35279443860054016, 0.4469793736934662, 0.07958246022462845, 0.07807701081037521, 0.06282146275043488, 0.4519966244697571, 0.37895020842552185, 0.2317376732826233, 0.44815006852149963, 0.2933139204978943, 0.02697020024061203], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_38448f52220394bd0877a96eca1a35f8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.19330722093582153, 0.3311571776866913, 0.047701120376586914, 0.18217015266418457, 0.34018969535827637, 0.10501229017972946, 0.4346584379673004, 0.4132143259048462, 0.03254563361406326, 0.13572098314762115, 0.004293842241168022, 0.15855275094509125, 0.33276990056037903, 0.05709877982735634, 0.4948651194572449, 0.2180202454328537], dtype='float32').reshape([16]),
            paddle.to_tensor([0.2465231567621231, 0.1376359611749649, 0.1002802923321724, 0.040108293294906616, 0.27469202876091003, 0.21270151436328888, 0.3942772150039673, 0.11661144345998764, 0.4366668164730072, 0.0663038119673729, 0.2777145206928253, 0.1312737911939621, 0.3349277079105377, 0.4391939342021942, 0.37470054626464844, 0.04875979572534561], dtype='float32').reshape([16]),
            paddle.to_tensor([0.024165133014321327, 0.055295348167419434, 0.3387836515903473, 0.25973954796791077, 0.16194231808185577, 0.31995001435279846, 0.11216531693935394, 0.07165470719337463, 0.04668683558702469, 0.34661784768104553, 0.4402410387992859, 0.24687911570072174, 0.04999721422791481, 0.18193651735782623, 0.35022932291030884, 0.06854987144470215], dtype='float32').reshape([16]),
            paddle.to_tensor([0.3812926113605499, 0.08006823807954788, 0.3391437232494354, 0.08336630463600159, 0.06030803546309471, 0.29264047741889954, 0.29831400513648987, 0.47037017345428467, 0.293276846408844, 0.18456381559371948, 0.4347773492336273, 0.10868029296398163, 0.32260265946388245, 0.28552722930908203, 0.22959209978580475, 0.1380757987499237], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d80ad80c6536ec5615ec97e69d0daee3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 112, 112], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_73d43573dd5365a61eac0fda48bfb886(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 480, 64, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_68c885aa9fd50f14fa1e73c99efaa7b6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 768, 32, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0553766af004ab2053e7e77bda0f041d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 120, 120], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_54f5a721ce9366e2ba070e62b0a00190(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_d589af777f5a8ae699611515c41499fb
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 40, 40], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a0adc5753af7fe143075eb6222fc1173(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 400, 672], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c182d0d414eff0cdd3eea0a8f368c4e0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cdc3b0a148b551f86897e4668e0bdb6b
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 32, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d585aca6cceb2924e36e898cb68c93c2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_af91295b59c5fca17f890a802deb001f
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 4, 16, 16], dtype='float16', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_59076b23a8a27d9d5e37f4a25a3ff217(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 8, 50], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d681682d8cd99b38c1cfe3605e3c21dd(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 19, 19], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_000345d3fef3fda8308d6553c21137f6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ab84b397f372b09312387df555d0f892(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.4458399713039398, 0.49912139773368835, 0.45139822363853455, 0.28040024638175964, 0.10354571789503098, 0.26235395669937134, 0.18570515513420105, 0.4085266888141632, 0.20621709525585175, 0.33089759945869446, 0.4256455600261688, 0.2699816823005676, 0.07836353033781052, 0.0411333367228508, 0.3025997281074524, 0.28415054082870483, 0.12166563421487808, 0.26722025871276855, 0.3113212287425995, 0.18614378571510315, 0.12545117735862732, 0.18353375792503357, 0.4680001437664032, 0.31014329195022583, 0.19015789031982422, 0.15717414021492004, 0.37517762184143066, 0.17242619395256042, 0.4357512295246124, 0.4036377966403961], dtype='float32').reshape([30]),
            paddle.to_tensor([0.35295116901397705, 0.1307324767112732, 0.41279029846191406, 0.08682624995708466, 0.36927467584609985, 0.1983199119567871, 0.44705837965011597, 0.29301387071609497, 0.15159961581230164, 0.4259089529514313, 0.16235294938087463, 0.4367327392101288, 0.19606541097164154, 0.02158566378057003, 0.005429913755506277, 0.14605236053466797, 0.3329431414604187, 0.42232316732406616, 0.3680487871170044, 0.2511955499649048, 0.2915526032447815, 0.45339471101760864, 0.07960624992847443, 0.16401903331279755, 0.170087531208992, 0.02033761888742447, 0.4568975269794464, 0.4320823848247528, 0.15488649904727936, 0.15579989552497864], dtype='float32').reshape([30]),
            paddle.to_tensor([0.30864447355270386, 0.4625546336174011, 0.4335777461528778, 0.06961079686880112, 0.08407008647918701, 0.4549139738082886, 0.355989933013916, 0.34253475069999695, 0.32285168766975403, 0.08624174445867538, 0.011371896602213383, 0.007005076855421066, 0.13258081674575806, 0.3480948805809021, 0.3155677616596222, 0.19340939819812775, 0.45114627480506897, 0.012587103992700577, 0.1584174633026123, 0.11684080958366394, 0.27474361658096313, 0.3447679579257965, 0.4412831962108612, 0.385430246591568, 0.011029199697077274, 0.47388362884521484, 0.48929983377456665, 0.122104212641716, 0.3710234463214874, 0.13717681169509888], dtype='float32').reshape([30]),
            paddle.to_tensor([0.3668444752693176, 0.14963309466838837, 0.2537075877189636, 0.2173549234867096, 0.07609277963638306, 0.13935640454292297, 0.46675264835357666, 0.3535400927066803, 0.2901804447174072, 0.0745561271905899, 0.10372977703809738, 0.13626335561275482, 0.1839735060930252, 0.05714789405465126, 0.45620492100715637, 0.03075987845659256, 0.3853721022605896, 0.1788395345211029, 0.36714237928390503, 0.15729764103889465, 0.22662733495235443, 0.017728637903928757, 0.48763614892959595, 0.16949187219142914, 0.07961002737283707, 0.43996191024780273, 0.3423343300819397, 0.27312976121902466, 0.2012408971786499, 0.4714336693286896], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_baa2acc5ac4f28070257c4799a8e0fbc(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6d9c2229e7e9d41ff87450b7d2f56244(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 72, 16, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_82275332dea673b796c5e1a503c00b02(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 28, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.11728816479444504, 0.3794322907924652, 0.37133702635765076, 0.24731029570102692, 0.4340965747833252, 0.16460388898849487, 0.24525229632854462, 0.03292836248874664, 0.07377907633781433, 0.323592871427536, 0.27900853753089905, 0.4362064599990845, 0.0988817885518074, 0.19745413959026337, 0.10622777044773102, 0.3817765712738037, 0.024394473060965538, 0.015600534155964851, 0.45055675506591797, 0.41865843534469604, 0.32843923568725586, 0.4330720901489258, 0.16531316936016083, 0.3419125974178314, 0.46070876717567444, 0.12019657343626022, 0.13823309540748596, 0.0889941081404686], dtype='float32').reshape([28]),
            paddle.to_tensor([0.1397997885942459, 0.48808708786964417, 0.436387300491333, 0.40416595339775085, 0.09488333761692047, 0.45368409156799316, 0.19955585896968842, 0.40258219838142395, 0.29612550139427185, 0.20938746631145477, 0.4277886152267456, 0.24226655066013336, 0.4510914087295532, 0.39075562357902527, 0.06149469316005707, 0.16549183428287506, 0.15929962694644928, 0.16193906962871552, 0.12738975882530212, 0.03211060166358948, 0.16494160890579224, 0.01199240144342184, 0.2613179087638855, 0.22508616745471954, 0.23808443546295166, 0.2588430941104889, 0.14039212465286255, 0.46721866726875305], dtype='float32').reshape([28]),
            paddle.to_tensor([0.22777070105075836, 0.054266683757305145, 0.42661625146865845, 0.49660760164260864, 0.2866387367248535, 0.35712283849716187, 0.16008852422237396, 0.2542042136192322, 0.31807422637939453, 0.19376717507839203, 0.17374549806118011, 0.12678273022174835, 0.493125855922699, 0.014496487565338612, 0.1212291419506073, 0.2897499203681946, 0.1329333335161209, 0.3035108745098114, 0.052214980125427246, 0.12749306857585907, 0.17371250689029694, 0.08682824671268463, 0.24322651326656342, 0.3344973623752594, 0.389778196811676, 0.44551539421081543, 0.34326186776161194, 0.49062836170196533], dtype='float32').reshape([28]),
            paddle.to_tensor([0.19572213292121887, 0.10651420056819916, 0.2288384884595871, 0.47134509682655334, 0.22894053161144257, 0.07362423092126846, 0.49624764919281006, 0.1373939961194992, 0.29686516523361206, 0.12984029948711395, 0.32083597779273987, 0.3323208689689636, 0.04310791939496994, 0.06779234111309052, 0.33549097180366516, 0.39989012479782104, 0.3223159611225128, 0.04782676696777344, 0.27651065587997437, 0.29447805881500244, 0.2633950412273407, 0.40968430042266846, 0.18609093129634857, 0.2807881236076355, 0.3133852481842041, 0.3282364308834076, 0.09480544924736023, 0.34523773193359375], dtype='float32').reshape([28]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3d95a95ad80002195bf4a7b21fc57d5d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 10, 10], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_aace3e61c17184d9d634af90b0fd2dcb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 16, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1d8d1fc3277f9823b1e9918846f576a1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 35, 35], dtype='float16', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2337d3e6f0690d181df064dd0ebbf595(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 32, 100], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d48e446100d42f4a5763b5af978cf037(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 512, 1024], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.22442123293876648, 0.04975297302007675, 0.3594405949115753, 0.3275631368160248, 0.06368738412857056, 0.127220019698143, 0.08952901512384415, 0.38879892230033875, 0.15491481125354767, 0.07255635410547256, 0.3700770437717438, 0.21163323521614075, 0.20463594794273376, 0.027814561501145363, 0.022336579859256744, 0.21936357021331787], dtype='float32').reshape([16]),
            paddle.to_tensor([0.43251296877861023, 0.2840625047683716, 0.062080755829811096, 0.3921006917953491, 0.39543813467025757, 0.2357434183359146, 0.21390824019908905, 0.04085744172334671, 0.3064362406730652, 0.3498440086841583, 0.18595077097415924, 0.21101121604442596, 0.17283621430397034, 0.22977013885974884, 0.3742494285106659, 0.4596530795097351], dtype='float32').reshape([16]),
            paddle.to_tensor([0.44344210624694824, 0.4054279625415802, 0.408550500869751, 0.27781108021736145, 0.14345262944698334, 0.10883038491010666, 0.42070603370666504, 0.06082431226968765, 0.29407763481140137, 0.47927409410476685, 0.03151940181851387, 0.1462291181087494, 0.24017933011054993, 0.4131775200366974, 0.495369017124176, 0.046825915575027466], dtype='float32').reshape([16]),
            paddle.to_tensor([0.19231809675693512, 0.12209630757570267, 0.005778012331575155, 0.21597643196582794, 0.04778687655925751, 0.39458194375038147, 0.2534312605857849, 0.46699070930480957, 0.10963626950979233, 0.20076781511306763, 0.40269801020622253, 0.14151358604431152, 0.17224037647247314, 0.2894705832004547, 0.010017077438533306, 0.29200229048728943], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bd8f33fd506f7f76fe0e0fb18663eee8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 960, 32, 16], dtype='float32', min=0, max=0.5),
            paddle.uniform([960], dtype='float32', min=0, max=0.5),
            paddle.uniform([960], dtype='float32', min=0, max=0.5),
            paddle.uniform([960], dtype='float32', min=0, max=0.5),
            paddle.uniform([960], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d29aa8aec2fcf454e2fa0a4beebfa4e4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 112, 199], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4ea669bf34021b132b3c36471aa0e272(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 160, 160], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c93e195426e7db47c1e96bb7ca009b65(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 16, 50], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1721df3500d1af4daf2710df3b5463af(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 160, 160], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.12394463270902634, 0.25115635991096497, 0.36681750416755676, 0.2879897952079773, 0.3710207939147949, 0.21474865078926086, 0.3010867238044739, 0.3531684875488281, 0.40531885623931885, 0.03792070224881172, 0.39606666564941406, 0.19444717466831207, 0.38426628708839417, 0.012604770250618458, 0.016644924879074097, 0.3236522376537323], dtype='float32').reshape([16]),
            paddle.to_tensor([0.11626183986663818, 0.40460631251335144, 0.04636547341942787, 0.46528860926628113, 0.4686557948589325, 0.15991564095020294, 0.49269983172416687, 0.17430807650089264, 0.12247931212186813, 0.20910698175430298, 0.2850874960422516, 0.3788916766643524, 0.007168634794652462, 0.11754760146141052, 0.16843093931674957, 0.12498017400503159], dtype='float32').reshape([16]),
            paddle.to_tensor([0.32860368490219116, 0.33823609352111816, 0.25321388244628906, 0.2734966576099396, 0.4730624854564667, 0.20386800169944763, 0.05705995485186577, 0.00962021667510271, 0.18690291047096252, 0.14844214916229248, 0.17400869727134705, 0.17046722769737244, 0.39140939712524414, 0.2820177972316742, 0.35403409600257874, 0.249524787068367], dtype='float32').reshape([16]),
            paddle.to_tensor([0.46452489495277405, 0.36635923385620117, 0.08275236189365387, 0.4930717349052429, 0.04519668221473694, 0.31793147325515747, 0.3652646839618683, 0.14987152814865112, 0.14158836007118225, 0.3369148373603821, 0.1635836362838745, 0.08052997291088104, 0.47036778926849365, 0.21717514097690582, 0.4598388373851776, 0.22423911094665527], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6ca5831be243ac16ca974555b1ef3ff7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 2, 240, 240], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.34972333908081055, 0.48405683040618896], dtype='float32').reshape([2]),
            paddle.to_tensor([0.3345395028591156, 0.19640633463859558], dtype='float32').reshape([2]),
            paddle.to_tensor([0.15275511145591736, 0.029400695115327835], dtype='float32').reshape([2]),
            paddle.to_tensor([0.07058747857809067, 0.24433904886245728], dtype='float32').reshape([2]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_930258db44ee8a84aced576bef591517(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_3a47a120d62a3f3a422a747f511df8d9
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 19, 19], dtype='float16', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a189650ad2cf49138ddade1316ee5cb6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_da4d85bc213f70819a72e68f21214cdb
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 4, 16, 16], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0abda57841c2812db700d44165f0e040(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 151, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([151], dtype='float32', min=0, max=0.5),
            paddle.uniform([151], dtype='float32', min=0, max=0.5),
            paddle.uniform([151], dtype='float32', min=0, max=0.5),
            paddle.uniform([151], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bb182dec18071514978b6fbe32bd98fa(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 40, 40], dtype='float16', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c0a42c4d0cd0acc18c313f59deac9fcf(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 336, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d6a492ff66269792d073f29e82524246(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_3a47a120d62a3f3a422a747f511df8d9
    def get_inputs(self):
        return [
            paddle.uniform([1, 728, 38, 38], dtype='float16', min=0, max=0.5),
            paddle.uniform([728], dtype='float32', min=0, max=0.5),
            paddle.uniform([728], dtype='float32', min=0, max=0.5),
            paddle.uniform([728], dtype='float32', min=0, max=0.5),
            paddle.uniform([728], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4949aff03c1cadda0d343bc73d7e294c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cb646497c72dad852a421a23ec70496d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.30439671874046326, 0.21781255304813385, 0.36766862869262695, 0.2104901820421219, 0.4999464154243469, 0.27939164638519287, 0.25988686084747314, 0.1189364343881607, 0.2994995415210724, 0.053464632481336594, 0.15223415195941925, 0.3140793740749359, 0.2756928503513336, 0.16657672822475433, 0.45385628938674927, 0.4420536756515503, 0.3041861653327942, 0.20290148258209229, 0.4818771779537201, 0.09814286977052689, 0.006484943442046642, 0.14960959553718567, 0.2962746322154999, 0.10335888713598251, 0.21512149274349213, 0.01948840357363224, 0.005684700794517994, 0.31248870491981506, 0.3682391345500946, 0.0008572197402827442], dtype='float32').reshape([30]),
            paddle.to_tensor([0.4877505600452423, 0.15114635229110718, 0.446728378534317, 0.09036405384540558, 0.4522232115268707, 0.43811047077178955, 0.0669967457652092, 0.45778125524520874, 0.4767971634864807, 0.3230224549770355, 0.18490855395793915, 0.16170549392700195, 0.4702025353908539, 0.07593823224306107, 0.4578814208507538, 0.14695997536182404, 0.0872478038072586, 0.3857846260070801, 0.37611493468284607, 0.48697084188461304, 0.12827762961387634, 0.26869261264801025, 0.4175511598587036, 0.38264158368110657, 0.1914849728345871, 0.19404582679271698, 0.22152753174304962, 0.025083597749471664, 0.15249720215797424, 0.34787890315055847], dtype='float32').reshape([30]),
            paddle.to_tensor([0.3343515396118164, 0.14352181553840637, 0.4886323809623718, 0.4281727075576782, 0.057526845484972, 0.2890574336051941, 0.40466323494911194, 0.180758535861969, 0.28371360898017883, 0.2129100263118744, 0.028183670714497566, 0.016609514132142067, 0.07082889974117279, 0.23481161892414093, 0.32430148124694824, 0.11748263984918594, 0.08418575674295425, 0.40537548065185547, 0.32698413729667664, 0.21991309523582458, 0.00552069116383791, 0.473913311958313, 0.4129345417022705, 0.04416869208216667, 0.2881474494934082, 0.4522542655467987, 0.24353796243667603, 0.10985036194324493, 0.49037668108940125, 0.3576831817626953], dtype='float32').reshape([30]),
            paddle.to_tensor([0.3855731189250946, 0.446935772895813, 0.35586777329444885, 0.2719752788543701, 0.2853534519672394, 0.4629982113838196, 0.3022303879261017, 0.12241311371326447, 0.3355977237224579, 0.4975229501724243, 0.29024481773376465, 0.1705539971590042, 0.07231853157281876, 0.14654441177845, 0.31277433037757874, 0.40339118242263794, 0.259806364774704, 0.4301293194293976, 0.20373068749904633, 0.13214322924613953, 0.40802931785583496, 0.24453997611999512, 0.24944132566452026, 0.10334198921918869, 0.22932632267475128, 0.36633381247520447, 0.30946892499923706, 0.21754692494869232, 0.24301087856292725, 0.39052140712738037], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_872be6a3cfe01b41d82eed6601fd638f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_df724aab73d32f779ee768feb64dfbba(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 12, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.010971969924867153, 0.11190253496170044, 0.14255784451961517, 0.4316464066505432, 0.24661660194396973, 0.059559166431427, 0.19167745113372803, 0.13211284577846527, 0.24965821206569672, 0.1479104459285736, 0.41888678073883057, 0.2855115234851837], dtype='float32').reshape([12]),
            paddle.to_tensor([0.3739595413208008, 0.17403843998908997, 0.21312670409679413, 0.42283564805984497, 0.1489141583442688, 0.4969649314880371, 0.22406168282032013, 0.26510071754455566, 0.1602078527212143, 0.4898854196071625, 0.3808620572090149, 0.03392396494746208], dtype='float32').reshape([12]),
            paddle.to_tensor([0.3255692720413208, 0.4917465150356293, 0.22167110443115234, 0.40061432123184204, 0.027838466688990593, 0.36265963315963745, 0.08092043548822403, 0.44316792488098145, 0.3396701514720917, 0.3973066210746765, 0.339073121547699, 0.1866510510444641], dtype='float32').reshape([12]),
            paddle.to_tensor([0.3666575849056244, 0.007306423969566822, 0.24310575425624847, 0.09229452162981033, 0.016001306474208832, 0.3642447292804718, 0.26409393548965454, 0.3855333924293518, 0.08300526440143585, 0.4008297026157379, 0.2685966491699219, 0.44784095883369446], dtype='float32').reshape([12]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f386632edf474f2e8ba32a4ddaaa4641(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 480, 480], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.3355080187320709, 0.0534588098526001, 0.3343733251094818, 0.07660822570323944, 0.20774562656879425, 0.28444620966911316, 0.4379711449146271, 0.13857778906822205], dtype='float32').reshape([8]),
            paddle.to_tensor([0.12598749995231628, 0.41215020418167114, 0.33739814162254333, 0.03990131989121437, 0.3279729187488556, 0.36606141924858093, 0.41294771432876587, 0.02921437658369541], dtype='float32').reshape([8]),
            paddle.to_tensor([0.11927767097949982, 0.13831084966659546, 0.3114088773727417, 0.45316773653030396, 0.4356798827648163, 0.4293316602706909, 0.3044930398464203, 0.06761150062084198], dtype='float32').reshape([8]),
            paddle.to_tensor([0.28556135296821594, 0.34483447670936584, 0.4094705283641815, 0.4569131135940552, 0.035575006157159805, 0.36354678869247437, 0.0018195880111306906, 0.1395665556192398], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7e411c1f8e5cce96c98178671359afd1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.22263921797275543, 0.2986160218715668, 0.39772558212280273, 0.09348073601722717, 0.13569249212741852, 0.35474926233291626, 0.16412478685379028, 0.4981621503829956, 0.04263049736618996, 0.27825868129730225, 0.091071717441082, 0.054429423063993454, 0.16840843856334686, 0.09707754850387573, 0.020465653389692307, 0.2746732532978058, 0.1882639229297638, 0.21662366390228271], dtype='float32').reshape([18]),
            paddle.to_tensor([0.11360657215118408, 0.13680681586265564, 0.01381014660000801, 0.45669758319854736, 0.06802158057689667, 0.11568927019834518, 0.11349376291036606, 0.06958858668804169, 0.42938512563705444, 0.2007310688495636, 0.04280786216259003, 0.07172282040119171, 0.3468884527683258, 0.30392348766326904, 0.15446457266807556, 0.08838683366775513, 0.3609609603881836, 0.35815757513046265], dtype='float32').reshape([18]),
            paddle.to_tensor([0.08800947666168213, 0.35096052289009094, 0.4112798571586609, 0.04460647329688072, 0.4948486089706421, 0.08711428195238113, 0.4731200337409973, 0.028083087876439095, 0.1094556450843811, 0.327955037355423, 0.1594269722700119, 0.4505996108055115, 0.23472218215465546, 0.47820591926574707, 0.23834149539470673, 0.3655134439468384, 0.22367624938488007, 0.17289207875728607], dtype='float32').reshape([18]),
            paddle.to_tensor([0.17638227343559265, 0.34276315569877625, 0.42428112030029297, 0.35874006152153015, 0.32644256949424744, 0.47887495160102844, 0.4861557185649872, 0.24022947251796722, 0.24688339233398438, 0.26638588309288025, 0.24278993904590607, 0.15586067736148834, 0.3425460457801819, 0.49331995844841003, 0.21018202602863312, 0.21723514795303345, 0.3303264379501343, 0.2280408889055252], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_01d361c32a0ccc44897e6a1c3387465a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 160, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e5b02729044a2278eef5229a1654b347(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 24, 80], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f1fec932848472eef5cc919be5c3758c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a8301b810bccba10e4dfdee0df73334
    def get_inputs(self):
        return [
            paddle.uniform([1, 240, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_71c7b1fe753fafee6d664abb64416340(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 352, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.uniform([352], dtype='float32', min=0, max=0.5),
            paddle.uniform([352], dtype='float32', min=0, max=0.5),
            paddle.uniform([352], dtype='float32', min=0, max=0.5),
            paddle.uniform([352], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a05e2f30ac6fac3cc1ccb22d88405d95(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 160, 160], dtype='float16', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_457e6d15decd13cffd7eb24d8e667f0e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 100, 168], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f2b24eb78fdb0b1da8118e8b9bbb2534(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 112, 64, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f7cde1e3dc2d36571261679838498427(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 80, 80], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d90075db91063089be25a465b952987a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 32, 64], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.24799279868602753, 0.14172625541687012, 0.19507771730422974, 0.1491866111755371, 0.4383167028427124, 0.22712595760822296, 0.1890081912279129, 0.4380200505256653, 0.1812446266412735, 0.17948724329471588, 0.2831886112689972, 0.2913941442966461, 0.05272555351257324, 0.36827605962753296, 0.1868695318698883, 0.194802388548851, 0.4113174378871918, 0.21268436312675476], dtype='float32').reshape([18]),
            paddle.to_tensor([0.36899930238723755, 0.02941683679819107, 0.26221054792404175, 0.19437788426876068, 0.23603856563568115, 0.2626349925994873, 0.44346439838409424, 0.14140579104423523, 0.09818880259990692, 0.09223663806915283, 0.3541621267795563, 0.44499289989471436, 0.49629905819892883, 0.0015617839526385069, 0.271797776222229, 0.368521124124527, 0.25235795974731445, 0.3567448556423187], dtype='float32').reshape([18]),
            paddle.to_tensor([0.4236850440502167, 0.44510769844055176, 0.47778400778770447, 0.4989960193634033, 0.37893351912498474, 0.14730586111545563, 0.18813760578632355, 0.2831434905529022, 0.2259894162416458, 0.35471731424331665, 0.4959689974784851, 0.07590552419424057, 0.11613811552524567, 0.22103643417358398, 0.4708389937877655, 0.07665649801492691, 0.15865245461463928, 0.40785592794418335], dtype='float32').reshape([18]),
            paddle.to_tensor([0.31738588213920593, 0.08971311897039413, 0.19477364420890808, 0.2470170557498932, 0.47583892941474915, 0.24774332344532013, 0.2861938774585724, 0.40980592370033264, 0.3365879952907562, 0.03744800016283989, 0.19547614455223083, 0.09618599712848663, 0.24457739293575287, 0.0528307780623436, 0.448303759098053, 0.4575270712375641, 0.04921085014939308, 0.3538724482059479], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e1705c35ab720cbb03a8364e6fab7b2b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.39954251050949097, 0.15127351880073547, 0.37985917925834656, 0.1983891725540161, 0.339495450258255, 0.057905301451683044, 0.17647014558315277, 0.07311281561851501, 0.32915163040161133, 0.38305649161338806, 0.25575873255729675, 0.22834402322769165, 0.4414893686771393, 0.14996489882469177, 0.3404552936553955, 0.062271393835544586, 0.2727900743484497, 0.31016087532043457, 0.28596657514572144, 0.2085258662700653], dtype='float32').reshape([20]),
            paddle.to_tensor([0.3730846643447876, 0.18108125030994415, 0.41482973098754883, 0.03159201890230179, 0.16264048218727112, 0.08050961792469025, 0.3367565870285034, 0.302109956741333, 0.3612684905529022, 0.24770425260066986, 0.24081356823444366, 0.06960392743349075, 0.04278367757797241, 0.28646335005760193, 0.04843856766819954, 0.28691744804382324, 0.0584227629005909, 0.4119460880756378, 0.3376295566558838, 0.1997697502374649], dtype='float32').reshape([20]),
            paddle.to_tensor([0.23389464616775513, 0.029715577140450478, 0.3676685392856598, 0.18778479099273682, 0.33721333742141724, 0.21527475118637085, 0.4513220191001892, 0.448011189699173, 0.3019184172153473, 0.4002567529678345, 0.10988368093967438, 0.40270861983299255, 0.031291134655475616, 0.15326836705207825, 0.4127669930458069, 0.491690456867218, 0.43008115887641907, 0.26175326108932495, 0.33959248661994934, 0.05257655307650566], dtype='float32').reshape([20]),
            paddle.to_tensor([0.20280681550502777, 0.11282870918512344, 0.002365127205848694, 0.19226936995983124, 0.46493691205978394, 0.12045587599277496, 0.4615086615085602, 0.2132083922624588, 0.44272539019584656, 0.4889204204082489, 0.1989990919828415, 0.16398774087429047, 0.1389104127883911, 0.17539070546627045, 0.37515556812286377, 0.0014008868020027876, 0.3956581652164459, 0.3847081959247589, 0.14323130249977112, 0.45977750420570374], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4fe2f7e1f9e5f7542105523a4dc622be(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_609f3848864125a9edc326d79663007d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.2007809579372406, 0.23915298283100128, 0.1081668883562088, 0.34331098198890686, 0.352370947599411, 0.3011734187602997, 0.3114052414894104, 0.463398814201355, 0.4300600588321686, 0.016770323738455772, 0.3045114278793335, 0.2963683307170868, 0.27874070405960083, 0.11156574636697769, 0.04652423784136772, 0.021809272468090057, 0.38747936487197876, 0.25035956501960754], dtype='float32').reshape([18]),
            paddle.to_tensor([0.424658864736557, 0.06450815498828888, 0.3275846838951111, 0.4952983260154724, 0.15771456062793732, 0.30693766474723816, 0.32846298813819885, 0.44170141220092773, 0.2348848581314087, 0.054501812905073166, 0.22721552848815918, 0.19708696007728577, 0.09619662165641785, 0.45305880904197693, 0.46212032437324524, 0.061483439058065414, 0.1818685680627823, 0.45800334215164185], dtype='float32').reshape([18]),
            paddle.to_tensor([0.144159197807312, 0.2568301260471344, 0.4755570590496063, 0.13807269930839539, 0.2892681062221527, 0.25775283575057983, 0.39581212401390076, 0.26633307337760925, 0.21405288577079773, 0.10400541126728058, 0.1410893350839615, 0.39775562286376953, 0.35469120740890503, 0.4091640114784241, 0.42234310507774353, 0.0793369710445404, 0.4308519959449768, 0.31704241037368774], dtype='float32').reshape([18]),
            paddle.to_tensor([0.4382997453212738, 0.3227030336856842, 0.36085307598114014, 0.24642181396484375, 0.20463261008262634, 0.0015634531155228615, 0.491957426071167, 0.2989373207092285, 0.11233823001384735, 0.05654294788837433, 0.3324660658836365, 0.14882932603359222, 0.44567662477493286, 0.2708792984485626, 0.3891890048980713, 0.2853148579597473, 0.19381771981716156, 0.12788815796375275], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5a6366bf16393e3cee09a2f68b437140(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_da4d85bc213f70819a72e68f21214cdb
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 32, 32, 32], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.28256654739379883, 0.3979782462120056, 0.4607546329498291, 0.14746227860450745, 0.2723684310913086, 0.16650494933128357, 0.3375545144081116, 0.3986313045024872, 0.30934402346611023, 0.2537785470485687, 0.03717183321714401, 0.4641837179660797, 0.2873374819755554, 0.4856111705303192, 0.03774118423461914, 0.16603830456733704], dtype='float32').reshape([16]),
            paddle.to_tensor([0.019944550469517708, 0.11179050803184509, 0.3441271185874939, 0.27392077445983887, 0.22293083369731903, 0.4278881847858429, 0.17312034964561462, 0.11149239540100098, 0.42957839369773865, 0.11829625815153122, 0.2889912724494934, 0.3930652439594269, 0.29174187779426575, 0.4774002134799957, 0.39693236351013184, 0.1548103541135788], dtype='float32').reshape([16]),
            paddle.to_tensor([0.08010503649711609, 0.32519614696502686, 0.36802515387535095, 0.012081549502909184, 0.4204356372356415, 0.40238046646118164, 0.46276310086250305, 0.4563332796096802, 0.017654692754149437, 0.10888227820396423, 0.2947086691856384, 0.38789671659469604, 0.41085726022720337, 0.0021407236345112324, 0.07688910514116287, 0.4947918653488159], dtype='float32').reshape([16]),
            paddle.to_tensor([0.3111701011657715, 0.08068632334470749, 0.2974948585033417, 0.057039931416511536, 0.16192717850208282, 0.45909205079078674, 0.4243197739124298, 0.31847381591796875, 0.05117382854223251, 0.038436975330114365, 0.1631823480129242, 0.4413357377052307, 0.017531855031847954, 0.2795260548591614, 0.4627324938774109, 0.08834078907966614], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_920a5b962387434be1113d06edc6ea78(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.04110073298215866, 0.07980474084615707, 0.19433511793613434, 0.2028803527355194, 0.03350970521569252, 0.4572429955005646, 0.2614762485027313, 0.24783305823802948, 0.49807503819465637, 0.34336236119270325, 0.44271349906921387, 0.15482527017593384, 0.3034730851650238, 0.29212433099746704, 0.362785667181015, 0.3101728856563568, 0.015626784414052963, 0.363288015127182], dtype='float32').reshape([18]),
            paddle.to_tensor([0.20310454070568085, 0.4278510510921478, 0.27609485387802124, 0.35320213437080383, 0.24958817660808563, 0.26986780762672424, 0.4570278227329254, 0.3844124376773834, 0.2840942144393921, 0.36331111192703247, 0.35189715027809143, 0.2212822139263153, 0.2121056765317917, 0.22423891723155975, 0.4386495351791382, 0.11250549554824829, 0.33229008316993713, 0.08931140601634979], dtype='float32').reshape([18]),
            paddle.to_tensor([0.15024231374263763, 0.05313399061560631, 0.06485848873853683, 0.19783146679401398, 0.4565262794494629, 0.3155386745929718, 0.27712786197662354, 0.09022169560194016, 0.14644160866737366, 0.08504678308963776, 0.12330540269613266, 0.20776768028736115, 0.43887919187545776, 0.05492640286684036, 0.12077908217906952, 0.38998129963874817, 0.35273146629333496, 0.4138531982898712], dtype='float32').reshape([18]),
            paddle.to_tensor([0.1910146325826645, 0.4254254698753357, 0.13251513242721558, 0.18529553711414337, 0.06462164968252182, 0.29646772146224976, 0.4582134187221527, 0.3214628994464874, 0.27389758825302124, 0.22860868275165558, 0.03395804017782211, 0.13162173330783844, 0.15179400146007538, 0.1664271503686905, 0.024542702361941338, 0.05151062086224556, 0.4908565282821655, 0.16697479784488678], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4d9a8a443e1c3d13ad1ad74bf2fbdee6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 128, 128], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.1905403435230255, 0.43634265661239624, 0.28273025155067444, 0.4873145520687103, 0.4018479883670807, 0.15850086510181427, 0.37519678473472595, 0.1946021169424057, 0.43304556608200073, 0.284395307302475, 0.3345547914505005, 0.05288126692175865, 0.26389503479003906, 0.4817720949649811, 0.3651069104671478, 0.2381967455148697], dtype='float32').reshape([16]),
            paddle.to_tensor([0.19221095740795135, 0.20330756902694702, 0.4185604453086853, 0.49140414595603943, 0.21781091392040253, 0.3717179298400879, 0.16527849435806274, 0.04531078785657883, 0.18090350925922394, 0.04925531893968582, 0.33748891949653625, 0.09933681041002274, 0.28208813071250916, 0.14614297449588776, 0.33266180753707886, 0.09540489315986633], dtype='float32').reshape([16]),
            paddle.to_tensor([0.257991760969162, 0.2350679337978363, 0.10583963990211487, 0.36485347151756287, 0.10181505978107452, 0.20656023919582367, 0.06788879632949829, 0.4023555815219879, 0.07851855456829071, 0.3830372989177704, 0.39371487498283386, 0.30428680777549744, 0.2917814552783966, 0.2908487617969513, 0.2161237895488739, 0.41098520159721375], dtype='float32').reshape([16]),
            paddle.to_tensor([0.24317830801010132, 0.2960241734981537, 0.13533881306648254, 0.25509509444236755, 0.4902603328227997, 0.3374309539794922, 0.41594943404197693, 0.13398391008377075, 0.09202152490615845, 0.05520746111869812, 0.33513501286506653, 0.010820126160979271, 0.2513609826564789, 0.055643726140260696, 0.12449011951684952, 0.46635955572128296], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_43897f2279f2df8871051a5e38681501(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 92, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([92], dtype='float32', min=0, max=0.5),
            paddle.uniform([92], dtype='float32', min=0, max=0.5),
            paddle.uniform([92], dtype='float32', min=0, max=0.5),
            paddle.uniform([92], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_351d489a08a600c6f0ba0a3e1de05cf8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 2688, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([2688], dtype='float32', min=0, max=0.5),
            paddle.uniform([2688], dtype='float32', min=0, max=0.5),
            paddle.uniform([2688], dtype='float32', min=0, max=0.5),
            paddle.uniform([2688], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0db739de350f7078b088c428efe1c90a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.18221749365329742, 0.027884386479854584, 0.09069342911243439, 0.06260400265455246, 0.27940404415130615, 0.26809337735176086, 0.25289666652679443, 0.2687426507472992, 0.49316343665122986, 0.2498364895582199, 0.4024624228477478, 0.14978307485580444, 0.061269037425518036, 0.04878753423690796, 0.201187863945961, 0.4215083122253418, 0.430092990398407, 0.4767833352088928, 0.3556042015552521, 0.043344125151634216, 0.10572664439678192, 0.45487353205680847, 0.3515204191207886, 0.18829640746116638, 0.32677504420280457, 0.3290795683860779, 0.24765461683273315, 0.4664349853992462, 0.002546965144574642, 0.03804337978363037], dtype='float32').reshape([30]),
            paddle.to_tensor([0.09250861406326294, 0.23466582596302032, 0.11964970082044601, 0.02589503675699234, 0.4667080044746399, 0.19897860288619995, 0.08998525142669678, 0.22565138339996338, 0.020085977390408516, 0.4271974265575409, 0.3455200791358948, 0.03728155419230461, 0.0013875628355890512, 0.27787551283836365, 0.13279210031032562, 0.44388848543167114, 0.4151539206504822, 0.00404586736112833, 0.3175371587276459, 0.22283287346363068, 0.2716113328933716, 0.3180884122848511, 0.45113489031791687, 0.0787414163351059, 0.0757506787776947, 0.0906440019607544, 0.4535024166107178, 0.10653428733348846, 0.4011232852935791, 0.19413968920707703], dtype='float32').reshape([30]),
            paddle.to_tensor([0.48264962434768677, 0.23397883772850037, 0.45071765780448914, 0.3570820391178131, 0.2945723533630371, 0.04261302947998047, 0.3394780158996582, 0.4542023539543152, 0.024913650006055832, 0.23390048742294312, 0.09722387790679932, 0.12048714607954025, 0.04416975751519203, 0.04198349639773369, 0.45208513736724854, 0.310738205909729, 0.3519769310951233, 0.03376207500696182, 0.4785330593585968, 0.3815457224845886, 0.421942800283432, 0.44026967883110046, 0.2624274790287018, 0.03609949350357056, 0.2888634204864502, 0.28292107582092285, 0.2959052324295044, 0.06086591258645058, 0.44119611382484436, 0.2326536625623703], dtype='float32').reshape([30]),
            paddle.to_tensor([0.02154427580535412, 0.3408670425415039, 0.2651621103286743, 0.32925546169281006, 0.4621373414993286, 0.0759907066822052, 0.3132810592651367, 0.2909940183162689, 0.15797854959964752, 0.06881022453308105, 0.4797218143939972, 0.3528888523578644, 0.1176532581448555, 0.08654823899269104, 0.2752058207988739, 0.02582290582358837, 0.3805060386657715, 0.005776942241936922, 0.17188574373722076, 0.2556692957878113, 0.14698529243469238, 0.4048204720020294, 0.3303859829902649, 0.46115681529045105, 0.037387192249298096, 0.2335107922554016, 0.43391430377960205, 0.48882871866226196, 0.3687888979911804, 0.31898728013038635], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ec9c5efc4345e6d97fad0ed47fe1f58b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 16, 50], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5f9b44244e4a66db980429f6c981bb22(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 128, 128], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_dda527b1541624a954c00902abd22584(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_da4d85bc213f70819a72e68f21214cdb
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 32, 64, 64], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.421832412481308, 0.2527824342250824, 0.48626330494880676, 0.3203010857105255, 0.42810681462287903, 0.33033061027526855, 0.2144729197025299, 0.37018606066703796], dtype='float32').reshape([8]),
            paddle.to_tensor([0.07189399749040604, 0.48849406838417053, 0.09975896775722504, 0.25124189257621765, 0.022613316774368286, 0.22457388043403625, 0.3547191917896271, 0.028862548992037773], dtype='float32').reshape([8]),
            paddle.to_tensor([0.22004340589046478, 0.15862597525119781, 0.4724113643169403, 0.33791249990463257, 0.44735267758369446, 0.4491106867790222, 0.4127785563468933, 0.002418187912553549], dtype='float32').reshape([8]),
            paddle.to_tensor([0.08802466094493866, 0.2891612648963928, 0.1525256484746933, 0.028777334839105606, 0.29614681005477905, 0.3766920268535614, 0.266349196434021, 0.3416917324066162], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7e746e3013ad885c814ef8db594b1a65(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 80, 16, 12], dtype='float16', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_52d1e63f4a78ab7f595fa033bcb00df1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 256, 512], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.2841070592403412, 0.05158810317516327, 0.41414782404899597, 0.2467608004808426, 0.01628127135336399, 0.11288353055715561, 0.0447397455573082, 0.0921068862080574, 0.2290669083595276, 0.44997742772102356, 0.29044467210769653, 0.4692121744155884, 0.14585484564304352, 0.09564375877380371, 0.4465300440788269, 0.19700276851654053], dtype='float32').reshape([16]),
            paddle.to_tensor([0.18881107866764069, 0.329091876745224, 0.01544775441288948, 0.3754009008407593, 0.01583736389875412, 0.2629702389240265, 0.2903956472873688, 0.3151063323020935, 0.4047122001647949, 0.3059264123439789, 0.08728624135255814, 0.08333130180835724, 0.3311116099357605, 0.41204002499580383, 0.2825748920440674, 0.02162238396704197], dtype='float32').reshape([16]),
            paddle.to_tensor([0.19966182112693787, 0.07338766753673553, 0.045419368892908096, 0.4169590175151825, 0.25989601016044617, 0.017291134223341942, 0.10818339884281158, 0.31683486700057983, 0.21548303961753845, 0.42047107219696045, 0.45499587059020996, 0.06833687424659729, 0.10053741931915283, 0.024446895346045494, 0.01782406121492386, 0.1800174117088318], dtype='float32').reshape([16]),
            paddle.to_tensor([0.07122008502483368, 0.15110325813293457, 0.041162680834531784, 0.20437094569206238, 0.253040075302124, 0.15545806288719177, 0.1788417249917984, 0.2983595132827759, 0.14173345267772675, 0.15623916685581207, 0.017373401671648026, 0.3787996172904968, 0.0007058459450490773, 0.2742902934551239, 0.40273144841194153, 0.1806335151195526], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4b7903e6ddf4c576d247b7195701864f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.24846923351287842, 0.28890812397003174, 0.2774921953678131, 0.00011139822890982032, 0.35125526785850525, 0.3514244258403778, 0.43226221203804016, 0.3552846312522888, 0.28787317872047424, 0.1430344432592392, 0.08084563910961151, 0.06466175615787506, 0.0474274642765522, 0.4503960907459259, 0.391886442899704, 0.48157310485839844], dtype='float32').reshape([16]),
            paddle.to_tensor([0.3217342495918274, 0.3353433907032013, 0.25868430733680725, 0.3603476583957672, 0.33353191614151, 0.04996732622385025, 0.3330519199371338, 0.18576370179653168, 0.07441051304340363, 0.35860633850097656, 0.11407496780157089, 0.39653733372688293, 0.07638707011938095, 0.08764549344778061, 0.26735711097717285, 0.15352153778076172], dtype='float32').reshape([16]),
            paddle.to_tensor([0.40966978669166565, 0.3064253628253937, 0.45799171924591064, 0.25113898515701294, 0.2746080458164215, 0.34668204188346863, 0.06343299895524979, 0.43812254071235657, 0.46276578307151794, 0.3197934627532959, 0.4475984573364258, 0.22418303787708282, 0.41455358266830444, 0.005218040198087692, 0.12312853336334229, 0.2537033259868622], dtype='float32').reshape([16]),
            paddle.to_tensor([0.007544658146798611, 0.41519200801849365, 0.3895297348499298, 0.1682150810956955, 0.3792531490325928, 0.11187458783388138, 0.10216014087200165, 0.08042945712804794, 0.2602338492870331, 0.16664916276931763, 0.05603671073913574, 0.07134684920310974, 0.13488493859767914, 0.23308533430099487, 0.061441559344530106, 0.4871307611465454], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d7a8d6faa735c9663483a6e184463e36(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 60, 60], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ad721699f743ed199858eb3f343bb7a7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 53, 1, 1], dtype='float32', min=0, max=0.5),
            paddle.uniform([53], dtype='float32', min=0, max=0.5),
            paddle.uniform([53], dtype='float32', min=0, max=0.5),
            paddle.uniform([53], dtype='float32', min=0, max=0.5),
            paddle.uniform([53], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6fa22984d305dc31090619ba02a64a93(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 1, 1], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ff3cf58da639b10a6278ccfa88bf753d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 80, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_51d4383fbf12e184cee6696049d34f6c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 35, 35], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ac434b6ae9300a06563f00b6ebcc6000(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.4598065912723541, 0.33019497990608215, 0.35036027431488037, 0.33309176564216614, 0.13857720792293549, 0.26505085825920105, 0.25889232754707336, 0.17412176728248596, 0.37654343247413635, 0.10787484794855118, 0.4647562503814697, 0.30280059576034546, 0.4726579487323761, 0.4899229407310486, 0.15222342312335968, 0.3591538369655609, 0.10665282607078552, 0.06894321739673615], dtype='float32').reshape([18]),
            paddle.to_tensor([0.48240944743156433, 0.04730997607111931, 0.139243483543396, 0.17873352766036987, 0.3369831144809723, 0.47521182894706726, 0.07203472405672073, 0.21683573722839355, 0.3935978412628174, 0.3998720645904541, 0.3132718801498413, 0.3289719521999359, 0.09461760520935059, 0.40247929096221924, 0.15972869098186493, 0.06345507502555847, 0.054536014795303345, 0.4502733051776886], dtype='float32').reshape([18]),
            paddle.to_tensor([0.06597411632537842, 0.33721086382865906, 0.0056871394626796246, 0.29806429147720337, 0.2414884716272354, 0.16303040087223053, 0.08345954865217209, 0.4256340265274048, 0.08541528135538101, 0.3843937814235687, 0.46513381600379944, 0.3570408821105957, 0.4558047354221344, 0.4666104018688202, 0.007127347402274609, 0.037312474101781845, 0.44719135761260986, 0.07335250824689865], dtype='float32').reshape([18]),
            paddle.to_tensor([0.08665196597576141, 0.18499191105365753, 0.09760475158691406, 0.12634487450122833, 0.149325892329216, 0.2737066447734833, 0.15373890101909637, 0.14134426414966583, 0.2781805396080017, 0.09104391187429428, 0.12166060507297516, 0.46261003613471985, 0.15693485736846924, 0.17575180530548096, 0.31624239683151245, 0.2716474235057831, 0.3911878168582916, 0.47094079852104187], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_569f2110adae05b92f833921bf9ea207(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.14710770547389984, 0.4043789207935333, 0.07800144702196121, 0.19344379007816315, 0.13762818276882172, 0.4301188588142395, 0.3255150616168976, 0.1391749083995819, 0.2204231470823288, 0.1950056254863739, 0.03833065927028656, 0.45257553458213806, 0.17183491587638855, 0.4056456387042999, 0.43884024024009705, 0.024966835975646973, 0.3934510052204132, 0.4189518690109253, 0.31755173206329346, 0.1602834314107895], dtype='float32').reshape([20]),
            paddle.to_tensor([0.2714584171772003, 0.49916496872901917, 0.4053553640842438, 0.39939096570014954, 0.3010199964046478, 0.18809829652309418, 0.021279405802488327, 0.41374096274375916, 0.18953341245651245, 0.4075284004211426, 0.020963968709111214, 0.27256178855895996, 0.2911760210990906, 0.03323112055659294, 0.20890246331691742, 0.31017884612083435, 0.07963541895151138, 0.48650795221328735, 0.4869252145290375, 0.19684623181819916], dtype='float32').reshape([20]),
            paddle.to_tensor([0.1567264348268509, 0.3002433776855469, 0.4023992717266083, 0.4340605139732361, 0.26673564314842224, 0.30865833163261414, 0.18853184580802917, 0.3520280122756958, 0.19214315712451935, 0.31894341111183167, 0.061565905809402466, 0.23411908745765686, 0.1170124039053917, 0.05017220228910446, 0.04041101410984993, 0.380598783493042, 0.414715439081192, 0.06077246740460396, 0.007212481927126646, 0.11852222681045532], dtype='float32').reshape([20]),
            paddle.to_tensor([0.38219645619392395, 0.1395908147096634, 0.06777824461460114, 0.479923278093338, 0.461467981338501, 0.3664647936820984, 0.3665672540664673, 0.26130780577659607, 0.31527817249298096, 0.380205363035202, 0.008744015358388424, 0.3632689416408539, 0.2807125151157379, 0.24646998941898346, 0.3439599871635437, 0.2629995346069336, 0.2935830056667328, 0.11679999530315399, 0.3104405105113983, 0.029148688539862633], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4d7b7d81a837269c3e4ec09bb17ee163(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 16, 16], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e9b09d7e83219ea210cef832a196ff27(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 256, 256], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.15654169023036957, 0.04095393791794777, 0.20566053688526154, 0.15093053877353668, 0.20651115477085114, 0.43065404891967773, 0.3526543378829956, 0.014867044053971767, 0.03404199331998825, 0.2839415669441223, 0.491491436958313, 0.3021153211593628, 0.2798033058643341, 0.24106863141059875, 0.46725207567214966, 0.4760304093360901], dtype='float32').reshape([16]),
            paddle.to_tensor([0.26805803179740906, 0.22997942566871643, 0.09309916198253632, 0.32629191875457764, 0.2862403690814972, 0.2357596904039383, 0.12960372865200043, 0.3764405846595764, 0.18814867734909058, 0.26039788126945496, 0.23679578304290771, 0.45632120966911316, 0.11300119012594223, 0.046398043632507324, 0.4027366638183594, 0.33062201738357544], dtype='float32').reshape([16]),
            paddle.to_tensor([0.27029943466186523, 0.49630942940711975, 0.329702764749527, 0.1308601200580597, 0.25335371494293213, 0.3085786998271942, 0.42154473066329956, 0.03564558923244476, 0.12070991843938828, 0.135796457529068, 0.3380061089992523, 0.36210519075393677, 0.30268827080726624, 0.20029672980308533, 0.40548965334892273, 0.07779071480035782], dtype='float32').reshape([16]),
            paddle.to_tensor([0.07342857122421265, 0.24914206564426422, 0.42736783623695374, 0.2701031267642975, 0.4149610996246338, 0.40304791927337646, 0.48059380054473877, 0.12805400788784027, 0.34238889813423157, 0.22698752582073212, 0.3959604501724243, 0.24382436275482178, 0.45397770404815674, 0.42221561074256897, 0.4514358639717102, 0.3946751058101654], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0c87b2188742ffe979f96cc12f100718(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.22652840614318848, 0.3166642487049103, 0.4505675733089447, 0.3137783706188202, 0.10271576792001724, 0.20641647279262543, 0.06484629213809967, 0.048054348677396774, 0.4332640767097473, 0.07156814634799957, 0.1990598440170288, 0.23125059902668, 0.37539732456207275, 0.18966898322105408, 0.12274560332298279, 0.4468165934085846, 0.01742309331893921, 0.22242112457752228, 0.13000334799289703, 0.017199959605932236, 0.019962230697274208, 0.39246001839637756, 0.049784161150455475, 0.3104124963283539, 0.21177291870117188, 0.30956533551216125, 0.2501697242259979, 0.2306261658668518, 0.15068283677101135, 0.40080398321151733], dtype='float32').reshape([30]),
            paddle.to_tensor([0.29524025321006775, 0.023141581565141678, 0.4414426386356354, 0.3646581470966339, 0.31709590554237366, 0.2763937711715698, 0.14620861411094666, 0.27298009395599365, 0.2894876301288605, 0.18397925794124603, 0.34515976905822754, 0.256679892539978, 0.40217986702919006, 0.22254949808120728, 0.36338356137275696, 0.09571443498134613, 0.40035635232925415, 0.3544953763484955, 0.11883514374494553, 0.3526189625263214, 0.3682982325553894, 0.20368163287639618, 0.1949504315853119, 0.003749351017177105, 0.10765251517295837, 0.3852761685848236, 0.2856394052505493, 0.47485172748565674, 0.04906081780791283, 0.14749504625797272], dtype='float32').reshape([30]),
            paddle.to_tensor([0.42616578936576843, 0.02842770144343376, 0.04449167475104332, 0.4301905333995819, 0.14309771358966827, 0.10750452429056168, 0.059284765273332596, 0.09133099019527435, 0.024579977616667747, 0.2637415826320648, 0.030796952545642853, 0.11340265721082687, 0.11421149224042892, 0.07177510112524033, 0.38121068477630615, 0.24509631097316742, 0.2541744112968445, 0.12421808391809464, 0.02527211606502533, 0.3203932046890259, 0.2978340983390808, 0.2811404764652252, 0.00574308168143034, 0.25810980796813965, 0.256593257188797, 0.13327310979366302, 0.16519670188426971, 0.04707585275173187, 0.05409829691052437, 0.20936724543571472], dtype='float32').reshape([30]),
            paddle.to_tensor([0.47894296050071716, 0.45094549655914307, 0.2599342465400696, 0.29785555601119995, 0.2316792905330658, 0.47356101870536804, 0.39790064096450806, 0.0018142671324312687, 0.3199644684791565, 0.4967423975467682, 0.10814768075942993, 0.1569146364927292, 0.46517670154571533, 0.1587831676006317, 0.3216966688632965, 0.31621354818344116, 0.3168928921222687, 0.3469478487968445, 0.21355415880680084, 0.46650081872940063, 0.4662807583808899, 0.22338582575321198, 0.11086767911911011, 0.39712366461753845, 0.0041160909458994865, 0.4446176588535309, 0.06597473472356796, 0.025692887604236603, 0.29883310198783875, 0.1779503971338272], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d24a0789445739f1042ec6b7f9754f50(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 112, 112], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.06870770454406738, 0.03457486256957054, 0.2489832639694214, 0.38661330938339233, 0.48081129789352417, 0.0356837622821331, 0.4227152466773987, 0.49241170287132263, 0.20020191371440887, 0.3599739670753479, 0.14188477396965027, 0.00599950086325407, 0.44939762353897095, 0.22855672240257263, 0.07622066140174866, 0.4864140748977661, 0.19651499390602112, 0.042629748582839966, 0.023206671699881554, 0.19244028627872467, 0.1659107655286789, 0.03432176262140274, 0.41190850734710693, 0.4680365025997162], dtype='float32').reshape([24]),
            paddle.to_tensor([0.33973604440689087, 0.09449790418148041, 0.23541349172592163, 0.4186634123325348, 0.002250443212687969, 0.3767870366573334, 0.44778385758399963, 0.47322219610214233, 0.29410606622695923, 0.0960729643702507, 0.25220996141433716, 0.3830757737159729, 0.33855658769607544, 0.4508135914802551, 0.38715076446533203, 0.49116116762161255, 0.42112839221954346, 0.4693698585033417, 0.3237314224243164, 0.23551076650619507, 0.28565120697021484, 0.2402632087469101, 0.1545013189315796, 0.32478514313697815], dtype='float32').reshape([24]),
            paddle.to_tensor([0.00367109221406281, 0.4199059009552002, 0.13237622380256653, 0.11715070903301239, 0.2247471660375595, 0.1513284295797348, 0.3582405149936676, 0.09318912774324417, 0.4389954209327698, 0.03185581788420677, 0.08147179335355759, 0.35966676473617554, 0.1903722584247589, 0.24445891380310059, 0.36591455340385437, 0.11801798641681671, 0.14216728508472443, 0.05913398042321205, 0.436002254486084, 0.3173075020313263, 0.22821921110153198, 0.32890966534614563, 0.005230074282735586, 0.3026920258998871], dtype='float32').reshape([24]),
            paddle.to_tensor([0.42007631063461304, 0.26801225543022156, 0.37806618213653564, 0.08337031304836273, 0.10714317113161087, 0.04736192524433136, 0.1308641880750656, 0.342129111289978, 0.02568628080189228, 0.012568900361657143, 0.007477098610252142, 0.2467462569475174, 0.3398122191429138, 0.3767983317375183, 0.3824492394924164, 0.058221448212862015, 0.1518440693616867, 0.20221444964408875, 0.03282453492283821, 0.41618216037750244, 0.10204355418682098, 0.4191493093967438, 0.42950439453125, 0.056866224855184555], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e17ccab8683234328cdfc2eb162c541c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 162, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.uniform([162], dtype='float32', min=0, max=0.5),
            paddle.uniform([162], dtype='float32', min=0, max=0.5),
            paddle.uniform([162], dtype='float32', min=0, max=0.5),
            paddle.uniform([162], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e70e2808df4f3a71533d7c8c4e295fea(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 104, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
            paddle.uniform([104], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_506f37d223f243152f90bc806a124c3a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cc1faf6f796f1bb90da08d6fb8b18275(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c05e3af1597b4a6ae7b92f7e3e77941e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 336, 60, 60], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_876d3d31e6507da55f65d8f3e0226555(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 80, 16, 12], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4d18c1cfcc58c5093d1779f277da386e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 16, 16], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_19bf7f9a6d526141121a149b4396b53e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 4, 50], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.07727601379156113, 0.4626077115535736, 0.13903507590293884, 0.14859361946582794, 0.20772621035575867, 0.27390336990356445, 0.3177374303340912, 0.40750211477279663, 0.2920309007167816, 0.14399117231369019, 0.10719922930002213, 0.4083929657936096, 0.29312312602996826, 0.21004794538021088, 0.3295398950576782, 0.313930481672287, 0.025004113093018532, 0.17422202229499817, 0.4999569356441498, 0.047319259494543076, 0.3075467348098755, 0.008491799235343933, 0.22080568969249725, 0.21685238182544708], dtype='float32').reshape([24]),
            paddle.to_tensor([0.4926542043685913, 0.10091962665319443, 0.0864095389842987, 0.18357719480991364, 0.16862702369689941, 0.39803647994995117, 0.03239114210009575, 0.12383083254098892, 0.16302864253520966, 0.18025195598602295, 0.26739856600761414, 0.41860926151275635, 0.015809539705514908, 0.0372760072350502, 0.0455969013273716, 0.34337759017944336, 0.23436671495437622, 0.3718791902065277, 0.22397826611995697, 0.11864030361175537, 0.3372179865837097, 0.3148716986179352, 0.10779713839292526, 0.3078821301460266], dtype='float32').reshape([24]),
            paddle.to_tensor([0.2788815200328827, 0.05032026022672653, 0.44466158747673035, 0.014139383099973202, 0.10586868971586227, 0.41146448254585266, 0.48897525668144226, 0.3397531509399414, 0.1717054694890976, 0.43262025713920593, 0.16647054255008698, 0.45571017265319824, 0.3251872658729553, 0.0967533141374588, 0.3389335572719574, 0.44240105152130127, 0.4235025942325592, 0.44340771436691284, 0.0797153040766716, 0.43074560165405273, 0.13354285061359406, 0.1851363629102707, 0.13104835152626038, 0.4335446357727051], dtype='float32').reshape([24]),
            paddle.to_tensor([0.277635395526886, 0.1340292990207672, 0.4263548254966736, 0.40129920840263367, 0.18929752707481384, 0.13215647637844086, 0.11503130942583084, 0.22042573988437653, 0.05006910488009453, 0.34812864661216736, 0.08361533284187317, 0.13650207221508026, 0.08742538094520569, 0.49452030658721924, 0.274489164352417, 0.48004859685897827, 0.20988816022872925, 0.27606672048568726, 0.23821084201335907, 0.3495253622531891, 0.05497997999191284, 0.20796729624271393, 0.3535315692424774, 0.14279985427856445], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_610ce5b56b1d3f7c5bdbb6dd3f2b9c52(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 336, 40, 40], dtype='float16', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_48604b98b1458fb0e9218ec4a4763146(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 128, 128], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5aa1d301a9c0bc7f66585b24d6eed571(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 16, 16], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2bcefde44639e3e32ddeb3fb6c2dff1b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_85d64e7a01b1c525040660bb4ac7d8ce(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.3409714698791504, 0.22161971032619476, 0.13616465032100677, 0.18750378489494324, 0.3853612542152405, 0.2803306579589844, 0.061764299869537354, 0.10739486664533615, 0.44840288162231445, 0.3641202747821808, 0.1687140166759491, 0.27525052428245544, 0.009280761703848839, 0.23272204399108887, 0.3929356634616852, 0.15067459642887115, 0.18881221115589142, 0.06721877306699753, 0.4306006133556366, 0.2175961285829544, 0.12969952821731567, 0.0009703385294415057, 0.08134440332651138, 0.3684741258621216, 0.3915930390357971, 0.19599321484565735, 0.11882113665342331, 0.03289435803890228, 0.17842267453670502, 0.48461979627609253], dtype='float32').reshape([30]),
            paddle.to_tensor([0.414869487285614, 0.47876134514808655, 0.4758942425251007, 0.07293187081813812, 0.36362332105636597, 0.2948148548603058, 0.41143563389778137, 0.09589359164237976, 0.07731804251670837, 0.11464503407478333, 0.34977519512176514, 0.22162896394729614, 0.035377852618694305, 0.041265662759542465, 0.2825075685977936, 0.04067450389266014, 0.43637171387672424, 0.05433954671025276, 0.20015689730644226, 0.07300090044736862, 0.02190851978957653, 0.3435197174549103, 0.07502210885286331, 0.19148188829421997, 0.16231952607631683, 0.027008172124624252, 0.16593429446220398, 0.030396761372685432, 0.16900058090686798, 0.36538267135620117], dtype='float32').reshape([30]),
            paddle.to_tensor([0.24308836460113525, 0.18236809968948364, 0.18008191883563995, 0.35156625509262085, 0.1532025784254074, 0.2390211522579193, 0.4861137270927429, 0.2673192620277405, 0.31157591938972473, 0.4354707598686218, 0.3267441689968109, 0.0689823105931282, 0.04255767911672592, 0.3247460424900055, 0.4629960358142853, 0.3449765741825104, 0.20074965059757233, 0.34861552715301514, 0.1726175844669342, 0.07257265597581863, 0.2132774144411087, 0.4028506278991699, 0.45401066541671753, 0.4640820324420929, 0.3197530508041382, 0.4552513659000397, 0.21873170137405396, 0.3936921954154968, 0.14087606966495514, 0.20877639949321747], dtype='float32').reshape([30]),
            paddle.to_tensor([0.1260605752468109, 0.31808215379714966, 0.3127637207508087, 0.4743736982345581, 0.1764998584985733, 0.4370565414428711, 0.4904298782348633, 0.042905211448669434, 0.3188907504081726, 0.41970449686050415, 0.14508040249347687, 0.24260388314723969, 0.3426834046840668, 0.09804116189479828, 0.165613055229187, 0.1549050658941269, 0.17945517599582672, 0.23487676680088043, 0.2938367426395416, 0.021917495876550674, 0.24119630455970764, 0.4031522274017334, 0.19156016409397125, 0.4883526563644409, 0.10161948949098587, 0.05739947035908699, 0.2970784902572632, 0.4229474663734436, 0.28472277522087097, 0.2584124803543091], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_aa228b272df0e2f11f9b6179c1e53cb8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 75, 1, 1], dtype='float32', min=0, max=0.5),
            paddle.uniform([75], dtype='float32', min=0, max=0.5),
            paddle.uniform([75], dtype='float32', min=0, max=0.5),
            paddle.uniform([75], dtype='float32', min=0, max=0.5),
            paddle.uniform([75], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a8c4923b7f259b91c90af626d096283d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 224, 224], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.0976649820804596, 0.23617660999298096, 0.3456140458583832, 0.36179280281066895, 0.4493712782859802, 0.4192814230918884, 0.35176703333854675, 0.17350484430789948, 0.0008350010612048209, 0.37849706411361694, 0.22212272882461548, 0.32736167311668396, 0.1260720193386078, 0.1840537190437317, 0.1387707144021988, 0.470390260219574], dtype='float32').reshape([16]),
            paddle.to_tensor([0.1961100846529007, 0.23815102875232697, 0.4908847510814667, 0.15763771533966064, 0.35344627499580383, 0.012841801159083843, 0.0837913304567337, 0.14637556672096252, 0.07899250090122223, 0.1248946338891983, 0.20674851536750793, 0.07311500608921051, 0.342835009098053, 0.3715333640575409, 0.0009106556535698473, 0.008317617699503899], dtype='float32').reshape([16]),
            paddle.to_tensor([0.4544447660446167, 0.21251970529556274, 0.3198157548904419, 0.004951345268636942, 0.38407662510871887, 0.4248732626438141, 0.09777427464723587, 0.24232256412506104, 0.14247021079063416, 0.12702731788158417, 0.04436163604259491, 0.21596944332122803, 0.0005933103966526687, 0.4426809847354889, 0.3165270686149597, 0.21946865320205688], dtype='float32').reshape([16]),
            paddle.to_tensor([0.07171973586082458, 0.3695254921913147, 0.0015764161944389343, 0.11460564285516739, 0.17320911586284637, 0.09790361672639847, 0.21312400698661804, 0.4592280089855194, 0.16738080978393555, 0.10368531942367554, 0.16912692785263062, 0.11021742969751358, 0.24553947150707245, 0.42682507634162903, 0.17610807716846466, 0.07580804824829102], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_9d5dde09230a5209a57565a3b7304f49(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
            paddle.static.InputSpec(shape=[None], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_84d855653cebe97f65b3559916b966d9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_9d5dde09230a5209a57565a3b7304f49
    def get_inputs(self):
        return [
            paddle.uniform([1, 50, 350], dtype='float32', min=0, max=0.5),
            paddle.uniform([50], dtype='float32', min=0, max=0.5),
            paddle.uniform([50], dtype='float32', min=0, max=0.5),
            paddle.uniform([50], dtype='float32', min=0, max=0.5),
            paddle.uniform([50], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_23c2b10d7a8abf9a67611a173566f3d2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 112, 112], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2b206aec1f440ef43cd77a185160fdc2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 31, 31], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9fbf45bc52a5e9effbf1909fdc573523(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 192, 8, 8], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
            paddle.uniform([192], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fe497642d62913c54b5e621201897ca9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 8, 50], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ba999752cd0dcc288c7e6e6412ef683f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 960, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([960], dtype='float32', min=0, max=0.5),
            paddle.uniform([960], dtype='float32', min=0, max=0.5),
            paddle.uniform([960], dtype='float32', min=0, max=0.5),
            paddle.uniform([960], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f50cc3fd399db6c10ce83b87a8e97109(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 200, 64, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([200], dtype='float32', min=0, max=0.5),
            paddle.uniform([200], dtype='float32', min=0, max=0.5),
            paddle.uniform([200], dtype='float32', min=0, max=0.5),
            paddle.uniform([200], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c9ed5a6b39a2d0153afc6860f6034d1e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 12, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.20468221604824066, 0.2536622881889343, 0.4912712574005127, 0.019483614712953568, 0.46522659063339233, 0.06021301448345184, 0.02366086281836033, 0.24143540859222412, 0.035597097128629684, 0.08351758122444153, 0.4049634635448456, 0.09610563516616821], dtype='float32').reshape([12]),
            paddle.to_tensor([0.40189310908317566, 0.005667477380484343, 0.1129237487912178, 0.10096815228462219, 0.33939188718795776, 0.3477475345134735, 0.3058944642543793, 0.25193309783935547, 0.129730686545372, 0.1563580185174942, 0.1194988265633583, 0.48935821652412415], dtype='float32').reshape([12]),
            paddle.to_tensor([0.28841596841812134, 0.4644812047481537, 0.4963093101978302, 0.29383066296577454, 0.01447009202092886, 0.4059099853038788, 0.15788695216178894, 0.26873478293418884, 0.3481057286262512, 0.3049217462539673, 0.29297974705696106, 0.229674831032753], dtype='float32').reshape([12]),
            paddle.to_tensor([0.3956868648529053, 0.31265944242477417, 0.16283434629440308, 0.4323107600212097, 0.41518816351890564, 0.20984959602355957, 0.02764839120209217, 0.10813002288341522, 0.49030160903930664, 0.2810627818107605, 0.260996550321579, 0.26433101296424866], dtype='float32').reshape([12]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_02fc409db1efd356d649c7a1c95ceca0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_623d84ccd2ed545d0bf6955d58c7d3e7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 1104, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([1104], dtype='float32', min=0, max=0.5),
            paddle.uniform([1104], dtype='float32', min=0, max=0.5),
            paddle.uniform([1104], dtype='float32', min=0, max=0.5),
            paddle.uniform([1104], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2bb51b61044b94b2def11c88470b108d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 1200, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([1200], dtype='float32', min=0, max=0.5),
            paddle.uniform([1200], dtype='float32', min=0, max=0.5),
            paddle.uniform([1200], dtype='float32', min=0, max=0.5),
            paddle.uniform([1200], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_60ac304885dcb2d44ec7dc1a1edcf864(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 1152, 6, 6], dtype='float16', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
            paddle.uniform([1152], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_98501d6a35cfb536f54cb81dc7989dc0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 112, 112], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.056613270193338394, 0.1036001443862915, 0.1664339154958725, 0.09714964777231216, 0.3826906681060791, 0.4672448933124542, 0.13624432682991028, 0.4299567639827728], dtype='float32').reshape([8]),
            paddle.to_tensor([0.07467231899499893, 0.13817031681537628, 0.38795754313468933, 0.4137129485607147, 0.052203934639692307, 0.45432692766189575, 0.1383797526359558, 0.05914498120546341], dtype='float32').reshape([8]),
            paddle.to_tensor([0.023552970960736275, 0.02485422044992447, 0.2434927076101303, 0.18448656797409058, 0.08795195072889328, 0.24623599648475647, 0.18611671030521393, 0.47461262345314026], dtype='float32').reshape([8]),
            paddle.to_tensor([0.4261905252933502, 0.4314700663089752, 0.06331325322389603, 0.19454260170459747, 0.1675250381231308, 0.0985385924577713, 0.06355959922075272, 0.42529428005218506], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5d2bd18e9dbb9b6b67f11e5f457b733b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 120, 120], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.3076832592487335, 0.2398305982351303, 0.08982452005147934, 0.46833857893943787, 0.0755876824259758, 0.43608003854751587, 0.031059743836522102, 0.4917753338813782, 0.4324693977832794, 0.17119182646274567, 0.09600208699703217, 0.30668216943740845, 0.31321465969085693, 0.29468145966529846, 0.4754544198513031, 0.3006599545478821, 0.006925660651177168, 0.15277592837810516, 0.44097664952278137, 0.07283392548561096, 0.17860396206378937, 0.2071317434310913, 0.1834876388311386, 0.12276585400104523], dtype='float32').reshape([24]),
            paddle.to_tensor([0.19145384430885315, 0.36677223443984985, 0.29484742879867554, 0.41985219717025757, 0.17706339061260223, 0.36210542917251587, 0.4495663642883301, 0.21630693972110748, 0.4838593006134033, 0.2688750624656677, 0.39514294266700745, 0.046656928956508636, 0.4643762409687042, 0.1751691848039627, 0.08562103658914566, 0.03373074159026146, 0.49033990502357483, 0.390846848487854, 0.35958877205848694, 0.4407327473163605, 0.10270855575799942, 0.28773877024650574, 0.21771498024463654, 0.1496465653181076], dtype='float32').reshape([24]),
            paddle.to_tensor([0.478595495223999, 0.38099566102027893, 0.25435328483581543, 0.048742298036813736, 0.248639315366745, 0.11368117481470108, 0.40711015462875366, 0.19077655673027039, 0.43947234749794006, 0.09415071457624435, 0.3063133955001831, 0.4088791012763977, 0.46008867025375366, 0.03791049122810364, 0.45901691913604736, 0.27289101481437683, 0.026636408641934395, 0.13632741570472717, 0.177413672208786, 0.2591366171836853, 0.083048976957798, 0.3827740252017975, 0.12152139842510223, 0.22401010990142822], dtype='float32').reshape([24]),
            paddle.to_tensor([0.2956397831439972, 0.06754926592111588, 0.287246435880661, 0.14571507275104523, 0.10865457355976105, 0.1359339952468872, 0.4067879915237427, 0.44481560587882996, 0.389309287071228, 0.14002081751823425, 0.22454512119293213, 0.11320517212152481, 0.3068511188030243, 0.029233843088150024, 0.47825852036476135, 0.08064053952693939, 0.4455118179321289, 0.36590054631233215, 0.37255942821502686, 0.035303372889757156, 0.4927195906639099, 0.16344057023525238, 0.04269488528370857, 0.28223803639411926], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_021252efb52ede89c8b44ec6099311e5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 112, 112], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5ed6de9e531cc6cc951a718a979a5d3c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 480, 20, 20], dtype='float16', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_845895442515f2fc13d56109f511aca8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 320, 320], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.37192803621292114, 0.032592762261629105, 0.36406898498535156, 0.059556908905506134, 0.30061912536621094, 0.0875694751739502, 0.08975029736757278, 0.3323182463645935], dtype='float32').reshape([8]),
            paddle.to_tensor([0.07506220042705536, 0.01806550659239292, 0.34572240710258484, 0.2298908531665802, 0.3360975682735443, 0.16086909174919128, 0.3905496597290039, 0.09902675449848175], dtype='float32').reshape([8]),
            paddle.to_tensor([0.453481525182724, 0.058514051139354706, 0.0413069985806942, 0.16790568828582764, 0.45359906554222107, 0.22472895681858063, 0.21844089031219482, 0.006946008186787367], dtype='float32').reshape([8]),
            paddle.to_tensor([0.3723197877407074, 0.3814813494682312, 0.1934361308813095, 0.09227921068668365, 0.3102753162384033, 0.22699661552906036, 0.3099820017814636, 0.4154869019985199], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9c52fb9aa9c59110f4b270132ac8e55e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8875d537d9bd492b8797cde815d29044(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 10, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.37436938285827637, 0.16062790155410767, 0.04861500486731529, 0.12108561396598816, 0.05051080510020256, 0.15980307757854462, 0.1545013040304184, 0.3901887536048889, 0.18414102494716644, 0.4878992438316345], dtype='float32').reshape([10]),
            paddle.to_tensor([0.19646623730659485, 0.21044136583805084, 0.1712404042482376, 0.006159941665828228, 0.4111512303352356, 0.4324166476726532, 0.350593626499176, 0.43481823801994324, 0.12584590911865234, 0.4206296503543854], dtype='float32').reshape([10]),
            paddle.to_tensor([0.05166323482990265, 0.024880301207304, 0.09694512188434601, 0.24370260536670685, 0.01578955352306366, 0.09937404841184616, 0.16758175194263458, 0.04196611046791077, 0.01508170086890459, 0.4502815008163452], dtype='float32').reshape([10]),
            paddle.to_tensor([0.37812355160713196, 0.11476729810237885, 0.11192356050014496, 0.2697056531906128, 0.3370354473590851, 0.438475638628006, 0.3368282914161682, 0.19723154604434967, 0.2970896065235138, 0.1342112123966217], dtype='float32').reshape([10]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_42706a870d1cfeac491fa245e0c819ef(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 704, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([704], dtype='float32', min=0, max=0.5),
            paddle.uniform([704], dtype='float32', min=0, max=0.5),
            paddle.uniform([704], dtype='float32', min=0, max=0.5),
            paddle.uniform([704], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d1e1cfb37873ec4cb56c3016780d145c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c4698e8e66bf4ae39b260cdc515db379(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 32, 100], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f255e1c80fa1db1a630c94efc732b327(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 256, 256], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_19bfeb9d1390554a176a988d47dbbeec(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 1280, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([1280], dtype='float32', min=0, max=0.5),
            paddle.uniform([1280], dtype='float32', min=0, max=0.5),
            paddle.uniform([1280], dtype='float32', min=0, max=0.5),
            paddle.uniform([1280], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7ea6a4c390ee132dbb7b325909e348b3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fcfa6e49b9c67856091b1a5ac5b92270(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.42766281962394714, 0.12650561332702637, 0.37135592103004456, 0.30343422293663025, 0.21958130598068237, 0.3168344795703888, 0.016684409230947495, 0.24620990455150604, 0.03458157926797867, 0.17570650577545166, 0.2742132246494293, 0.08434206247329712, 0.08580219745635986, 0.22643469274044037, 0.4928056001663208, 0.344691663980484, 0.46859124302864075, 0.025116588920354843], dtype='float32').reshape([18]),
            paddle.to_tensor([0.3145536184310913, 0.14499226212501526, 0.15103939175605774, 0.34467244148254395, 0.446471631526947, 0.15613889694213867, 0.4187062382698059, 0.3305586278438568, 0.1325715184211731, 0.48512640595436096, 0.16716253757476807, 0.015543035231530666, 0.42395472526550293, 0.15832901000976562, 0.15563146770000458, 0.24960625171661377, 0.2535829246044159, 0.4467298984527588], dtype='float32').reshape([18]),
            paddle.to_tensor([0.13556823134422302, 0.016396891325712204, 0.23891021311283112, 0.11853823065757751, 0.3380109369754791, 0.3716205358505249, 0.47579339146614075, 0.26819750666618347, 0.22861599922180176, 0.3566648066043854, 0.27144527435302734, 0.44794774055480957, 0.23966148495674133, 0.4641282558441162, 0.15115290880203247, 0.01330794021487236, 0.2485901564359665, 0.3395980894565582], dtype='float32').reshape([18]),
            paddle.to_tensor([0.18060128390789032, 0.04230109974741936, 0.42906829714775085, 0.26014575362205505, 0.26969748735427856, 0.31533750891685486, 0.22051149606704712, 0.44028130173683167, 0.06474699079990387, 0.07708320021629333, 0.43442702293395996, 0.2285049706697464, 0.4810100495815277, 0.3053273856639862, 0.3562201261520386, 0.41070085763931274, 0.02518049255013466, 0.27383220195770264], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ff0ee26c44be4b88da1ba897cca2c53a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 576, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([576], dtype='float32', min=0, max=0.5),
            paddle.uniform([576], dtype='float32', min=0, max=0.5),
            paddle.uniform([576], dtype='float32', min=0, max=0.5),
            paddle.uniform([576], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fb3adf16d3f9d3d744ae5d8fd754f72c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([8, 1024, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_32219773063578c8ad53656684731374(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_da4d85bc213f70819a72e68f21214cdb
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 32, 16, 16], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e62051d30473a8835c28f0200d96df4f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 2048, 10, 10], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1daf6b109f0a7cad6d10eebff4a5420f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 16, 50], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.31688329577445984, 0.4239937365055084, 0.163129985332489, 0.49654287099838257, 0.24878010153770447, 0.08251246809959412, 0.17672230303287506, 0.08638761937618256], dtype='float32').reshape([8]),
            paddle.to_tensor([0.36120519042015076, 0.1477004885673523, 0.021180007606744766, 0.11997948586940765, 0.4876387417316437, 0.1642913818359375, 0.2079324573278427, 0.30032503604888916], dtype='float32').reshape([8]),
            paddle.to_tensor([0.23389452695846558, 0.4444618225097656, 0.22816425561904907, 0.22144073247909546, 0.48360326886177063, 0.3106992244720459, 0.003734841011464596, 0.02672434039413929], dtype='float32').reshape([8]),
            paddle.to_tensor([0.33812016248703003, 0.2819894850254059, 0.4359188377857208, 0.18510763347148895, 0.2587635815143585, 0.03287492319941521, 0.4790809750556946, 0.06952474266290665], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ba9f31408f96cf244898474aa8360419(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.23533348739147186, 0.31506606936454773, 0.03435583412647247, 0.04132542386651039, 0.2674749791622162, 0.21002472937107086, 0.13032571971416473, 0.1050562858581543, 0.4746582806110382, 0.1958744376897812, 0.38443732261657715, 0.007579511031508446, 0.1903546303510666, 0.39156004786491394, 6.266176933422685e-05, 0.46997320652008057, 0.39221128821372986, 0.0034549576230347157, 0.3231128752231598, 0.2242753803730011, 0.1373838186264038, 0.2581767439842224, 0.4571051001548767, 0.4833131730556488], dtype='float32').reshape([24]),
            paddle.to_tensor([0.23110727965831757, 0.3948252201080322, 0.43388861417770386, 0.393754780292511, 0.2234860062599182, 0.35383448004722595, 0.4866023659706116, 0.3054259717464447, 0.2645679712295532, 0.3197914958000183, 0.21189957857131958, 0.34985387325286865, 0.3597550392150879, 0.12475629150867462, 0.38816237449645996, 0.09368617832660675, 0.300818532705307, 0.31045594811439514, 0.3992258608341217, 0.4896561801433563, 0.2998631000518799, 0.40125903487205505, 0.15945693850517273, 0.44929632544517517], dtype='float32').reshape([24]),
            paddle.to_tensor([0.4680517911911011, 0.3579433560371399, 0.05553781986236572, 0.2685360014438629, 0.1305607557296753, 0.4050348997116089, 0.41145479679107666, 0.3693763315677643, 0.3858642876148224, 0.21039390563964844, 0.0461144745349884, 0.3917446732521057, 0.08760426938533783, 0.3482493460178375, 0.17453336715698242, 0.05714866891503334, 0.023851897567510605, 0.45131915807724, 0.173579603433609, 0.22017104923725128, 0.4935011565685272, 0.2809513509273529, 0.17036141455173492, 0.3933624029159546], dtype='float32').reshape([24]),
            paddle.to_tensor([0.43206748366355896, 0.30971038341522217, 0.4011095464229584, 0.3839275538921356, 0.19399744272232056, 0.023437410593032837, 0.4358484745025635, 0.07169608771800995, 0.21524600684642792, 0.4876810908317566, 0.010687075555324554, 0.4982993006706238, 0.06968267261981964, 0.17004938423633575, 0.48843657970428467, 0.10130918025970459, 0.27323347330093384, 0.4120211899280548, 0.18136821687221527, 0.32000499963760376, 0.24363072216510773, 0.1791282445192337, 0.20798161625862122, 0.243788942694664], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e7a9764e0434639b5aa0462bb4e10c00(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 1, 1], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2428fe41e069348e823b33e06ac6ea70(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e4dada82336f848d47d879b66705e648(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 52, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([52], dtype='float32', min=0, max=0.5),
            paddle.uniform([52], dtype='float32', min=0, max=0.5),
            paddle.uniform([52], dtype='float32', min=0, max=0.5),
            paddle.uniform([52], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7b43bd1c435100a8ae419ce2be591392(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 256, 512], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.14695397019386292, 0.2586841583251953, 0.058409690856933594, 0.12755046784877777, 0.31527119874954224, 0.19343039393424988, 0.18708796799182892, 0.34421467781066895, 0.3678618371486664, 0.3163703382015228, 0.2376958280801773, 0.26747843623161316, 0.05363382399082184, 0.3576444983482361, 0.21147923171520233, 0.39126908779144287], dtype='float32').reshape([16]),
            paddle.to_tensor([0.21544133126735687, 0.12990392744541168, 0.17544855177402496, 0.26694339513778687, 0.33041051030158997, 0.12114810943603516, 0.24053725600242615, 0.1573152095079422, 0.3248526155948639, 0.43808743357658386, 0.14376017451286316, 0.17235392332077026, 0.10063618421554565, 0.387660413980484, 0.34431612491607666, 0.01400711014866829], dtype='float32').reshape([16]),
            paddle.to_tensor([0.4966626763343811, 0.25744593143463135, 0.3426036536693573, 0.43985068798065186, 0.22914130985736847, 0.40373244881629944, 0.47463321685791016, 0.05966908112168312, 0.3763192892074585, 0.1894221305847168, 0.3538073003292084, 0.4610082507133484, 0.4696105122566223, 0.25417831540107727, 0.26363280415534973, 0.31587639451026917], dtype='float32').reshape([16]),
            paddle.to_tensor([0.15685105323791504, 0.3940292298793793, 0.04023417830467224, 0.10341990739107132, 0.3161331117153168, 0.43373531103134155, 0.09477891027927399, 0.32416832447052, 0.38660261034965515, 0.21919865906238556, 0.4053646922111511, 0.24656681716442108, 0.49319756031036377, 0.4745771884918213, 0.4980245530605316, 0.14642606675624847], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_aaf6aa6ae8a0a9eeaec78880fc38fbd4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 120, 120], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_849cdf651ac47a2e10e3c9852ebd6703(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 112, 12, 12], dtype='float16', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
            paddle.uniform([112], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_688d1223a457607d1f272357c350ca62(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cdc3b0a148b551f86897e4668e0bdb6b
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 16, 16], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bf6bb842a03f16570ef598b7fc789a32(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 1, 1], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c53e736e9e341d73d20852424aa00eef(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_da4d85bc213f70819a72e68f21214cdb
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 32, 64, 64], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.05142014101147652, 0.13105399906635284, 0.07851973176002502, 0.10221171379089355, 0.4827289283275604, 0.2645960748195648, 0.12511031329631805, 0.3057035207748413], dtype='float32').reshape([8]),
            paddle.to_tensor([0.17837977409362793, 0.4848180115222931, 0.24470825493335724, 0.355845183134079, 0.3124638795852661, 0.40127095580101013, 0.10990352928638458, 0.301351934671402], dtype='float32').reshape([8]),
            paddle.to_tensor([0.37947726249694824, 0.28326907753944397, 0.042105089873075485, 0.03903720900416374, 0.1462898999452591, 0.3318634629249573, 0.2815316915512085, 0.14922811090946198], dtype='float32').reshape([8]),
            paddle.to_tensor([0.32423633337020874, 0.3157745897769928, 0.3939964175224304, 0.3779418468475342, 0.19814594089984894, 0.018262522295117378, 0.2542611360549927, 0.3476412296295166], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_77eb72090d9031d6a9ec5700f4d325b9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 40, 40], dtype='float16', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_17467706fb9f3865a4fec1e081c62c8e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 32, 64], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.4147466719150543, 0.4684407413005829, 0.37761974334716797, 0.06817129999399185, 0.21018637716770172, 0.4637147784233093, 0.2818199694156647, 0.40314939618110657, 0.44346392154693604, 0.3802662789821625, 0.37261196970939636, 0.22723422944545746, 0.28161031007766724, 0.21948689222335815, 0.23006944358348846, 0.39650261402130127, 0.012876426801085472, 0.4060233533382416], dtype='float32').reshape([18]),
            paddle.to_tensor([0.12092169374227524, 0.3624633550643921, 0.45348596572875977, 0.3473249077796936, 0.06658662110567093, 0.11633658409118652, 0.09646455198526382, 0.03685188293457031, 0.061351485550403595, 0.006260200869292021, 0.2269420176744461, 0.02647268772125244, 0.4693724513053894, 0.0638776496052742, 0.350588321685791, 0.4403800368309021, 0.28692060708999634, 0.14046365022659302], dtype='float32').reshape([18]),
            paddle.to_tensor([0.26042550802230835, 0.29240214824676514, 0.07731752097606659, 0.4225768446922302, 0.12057843059301376, 0.19324129819869995, 0.3226916491985321, 0.1320241093635559, 0.024021781980991364, 0.2897478938102722, 0.43642666935920715, 0.24096855521202087, 0.440872460603714, 0.2635137736797333, 0.34431782364845276, 0.4005966782569885, 0.43166789412498474, 0.10423849523067474], dtype='float32').reshape([18]),
            paddle.to_tensor([0.10874216258525848, 0.14637337625026703, 0.28494730591773987, 0.017487825825810432, 0.3678358495235443, 0.36534351110458374, 0.44171929359436035, 0.13186420500278473, 0.01095564290881157, 0.4127013087272644, 0.37040242552757263, 0.38575124740600586, 0.24651318788528442, 0.3574659526348114, 0.2047906070947647, 0.21649248898029327, 0.4598648250102997, 0.0733756572008133], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c8255f67f9cf5c116c8d83bbab8c58c7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 240, 40, 40], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1b1c7e4c8d991282aa0d0aa711fd3448(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 175, 25], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f7bff48077e0599a1d5a55e2ae19851b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.3497108817100525, 0.2972128093242645, 0.40742865204811096, 0.4818175733089447, 0.1942264884710312, 0.4763771593570709, 0.15285377204418182, 0.2283594161272049, 0.4290488362312317, 0.23222170770168304, 0.11165595799684525, 0.12774518132209778, 0.08081624656915665, 0.47626355290412903, 0.41022226214408875, 0.27920302748680115, 0.48873916268348694, 0.3984526991844177, 0.08588805049657822, 0.05148405581712723, 0.17091083526611328, 0.4850679337978363, 0.16544203460216522, 0.11561987549066544], dtype='float32').reshape([24]),
            paddle.to_tensor([0.3661627471446991, 0.4239063560962677, 0.2063927948474884, 0.040059011429548264, 0.23939092457294464, 0.2643526792526245, 0.05215347185730934, 0.11758889257907867, 0.04660138860344887, 0.3560762405395508, 0.3713960647583008, 0.0129772387444973, 0.13758523762226105, 0.4665074646472931, 0.4585303068161011, 0.4664272964000702, 0.015994859859347343, 0.45196861028671265, 0.4175983667373657, 0.40941354632377625, 0.4355378746986389, 0.09040796756744385, 0.2903195321559906, 0.03481007367372513], dtype='float32').reshape([24]),
            paddle.to_tensor([0.2754937708377838, 0.3479728400707245, 0.15653137862682343, 0.34315794706344604, 0.07886287569999695, 0.20528189837932587, 0.14778012037277222, 0.13875244557857513, 0.24914810061454773, 0.32166630029678345, 0.17957676947116852, 0.3526609539985657, 0.06335363537073135, 0.2649545967578888, 0.45872727036476135, 0.08385302126407623, 0.3573344647884369, 0.21594791114330292, 0.16808214783668518, 0.13082410395145416, 0.22638218104839325, 0.2849888503551483, 0.39510905742645264, 0.2260151207447052], dtype='float32').reshape([24]),
            paddle.to_tensor([0.38259613513946533, 0.07544635981321335, 0.40732336044311523, 0.054194558411836624, 0.12841454148292542, 0.32845693826675415, 0.21723516285419464, 0.017790377140045166, 0.27063608169555664, 0.06580750644207001, 0.4979363679885864, 0.16137319803237915, 0.05298822745680809, 0.29718017578125, 0.4248575270175934, 0.1796616017818451, 0.3741775155067444, 0.059686239808797836, 0.49813705682754517, 0.24270355701446533, 0.39038243889808655, 0.3608960509300232, 0.18558548390865326, 0.4784128963947296], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_aa47231b4bb80ea3e6aea444b79a2cbc(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 152, 152], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6825698b415c45f6c8afc716b868e78f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_5e7cb23119f65b33bdbad71952f6dd12
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 4, 4], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_370df69b9dcc85dc0804c17dc8de431e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 82, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([82], dtype='float32', min=0, max=0.5),
            paddle.uniform([82], dtype='float32', min=0, max=0.5),
            paddle.uniform([82], dtype='float32', min=0, max=0.5),
            paddle.uniform([82], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6b54c24938f44d9a57c4f955ec1c023e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.28749027848243713, 0.19257089495658875, 0.4570263922214508, 0.2350829839706421, 0.11965487152338028, 0.20590513944625854, 0.4182504117488861, 0.01998896710574627, 0.31012046337127686, 0.044712770730257034, 0.49101293087005615, 0.30293482542037964, 0.35398295521736145, 0.16753722727298737, 0.3739153742790222, 0.15193147957324982], dtype='float32').reshape([16]),
            paddle.to_tensor([0.22377386689186096, 0.12903040647506714, 0.3488973081111908, 0.18532370030879974, 0.12844865024089813, 0.30923476815223694, 0.3128046989440918, 0.03040543757379055, 0.276547372341156, 0.4380645155906677, 0.21800395846366882, 0.30431681871414185, 0.021052153781056404, 0.05436643213033676, 0.4328029453754425, 0.32977551221847534], dtype='float32').reshape([16]),
            paddle.to_tensor([0.4708232879638672, 0.2963787317276001, 0.45324623584747314, 0.4789246618747711, 0.33541423082351685, 0.15950703620910645, 0.3260650634765625, 0.0815773606300354, 0.38754093647003174, 0.46215102076530457, 0.20924930274486542, 0.27009230852127075, 0.19888979196548462, 0.17838215827941895, 0.3559666574001312, 0.17849372327327728], dtype='float32').reshape([16]),
            paddle.to_tensor([0.15892091393470764, 0.021456053480505943, 0.07282797992229462, 0.4475376605987549, 0.08573678880929947, 0.34430429339408875, 0.2836500108242035, 0.3635454475879669, 0.21790890395641327, 0.146846204996109, 0.25752875208854675, 0.28783586621284485, 0.02903110533952713, 0.1260410100221634, 0.34685975313186646, 0.14846809208393097], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c91106b3a10dca6aa63c7948fca33bd3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 2048, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_731d72c7410aab85c8a4bb6730ea45ba(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 320, 320], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.19170300662517548, 0.21634984016418457, 0.14812588691711426, 0.3599798083305359, 0.0673409178853035, 0.10585509240627289, 0.24176780879497528, 0.07534297555685043], dtype='float32').reshape([8]),
            paddle.to_tensor([0.22832649946212769, 0.34488385915756226, 0.26931047439575195, 0.39252564311027527, 0.047801773995161057, 0.0033056391403079033, 0.43176984786987305, 0.08047650754451752], dtype='float32').reshape([8]),
            paddle.to_tensor([0.39177757501602173, 0.28487420082092285, 0.3957374691963196, 0.4777330756187439, 0.1691277176141739, 0.12384667992591858, 0.4247342646121979, 0.4775155484676361], dtype='float32').reshape([8]),
            paddle.to_tensor([0.4211759865283966, 0.02285650745034218, 0.3025083541870117, 0.1275191754102707, 0.4069538414478302, 0.3278164267539978, 0.015784140676259995, 0.020825112238526344], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9fd003444bad580c146954150c333c1d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 112, 112], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f4126f3aec0bb1b7771ee4a1b89c75ff(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 56, 4, 50], dtype='float16', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_96ba1c3e30c421b01bd8e88b324271ea(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 1568, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([1568], dtype='float32', min=0, max=0.5),
            paddle.uniform([1568], dtype='float32', min=0, max=0.5),
            paddle.uniform([1568], dtype='float32', min=0, max=0.5),
            paddle.uniform([1568], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_95a9daf1596c05056f082b2d77188925(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 1, 3], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d5f6efd70ddafebbe0f7a15bb2c3ba3c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 28, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.27055343985557556, 0.3240574896335602, 0.21805839240550995, 0.0899236798286438, 0.49865415692329407, 0.14813214540481567, 0.002083045896142721, 0.17672309279441833, 0.38046368956565857, 0.3069570064544678, 0.32380029559135437, 0.10412953794002533, 0.48871591687202454, 0.07709450274705887, 0.2953909933567047, 0.1476738452911377, 0.212232768535614, 0.051484666764736176, 0.29692143201828003, 0.17061714828014374, 0.19476521015167236, 0.2487160712480545, 0.39216965436935425, 0.20859111845493317, 0.40012454986572266, 0.4633551239967346, 0.32270917296409607, 0.3369652330875397], dtype='float32').reshape([28]),
            paddle.to_tensor([0.1239505410194397, 0.10542173683643341, 0.4703482985496521, 0.3590462803840637, 0.1453639417886734, 0.11084439605474472, 0.302556574344635, 0.04470745846629143, 0.19457338750362396, 0.4001605212688446, 0.4824044704437256, 0.026893150061368942, 0.4287010431289673, 0.08220060914754868, 0.20099958777427673, 0.37006229162216187, 0.1051560714840889, 0.459030419588089, 0.11774573475122452, 0.14349791407585144, 0.4518883526325226, 0.40091004967689514, 0.13600313663482666, 0.4610772430896759, 0.023657646030187607, 0.37807947397232056, 0.03824176266789436, 0.4687100946903229], dtype='float32').reshape([28]),
            paddle.to_tensor([0.42497554421424866, 0.17400652170181274, 0.16198104619979858, 0.28172236680984497, 0.4696204364299774, 0.07145135849714279, 0.09091418981552124, 0.02675430290400982, 0.0011913988273590803, 0.31005629897117615, 0.31909850239753723, 0.22157637774944305, 0.4434549808502197, 0.39077314734458923, 0.08642705529928207, 0.3426613211631775, 0.08091028034687042, 0.14122341573238373, 0.4329145550727844, 0.05518215522170067, 0.3630676865577698, 0.35013243556022644, 0.4850027859210968, 0.41968926787376404, 0.3345392048358917, 0.0009067385108210146, 0.223550483584404, 0.2918137013912201], dtype='float32').reshape([28]),
            paddle.to_tensor([0.10757526755332947, 0.46142083406448364, 0.4125477075576782, 0.2176133543252945, 0.27655133605003357, 0.3196488916873932, 0.4059293270111084, 0.023274414241313934, 0.10358141362667084, 0.48835286498069763, 0.06461784988641739, 0.12925942242145538, 0.16153381764888763, 0.4829705059528351, 0.19333074986934662, 0.08442753553390503, 0.4668627679347992, 0.03514701500535011, 0.04190382733941078, 0.09810402989387512, 0.01659528538584709, 0.4083481431007385, 0.20388522744178772, 0.16549451649188995, 0.0006911939126439393, 0.4689388573169708, 0.0800115093588829, 0.4042797088623047], dtype='float32').reshape([28]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c8db9b407d0640a47e1033158c4f1c0c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 174, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([174], dtype='float32', min=0, max=0.5),
            paddle.uniform([174], dtype='float32', min=0, max=0.5),
            paddle.uniform([174], dtype='float32', min=0, max=0.5),
            paddle.uniform([174], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f755362f87af70121435839dfe30a02e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 240, 31, 31], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_088c7325ceea18ad4079d7545dba4775(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 256, 512], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.45928022265434265, 0.3904736340045929, 0.2808667719364166, 0.4504765272140503, 0.3683357536792755, 0.3889272212982178, 0.4860570728778839, 0.2082287073135376, 0.38394472002983093, 0.25447067618370056, 0.22716665267944336, 0.36580324172973633, 0.3165105879306793, 0.28209707140922546, 0.38996657729148865, 0.46470609307289124], dtype='float32').reshape([16]),
            paddle.to_tensor([0.2772737145423889, 0.008664594031870365, 0.27116677165031433, 0.10922196507453918, 0.46636125445365906, 0.34657639265060425, 0.20364898443222046, 0.48907411098480225, 0.1214916929602623, 0.052947551012039185, 0.3734426498413086, 0.01569770835340023, 0.03233318403363228, 0.4969390332698822, 0.3440883755683899, 0.061301663517951965], dtype='float32').reshape([16]),
            paddle.to_tensor([0.16430820524692535, 0.4265882670879364, 0.4042595326900482, 0.0879141092300415, 0.30726271867752075, 0.2729908227920532, 0.09996557980775833, 0.2189052551984787, 0.06785745173692703, 0.2121993452310562, 0.08386950939893723, 0.19891279935836792, 0.3791796565055847, 0.35645177960395813, 0.06316196173429489, 0.19600217044353485], dtype='float32').reshape([16]),
            paddle.to_tensor([0.4359804093837738, 0.018751345574855804, 0.041587330400943756, 0.36735105514526367, 0.09326770901679993, 0.11185906082391739, 0.2757706642150879, 0.21651865541934967, 0.3761105537414551, 0.24123245477676392, 0.0736672431230545, 0.018580486997961998, 0.008319326676428318, 0.2137085497379303, 0.2901132106781006, 0.4498550593852997], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3b2c3b79875bf41a299c147e41911da4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 92, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([92], dtype='float32', min=0, max=0.5),
            paddle.uniform([92], dtype='float32', min=0, max=0.5),
            paddle.uniform([92], dtype='float32', min=0, max=0.5),
            paddle.uniform([92], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1a053da07c1a1887a7d1207207dbc5b2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 16, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_069e7cf64d651f596cba675b352cd136(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 160, 160], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5b758db868987a1e9fa38aaaac010e03(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 480, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_46f8fe3ce61c7bc841074fcff8bf5327(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 960, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([960], dtype='float32', min=0, max=0.5),
            paddle.uniform([960], dtype='float32', min=0, max=0.5),
            paddle.uniform([960], dtype='float32', min=0, max=0.5),
            paddle.uniform([960], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a178a05d6a979c85755fadc460b7171a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9e7c17f9ba7391848adf4d3942ea6d30(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 64, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1ad2b47f7bf039f89465d61626278397(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 32, 16], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bd6e0669c42e2722bb6a19dc9b64ecb4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([8, 128, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_76fab50cdf5245bdbced2ef6c5022d95(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 8, 25], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a4dfb996bf499d5d97f3f819b4d17d2c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 16, 32], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.01707882434129715, 0.4861084222793579, 0.4738156795501709, 0.08330842107534409, 0.12962885200977325, 0.3042057752609253, 0.21204707026481628, 0.23876547813415527, 0.2362901270389557, 0.14799004793167114, 0.016029903665184975, 0.07386194914579391, 0.21284586191177368, 0.19536477327346802, 0.31399616599082947, 0.4485376179218292, 0.1614803820848465, 0.13564956188201904], dtype='float32').reshape([18]),
            paddle.to_tensor([0.26216939091682434, 0.4228413999080658, 0.11382386833429337, 0.35446763038635254, 0.21534155309200287, 0.03699714317917824, 0.0496143214404583, 0.11230577528476715, 0.2641577422618866, 0.12366224080324173, 0.4716685712337494, 0.055578310042619705, 0.1177515760064125, 0.35843750834465027, 0.3196979761123657, 0.059867169708013535, 0.004391923081129789, 0.38316062092781067], dtype='float32').reshape([18]),
            paddle.to_tensor([0.2387801557779312, 0.25192388892173767, 0.06866660714149475, 0.1969902068376541, 0.40562552213668823, 0.48381495475769043, 0.48262521624565125, 0.4558928906917572, 0.4380781650543213, 0.03616303205490112, 0.15106473863124847, 0.43221738934516907, 0.25122666358947754, 0.3447968363761902, 0.19712622463703156, 0.3211536407470703, 0.3487238883972168, 0.03711133450269699], dtype='float32').reshape([18]),
            paddle.to_tensor([0.2171265184879303, 0.15624815225601196, 0.23309345543384552, 0.15241695940494537, 0.13312923908233643, 0.296428382396698, 0.07284992188215256, 0.24932780861854553, 0.16350336372852325, 0.24333015084266663, 0.28505945205688477, 0.03283528611063957, 0.03344547376036644, 0.19418081641197205, 0.47838133573532104, 0.1969587653875351, 0.1996643841266632, 0.009566148743033409], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9e855878e0c3e2d87c143f63acb77104(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 80, 80], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0b2abbe4d511ce8f7fc749f3afa06e5c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.48738089203834534, 0.4889049530029297, 0.3224408030509949, 0.29664573073387146, 0.49986693263053894, 0.20245756208896637, 0.2479705810546875, 0.25791677832603455, 0.16395150125026703, 0.45716845989227295, 0.11689016968011856, 0.4570024907588959, 0.04652116075158119, 0.2549054026603699, 0.0060049607418477535, 0.009557751938700676, 0.3703991174697876, 0.2916989326477051, 0.22756312787532806, 0.30927738547325134, 0.39691081643104553, 0.2539328634738922, 0.047069404274225235, 0.4010445177555084, 0.49438509345054626, 0.006332955323159695, 0.03310179337859154, 0.3374742865562439, 0.21327374875545502, 0.1374249905347824], dtype='float32').reshape([30]),
            paddle.to_tensor([0.3934166133403778, 0.37759754061698914, 0.04819761961698532, 0.40955713391304016, 0.043950457125902176, 0.34066563844680786, 0.10869116336107254, 0.4655986428260803, 0.44917234778404236, 0.0616091787815094, 0.11442270874977112, 0.06634583324193954, 0.21370424330234528, 0.46984487771987915, 0.061048686504364014, 0.21393504738807678, 0.4653656482696533, 0.18904510140419006, 0.29914289712905884, 0.3488917350769043, 0.0049366336315870285, 0.35736528038978577, 0.3612874448299408, 0.21593333780765533, 0.39565905928611755, 0.42334872484207153, 0.32123905420303345, 0.331073135137558, 0.31671270728111267, 0.48679715394973755], dtype='float32').reshape([30]),
            paddle.to_tensor([0.21540826559066772, 0.03885328769683838, 0.2560858428478241, 0.29852357506752014, 0.10498189181089401, 0.10091865062713623, 0.005922093987464905, 0.469318687915802, 0.4393877387046814, 0.015229281038045883, 0.3433034420013428, 0.36886340379714966, 0.33195602893829346, 0.12557563185691833, 0.14607654511928558, 0.0709763765335083, 0.3495499789714813, 0.2543274760246277, 0.12397925555706024, 0.0048886314034461975, 0.035023972392082214, 0.23033101856708527, 0.012533299624919891, 0.1796007752418518, 0.013118044473230839, 0.27242156863212585, 0.3679284453392029, 0.2773464620113373, 0.37621599435806274, 0.2138872742652893], dtype='float32').reshape([30]),
            paddle.to_tensor([0.21027804911136627, 0.17331410944461823, 0.49717557430267334, 0.1315118670463562, 0.2538514733314514, 0.06218407303094864, 0.44266799092292786, 0.13611768186092377, 0.29198190569877625, 0.41009125113487244, 0.23534293472766876, 0.1017155572772026, 0.29886749386787415, 0.36214715242385864, 0.0007410096586681902, 0.028441019356250763, 0.20455574989318848, 0.45167452096939087, 0.27425462007522583, 0.15039601922035217, 0.3629760146141052, 0.13794109225273132, 0.023021630942821503, 0.357364684343338, 0.007073553744703531, 0.2997593879699707, 0.025786465033888817, 0.0245832372456789, 0.007925081066787243, 0.1335240751504898], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_54a62a2b6084718ed110d0fd0c7d4061(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 160, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_92401916800b421a48c54bfd781a24bc(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 76, 76], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_dcb932ddc8c4b6b18124029b87be9b57(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 224, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([224], dtype='float32', min=0, max=0.5),
            paddle.uniform([224], dtype='float32', min=0, max=0.5),
            paddle.uniform([224], dtype='float32', min=0, max=0.5),
            paddle.uniform([224], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ac57af21732dbdfb80431ca78f7f3b6f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 2048, 20, 20], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4281b762f75ca27c88109e9cff2e3397(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.4269542098045349, 0.28521478176116943, 0.15134431421756744, 0.1768672615289688, 0.4695432782173157, 0.21818549931049347, 0.3673296570777893, 0.31527864933013916, 0.4849682152271271, 0.36461690068244934, 0.10164633393287659, 0.3659038245677948, 0.028823690488934517, 0.3414773941040039, 0.3139674663543701, 0.26231783628463745, 0.49112123250961304, 0.19724509119987488, 0.09138678759336472, 0.3466850817203522], dtype='float32').reshape([20]),
            paddle.to_tensor([0.029294854030013084, 0.00730014406144619, 0.42551732063293457, 0.33647435903549194, 0.2794761061668396, 0.27373939752578735, 0.49599507451057434, 0.10601375997066498, 0.03968394547700882, 0.1425444781780243, 0.3740035891532898, 0.4268903434276581, 0.29751700162887573, 0.423776775598526, 0.34749341011047363, 0.27761387825012207, 0.3326234519481659, 0.26016807556152344, 0.31141892075538635, 0.42227962613105774], dtype='float32').reshape([20]),
            paddle.to_tensor([0.029305636882781982, 0.1287262737751007, 0.24741844832897186, 0.025889964774250984, 0.14912767708301544, 0.22753548622131348, 0.26127758622169495, 0.3588404059410095, 0.20693019032478333, 0.035328906029462814, 0.33686286211013794, 0.39582955837249756, 0.1671745777130127, 0.3898501992225647, 0.41597163677215576, 0.08349163830280304, 0.15000474452972412, 0.3486316502094269, 0.10131093114614487, 0.15793295204639435], dtype='float32').reshape([20]),
            paddle.to_tensor([0.16296735405921936, 0.2537307143211365, 0.10220544040203094, 0.2298017144203186, 0.34540536999702454, 0.015382216311991215, 0.17075100541114807, 0.4047044515609741, 0.0900716558098793, 0.38349461555480957, 0.32687118649482727, 0.3248467743396759, 0.48685675859451294, 0.023950982838869095, 0.05951523035764694, 0.429723858833313, 0.4123036861419678, 0.2918248176574707, 0.28669095039367676, 0.22879360616207123], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6ba2f96cfe824092616a1c18d5693395(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 174, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([174], dtype='float32', min=0, max=0.5),
            paddle.uniform([174], dtype='float32', min=0, max=0.5),
            paddle.uniform([174], dtype='float32', min=0, max=0.5),
            paddle.uniform([174], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_faa499b21db571509b86401617f3e8a9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 320, 320], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.491838663816452, 0.03547292947769165, 0.3609066903591156, 0.25388115644454956, 0.25845974683761597, 0.021159544587135315, 0.08551926165819168, 0.1603606939315796, 0.27645814418792725, 0.17474620044231415, 0.38986098766326904, 0.41233229637145996, 0.4333619475364685, 0.2200516015291214, 0.3515142500400543, 0.16006651520729065, 0.4684000015258789, 0.21059182286262512, 0.45974984765052795, 0.22474612295627594, 0.11798135191202164, 0.02140117809176445, 0.21147558093070984, 0.43162620067596436], dtype='float32').reshape([24]),
            paddle.to_tensor([0.1575586497783661, 0.09178630262613297, 0.22251735627651215, 0.005984893534332514, 0.19979546964168549, 0.3114355206489563, 0.490450918674469, 0.05221368744969368, 0.19578887522220612, 0.2812045216560364, 0.3031367361545563, 0.4317221939563751, 0.43455010652542114, 0.20510369539260864, 0.46274086833000183, 0.36712729930877686, 0.08631618320941925, 0.4249473810195923, 0.3819586932659149, 0.09464438259601593, 0.018038179725408554, 0.19347576797008514, 0.12164827436208725, 0.47914060950279236], dtype='float32').reshape([24]),
            paddle.to_tensor([0.2123800814151764, 0.14009860157966614, 0.44391801953315735, 0.18098396062850952, 0.48528146743774414, 0.33718863129615784, 0.2841911017894745, 0.16981817781925201, 0.42432403564453125, 0.29548028111457825, 0.445811927318573, 0.07734239846467972, 0.005317773669958115, 0.43096503615379333, 0.042529840022325516, 0.026722485199570656, 0.07569614052772522, 0.10760613530874252, 0.48836129903793335, 0.4780399799346924, 0.43814167380332947, 0.1718595027923584, 0.4488520920276642, 0.37029513716697693], dtype='float32').reshape([24]),
            paddle.to_tensor([0.08032835274934769, 0.26831555366516113, 0.3279876112937927, 0.14818091690540314, 0.4894154369831085, 0.09862162172794342, 0.16283708810806274, 0.25080329179763794, 0.3337763249874115, 0.41507285833358765, 0.2331124246120453, 0.0024151059333235025, 0.3356689214706421, 0.01642523519694805, 0.28987887501716614, 0.39560791850090027, 0.20837648212909698, 0.05746585130691528, 0.40842995047569275, 0.023983178660273552, 0.047094423323869705, 0.2724613845348358, 0.33362749218940735, 0.3992570638656616], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_013ddcec6017c89b64f9987936c1a751(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 256, 512], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ccfc6d628a4e2df682ed870a51a638c4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.314710408449173, 0.06416672468185425, 0.2097308337688446, 0.3018539845943451, 0.3077201545238495, 0.38543328642845154, 0.2777332365512848, 0.028121942654252052, 0.43865808844566345, 0.46882420778274536, 0.2601540982723236, 0.2820857763290405, 0.1559409350156784, 0.4568096995353699, 0.10038777440786362, 0.14427092671394348, 0.40776556730270386, 0.31589460372924805, 0.10530773550271988, 0.35555005073547363], dtype='float32').reshape([20]),
            paddle.to_tensor([0.2334134876728058, 0.44114091992378235, 0.12872549891471863, 0.23981010913848877, 0.44972074031829834, 0.10003453493118286, 0.04715563356876373, 0.4917718470096588, 0.22881385684013367, 0.32695847749710083, 0.17645661532878876, 0.2621958255767822, 0.02113303542137146, 0.21127521991729736, 0.07107841968536377, 0.49428728222846985, 0.1974862813949585, 0.40113499760627747, 0.12282878160476685, 0.39302879571914673], dtype='float32').reshape([20]),
            paddle.to_tensor([0.3713676333427429, 0.1507992148399353, 0.4223719537258148, 0.08001867681741714, 0.31264328956604004, 0.21759118139743805, 0.15014393627643585, 0.20762479305267334, 0.47793859243392944, 0.1587098091840744, 0.4947580397129059, 0.4796546399593353, 0.062292568385601044, 0.2841520607471466, 0.12823748588562012, 0.1539691686630249, 0.3164801597595215, 0.28709566593170166, 0.2689625322818756, 0.499309778213501], dtype='float32').reshape([20]),
            paddle.to_tensor([0.214827761054039, 0.2289607673883438, 0.18351954221725464, 0.39091259241104126, 0.386564701795578, 0.21566826105117798, 0.49922746419906616, 0.1536087840795517, 0.21370048820972443, 0.027975711971521378, 0.4042951762676239, 0.05233653262257576, 0.0059801144525408745, 0.4158638119697571, 0.2952522337436676, 0.2696061134338379, 0.1583564579486847, 0.3839857280254364, 0.40171006321907043, 0.4375087022781372], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b87150ecfabc7a19e7e2e1d5f75b0af5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_145ad20e7a3b3c8aa31f6b40d4fe3d45
    def get_inputs(self):
        return [
            paddle.uniform([16, 256], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a16b2bdf1df903228b5ef33a8469352b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 80, 80], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8d2a39122b7a526c9e2e4525cdb487bb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 102, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([102], dtype='float32', min=0, max=0.5),
            paddle.uniform([102], dtype='float32', min=0, max=0.5),
            paddle.uniform([102], dtype='float32', min=0, max=0.5),
            paddle.uniform([102], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c241f252de1ab2ef5efabc745ef603bf(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 224, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([224], dtype='float32', min=0, max=0.5),
            paddle.uniform([224], dtype='float32', min=0, max=0.5),
            paddle.uniform([224], dtype='float32', min=0, max=0.5),
            paddle.uniform([224], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b59f47c4d53477462af5d042f93103b4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 76, 76], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b85e75b2975728d83795b5ff050672a7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 8, 8], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_38f4324dc9938150114f7adb99467e8c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7d2c19116e813ba08666e0d0c9aeb730(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 32, 64], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.08727995306253433, 0.3115723729133606, 0.2568989396095276, 0.09247159212827682, 0.3026641309261322, 0.42445918917655945, 0.14498990774154663, 0.4055476784706116, 0.4143112599849701, 0.1837581992149353, 0.2858794629573822, 0.1694660782814026, 0.2569742202758789, 0.33269402384757996, 0.3843045234680176, 0.305637001991272, 0.3221683204174042, 0.2789083421230316], dtype='float32').reshape([18]),
            paddle.to_tensor([0.0070621962659060955, 0.14912734925746918, 0.2927630543708801, 0.09115595370531082, 0.35614272952079773, 0.3775066137313843, 0.4382753372192383, 0.33573752641677856, 0.4261503219604492, 0.32020655274391174, 0.43762433528900146, 0.18251828849315643, 0.3621053397655487, 0.24267734587192535, 0.3641965985298157, 0.1313740760087967, 0.28682711720466614, 0.1798975169658661], dtype='float32').reshape([18]),
            paddle.to_tensor([0.4505169689655304, 0.40845122933387756, 0.07339303940534592, 0.14685238897800446, 0.15027301013469696, 0.38439661264419556, 0.3226683437824249, 0.29393020272254944, 0.2818801701068878, 0.38603395223617554, 0.14904199540615082, 0.36378297209739685, 0.3581219017505646, 0.36827704310417175, 0.10649782419204712, 0.49129122495651245, 0.04577379673719406, 0.44252336025238037], dtype='float32').reshape([18]),
            paddle.to_tensor([0.24341227114200592, 0.27076205611228943, 0.40364211797714233, 0.4709976315498352, 0.48267099261283875, 0.3581545948982239, 0.0921669602394104, 0.14127345383167267, 0.3571106195449829, 0.1610712707042694, 0.22534184157848358, 0.19968914985656738, 0.38960397243499756, 0.06816301494836807, 0.11786216497421265, 0.23148813843727112, 0.017195995897054672, 0.3816300630569458], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5912a02208a4c36696d52578864f5cb1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_b4f4da3a8d44d1b5b3a2903a097df186
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 4, 4], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e4db4e07e68f251d9ba9ce5099a65c0f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 80, 80], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.4760424494743347, 0.48524969816207886, 0.002009844873100519, 0.21154886484146118, 0.40870076417922974, 0.0030993041582405567, 0.27412718534469604, 0.0425754077732563, 0.28004640340805054, 0.3897590935230255, 0.19768764078617096, 0.07866464555263519, 0.2356078177690506, 0.16777752339839935, 0.41802385449409485, 0.20627979934215546, 0.15717151761054993, 0.25885531306266785, 0.4365106523036957, 0.41318121552467346, 0.48641344904899597, 0.05154742673039436, 0.26991063356399536, 0.07397269457578659], dtype='float32').reshape([24]),
            paddle.to_tensor([0.4942610561847687, 0.16785667836666107, 0.3027103543281555, 0.31665289402008057, 0.47922515869140625, 0.27008897066116333, 0.12935993075370789, 0.4001200497150421, 0.33119213581085205, 0.4173677861690521, 0.19776785373687744, 0.2518433928489685, 0.4804530441761017, 0.1335139125585556, 0.055479880422353745, 0.0626051276922226, 0.08155275881290436, 0.13028261065483093, 0.0539524108171463, 0.06766276806592941, 0.012181186117231846, 0.1591225564479828, 0.4205416440963745, 0.4069628417491913], dtype='float32').reshape([24]),
            paddle.to_tensor([0.34171855449676514, 0.1966378539800644, 0.4014098346233368, 0.3176187574863434, 0.29479315876960754, 0.1268816888332367, 0.264634907245636, 0.22468982636928558, 0.3256657123565674, 0.30124977231025696, 0.008959616534411907, 0.0006741240504197776, 0.3962780833244324, 0.26890796422958374, 0.48094725608825684, 0.4280053675174713, 0.22867339849472046, 0.05151212587952614, 0.16563944518566132, 0.05480481684207916, 0.1317194700241089, 0.30949074029922485, 0.08085108548402786, 0.01696769893169403], dtype='float32').reshape([24]),
            paddle.to_tensor([0.1879921555519104, 0.1879948228597641, 0.3895679712295532, 0.47331520915031433, 0.3711880147457123, 0.49683767557144165, 0.24911144375801086, 0.11771900206804276, 0.2721077799797058, 0.2315690666437149, 0.3224378228187561, 0.41737934947013855, 0.22224870324134827, 0.37122398614883423, 0.06009901687502861, 0.38358500599861145, 0.4128367006778717, 0.2496660053730011, 0.2538734972476959, 0.3415200412273407, 0.44013506174087524, 0.14946947991847992, 0.3218631148338318, 0.010896185413002968], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7f491496970c567562a5dc2ca893bf2f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 56, 60, 60], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
            paddle.uniform([56], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0e3780199d6931d7b2877934a2cf5297(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 832, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([832], dtype='float32', min=0, max=0.5),
            paddle.uniform([832], dtype='float32', min=0, max=0.5),
            paddle.uniform([832], dtype='float32', min=0, max=0.5),
            paddle.uniform([832], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_533b48e0a2606fa61cfb2a15727e4d92(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 82, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([82], dtype='float32', min=0, max=0.5),
            paddle.uniform([82], dtype='float32', min=0, max=0.5),
            paddle.uniform([82], dtype='float32', min=0, max=0.5),
            paddle.uniform([82], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_27feefa6e6dba8d8170cd03040883c78(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.19620753824710846, 0.45813998579978943, 0.20171958208084106, 0.2992435395717621, 0.37505432963371277, 0.270245224237442, 0.2327636182308197, 0.265588641166687, 0.03488349914550781, 0.47239428758621216, 0.4498855173587799, 0.30861029028892517, 0.3083689212799072, 0.27820655703544617, 0.2324001044034958, 0.028061259537935257, 0.2666180729866028, 0.0480329804122448, 0.4641360640525818, 0.2114545851945877], dtype='float32').reshape([20]),
            paddle.to_tensor([0.421753853559494, 0.4865666925907135, 0.21288704872131348, 0.09795312583446503, 0.18309670686721802, 0.36366814374923706, 0.053637925535440445, 0.37905529141426086, 0.3061903417110443, 0.37511613965034485, 0.0008519680122844875, 0.18796759843826294, 0.07436122000217438, 0.09745554625988007, 0.43638068437576294, 0.3369889557361603, 0.39196109771728516, 0.3061322569847107, 0.2138454169034958, 0.09443467110395432], dtype='float32').reshape([20]),
            paddle.to_tensor([0.3499773144721985, 0.0426032729446888, 0.3133814334869385, 0.0853329673409462, 0.4805872440338135, 0.1602289080619812, 0.06831672042608261, 0.05351365730166435, 0.1474308967590332, 0.013313837349414825, 0.1395190805196762, 0.08572715520858765, 0.13196668028831482, 0.022478103637695312, 0.49809911847114563, 0.43326449394226074, 0.23577161133289337, 0.25774577260017395, 0.1375434845685959, 0.4181838929653168], dtype='float32').reshape([20]),
            paddle.to_tensor([0.04866325482726097, 0.377929151058197, 0.4725552797317505, 0.41331368684768677, 0.26059481501579285, 0.2913784980773926, 0.21663793921470642, 0.10053294897079468, 0.3753672242164612, 0.07130951434373856, 0.06270796805620193, 0.03528914973139763, 0.06021444499492645, 0.3750806450843811, 0.4884616732597351, 0.009221751242876053, 0.419238418340683, 0.31819814443588257, 0.4115229845046997, 0.27686670422554016], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ff337c11653720880c8b42d176c0ed19(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_af91295b59c5fca17f890a802deb001f
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 4, 64, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9225e615465c1c888698e73a2fe2d888(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_3a47a120d62a3f3a422a747f511df8d9
    def get_inputs(self):
        return [
            paddle.uniform([1, 80, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5608b3b7b344235002e92ff2b7205172(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([8, 2048, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_92658b05560d4b89956c248facace849(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 702, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([702], dtype='float32', min=0, max=0.5),
            paddle.uniform([702], dtype='float32', min=0, max=0.5),
            paddle.uniform([702], dtype='float32', min=0, max=0.5),
            paddle.uniform([702], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a82eb4e924b0436836f0eae87b951a92(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 512, 256], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.34065449237823486, 0.42299824953079224, 0.3526168167591095, 0.007321098353713751, 0.1046183854341507, 0.43448299169540405, 0.16817845404148102, 0.30453866720199585, 0.2028011977672577, 0.1738460212945938, 0.0436759851872921, 0.43737852573394775, 0.19658921658992767, 0.18167680501937866, 0.4588698446750641, 0.1536923497915268], dtype='float32').reshape([16]),
            paddle.to_tensor([0.3973209857940674, 0.36372607946395874, 0.3781336545944214, 0.11725249141454697, 0.0671924352645874, 0.008404959924519062, 0.2619532644748688, 0.17969703674316406, 0.4679117500782013, 0.3649923503398895, 0.21182982623577118, 0.006698811426758766, 0.07252933084964752, 0.07975137233734131, 0.35421210527420044, 0.21526330709457397], dtype='float32').reshape([16]),
            paddle.to_tensor([0.36381223797798157, 0.02943592518568039, 0.09348192065954208, 0.2673557996749878, 0.4814379811286926, 0.021338583901524544, 0.35935646295547485, 0.025009924545884132, 0.19751305878162384, 0.2132180780172348, 0.1043381541967392, 0.464898943901062, 0.4830804169178009, 0.2233525663614273, 0.4105360507965088, 0.3398919999599457], dtype='float32').reshape([16]),
            paddle.to_tensor([0.3419407308101654, 0.24226756393909454, 0.21240219473838806, 0.3543379306793213, 0.15069706737995148, 0.20382149517536163, 0.36033743619918823, 0.292012482881546, 0.2505806088447571, 0.3083159923553467, 0.32588499784469604, 0.2895325720310211, 0.04136795178055763, 0.4429094195365906, 0.16826564073562622, 0.39415284991264343], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5bf3acb5a9fd6e6dcbf920a4f49b3c3c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 72, 128, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7d8a459993877583d69e911cdac661a1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.39540866017341614, 0.04244876280426979, 0.4647851586341858, 0.010055108927190304, 0.33055680990219116, 0.4108414053916931, 0.34358280897140503, 0.32828161120414734, 0.08453354239463806, 0.260023295879364, 0.14045533537864685, 0.04053357243537903, 0.17104697227478027, 0.197008416056633, 0.28396883606910706, 0.4549807012081146, 0.26158103346824646, 0.24383817613124847, 0.2702772617340088, 0.12223365902900696, 0.17089037597179413, 0.46241024136543274, 0.17832477390766144, 0.4045260548591614, 0.4994095265865326, 0.4399697780609131, 0.3326322138309479, 0.4750231206417084, 0.19240209460258484, 0.4191708266735077], dtype='float32').reshape([30]),
            paddle.to_tensor([0.3908115029335022, 0.21326136589050293, 0.02723599411547184, 0.07799354940652847, 0.3628165125846863, 0.36254826188087463, 0.4660426080226898, 0.3871261477470398, 0.22971995174884796, 0.2563927173614502, 0.34764668345451355, 0.030041148886084557, 0.34037548303604126, 0.09337995201349258, 0.3582691550254822, 0.42905962467193604, 0.1182582825422287, 0.16097566485404968, 0.28643396496772766, 0.25637316703796387, 0.3274199664592743, 0.47465288639068604, 0.08582162111997604, 0.37947729229927063, 0.3058415949344635, 0.4214341640472412, 0.2104068547487259, 0.31142839789390564, 0.17880922555923462, 0.450678288936615], dtype='float32').reshape([30]),
            paddle.to_tensor([0.06143299862742424, 0.33508551120758057, 0.4890533685684204, 0.3828948140144348, 0.14889100193977356, 0.2946261465549469, 0.18463246524333954, 0.3530607521533966, 0.10935016721487045, 0.1794300228357315, 0.3138652741909027, 0.4913521409034729, 0.28789299726486206, 0.15572765469551086, 0.16214759647846222, 0.3082212209701538, 0.4034029543399811, 0.3103708326816559, 0.259192556142807, 0.3736116886138916, 0.3862158954143524, 0.24311742186546326, 0.309612900018692, 0.3849579095840454, 0.37858930230140686, 0.06816250085830688, 0.1986270695924759, 0.43661239743232727, 0.2606831192970276, 0.45039114356040955], dtype='float32').reshape([30]),
            paddle.to_tensor([0.3892724812030792, 0.3735119700431824, 0.45945265889167786, 0.09784312546253204, 0.3573147654533386, 0.1805325150489807, 0.45416224002838135, 0.425553560256958, 0.08115900307893753, 0.12789709866046906, 0.34226348996162415, 0.2890423834323883, 0.14191743731498718, 0.14299257099628448, 0.218035489320755, 0.21106234192848206, 0.48094937205314636, 0.40260452032089233, 0.39569300413131714, 0.432223379611969, 0.06506430357694626, 0.2540711462497711, 0.33696720004081726, 0.4165818691253662, 0.18123896420001984, 0.10696651041507721, 0.022901151329278946, 0.43088412284851074, 0.24625572562217712, 0.4176986515522003], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8e0409ce6f1d287aed1b82add7f827e6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1baf9be6e395e6e6387fcb8474d56fa7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.07959316670894623, 0.003492280375212431, 0.08046150952577591, 0.47118839621543884, 0.4789477288722992, 0.1902516633272171, 0.06121499091386795, 0.2871161699295044, 0.002457655966281891, 0.10206081718206406, 0.03807121515274048, 0.46744784712791443, 0.24950078129768372, 0.3866279423236847, 0.3648768961429596, 0.2305230349302292, 0.0006907666684128344, 0.001453085569664836, 0.335435688495636, 0.49110689759254456, 0.44046643376350403, 0.05643012374639511, 0.4307067394256592, 0.43023788928985596, 0.0833234190940857, 0.2320738136768341, 0.026950186118483543, 0.2544432580471039, 0.34743931889533997, 0.34184420108795166], dtype='float32').reshape([30]),
            paddle.to_tensor([0.33362409472465515, 0.20291776955127716, 0.24872691929340363, 0.2785438597202301, 0.22796204686164856, 0.12134508788585663, 0.21450920403003693, 0.47117677330970764, 0.34516167640686035, 0.15380436182022095, 0.2497202306985855, 0.02367439493536949, 0.4096820652484894, 0.0819239690899849, 0.21280784904956818, 0.40926218032836914, 0.12355764210224152, 0.2796686291694641, 0.4670616686344147, 0.25039026141166687, 0.46151477098464966, 0.23643505573272705, 0.26374849677085876, 0.3928356170654297, 0.4077344834804535, 0.18052303791046143, 0.15386344492435455, 0.20946556329727173, 0.1870945692062378, 0.4637877643108368], dtype='float32').reshape([30]),
            paddle.to_tensor([0.19749808311462402, 0.39692118763923645, 0.19916096329689026, 0.40008676052093506, 0.2648649215698242, 0.40414953231811523, 0.31108370423316956, 0.2995488941669464, 0.050986070185899734, 0.18946026265621185, 0.3873789608478546, 0.3002113103866577, 0.09638363122940063, 0.2236202359199524, 0.3959079682826996, 0.3709987699985504, 0.37753474712371826, 0.2582436501979828, 0.17091405391693115, 0.49557018280029297, 0.1849675178527832, 0.17473924160003662, 0.4823334515094757, 0.3610609769821167, 0.1357106864452362, 0.3088102638721466, 0.31251853704452515, 0.26815617084503174, 0.10387160629034042, 0.3164524734020233], dtype='float32').reshape([30]),
            paddle.to_tensor([0.3532179892063141, 0.3531680703163147, 0.12764398753643036, 0.14081606268882751, 0.2863749563694, 0.07944060117006302, 0.49645981192588806, 0.03185495734214783, 0.12873443961143494, 0.01855672523379326, 0.13494861125946045, 0.07447133958339691, 0.3772691488265991, 0.263431578874588, 0.4788053333759308, 0.2446368783712387, 0.3860245943069458, 0.16247566044330597, 0.07085200399160385, 0.17847225069999695, 0.19954551756381989, 0.4265519976615906, 0.20741158723831177, 0.041216619312763214, 0.22975824773311615, 0.38280463218688965, 0.4644758105278015, 0.021923862397670746, 0.3424595594406128, 0.3649490177631378], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_63bdb7a7da4ce3cbb4e9a97af293b96c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 160, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6bb6b5e80307b55610b6f77d4226c2ec(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.08716892451047897, 0.2747562527656555, 0.382252961397171, 0.013242479413747787, 0.04215031489729881, 0.3151552677154541, 0.10220228135585785, 0.45654618740081787, 0.48880550265312195, 0.25470611453056335, 0.4231697618961334, 0.30502787232398987, 0.4086945354938507, 0.06601271778345108, 0.12092210352420807, 0.35938897728919983, 0.008363144472241402, 0.30474182963371277], dtype='float32').reshape([18]),
            paddle.to_tensor([0.28271204233169556, 0.22892412543296814, 0.2313404530286789, 0.387432724237442, 0.4929327070713043, 0.2002917379140854, 0.11949282139539719, 0.02565193735063076, 0.48921823501586914, 0.24082712829113007, 0.36457714438438416, 0.23203685879707336, 0.41685330867767334, 0.11091464757919312, 0.3487091064453125, 0.17094969749450684, 0.2876507043838501, 0.4086202383041382], dtype='float32').reshape([18]),
            paddle.to_tensor([0.4968653917312622, 0.050681572407484055, 0.16317956149578094, 0.06343793123960495, 0.23704573512077332, 0.22477948665618896, 0.3902846574783325, 0.22343981266021729, 0.26493969559669495, 0.19217024743556976, 0.024398718029260635, 0.48873889446258545, 0.08638426661491394, 0.33764615654945374, 0.34980684518814087, 0.23138739168643951, 0.26217958331108093, 0.2869727909564972], dtype='float32').reshape([18]),
            paddle.to_tensor([0.020068058744072914, 0.4827519357204437, 0.33917149901390076, 0.3292830288410187, 0.3927510380744934, 0.45879510045051575, 0.16938039660453796, 0.21173371374607086, 0.0767381563782692, 0.12728628516197205, 0.262641578912735, 0.3301168978214264, 0.26901575922966003, 0.14757800102233887, 0.09789098054170609, 0.416557252407074, 0.16979792714118958, 0.25365933775901794], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e019fb13c2774a1ba9c3252289915b90(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 256, 512], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_96514297c4dcdafe71892fd6aa27812a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 32, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bf67c8145e2df8d9a0f6b249f6f252a8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 160, 160], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.4119497239589691, 0.27770763635635376, 0.4409843683242798, 0.37532511353492737, 0.08546261489391327, 0.3412858247756958, 0.39948561787605286, 0.3877311944961548, 0.4078519344329834, 0.12578214704990387, 0.13927295804023743, 0.47288450598716736, 0.4019257128238678, 0.06844235956668854, 0.490522563457489, 0.3811776340007782, 0.3893822133541107, 0.14720745384693146, 0.26895666122436523, 0.41371238231658936, 0.0025441418401896954, 0.390747606754303, 0.001387882512062788, 0.3971284329891205], dtype='float32').reshape([24]),
            paddle.to_tensor([0.49858683347702026, 0.09417324513196945, 0.46304991841316223, 0.02802295796573162, 0.28364822268486023, 0.13170549273490906, 0.3537306785583496, 0.03531152009963989, 0.4701496660709381, 0.22111618518829346, 0.09231594204902649, 0.4019242823123932, 0.15871362388134003, 0.08509495854377747, 0.17120087146759033, 0.13438120484352112, 0.30170029401779175, 0.23408517241477966, 0.3100568354129791, 0.38815438747406006, 0.19590003788471222, 0.17254410684108734, 0.3754878342151642, 0.22510239481925964], dtype='float32').reshape([24]),
            paddle.to_tensor([0.3054518699645996, 0.4004093408584595, 0.3271883428096771, 0.2903115749359131, 0.22417977452278137, 0.19232173264026642, 0.06488920748233795, 0.3981775641441345, 0.4596420228481293, 0.01420610211789608, 0.017303667962551117, 0.00047243578592315316, 0.2578359544277191, 0.20894522964954376, 0.33849021792411804, 0.3004505932331085, 0.4949011206626892, 0.05082189664244652, 0.04126029834151268, 0.3714295029640198, 0.20116668939590454, 0.4261877238750458, 0.3784853518009186, 0.06553508341312408], dtype='float32').reshape([24]),
            paddle.to_tensor([0.1039816290140152, 0.019554754719138145, 0.15436618030071259, 0.37891972064971924, 0.16939128935337067, 0.04081517457962036, 0.14762403070926666, 0.3508952856063843, 0.21612043678760529, 0.23210708796977997, 0.10178312659263611, 0.10509247332811356, 0.12357678264379501, 0.10871575027704239, 0.19459721446037292, 0.01638464629650116, 0.006007855292409658, 0.04983900114893913, 0.24387404322624207, 0.25277090072631836, 0.16762331128120422, 0.16087108850479126, 0.46529698371887207, 0.30141761898994446], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_59550b940c85dd549d5da0830a0f50b8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 64, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b4ecc86a1e34bdabe624612b6f928c4c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a8301b810bccba10e4dfdee0df73334
    def get_inputs(self):
        return [
            paddle.uniform([1, 320, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
            paddle.uniform([320], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4f13a709635c9bd78888cc8dc244745e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.41593679785728455, 0.3231254816055298, 0.34985220432281494, 0.3984311521053314, 0.4756488502025604, 0.31790685653686523, 0.20749887824058533, 0.28874441981315613, 0.2382092922925949, 0.0003397907712496817, 0.054553695023059845, 0.1091606467962265, 0.27904295921325684, 0.35082000494003296, 0.1821097731590271, 0.19368699193000793, 0.1621180772781372, 0.21875546872615814, 0.06751671433448792, 2.9169314075261354e-05, 0.4101163446903229, 0.4174511134624481, 0.21403658390045166, 0.29017359018325806], dtype='float32').reshape([24]),
            paddle.to_tensor([0.23955291509628296, 0.1250922977924347, 0.4262375831604004, 0.4191329777240753, 0.48948684334754944, 0.3886618912220001, 0.39430147409439087, 0.2757144868373871, 0.03677651658654213, 0.4074716567993164, 0.36313143372535706, 0.4708501398563385, 0.08302858471870422, 0.33468493819236755, 0.2878340482711792, 0.10650518536567688, 0.05158548057079315, 0.12607182562351227, 0.34986260533332825, 0.25189757347106934, 0.1056021973490715, 0.33765098452568054, 0.1760464459657669, 0.4784315228462219], dtype='float32').reshape([24]),
            paddle.to_tensor([0.42227473855018616, 0.304455041885376, 0.3204461634159088, 0.48003503680229187, 0.24253961443901062, 0.12612222135066986, 0.27728357911109924, 0.4207451343536377, 0.14555373787879944, 0.036537691950798035, 0.23071983456611633, 0.4289722442626953, 0.01155728381127119, 0.20124830305576324, 0.06508840620517731, 0.19115501642227173, 0.43764615058898926, 0.24198195338249207, 0.036154404282569885, 0.12211904674768448, 0.07896453887224197, 0.34900400042533875, 0.0928969457745552, 0.47990119457244873], dtype='float32').reshape([24]),
            paddle.to_tensor([0.4027276039123535, 0.01562507450580597, 0.31878960132598877, 0.3042995035648346, 0.04216393455862999, 0.3899063766002655, 0.3332522511482239, 0.32641974091529846, 0.15379126369953156, 0.3440553843975067, 0.10193590819835663, 0.2634130120277405, 0.3249770402908325, 0.03672166168689728, 0.4239997863769531, 0.2826928496360779, 0.3580344021320343, 0.4434906542301178, 0.4063873589038849, 0.4982944130897522, 0.07919329404830933, 0.06103275343775749, 0.2547610402107239, 0.4994681179523468], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6c7ea4e2c2961211accbccc82a9ddfa1(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 256, 512], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.460455983877182, 0.3258041441440582, 0.42085424065589905, 0.1825629472732544, 0.46268585324287415, 0.1508932262659073, 0.3006163537502289, 0.07508978992700577, 0.15155787765979767, 0.033093713223934174, 0.4921491742134094, 0.3845517039299011, 0.3379315733909607, 0.019290229305624962, 0.0386483296751976, 0.43008023500442505], dtype='float32').reshape([16]),
            paddle.to_tensor([0.4281443953514099, 0.3660159111022949, 0.42029863595962524, 0.42972511053085327, 0.3611556887626648, 0.20536385476589203, 0.0933285504579544, 0.44711801409721375, 0.472463458776474, 0.34771305322647095, 0.20947527885437012, 0.22315505146980286, 0.45215076208114624, 0.23292754590511322, 0.06712682545185089, 0.10229583084583282], dtype='float32').reshape([16]),
            paddle.to_tensor([0.23594588041305542, 0.22580046951770782, 0.43833982944488525, 0.13692247867584229, 0.3427271246910095, 0.11939708143472672, 0.49079370498657227, 0.14525191485881805, 0.3904980421066284, 0.05659627914428711, 0.1958024501800537, 0.2443195879459381, 0.41787105798721313, 0.42135336995124817, 0.3898906409740448, 0.17778146266937256], dtype='float32').reshape([16]),
            paddle.to_tensor([0.4130465090274811, 0.12354102730751038, 0.44242042303085327, 0.4180437922477722, 0.34564316272735596, 0.4025505483150482, 0.327503502368927, 0.16354259848594666, 0.3209320902824402, 0.007033756468445063, 0.2887943387031555, 0.16877560317516327, 0.21815362572669983, 0.31019896268844604, 0.33484959602355957, 0.4731602072715759], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4342d2501d2cd0876870d79d2ab66e74(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 120, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_84d939733796fc236cb10d79b8053a56(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 384, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_27afbd7565710973cef21b87758f6fef(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 28, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.16943597793579102, 0.39595919847488403, 0.47177591919898987, 0.3534198999404907, 0.3460676074028015, 0.10340853035449982, 0.4987395107746124, 0.1516023874282837, 0.36189597845077515, 0.05891121178865433, 0.3542736768722534, 0.36298128962516785, 0.06501661241054535, 0.20121408998966217, 0.10552553832530975, 0.056826408952474594, 0.07676274329423904, 0.18534401059150696, 0.4692034125328064, 0.19624488055706024, 0.3597409427165985, 0.4785219132900238, 0.045976489782333374, 0.4652823507785797, 0.22409433126449585, 0.45030057430267334, 0.1072610467672348, 0.37830638885498047], dtype='float32').reshape([28]),
            paddle.to_tensor([0.3691380023956299, 0.09413500130176544, 0.47792428731918335, 0.35304272174835205, 0.04138990119099617, 0.27180030941963196, 0.2764336168766022, 0.04055316001176834, 0.25686371326446533, 0.2088208645582199, 0.31426021456718445, 0.46254852414131165, 0.1349475234746933, 0.4596765637397766, 0.17729350924491882, 0.429922878742218, 0.23315656185150146, 0.270359605550766, 0.1847989708185196, 0.05550942197442055, 0.25362834334373474, 0.431159108877182, 0.3695080578327179, 0.33043017983436584, 0.35359859466552734, 0.4368472397327423, 0.35829222202301025, 0.13410933315753937], dtype='float32').reshape([28]),
            paddle.to_tensor([0.05557096004486084, 0.08968588709831238, 0.40979859232902527, 0.4414963722229004, 0.46304193139076233, 0.32928526401519775, 0.10033035278320312, 0.08108226954936981, 0.1265133172273636, 0.23000645637512207, 0.35452139377593994, 0.49992674589157104, 0.11310021579265594, 0.3932909667491913, 0.10499320924282074, 0.12210512161254883, 0.45593854784965515, 0.14005468785762787, 0.07782470434904099, 0.07606108486652374, 0.3357319235801697, 0.4693761467933655, 0.13830608129501343, 0.20203177630901337, 0.2558981478214264, 0.18136613070964813, 0.040102481842041016, 0.43317359685897827], dtype='float32').reshape([28]),
            paddle.to_tensor([0.004256925079971552, 0.2850172221660614, 0.15463247895240784, 0.2426149994134903, 0.44280025362968445, 0.210100919008255, 0.4582255780696869, 0.34621569514274597, 0.25725871324539185, 0.28809016942977905, 0.37404707074165344, 0.43636152148246765, 0.24769750237464905, 0.41371724009513855, 0.061462245881557465, 0.36044907569885254, 0.43334120512008667, 0.1413988322019577, 0.03778975456953049, 0.02485988475382328, 0.4707949459552765, 0.14209461212158203, 0.05640260502696037, 0.3197653889656067, 0.042300816625356674, 0.016470223665237427, 0.3683938980102539, 0.08494774252176285], dtype='float32').reshape([28]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_955e1571a68c2039cd04a0e3f11eb212(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_6538b48a7528f066c6653f73464f7a0b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 256, 512], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.06957463175058365, 0.2693580090999603, 0.007607928477227688, 0.2345806211233139, 0.16359403729438782, 0.29042112827301025, 0.08685663342475891, 0.30207422375679016, 0.2064817100763321, 0.3494563102722168, 0.3266875147819519, 0.12067962437868118, 0.24613045156002045, 0.34776005148887634, 0.43933016061782837, 0.41167253255844116], dtype='float32').reshape([16]),
            paddle.to_tensor([0.16396281123161316, 0.4929197430610657, 0.1519419401884079, 0.4386049807071686, 0.22346577048301697, 0.05249692127108574, 0.1984991878271103, 0.3625791668891907, 0.12560544908046722, 0.39380475878715515, 0.10581542551517487, 0.20158463716506958, 0.3425115644931793, 0.3240610957145691, 0.20868675410747528, 0.14841938018798828], dtype='float32').reshape([16]),
            paddle.to_tensor([0.36742669343948364, 0.38706448674201965, 0.1456773430109024, 0.28011661767959595, 0.1809292882680893, 0.3765343129634857, 0.28874027729034424, 0.050867754966020584, 0.49082180857658386, 0.07147680222988129, 0.10822122544050217, 0.3785264492034912, 0.1491905152797699, 0.07948128879070282, 0.13068342208862305, 0.46472641825675964], dtype='float32').reshape([16]),
            paddle.to_tensor([0.27522242069244385, 0.2727971076965332, 0.042203593999147415, 0.48161256313323975, 0.4619990587234497, 0.25812771916389465, 0.2541239857673645, 0.3635568916797638, 0.21141169965267181, 0.08372902870178223, 0.2245449721813202, 0.04731796309351921, 0.2882307171821594, 0.2466324269771576, 0.29473233222961426, 0.3651428818702698], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_13d230d777b0b82757291afee8389fbf(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 96, 5, 5], dtype='float16', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
            paddle.uniform([96], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ac05f216d1386a0e44bcefaf74ec5bdb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 40, 40], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.13903258740901947, 0.05628363788127899, 0.304659903049469, 0.4241171181201935, 0.34665387868881226, 0.4432891309261322, 0.17443540692329407, 0.49229690432548523, 0.3534581959247589, 0.07426328957080841, 0.4207060635089874, 0.19280141592025757, 0.39290353655815125, 0.13952116668224335, 0.23002220690250397, 0.42231711745262146, 0.365020215511322, 0.022504378110170364, 0.008979427628219128, 0.29982665181159973, 0.46691861748695374, 0.3400227129459381, 0.2213335633277893, 0.37564149498939514], dtype='float32').reshape([24]),
            paddle.to_tensor([0.3736061155796051, 0.13946066796779633, 0.22242769598960876, 0.04351964592933655, 0.3980250358581543, 0.014978180639445782, 0.2773241400718689, 0.4103530943393707, 0.0010987936984747648, 0.3951767683029175, 0.2752801775932312, 0.4856913983821869, 0.4958024322986603, 0.1808958500623703, 0.13580010831356049, 0.2511652112007141, 0.4851546883583069, 0.10639339685440063, 0.3139166831970215, 0.44516491889953613, 0.4686475694179535, 0.2836206257343292, 0.1258707493543625, 0.2413640171289444], dtype='float32').reshape([24]),
            paddle.to_tensor([0.15177543461322784, 0.2770598828792572, 0.452624648809433, 0.42780449986457825, 0.21828913688659668, 0.09518259018659592, 0.17283667623996735, 0.28631407022476196, 0.1909283846616745, 0.22050634026527405, 0.04623951017856598, 0.26198288798332214, 0.34842559695243835, 0.12329919636249542, 0.24026355147361755, 0.44192856550216675, 0.32328301668167114, 0.24986644089221954, 0.21453920006752014, 0.3889833390712738, 0.22366595268249512, 0.38124701380729675, 0.15601807832717896, 0.13895857334136963], dtype='float32').reshape([24]),
            paddle.to_tensor([0.14915713667869568, 0.4758327901363373, 0.3246719241142273, 0.4603270888328552, 0.03323937579989433, 0.20456641912460327, 0.008984200656414032, 0.14748434722423553, 0.14568354189395905, 0.032728299498558044, 0.31368398666381836, 0.3572676479816437, 0.1285155713558197, 0.39359045028686523, 0.048685766756534576, 0.04322744533419609, 0.44546034932136536, 0.13159027695655823, 0.23739053308963776, 0.25006580352783203, 0.3425914943218231, 0.26075229048728943, 0.37997135519981384, 0.23169033229351044], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_465b4e553d25ce9c9e78e9db40191ecc(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 16, 50], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d0ab2186627111251b8e63d180616f51(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 8, 25], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4cff028f0dc1f89ec30587f579de486b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_da4d85bc213f70819a72e68f21214cdb
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 4, 16, 16], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_51fbf3b8996ef65f43a437fd2c6b0d44(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 1952, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([1952], dtype='float32', min=0, max=0.5),
            paddle.uniform([1952], dtype='float32', min=0, max=0.5),
            paddle.uniform([1952], dtype='float32', min=0, max=0.5),
            paddle.uniform([1952], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b3cca576a17d82a8444b64b428f98715(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 1280, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([1280], dtype='float32', min=0, max=0.5),
            paddle.uniform([1280], dtype='float32', min=0, max=0.5),
            paddle.uniform([1280], dtype='float32', min=0, max=0.5),
            paddle.uniform([1280], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_767f75f081638809e61f4c7584ccc28a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.14214979112148285, 0.3639259338378906, 0.14177149534225464, 0.41676875948905945, 0.4135076403617859, 0.4211595952510834, 0.10431653261184692, 0.4456672668457031, 0.15130318701267242, 0.11603689193725586, 0.11706017702817917, 0.04769919440150261, 0.35227468609809875, 0.3510102331638336, 0.08535054326057434, 0.28484398126602173, 0.04383601248264313, 0.05880287289619446, 0.017615649849176407, 0.2531561255455017], dtype='float32').reshape([20]),
            paddle.to_tensor([0.2521248459815979, 0.29958978295326233, 0.027111554518342018, 0.21423165500164032, 0.23612180352210999, 0.1859154850244522, 0.11142012476921082, 0.369003027677536, 0.36037978529930115, 0.1501607447862625, 0.20006899535655975, 0.2556142807006836, 0.47924184799194336, 0.18366971611976624, 0.17492884397506714, 0.11240916699171066, 0.44192060828208923, 0.09061232954263687, 0.21818798780441284, 0.23835009336471558], dtype='float32').reshape([20]),
            paddle.to_tensor([0.2693130671977997, 0.25792208313941956, 0.0420527420938015, 0.24203476309776306, 0.3017510175704956, 0.23309071362018585, 0.22503678500652313, 0.24036537110805511, 0.45278841257095337, 0.09827490150928497, 0.163249671459198, 0.1538882553577423, 0.25705137848854065, 0.3834947943687439, 0.12237898260354996, 0.02778434194624424, 0.027676556259393692, 0.2806372344493866, 0.23819129168987274, 0.02292170561850071], dtype='float32').reshape([20]),
            paddle.to_tensor([0.23896023631095886, 0.4987260401248932, 0.3844190835952759, 0.34478914737701416, 0.02732265554368496, 0.36820003390312195, 0.3125758469104767, 0.3124637305736542, 0.4569036662578583, 0.3609047830104828, 0.35704851150512695, 0.10647787153720856, 0.18621672689914703, 0.07535312324762344, 0.08141464740037918, 0.25410032272338867, 0.3495868146419525, 0.4545900821685791, 0.32243645191192627, 0.4640244245529175], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7d7fc51e65ccbd9a2c21e22a7c425b62(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 336, 4, 50], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_26efaf8d5bcabc8f8b9ee03ab166b082(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 16, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9ec3947b599c84b5e222e8fdaf1f1e07(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 112, 112], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d1a580e0e31cb2f8c03da4704ab4bb62(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 336, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
            paddle.uniform([336], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_960900ebc0fb71a855f4663f2d18faa2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_af91295b59c5fca17f890a802deb001f
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 32, 64, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_03579653d79a1ba8bf08e31bd37b81ed(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.48737552762031555, 0.37038952112197876, 0.2929771840572357, 0.43197014927864075, 0.0330171212553978, 0.22856459021568298, 0.00787331536412239, 0.25261515378952026, 0.1644562929868698, 0.42116686701774597, 0.40186256170272827, 0.4449106454849243, 0.4110330641269684, 0.38222360610961914, 0.4020058214664459, 0.2560344636440277, 0.2642906904220581, 0.13076788187026978, 0.29044824838638306, 0.24051126837730408, 0.24309517443180084, 0.35981646180152893, 0.2103300839662552, 0.15448932349681854, 0.19140447676181793, 0.4047066867351532, 0.09384504705667496, 0.45193344354629517, 0.037844642996788025, 0.3592281937599182], dtype='float32').reshape([30]),
            paddle.to_tensor([0.11293212324380875, 0.28525909781455994, 0.08292238414287567, 0.056615255773067474, 0.16685771942138672, 0.3979833126068115, 0.28451991081237793, 0.19569046795368195, 0.08868767321109772, 0.25795796513557434, 0.31608134508132935, 0.2907949388027191, 0.18625733256340027, 0.47133639454841614, 0.3761317729949951, 0.08092774450778961, 0.037868112325668335, 0.10796661674976349, 0.3302968442440033, 0.4388766586780548, 0.30412670969963074, 0.4353386461734772, 0.41041722893714905, 0.2226434051990509, 0.1675683557987213, 0.16882607340812683, 0.2388598769903183, 0.17124442756175995, 0.1316584199666977, 0.07910388708114624], dtype='float32').reshape([30]),
            paddle.to_tensor([0.48570865392684937, 0.4422001242637634, 0.20351789891719818, 0.2456876039505005, 0.3377332091331482, 0.35412245988845825, 0.4271007180213928, 0.38411757349967957, 0.18575331568717957, 0.07253523170948029, 0.12165684252977371, 0.23552870750427246, 0.2715209424495697, 0.40873023867607117, 0.4147029221057892, 0.4445178508758545, 0.09084012359380722, 0.486601322889328, 0.37618502974510193, 0.03182097524404526, 0.19586126506328583, 0.026033805683255196, 0.022265974432229996, 0.14133432507514954, 0.2701074182987213, 0.41027185320854187, 0.394814670085907, 0.33086562156677246, 0.45981305837631226, 0.07458104193210602], dtype='float32').reshape([30]),
            paddle.to_tensor([0.3751968741416931, 0.04433276131749153, 0.11832074075937271, 0.19614817202091217, 0.41812336444854736, 0.22344061732292175, 0.14568711817264557, 0.03383984789252281, 0.25284823775291443, 0.08608301728963852, 0.27378347516059875, 0.16166475415229797, 0.1911695897579193, 0.35462647676467896, 0.42063233256340027, 0.46203961968421936, 0.46935009956359863, 0.27285873889923096, 0.27907538414001465, 0.14288823306560516, 0.2976607084274292, 0.07731390744447708, 0.016084957867860794, 0.3393622040748596, 0.09431426972150803, 0.24296952784061432, 0.26062190532684326, 0.35530754923820496, 0.3670066297054291, 0.2696003019809723], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_93525d94ea551542a1ce1b3867134a97(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_da4d85bc213f70819a72e68f21214cdb
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 32, 16, 16], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5c5e6a64543906aa2b4c85d2e311d23c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 504, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([504], dtype='float32', min=0, max=0.5),
            paddle.uniform([504], dtype='float32', min=0, max=0.5),
            paddle.uniform([504], dtype='float32', min=0, max=0.5),
            paddle.uniform([504], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_48979174d183fd5afb16613b80589c74(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_145ad20e7a3b3c8aa31f6b40d4fe3d45
    def get_inputs(self):
        return [
            paddle.uniform([49, 256], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_00098c6c84999a37ffdd79d5fc0220a7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.25933393836021423, 0.4270317554473877, 0.04060373827815056, 0.33173370361328125, 0.47304797172546387, 0.14558839797973633, 0.4365278482437134, 0.400701642036438, 0.4854549169540405, 0.32257598638534546, 0.3680911660194397, 0.20568855106830597, 0.06726949661970139, 0.25282809138298035, 0.4505070745944977, 0.30079373717308044, 0.10697037726640701, 0.3995541036128998], dtype='float32').reshape([18]),
            paddle.to_tensor([0.20598284900188446, 0.3707277178764343, 0.2079690843820572, 0.3310324251651764, 0.33462363481521606, 0.04711758717894554, 0.37392401695251465, 0.19167879223823547, 0.1594272404909134, 0.09617900848388672, 0.47732481360435486, 0.3698802590370178, 0.3154182434082031, 0.20766907930374146, 0.07297848165035248, 0.29016077518463135, 0.24555203318595886, 0.10694194585084915], dtype='float32').reshape([18]),
            paddle.to_tensor([0.183976411819458, 0.1039058193564415, 0.09498340636491776, 0.22472552955150604, 0.3023381233215332, 0.4813959002494812, 0.4715644121170044, 0.24255916476249695, 0.07937347143888474, 0.4080798923969269, 0.18817922472953796, 0.12618520855903625, 0.04962727800011635, 0.4138442277908325, 0.027303587645292282, 0.36351579427719116, 0.3302876651287079, 0.4295905828475952], dtype='float32').reshape([18]),
            paddle.to_tensor([0.07063057273626328, 0.14352071285247803, 0.25501886010169983, 0.3665362596511841, 0.1485522836446762, 0.20383954048156738, 0.20848166942596436, 0.4232027530670166, 0.18119439482688904, 0.1969151794910431, 0.40371692180633545, 0.0022985811810940504, 0.33065471053123474, 0.2648439407348633, 0.26561564207077026, 0.44911304116249084, 0.4770871102809906, 0.09106002748012543], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_20bf4a85fd8acf2d8c118ef434c1e04c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 20, 20], dtype='float16', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_110e15224f9b2ef08be33af9ab69ae45(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.17492593824863434, 0.08223314583301544, 0.32022202014923096, 0.22910340130329132, 0.2658535838127136, 0.40171223878860474, 0.36253562569618225, 0.04205050691962242, 0.3817601501941681, 0.4328356087207794, 0.326712965965271, 0.013212657533586025, 0.39833468198776245, 0.2377236783504486, 0.34612786769866943, 0.27961134910583496, 0.48126763105392456, 0.3363286256790161, 0.03216371685266495, 0.26733851432800293], dtype='float32').reshape([20]),
            paddle.to_tensor([0.2143525630235672, 0.1837369054555893, 0.23447725176811218, 0.4920486509799957, 0.4099375903606415, 0.49217796325683594, 0.07140489667654037, 0.3314629793167114, 0.10299531370401382, 0.35698410868644714, 0.25879034399986267, 0.1525326371192932, 0.4993685483932495, 0.4912129342556, 0.4815918207168579, 0.434161514043808, 0.4667523205280304, 0.4963609278202057, 0.10718121379613876, 0.44992879033088684], dtype='float32').reshape([20]),
            paddle.to_tensor([0.09030502289533615, 0.4152119755744934, 0.02116275019943714, 0.2780313491821289, 0.2474433183670044, 0.05860275775194168, 0.386761873960495, 0.23911404609680176, 0.09297051280736923, 0.237071692943573, 0.4078485667705536, 0.2064843773841858, 0.3911910653114319, 0.42349323630332947, 0.1988578885793686, 0.2957366108894348, 0.3202350437641144, 0.45252934098243713, 0.4749467074871063, 0.15170349180698395], dtype='float32').reshape([20]),
            paddle.to_tensor([0.39378151297569275, 0.4558534622192383, 0.304231196641922, 0.11299571394920349, 0.4469059407711029, 0.2380347102880478, 0.08813952654600143, 0.3866117298603058, 0.20876546204090118, 0.1106911301612854, 0.38588815927505493, 0.24378906190395355, 0.14299799501895905, 0.29353705048561096, 0.3527834415435791, 0.287288635969162, 0.34248387813568115, 0.15287284553050995, 0.37265342473983765, 0.35696274042129517], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3498e87aa7e6a6f199308a31b549d4c6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 160, 160], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.0075057027861475945, 0.29935577511787415, 0.21666572988033295, 0.34411659836769104, 0.10377120226621628, 0.2580960690975189, 0.070846788585186, 0.41128435730934143, 0.04561338573694229, 0.3222939074039459, 0.34604915976524353, 0.49706849455833435, 0.4579702913761139, 0.012608230113983154, 0.011913858354091644, 0.40793344378471375], dtype='float32').reshape([16]),
            paddle.to_tensor([0.4622468650341034, 0.21661126613616943, 0.10137545317411423, 0.3523564040660858, 0.2541758418083191, 0.3596709966659546, 0.36350494623184204, 0.4332371950149536, 0.3594099283218384, 0.4758179783821106, 0.03899526968598366, 0.492061048746109, 0.35631099343299866, 0.4106981158256531, 0.07383640110492706, 0.4006960690021515], dtype='float32').reshape([16]),
            paddle.to_tensor([0.4818668067455292, 0.057370491325855255, 0.17835882306098938, 0.21697013080120087, 0.4329129159450531, 0.42415255308151245, 0.41313445568084717, 0.02068009227514267, 0.2443743199110031, 0.12123750150203705, 0.11487536132335663, 0.07615316659212112, 0.3181241750717163, 0.14603479206562042, 0.29486969113349915, 0.3183562755584717], dtype='float32').reshape([16]),
            paddle.to_tensor([0.21126100420951843, 0.055746156722307205, 0.09293514490127563, 0.3822736442089081, 0.4465416669845581, 0.46597111225128174, 0.2037867158651352, 0.4468226432800293, 0.21311122179031372, 0.4908055067062378, 0.3796435296535492, 0.30692604184150696, 0.3116193115711212, 0.42398667335510254, 0.1122652217745781, 0.3118472993373871], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_72b3ca841922e574f771cd7265014232(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_0edd578f48a07aa5d1d79f9f678b472e
    def get_inputs(self):
        return [
            paddle.uniform([49, 256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8eae295f5c2e5c3f259f5573438583fa(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 32, 16], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d9ea0719a3161af562549ff5d6b9c451(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 2048, 128, 256], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_10af86e736d31d8c8f822b63b68c96b5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 64, 128], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.2658407688140869, 0.06981386989355087, 0.07894209027290344, 0.2629556953907013, 0.3282913267612457, 0.009020388126373291, 0.32734572887420654, 0.26904296875, 0.41899681091308594, 0.02988455258309841, 0.30599841475486755, 0.4385641813278198, 0.2205199897289276, 0.44551733136177063, 0.023689446970820427, 0.3272143006324768, 0.4165889620780945, 0.40417420864105225], dtype='float32').reshape([18]),
            paddle.to_tensor([0.4180223345756531, 0.2290998101234436, 0.15515239536762238, 0.10105954110622406, 0.11278192698955536, 0.43879371881484985, 0.07235568016767502, 0.13464713096618652, 0.047078851610422134, 0.14860443770885468, 0.24144701659679413, 0.30195483565330505, 0.3962138295173645, 0.4142386317253113, 0.21034345030784607, 0.24514365196228027, 0.40240371227264404, 0.28089165687561035], dtype='float32').reshape([18]),
            paddle.to_tensor([0.28749629855155945, 0.13610103726387024, 0.0003094352432526648, 0.1598660796880722, 0.3673824667930603, 0.040982700884342194, 0.12100239843130112, 0.1593581736087799, 0.4209560751914978, 0.4434102177619934, 0.3369098901748657, 0.06778824329376221, 0.07802163809537888, 0.24976351857185364, 0.328895628452301, 0.42856064438819885, 0.2525481879711151, 0.04658319428563118], dtype='float32').reshape([18]),
            paddle.to_tensor([0.44426918029785156, 0.028388097882270813, 0.0734255239367485, 0.48834511637687683, 0.153133362531662, 0.12204448133707047, 0.36444053053855896, 0.1466507464647293, 0.42520493268966675, 0.29385340213775635, 0.05211472883820534, 0.03678659722208977, 0.42120397090911865, 0.1839504987001419, 0.34742894768714905, 0.48288705945014954, 0.34284767508506775, 0.16390708088874817], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_faece81c88b8cb5cfee46d23390eb268(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 80, 80], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_42aacc9dd98d617455ce196c4f33e31b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.11332863569259644, 0.2852092981338501, 0.14539982378482819, 0.3435695469379425, 0.33968162536621094, 0.11909468472003937, 0.14363500475883484, 0.3013315200805664, 0.27093276381492615, 0.05406557768583298, 0.22536560893058777, 0.08286838233470917, 0.18654224276542664, 0.45999929308891296, 0.03722422197461128, 0.35679182410240173, 0.4030681848526001, 0.3586171567440033, 0.16624847054481506, 0.4756237864494324, 0.12430422008037567, 0.13838857412338257, 0.18806754052639008, 0.2094494253396988], dtype='float32').reshape([24]),
            paddle.to_tensor([0.229668527841568, 0.13331001996994019, 0.003254699520766735, 0.2855781614780426, 0.0036920239217579365, 0.382296085357666, 0.1432403177022934, 0.11647837609052658, 0.18720826506614685, 0.15366366505622864, 0.1959764063358307, 0.22614054381847382, 0.09820275753736496, 0.3197827935218811, 0.4958265721797943, 0.46119117736816406, 0.424214631319046, 0.44059717655181885, 0.23277749121189117, 0.10850998014211655, 0.4217278063297272, 0.012736637145280838, 0.43156468868255615, 0.30541542172431946], dtype='float32').reshape([24]),
            paddle.to_tensor([0.22520898282527924, 0.18711255490779877, 0.3699609041213989, 0.07018204778432846, 0.2532355487346649, 0.31962382793426514, 0.29072239995002747, 0.034647949039936066, 0.050175413489341736, 0.09999332576990128, 0.18444466590881348, 0.07311224937438965, 0.2742522358894348, 0.42545127868652344, 0.24939653277397156, 0.07288598269224167, 0.063571996986866, 0.4861731231212616, 0.3744416832923889, 0.06367459148168564, 0.4058012068271637, 0.21957333385944366, 0.302389919757843, 0.01600344106554985], dtype='float32').reshape([24]),
            paddle.to_tensor([0.15681158006191254, 0.22741810977458954, 0.1267077624797821, 0.36644965410232544, 0.05213771387934685, 0.4525020122528076, 0.016522925347089767, 0.0030872137285768986, 0.05313514545559883, 0.15729206800460815, 0.03287523239850998, 0.26435232162475586, 0.328620046377182, 0.3885119557380676, 0.07072537392377853, 0.06088484078645706, 0.15293611586093903, 0.15025372803211212, 0.1528668999671936, 0.28828293085098267, 0.02694106474518776, 0.39780959486961365, 0.48913314938545227, 0.4742310345172882], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5ed428bc3ca427bdb538b732d236b92b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 208, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.uniform([208], dtype='float32', min=0, max=0.5),
            paddle.uniform([208], dtype='float32', min=0, max=0.5),
            paddle.uniform([208], dtype='float32', min=0, max=0.5),
            paddle.uniform([208], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_66d7d5cc5731157a8114087fcc1345e5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.30649837851524353, 0.46235325932502747, 0.12569668889045715, 0.45123612880706787, 0.32113829255104065, 0.16403426229953766, 0.2739903926849365, 0.3360518515110016, 0.0793033316731453, 0.27192389965057373, 0.45986127853393555, 0.3349605202674866, 0.3875463902950287, 0.027970729395747185, 0.3907143175601959, 0.3549496829509735, 0.07021098583936691, 0.14623413980007172, 0.01908787712454796, 0.36221256852149963, 0.3963336646556854, 0.3210957944393158, 0.3586273789405823, 0.05063363537192345], dtype='float32').reshape([24]),
            paddle.to_tensor([0.18419034779071808, 0.21689462661743164, 0.10934514552354813, 0.35051947832107544, 0.29221680760383606, 0.426658570766449, 0.10244465619325638, 0.4399649500846863, 0.3300687074661255, 0.2672674357891083, 0.15744173526763916, 0.4874909520149231, 0.056147221475839615, 0.4446224570274353, 0.10014429688453674, 0.015114588662981987, 0.4986315965652466, 0.01947961002588272, 0.3556162714958191, 0.24359969794750214, 0.11177881807088852, 0.3677765727043152, 0.12786465883255005, 0.1133689284324646], dtype='float32').reshape([24]),
            paddle.to_tensor([0.4273337125778198, 0.49278905987739563, 0.42773354053497314, 0.3548845946788788, 0.158016636967659, 0.05770760774612427, 0.12107204645872116, 0.23524078726768494, 0.07113867253065109, 0.09075111895799637, 0.459488183259964, 0.3800444006919861, 0.3717644214630127, 0.4078374207019806, 0.49392449855804443, 0.0011573547963052988, 0.4506891667842865, 0.09605931490659714, 0.31610482931137085, 0.4925512969493866, 0.16171061992645264, 0.053279876708984375, 0.262923002243042, 0.3454105257987976], dtype='float32').reshape([24]),
            paddle.to_tensor([0.1946384161710739, 0.040441639721393585, 0.2140306979417801, 0.3358222544193268, 0.31711989641189575, 0.0515972264111042, 0.3868575394153595, 0.00029106909641996026, 0.18240448832511902, 0.11905254423618317, 0.4800936281681061, 0.26337167620658875, 0.20318390429019928, 0.47930192947387695, 0.04835616052150726, 0.3219568133354187, 0.2449110448360443, 0.4169704020023346, 0.27415868639945984, 0.16747641563415527, 0.1660109907388687, 0.31262052059173584, 0.09579646587371826, 0.23190924525260925], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1e9e2d8c307da4aed98572186b931b75(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 80, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
            paddle.uniform([80], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cdbb6689269b63fc103c7e1d1a7dd656(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.09216026961803436, 0.16488055884838104, 0.239617720246315, 0.44817227125167847, 0.3956904411315918, 0.28027355670928955, 0.3813973069190979, 0.4124147295951843, 0.3831045925617218, 0.304220050573349, 0.3428688049316406, 0.3276115357875824, 0.14234481751918793, 0.038415905088186264, 0.3412645161151886, 0.1693674474954605], dtype='float32').reshape([16]),
            paddle.to_tensor([0.3574409782886505, 0.3954763412475586, 0.37661904096603394, 0.14882570505142212, 0.21728605031967163, 0.20255476236343384, 0.4496549665927887, 0.3591940104961395, 0.4817468225955963, 0.2978670001029968, 0.18512816727161407, 0.40075504779815674, 0.3232286870479584, 0.4913976788520813, 0.059122420847415924, 0.29500868916511536], dtype='float32').reshape([16]),
            paddle.to_tensor([0.4595491886138916, 0.0049051884561777115, 0.11436890065670013, 0.1521875560283661, 0.2090350240468979, 0.3117656111717224, 0.32448479533195496, 0.3325606882572174, 0.41278788447380066, 0.130575492978096, 0.3652006983757019, 0.32846635580062866, 0.08685695379972458, 0.3462909460067749, 0.12810441851615906, 0.1900624930858612], dtype='float32').reshape([16]),
            paddle.to_tensor([0.44086265563964844, 0.3551276624202728, 0.09470023959875107, 0.4561956524848938, 0.23514582216739655, 0.07463589310646057, 0.46745386719703674, 0.006994906812906265, 0.0012929048389196396, 0.023835767060518265, 0.2753884792327881, 0.03957252576947212, 0.13661962747573853, 0.3507654368877411, 0.10947192460298538, 0.05809991806745529], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bfdbc31c54556d244ebaa48ff6258665(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 2, 4], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_736fce276554af606e7f8ff945e79754(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.03985441103577614, 0.2937864065170288, 0.1591288447380066, 0.19140832126140594, 0.05517485737800598, 0.13762548565864563, 0.17474259436130524, 0.08286727219820023, 0.04858289659023285, 0.2743244469165802, 0.22561606764793396, 0.16005204617977142, 0.41977497935295105, 0.10955385863780975, 0.04879310354590416, 0.045570820569992065, 0.28518345952033997, 0.1336541324853897, 0.49320945143699646, 0.4333694875240326, 0.297821581363678, 0.2539539635181427, 0.4881376028060913, 0.423803448677063], dtype='float32').reshape([24]),
            paddle.to_tensor([0.16693204641342163, 0.13581858575344086, 0.4061949849128723, 0.04583967477083206, 0.4935744106769562, 0.24064721167087555, 0.19781051576137543, 0.42770078778266907, 0.46300920844078064, 0.368455708026886, 0.22387506067752838, 0.13649097084999084, 0.4117717742919922, 0.44784727692604065, 0.3527373671531677, 0.1359136700630188, 0.15995080769062042, 0.44919463992118835, 0.11968236416578293, 0.11151639372110367, 0.104923777282238, 0.36611729860305786, 0.46415385603904724, 0.1306680291891098], dtype='float32').reshape([24]),
            paddle.to_tensor([0.04864469915628433, 0.44281283020973206, 0.07445704191923141, 0.20649556815624237, 0.195307195186615, 0.38599151372909546, 0.21631647646427155, 0.42690756916999817, 0.4025436043739319, 0.021511178463697433, 0.17735622823238373, 0.04602065309882164, 0.23587818443775177, 0.18208402395248413, 0.3797244429588318, 0.22092002630233765, 0.22451834380626678, 0.34108084440231323, 0.46579501032829285, 0.4061734080314636, 0.36230674386024475, 0.07541890442371368, 0.2948160767555237, 0.397400826215744], dtype='float32').reshape([24]),
            paddle.to_tensor([0.06402681767940521, 0.11394500732421875, 0.10915800929069519, 0.10862236469984055, 0.413679301738739, 0.02483793906867504, 0.14721012115478516, 0.10016686469316483, 0.23744623363018036, 0.0007936680340208113, 0.294736385345459, 0.34295982122421265, 0.37293490767478943, 0.06584937870502472, 0.19269025325775146, 0.43768444657325745, 0.24313318729400635, 0.2397841215133667, 0.4093812108039856, 0.04385652020573616, 0.2403992861509323, 0.34008821845054626, 0.27718397974967957, 0.40238603949546814], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f53a80c1faf4bc16242ddf168b186247(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 672, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_61b1431221310ed886dfbde896abd03d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.3810195028781891, 0.010606847703456879, 0.1495044082403183, 0.094141386449337, 0.17977042496204376, 0.4034121334552765, 0.46720829606056213, 0.24474605917930603, 0.4508684277534485, 0.09216813743114471, 0.31437045335769653, 0.33699890971183777, 0.006408710964024067, 0.001743928762152791, 0.20185445249080658, 0.05476991832256317, 0.3122168481349945, 0.46052679419517517, 0.17818844318389893, 0.24419790506362915, 0.26060110330581665, 0.4789317846298218, 0.3381468653678894, 0.38018572330474854, 0.06027514487504959, 0.19359254837036133, 0.1660899817943573, 0.30635273456573486, 0.027662882581353188, 0.36759647727012634], dtype='float32').reshape([30]),
            paddle.to_tensor([0.3098454177379608, 0.33715322613716125, 0.3718946874141693, 0.12781468033790588, 0.19468799233436584, 0.39211514592170715, 0.4438300132751465, 0.3683077096939087, 0.2863064408302307, 0.11053670197725296, 0.24974821507930756, 0.39066630601882935, 0.4166699945926666, 0.47462770342826843, 0.1714731752872467, 0.007907445542514324, 0.22837400436401367, 0.18011103570461273, 0.32846733927726746, 0.281568706035614, 0.18939653038978577, 0.10356369614601135, 0.0010148216970264912, 0.4715375006198883, 0.04808512330055237, 0.4442269802093506, 0.3373643457889557, 0.41021299362182617, 0.31787487864494324, 0.22821854054927826], dtype='float32').reshape([30]),
            paddle.to_tensor([0.4540763199329376, 0.4180575907230377, 0.1734360009431839, 0.246690571308136, 0.4485841691493988, 0.33127444982528687, 0.22362497448921204, 0.40627092123031616, 0.04343489184975624, 0.46197745203971863, 0.008050349541008472, 0.024797596037387848, 0.11332902312278748, 0.3398861289024353, 0.35893622040748596, 0.36494073271751404, 0.20632292330265045, 0.1557030975818634, 0.22569534182548523, 0.09193675965070724, 0.3861309587955475, 0.13340330123901367, 0.20925214886665344, 0.07912943512201309, 0.0010520333889871836, 0.11281965672969818, 0.09840318560600281, 0.2251589149236679, 0.40641549229621887, 0.32539358735084534], dtype='float32').reshape([30]),
            paddle.to_tensor([0.28825220465660095, 0.26679643988609314, 0.1210208460688591, 0.0870327427983284, 0.3650294244289398, 0.48268717527389526, 0.10379485785961151, 0.2605193257331848, 0.37024328112602234, 0.1334623098373413, 0.04213184863328934, 0.32062584161758423, 0.2983162999153137, 0.1353391706943512, 0.3763861358165741, 0.09240074455738068, 0.22609776258468628, 0.027957316488027573, 0.07282768189907074, 0.21376413106918335, 0.43022751808166504, 0.013514429330825806, 0.300100713968277, 0.21571025252342224, 0.26074329018592834, 0.030310381203889847, 0.3791087567806244, 0.21734361350536346, 0.16586092114448547, 0.3976374566555023], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3c7de9a2e7d41a4f6ae81e6c61f3459c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 64, 64], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f72c87d6e520a8de2880adc253f317f6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.4463045299053192, 0.20297589898109436, 0.3116696774959564, 0.39716875553131104, 0.37678536772727966, 0.35435950756073, 0.17403656244277954, 0.19328315556049347, 0.2344382107257843, 0.18751338124275208, 0.1628311574459076, 0.24196572601795197, 0.2913140058517456, 0.3012894093990326, 0.08719564229249954, 0.2733503580093384, 0.33461493253707886, 0.4007520377635956, 0.24952062964439392, 0.01826414465904236], dtype='float32').reshape([20]),
            paddle.to_tensor([0.23144221305847168, 0.38474422693252563, 0.22738833725452423, 0.1639064997434616, 0.17024202644824982, 0.257724791765213, 0.16320142149925232, 0.45188671350479126, 0.0745847225189209, 0.3596499562263489, 0.17413388192653656, 0.23754730820655823, 0.2851201593875885, 0.41378143429756165, 0.3698466718196869, 0.21845132112503052, 0.46925461292266846, 0.25783348083496094, 0.14873404800891876, 0.4113614559173584], dtype='float32').reshape([20]),
            paddle.to_tensor([0.18943119049072266, 0.41484856605529785, 0.48666754364967346, 0.22988317906856537, 0.4515126645565033, 0.358685702085495, 0.03731929510831833, 0.025022396817803383, 0.059017498046159744, 0.011413341388106346, 0.42697227001190186, 0.2670406997203827, 0.03936193510890007, 0.46188920736312866, 0.028158506378531456, 0.49718722701072693, 0.07102151960134506, 0.013540348038077354, 0.0724964514374733, 0.1591060608625412], dtype='float32').reshape([20]),
            paddle.to_tensor([0.24446019530296326, 0.4109063446521759, 0.1826907992362976, 0.33241233229637146, 0.22617240250110626, 0.08303550630807877, 0.42408642172813416, 0.38209593296051025, 0.2823675274848938, 0.42883822321891785, 0.4149206578731537, 0.4851095378398895, 0.3766009211540222, 0.15132324397563934, 0.08188291639089584, 0.41886255145072937, 0.18245577812194824, 0.4836369454860687, 0.04336987063288689, 0.050503477454185486], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_94771e775085439bcf688b078c81ff23(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 122, 122], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.07749823480844498, 0.3925786316394806, 0.3049207925796509, 0.1652437448501587, 0.4040561020374298, 0.2946202754974365, 0.22426864504814148, 0.14685022830963135, 0.341387003660202, 0.16853412985801697, 0.45466795563697815, 0.4785820245742798, 0.29494139552116394, 0.11532418429851532, 0.013195998035371304, 0.3584086000919342], dtype='float32').reshape([16]),
            paddle.to_tensor([0.04978526383638382, 0.3741600513458252, 0.2964489758014679, 0.143005833029747, 0.09456170350313187, 0.18866828083992004, 0.05497296154499054, 0.4065607488155365, 0.12504494190216064, 0.06492418795824051, 0.3608287572860718, 0.18013881146907806, 0.0515839122235775, 0.42514607310295105, 0.36982378363609314, 0.4323754608631134], dtype='float32').reshape([16]),
            paddle.to_tensor([0.2743213474750519, 0.2968289852142334, 0.37423714995384216, 0.05831579864025116, 0.31343477964401245, 0.25802308320999146, 0.10116498172283173, 0.06744714826345444, 0.42086124420166016, 0.2340216338634491, 0.46089863777160645, 0.4082399904727936, 0.34859421849250793, 0.49662265181541443, 0.21710966527462006, 0.0045452783815562725], dtype='float32').reshape([16]),
            paddle.to_tensor([0.36238259077072144, 0.24448813498020172, 0.16462944447994232, 0.4491027891635895, 0.21249742805957794, 0.45158088207244873, 0.1712275594472885, 0.28878387808799744, 0.472334086894989, 0.21644315123558044, 0.48889362812042236, 0.019909555092453957, 0.08090348541736603, 0.3637588620185852, 0.3154156804084778, 0.4713946580886841], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0ab6ca355d887643a90820031f070b42(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_d589af777f5a8ae699611515c41499fb
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 20, 20], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3a661a55a7fad362bb165f61941e5011(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 144, 31, 31], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
            paddle.uniform([144], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_f45a9f65212f712d627b53a023240e28(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 16, 16], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_446dbd9ab454b2f4bb089621fcdc978d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 16, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a0e002b18d39f95c5e93199a0e88016f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 44, 40, 40], dtype='float16', min=0, max=0.5),
            paddle.uniform([44], dtype='float32', min=0, max=0.5),
            paddle.uniform([44], dtype='float32', min=0, max=0.5),
            paddle.uniform([44], dtype='float32', min=0, max=0.5),
            paddle.uniform([44], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a8ff5193cd45fc1bc8ca75ed4bb74723(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_0edd578f48a07aa5d1d79f9f678b472e
    def get_inputs(self):
        return [
            paddle.uniform([1, 1000], dtype='float32', min=0, max=0.5),
            paddle.uniform([1000], dtype='float32', min=0, max=0.5),
            paddle.uniform([1000], dtype='float32', min=0, max=0.5),
            paddle.uniform([1000], dtype='float32', min=0, max=0.5),
            paddle.uniform([1000], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bf66b1c5b32f7fc6bda3d211a9848a83(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 256, 256], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_c959d47e4759e1cb6f1d51b8a568b5d6(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 10, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.3594059348106384, 0.02319997362792492, 0.18257321417331696, 0.349458247423172, 0.29396679997444153, 0.37940967082977295, 0.44679558277130127, 0.23784500360488892, 0.19561825692653656, 0.12368534505367279], dtype='float32').reshape([10]),
            paddle.to_tensor([0.15378303825855255, 0.3084745705127716, 0.28996869921684265, 0.2124924510717392, 0.05393151193857193, 0.30392202734947205, 0.2343498170375824, 0.24386680126190186, 0.36631277203559875, 0.3641701638698578], dtype='float32').reshape([10]),
            paddle.to_tensor([0.31553229689598083, 0.01589980721473694, 0.05506693571805954, 0.019728804007172585, 0.11811677366495132, 0.1607091724872589, 0.39099782705307007, 0.3448096215724945, 0.08113162964582443, 0.18220680952072144], dtype='float32').reshape([10]),
            paddle.to_tensor([0.21658553183078766, 0.3550552725791931, 0.015581602230668068, 0.22390155494213104, 0.006487896200269461, 0.16857562959194183, 0.060327280312776566, 0.4102313816547394, 0.1787998527288437, 0.44245442748069763], dtype='float32').reshape([10]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_47cafdf047cd3893c16699258564dcd2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 112, 112], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_851610f427c8b281219a52af41346ca0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 240, 24, 24], dtype='float16', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_73f3463712697a1e02304099f4a7ba4b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.20479349792003632, 0.2253814935684204, 0.34326669573783875, 0.023989250883460045, 0.4993332028388977, 0.3777620494365692, 0.4608784019947052, 0.28354412317276, 0.1345529407262802, 0.30315038561820984, 0.015519862063229084, 0.1288868635892868, 0.3117024600505829, 0.39347943663597107, 0.31291234493255615, 0.10612496733665466, 0.39984917640686035, 0.48265597224235535, 0.31598615646362305, 0.10850812494754791], dtype='float32').reshape([20]),
            paddle.to_tensor([0.4525308310985565, 0.029801230877637863, 0.36656665802001953, 0.2400515228509903, 0.16693399846553802, 0.43865394592285156, 0.4880908727645874, 0.48120659589767456, 0.06081121787428856, 0.14325878024101257, 0.08914769440889359, 0.4617767035961151, 0.18051674962043762, 0.3367944061756134, 0.29190173745155334, 0.03903459757566452, 0.08571945875883102, 0.43042412400245667, 0.3246075212955475, 0.15552644431591034], dtype='float32').reshape([20]),
            paddle.to_tensor([0.4870905876159668, 0.447393536567688, 0.25325846672058105, 0.4688599109649658, 0.06962324678897858, 0.204634428024292, 0.25503435730934143, 0.08115140348672867, 0.3452446460723877, 0.46968021988868713, 0.2953789234161377, 0.3225369155406952, 0.41299304366111755, 0.31600746512413025, 0.2464361935853958, 0.2948693037033081, 0.15435093641281128, 0.11947652697563171, 0.30601608753204346, 0.27849581837654114], dtype='float32').reshape([20]),
            paddle.to_tensor([0.4700261056423187, 0.16000831127166748, 0.15283706784248352, 0.36746543645858765, 0.4062787890434265, 0.27743542194366455, 0.23208558559417725, 0.4985003173351288, 0.3275720179080963, 0.20743316411972046, 0.47147050499916077, 0.25838878750801086, 0.36331745982170105, 0.19537243247032166, 0.4821179509162903, 0.4847409129142761, 0.25130507349967957, 0.1371477246284485, 0.20541250705718994, 0.3617609143257141], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fc863bb932e492aa15198f273ccc1ab9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 12, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.23241814970970154, 0.2555374801158905, 0.0003707119612954557, 0.23511379957199097, 0.4516519606113434, 0.2928416430950165, 0.3882034122943878, 0.2779788076877594, 0.03613710030913353, 0.04086862504482269, 0.39097559452056885, 0.2021654099225998], dtype='float32').reshape([12]),
            paddle.to_tensor([0.07033765316009521, 0.27518346905708313, 0.11179865151643753, 0.11491360515356064, 0.16179722547531128, 0.3363892138004303, 0.18856720626354218, 0.4296489953994751, 0.4467286467552185, 0.4798223674297333, 0.3892250657081604, 0.31503328680992126], dtype='float32').reshape([12]),
            paddle.to_tensor([0.4038544297218323, 0.22709274291992188, 0.16915330290794373, 0.2681385576725006, 0.1708158552646637, 0.25192883610725403, 0.22149889171123505, 0.015092930756509304, 0.44341912865638733, 0.4458787441253662, 0.40479224920272827, 0.005476436577737331], dtype='float32').reshape([12]),
            paddle.to_tensor([0.33409348130226135, 0.4659464955329895, 0.29014167189598083, 0.11187666654586792, 0.435992568731308, 0.16814230382442474, 0.29469022154808044, 0.15162838995456696, 0.35212674736976624, 0.09031239151954651, 0.22665835916996002, 0.347971647977829], dtype='float32').reshape([12]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a5a3048c2f779d9725753401037bcde8(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_af91295b59c5fca17f890a802deb001f
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 32, 32, 32], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_78ee2e26ae0f78d9914b70443d5e057e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 200, 336], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_384c10226ee4edeb58edadfaa0e6664c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 244, 244], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.4594477117061615, 0.39793330430984497, 0.4710063338279724, 0.11802516877651215, 0.485836923122406, 0.0027834936045110226, 0.26950371265411377, 0.39641788601875305, 0.04188960790634155, 0.23261548578739166, 0.42471665143966675, 0.017064370214939117, 0.0708179771900177, 0.47578945755958557, 0.1009882241487503, 0.015862172469496727], dtype='float32').reshape([16]),
            paddle.to_tensor([0.07789333909749985, 0.16977638006210327, 0.24543246626853943, 0.11941507458686829, 0.026366891339421272, 0.0025737485848367214, 0.4523186683654785, 0.26425477862358093, 0.2752548158168793, 0.21579153835773468, 0.24792073667049408, 0.2770301401615143, 0.34357088804244995, 0.047201983630657196, 0.2733111083507538, 0.364338219165802], dtype='float32').reshape([16]),
            paddle.to_tensor([0.19001229107379913, 0.2428266853094101, 0.2125091403722763, 0.49242132902145386, 0.09526095539331436, 0.02284766174852848, 0.35819512605667114, 0.08746980875730515, 0.18793241679668427, 0.33290600776672363, 0.03615611419081688, 0.30204716324806213, 0.24009673297405243, 0.30635982751846313, 0.037178754806518555, 0.05837443843483925], dtype='float32').reshape([16]),
            paddle.to_tensor([0.37721505761146545, 0.0341062992811203, 0.3290386497974396, 0.368766725063324, 0.27014315128326416, 0.022943269461393356, 0.33823317289352417, 0.10731668025255203, 0.19401517510414124, 0.10810261964797974, 0.01493005733937025, 0.052068695425987244, 0.28847235441207886, 0.44973570108413696, 0.019937338307499886, 0.4437134563922882], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_80618ec09d85a360ca4bb9a6cdd5ddd5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_3a47a120d62a3f3a422a747f511df8d9
    def get_inputs(self):
        return [
            paddle.uniform([1, 1536, 10, 10], dtype='float16', min=0, max=0.5),
            paddle.uniform([1536], dtype='float32', min=0, max=0.5),
            paddle.uniform([1536], dtype='float32', min=0, max=0.5),
            paddle.uniform([1536], dtype='float32', min=0, max=0.5),
            paddle.uniform([1536], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a845c79765aaf7de495686721fe77c52(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 1, 1], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ea919c9ee6ea2b45f4a2fb2a55647678(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 120, 80, 80], dtype='float16', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
            paddle.uniform([120], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_602eee06ccf2da9d31916d7b8695d14c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 504, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([504], dtype='float32', min=0, max=0.5),
            paddle.uniform([504], dtype='float32', min=0, max=0.5),
            paddle.uniform([504], dtype='float32', min=0, max=0.5),
            paddle.uniform([504], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2a79e481912dcb6cd9247eee4135b531(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.41576001048088074, 0.10000333935022354, 0.3617406189441681, 0.18583816289901733, 0.303725004196167, 0.2736740708351135, 0.4416223168373108, 0.17642563581466675, 0.37190088629722595, 0.304394394159317, 0.48224055767059326, 0.4584444463253021, 0.05205921828746796, 0.3091231882572174, 0.13465464115142822, 0.0773499608039856, 0.13820119202136993, 0.398281991481781, 0.25070181488990784, 0.39141929149627686, 0.38995644450187683, 0.3791627883911133, 0.4984089732170105, 0.2592542767524719, 0.2575569152832031, 0.29484260082244873, 0.2729650139808655, 0.47102439403533936, 0.21595527231693268, 0.14579521119594574], dtype='float32').reshape([30]),
            paddle.to_tensor([0.27039700746536255, 0.43065837025642395, 0.2897668778896332, 0.34564629197120667, 0.412906289100647, 0.3713582456111908, 0.035404570400714874, 0.038538120687007904, 0.2804011106491089, 0.22679860889911652, 0.48597243428230286, 0.07179855555295944, 0.18487834930419922, 0.2350407838821411, 0.24380628764629364, 0.05104093998670578, 0.219744011759758, 0.48168155550956726, 0.10619552433490753, 0.014183765277266502, 0.08026576787233353, 0.058901023119688034, 0.17431572079658508, 0.0922161415219307, 0.12264331430196762, 0.3836144208908081, 0.33007460832595825, 0.0805019736289978, 0.00851759035140276, 0.07302460074424744], dtype='float32').reshape([30]),
            paddle.to_tensor([0.4519522786140442, 0.0005183547618798912, 0.389875590801239, 0.3717016875743866, 0.1650593876838684, 0.30092746019363403, 0.3204177916049957, 0.3820105493068695, 0.10501916706562042, 0.0044960444793105125, 0.3693988621234894, 0.38632628321647644, 0.18926449120044708, 0.167074054479599, 0.040254779160022736, 0.4640902876853943, 0.16647213697433472, 0.4185250997543335, 0.4702312648296356, 0.24701084196567535, 0.1767290234565735, 0.25621774792671204, 0.03796874359250069, 0.20335568487644196, 0.08035401254892349, 0.061516229063272476, 0.36565402150154114, 0.1381116509437561, 0.26125118136405945, 0.48174846172332764], dtype='float32').reshape([30]),
            paddle.to_tensor([0.26423466205596924, 0.25927820801734924, 0.24395254254341125, 0.4162203371524811, 0.18918377161026, 0.007553811650723219, 0.3224928081035614, 0.4110758304595947, 0.4716332256793976, 0.19920596480369568, 0.16023461520671844, 0.12585385143756866, 0.19799090921878815, 0.4651387929916382, 0.12585559487342834, 0.23357003927230835, 0.18821822106838226, 0.1627660095691681, 0.2391330450773239, 0.15685613453388214, 0.20179355144500732, 0.0725679025053978, 0.05148445442318916, 0.13514791429042816, 0.18775664269924164, 0.1823750138282776, 0.4365222752094269, 0.432793527841568, 0.1740119457244873, 0.13570638000965118], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_546d57d58adbde1715af6dce37e0207a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 2, 6], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_69f6c4c1c483a7ba9ca4e4741b56c450(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 384, 32, 32], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_e7233c4349683410a3523d100830d9f7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 384, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_77c96a32af44d736ea42aaa65d0ef19d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 1024, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([1024], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ebc840a3cba0fd337cbd0d13466ca214(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 16, 50], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bace87724506ed61238291d590778c8a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 128, 128], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_dc151cbda89ed5ffdef6d859df0ba5b5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.2597765028476715, 0.45470550656318665, 0.11798588931560516, 0.12887927889823914, 0.020065585151314735, 0.15574362874031067, 0.2135743945837021, 0.17766429483890533, 0.14113236963748932, 0.22653651237487793, 0.14493946731090546, 0.13440600037574768, 0.2434026151895523, 0.11377841234207153, 0.41384178400039673, 0.23910461366176605, 0.03146842122077942, 0.005913934670388699, 0.42516180872917175, 0.23241585493087769, 0.43550580739974976, 0.24492666125297546, 0.41662830114364624, 0.3700472116470337, 0.14379628002643585, 0.4185296893119812, 0.020721247419714928, 0.3013533353805542, 0.14302675426006317, 0.302831768989563], dtype='float32').reshape([30]),
            paddle.to_tensor([0.32150328159332275, 0.0456363670527935, 0.4171980023384094, 0.2752203047275543, 0.4129781723022461, 0.1596488505601883, 0.49906763434410095, 0.4243617355823517, 0.17415949702262878, 0.13193261623382568, 0.28504133224487305, 0.35495662689208984, 0.4877970218658447, 0.4006928503513336, 0.28534141182899475, 0.26803329586982727, 0.4332127571105957, 0.19799360632896423, 0.19393765926361084, 0.4917214512825012, 0.014143923297524452, 0.20969989895820618, 0.387635737657547, 0.22821779549121857, 0.166480153799057, 0.0392795130610466, 0.33852311968803406, 0.1661900132894516, 0.12944726645946503, 0.08418716490268707], dtype='float32').reshape([30]),
            paddle.to_tensor([0.10232612490653992, 0.24724730849266052, 0.19009527564048767, 0.3615996241569519, 0.30285343527793884, 0.33536481857299805, 0.04941132292151451, 0.23431211709976196, 0.30072852969169617, 0.13856475055217743, 0.018150826916098595, 0.3243066668510437, 0.21586957573890686, 0.006629035808146, 0.20264294743537903, 0.4850925803184509, 0.07041200995445251, 0.19713804125785828, 0.26575997471809387, 0.3087683618068695, 0.13186857104301453, 0.26723992824554443, 0.1676435023546219, 0.3922049403190613, 0.3236061930656433, 0.41139546036720276, 0.3952515423297882, 0.42189544439315796, 0.0653691217303276, 0.19231733679771423], dtype='float32').reshape([30]),
            paddle.to_tensor([0.3151624798774719, 0.3846258521080017, 0.03639712929725647, 0.25037455558776855, 0.1725001186132431, 0.2120848149061203, 0.13327573239803314, 0.4311078190803528, 0.2842770516872406, 0.22223879396915436, 0.25098833441734314, 0.16778121888637543, 0.295294851064682, 0.4920060336589813, 0.1603454053401947, 0.2816539704799652, 0.14246973395347595, 0.12532632052898407, 0.4200851619243622, 0.47272613644599915, 0.00777457095682621, 0.14672879874706268, 0.4723754823207855, 0.4713652729988098, 0.46165305376052856, 0.2950730323791504, 0.39698585867881775, 0.1798831969499588, 0.022935736924409866, 0.42304372787475586], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2df3c32e3e602e2fb89efe88f153fcfa(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 26, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.31663453578948975, 0.4798572361469269, 0.020493904128670692, 0.2869206964969635, 0.14079448580741882, 0.3785996437072754, 0.4012230634689331, 0.4323674142360687, 0.12336356192827225, 0.09344460815191269, 0.2823309898376465, 0.4660576283931732, 0.12941443920135498, 0.369327187538147, 0.4039629101753235, 0.12195748835802078, 0.4564732015132904, 0.23727631568908691, 0.15963159501552582, 0.140971839427948, 0.043028607964515686, 0.01622474193572998, 0.19530309736728668, 0.12080467492341995, 0.3493262231349945, 0.2243300825357437], dtype='float32').reshape([26]),
            paddle.to_tensor([0.22198042273521423, 0.4895192086696625, 0.27287647128105164, 0.2421780526638031, 0.01804152876138687, 0.35996678471565247, 0.37986835837364197, 0.250436931848526, 0.41775792837142944, 0.4946366846561432, 0.032798830419778824, 0.4966306984424591, 0.18029016256332397, 0.49327483773231506, 0.09833884239196777, 0.24725942313671112, 0.41541168093681335, 0.06672433763742447, 0.18162725865840912, 0.08894802629947662, 0.4062204658985138, 0.46549859642982483, 0.4894404709339142, 0.0484987273812294, 0.2153315246105194, 0.25929439067840576], dtype='float32').reshape([26]),
            paddle.to_tensor([0.1065753921866417, 0.08045049011707306, 0.1497209072113037, 0.48370325565338135, 0.045665133744478226, 0.36230653524398804, 0.4836311638355255, 0.25997859239578247, 0.4228004217147827, 0.4960174560546875, 0.08892429620027542, 0.42633649706840515, 0.27209195494651794, 0.29325127601623535, 0.3085063397884369, 0.44267526268959045, 0.3072437644004822, 0.020876813679933548, 0.4365655183792114, 0.3625173568725586, 0.43235236406326294, 0.10436904430389404, 0.07076317816972733, 0.06018364429473877, 0.2268482744693756, 0.4956822395324707], dtype='float32').reshape([26]),
            paddle.to_tensor([0.1961570531129837, 0.08187098056077957, 0.29390949010849, 0.2543138563632965, 0.2912537753582001, 0.19234907627105713, 0.21200527250766754, 0.40180009603500366, 0.18832218647003174, 0.3628450632095337, 0.009770967066287994, 0.42549648880958557, 0.05871792882680893, 0.4349491000175476, 0.1584465205669403, 0.19072270393371582, 0.3869628310203552, 0.23215052485466003, 0.08643337339162827, 0.19471392035484314, 0.17197297513484955, 0.25821298360824585, 0.05715222656726837, 0.15868566930294037, 0.07196810096502304, 0.3431500196456909], dtype='float32').reshape([26]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_422d4d3c81eb74b6218225a3163c991d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.01683821715414524, 0.09447208791971207, 0.07016526907682419, 0.08202602714300156, 0.3009457588195801, 0.2107732743024826, 0.2642141580581665, 0.25664907693862915, 0.16094335913658142, 0.09567560255527496, 0.26550042629241943, 0.3448048532009125, 0.07127226889133453, 0.22043775022029877, 0.2779553532600403, 0.48784539103507996, 0.04496535286307335, 0.2997593581676483, 0.421597421169281, 0.24819393455982208, 0.27876171469688416, 0.36072632670402527, 0.08778231590986252, 0.05190087854862213, 0.06658072769641876, 0.28826311230659485, 0.0656692236661911, 0.44224098324775696, 0.23419322073459625, 0.4855830669403076], dtype='float32').reshape([30]),
            paddle.to_tensor([0.11858691275119781, 0.02909945510327816, 0.1372784525156021, 0.24953915178775787, 0.24029433727264404, 0.06422952562570572, 0.019999926909804344, 0.4157228171825409, 0.01779339276254177, 0.1319393664598465, 0.19951587915420532, 0.09767836332321167, 0.49710217118263245, 0.005031351465731859, 0.08712661266326904, 0.130531907081604, 0.2516252100467682, 0.467306524515152, 0.23196105659008026, 0.4193499684333801, 0.3555724322795868, 0.09275553375482559, 0.23871755599975586, 0.45124921202659607, 0.32945168018341064, 0.39244744181632996, 0.10364192724227905, 0.4573342204093933, 0.0740460678935051, 0.17686013877391815], dtype='float32').reshape([30]),
            paddle.to_tensor([0.05170779675245285, 0.263077050447464, 0.48918628692626953, 0.21832530200481415, 0.2775108814239502, 0.42332780361175537, 0.3624371290206909, 0.30708062648773193, 0.17191824316978455, 0.4674406051635742, 0.013291816227138042, 0.2758638858795166, 0.34789785742759705, 0.25220075249671936, 0.36983823776245117, 0.1891196221113205, 0.427674263715744, 0.06595808267593384, 0.3509189784526825, 0.29032275080680847, 0.29319193959236145, 0.3177371025085449, 0.35190314054489136, 0.042511261999607086, 0.3155541718006134, 0.27742627263069153, 0.26026469469070435, 0.3293144702911377, 0.2501680552959442, 0.113270103931427], dtype='float32').reshape([30]),
            paddle.to_tensor([0.15787392854690552, 0.10660301893949509, 0.041568346321582794, 0.4638247489929199, 0.10484202951192856, 0.19103144109249115, 0.14150340855121613, 0.3676854372024536, 0.46312451362609863, 0.48508918285369873, 0.11960993707180023, 0.1574118286371231, 0.4237512946128845, 0.24803239107131958, 0.4937380850315094, 0.05201689153909683, 0.37515518069267273, 0.26126620173454285, 0.10202629864215851, 0.0812852531671524, 0.38293197751045227, 0.21243996918201447, 0.2887513339519501, 0.04670179262757301, 0.29248204827308655, 0.11735687404870987, 0.2954450845718384, 0.2335827648639679, 0.03719401732087135, 0.3000486493110657], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_157a00729c898c8994289990d1cee6ab(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 2048, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fe357bd2132e30fdfeb88040c3888d08(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 128, 128], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_47aa6f452775a174c4c49835e7ff1a3f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 720, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([720], dtype='float32', min=0, max=0.5),
            paddle.uniform([720], dtype='float32', min=0, max=0.5),
            paddle.uniform([720], dtype='float32', min=0, max=0.5),
            paddle.uniform([720], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_577e7fdafa0ede6e8b29ab6af134cb42(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 4, 50], dtype='float16', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1a2f94c47a382194886a6a7cd8480f8c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 40, 8, 50], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
            paddle.uniform([40], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5edf2f0c0b159c52348f728a0d9f4396(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 512, 1024], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.37054598331451416, 0.3004034757614136, 0.36073634028434753, 0.28797823190689087, 0.36973169445991516, 0.2620905637741089, 0.19957582652568817, 0.27757543325424194, 0.3846372067928314, 0.32799971103668213, 0.30411022901535034, 0.26177820563316345, 0.0928601622581482, 0.38294103741645813, 0.46108800172805786, 0.09227459132671356], dtype='float32').reshape([16]),
            paddle.to_tensor([0.1875702142715454, 0.01638462394475937, 0.010647136718034744, 0.00736524723470211, 0.25135695934295654, 0.08284176141023636, 0.34814196825027466, 0.32720646262168884, 0.25433582067489624, 0.34749001264572144, 0.4065961241722107, 0.2572247087955475, 0.49289530515670776, 0.11586270481348038, 0.009165197610855103, 0.06493624299764633], dtype='float32').reshape([16]),
            paddle.to_tensor([0.009387480095028877, 0.20592471957206726, 0.41164809465408325, 0.4586261510848999, 0.3885120749473572, 0.2517792582511902, 0.21095450222492218, 0.1810624897480011, 0.24061481654644012, 0.47636374831199646, 0.0960107296705246, 0.02284868434071541, 0.2305881232023239, 0.15753059089183807, 0.4849205017089844, 0.30454230308532715], dtype='float32').reshape([16]),
            paddle.to_tensor([0.41972237825393677, 0.08134663850069046, 0.009900852106511593, 0.49440014362335205, 0.3863065242767334, 0.46658855676651, 0.21357668936252594, 0.4492649734020233, 0.22158220410346985, 0.4456111490726471, 0.4228990375995636, 0.12054239213466644, 0.21371865272521973, 0.26316535472869873, 0.35210952162742615, 0.30295950174331665], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0c5ac0955e67428cbb438493b869f077(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 12, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.14862267673015594, 0.13618707656860352, 0.12121688574552536, 0.3159422278404236, 0.1601230800151825, 0.49324214458465576, 0.15494123101234436, 0.136725053191185, 0.017790520563721657, 0.0018615610897541046, 0.3583475351333618, 0.3753281533718109], dtype='float32').reshape([12]),
            paddle.to_tensor([0.2622189521789551, 0.32688337564468384, 0.19344742596149445, 0.29884013533592224, 0.20643149316310883, 0.04654712229967117, 0.08730977028608322, 0.28316015005111694, 0.2463606595993042, 0.09528005123138428, 0.4792705476284027, 0.10922855883836746], dtype='float32').reshape([12]),
            paddle.to_tensor([0.394323468208313, 0.24245785176753998, 0.292663037776947, 0.3531898558139801, 0.0517483614385128, 0.44799894094467163, 0.2494421750307083, 0.30283796787261963, 0.2941492795944214, 0.041008953005075455, 0.005593357607722282, 0.08210857212543488], dtype='float32').reshape([12]),
            paddle.to_tensor([0.10103197395801544, 0.3669552803039551, 0.3598272204399109, 0.16696399450302124, 0.36546236276626587, 0.2126137614250183, 0.39701536297798157, 0.42168644070625305, 0.042258940637111664, 0.30758509039878845, 0.44777336716651917, 0.3496261537075043], dtype='float32').reshape([12]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_142bd3fe899df8a44e48bb02308d27b9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 512, 1024], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_de9e3dd48ac4cada0fad4218bb34f3f5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 28, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.30109015107154846, 0.4525509178638458, 0.019241681322455406, 0.07441824674606323, 0.4422428011894226, 0.3447861671447754, 0.1713894009590149, 0.18949836492538452, 0.1723518818616867, 0.4298386871814728, 0.12830886244773865, 0.21543475985527039, 0.494538277387619, 0.2101689577102661, 0.21651601791381836, 0.35282981395721436, 0.24001973867416382, 0.35066965222358704, 0.42817389965057373, 0.014404517598450184, 0.46233057975769043, 0.11880802363157272, 0.24665403366088867, 0.32120901346206665, 0.0761832445859909, 0.43632006645202637, 0.002822727896273136, 0.40076208114624023], dtype='float32').reshape([28]),
            paddle.to_tensor([0.3223475515842438, 0.44929975271224976, 0.11528850346803665, 0.36599165201187134, 0.06668594479560852, 0.34894227981567383, 0.01807275041937828, 0.2893662452697754, 0.48862820863723755, 0.41434335708618164, 0.0026359879411756992, 0.09438001364469528, 0.3860947787761688, 0.08404204249382019, 0.06464899331331253, 0.4717258810997009, 0.2247816026210785, 0.4236467778682709, 0.41276130080223083, 0.3491719961166382, 0.38784298300743103, 0.1721811443567276, 0.48634424805641174, 0.014619885943830013, 0.3120933175086975, 0.16489726305007935, 0.24790577590465546, 0.09307555109262466], dtype='float32').reshape([28]),
            paddle.to_tensor([0.1414051055908203, 0.4976974129676819, 0.18207907676696777, 0.04777093604207039, 0.2877380847930908, 0.2909972667694092, 0.4293501377105713, 0.49421483278274536, 0.3632136583328247, 0.4405565559864044, 0.4769960939884186, 0.26180845499038696, 0.2603917419910431, 0.34816840291023254, 0.13259713351726532, 0.2926750183105469, 0.37191489338874817, 0.06393537670373917, 0.18656322360038757, 0.32816049456596375, 0.10828088968992233, 0.3883361518383026, 0.4171315133571625, 0.01590813510119915, 0.2851124405860901, 0.1498374193906784, 0.2822611629962921, 0.21075358986854553], dtype='float32').reshape([28]),
            paddle.to_tensor([0.011251583695411682, 0.35907629132270813, 0.3061369061470032, 0.09613557159900665, 0.1728953868150711, 0.42289701104164124, 0.016365094110369682, 0.19359144568443298, 0.4322088956832886, 0.09319347143173218, 0.3713028132915497, 0.2981347441673279, 0.1470041573047638, 0.33278360962867737, 0.44393953680992126, 0.40709802508354187, 0.055619485676288605, 0.006761699914932251, 0.07630540430545807, 0.3795267343521118, 0.07546133548021317, 0.2587389349937439, 0.49907493591308594, 0.020336434245109558, 0.39693036675453186, 0.3255364000797272, 0.18502506613731384, 0.13103114068508148], dtype='float32').reshape([28]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_41ba6148a087f33d277b60f248d69f8e(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 512, 256], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.07018058001995087, 0.020947445183992386, 0.3949703276157379, 0.17254945635795593, 0.1165318489074707, 0.37367671728134155, 0.30476170778274536, 0.4945967197418213, 0.31134089827537537, 0.20631515979766846, 0.39026567339897156, 0.031535934656858444, 0.02343958057463169, 0.25081443786621094, 0.2641243636608124, 0.19173701107501984], dtype='float32').reshape([16]),
            paddle.to_tensor([0.463748037815094, 0.4385305345058441, 0.33821901679039, 0.22341910004615784, 0.4576816260814667, 0.33619141578674316, 0.06485164910554886, 0.37101367115974426, 0.027822323143482208, 0.09250718355178833, 0.3521142303943634, 0.1518094539642334, 0.4770796298980713, 0.042052801698446274, 0.15006177127361298, 0.15163275599479675], dtype='float32').reshape([16]),
            paddle.to_tensor([0.47771012783050537, 0.3406626582145691, 0.4038652777671814, 0.1958221048116684, 0.35775840282440186, 0.44377052783966064, 0.22165706753730774, 0.12753821909427643, 0.22586853802204132, 0.3004816174507141, 0.3531942367553711, 0.3505972921848297, 0.13621732592582703, 0.07670179754495621, 0.3485919237136841, 0.44638314843177795], dtype='float32').reshape([16]),
            paddle.to_tensor([0.1856202930212021, 0.4384966790676117, 0.30393239855766296, 0.28637704253196716, 0.040081314742565155, 0.2977690100669861, 0.28785979747772217, 0.36525958776474, 0.277120977640152, 0.10419274866580963, 0.4984133243560791, 0.23136381804943085, 0.029291599988937378, 0.34299230575561523, 0.15071648359298706, 0.40950506925582886], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_924480489ef6cd24267b8fa2bd147c72(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 80, 80], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b62888419ace5e97051364cca9bf9d50(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 60, 60], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_55f6309c904b67ae9325a31d81b30395(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 36, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
            paddle.uniform([36], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_cdf543476e4156bf1d278fb16b955099(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([8, 512, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_503f602f0bf6371280703e84e7f6b50d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 88, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([88], dtype='float32', min=0, max=0.5),
            paddle.uniform([88], dtype='float32', min=0, max=0.5),
            paddle.uniform([88], dtype='float32', min=0, max=0.5),
            paddle.uniform([88], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0b256304ba9484cbe25651fee7a9935f(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 120, 120], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.2663467824459076, 0.4976952075958252, 0.01762637123465538, 0.4220578372478485, 0.23862920701503754, 0.22154246270656586, 0.06731360405683517, 0.2531149089336395, 0.024134458974003792, 0.19277237355709076, 0.199326753616333, 0.18955489993095398, 0.28167569637298584, 0.26700273156166077, 0.20647616684436798, 0.14522506296634674, 0.3571803569793701, 0.11434045433998108, 0.17207780480384827, 0.010055950842797756, 0.005990516394376755, 0.40174832940101624, 0.41920948028564453, 0.4842623770236969], dtype='float32').reshape([24]),
            paddle.to_tensor([0.2788851261138916, 0.14872586727142334, 0.4769812822341919, 0.4043138325214386, 0.3224262297153473, 0.20885509252548218, 0.4495214521884918, 0.4136927127838135, 0.3192780613899231, 0.13909536600112915, 0.1816757470369339, 0.17243680357933044, 0.4514378011226654, 0.11035439372062683, 0.2560614347457886, 0.24674534797668457, 0.3724687695503235, 0.49719154834747314, 0.3911842107772827, 0.13923612236976624, 0.031486041843891144, 0.44507405161857605, 0.10875600576400757, 0.24378880858421326], dtype='float32').reshape([24]),
            paddle.to_tensor([0.4042821526527405, 0.49508136510849, 0.4673615097999573, 0.3565851151943207, 0.20902971923351288, 0.2039574235677719, 0.0731768012046814, 0.06898908317089081, 0.168507382273674, 0.3161943554878235, 0.09998197108507156, 0.18728505074977875, 0.22847819328308105, 0.2766527533531189, 0.39627841114997864, 0.27121514081954956, 0.2322334200143814, 0.4243801534175873, 0.4658929109573364, 0.26620161533355713, 0.22559548914432526, 0.49228763580322266, 0.4897429943084717, 0.04732445627450943], dtype='float32').reshape([24]),
            paddle.to_tensor([0.18515221774578094, 0.06223341450095177, 0.08013488352298737, 0.1613272875547409, 0.39603424072265625, 0.36474528908729553, 0.17390522360801697, 0.013154528103768826, 0.3037387430667877, 0.12590140104293823, 0.27639615535736084, 0.333039790391922, 0.009326310828328133, 0.11489498615264893, 0.3690508008003235, 0.44073018431663513, 0.044518087059259415, 0.022834276780486107, 0.20440499484539032, 0.08023741096258163, 0.08165383338928223, 0.015560132451355457, 0.3622700572013855, 0.08468583971261978], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a9efed97d2578016508328964aec588c(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 672, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
            paddle.uniform([672], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fa4398b23912a38d92abb67a2532eb7b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 48, 112, 112], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
            paddle.uniform([48], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_58bb9af4fbe307924ddbad611e35bedd(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 270, 128, 256], dtype='float16', min=0, max=0.5),
            paddle.uniform([270], dtype='float32', min=0, max=0.5),
            paddle.uniform([270], dtype='float32', min=0, max=0.5),
            paddle.uniform([270], dtype='float32', min=0, max=0.5),
            paddle.uniform([270], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8f59aa7663625a1bfb2ea9232682d630(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 2048, 20, 20], dtype='float16', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_1607281dae9b06cea99104cef7fb27c9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 160, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
            paddle.uniform([160], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8286aae06ed3be5917dc00b49dca5a28(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_cdc3b0a148b551f86897e4668e0bdb6b
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 128, 128], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_ec82175f2fe8c89e54a7aa6733150e57(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_f5f2213be92f20fda5378beeed72688f
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 128, 128], dtype='float16', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9f3e2383158e0ef21b677e7d2b5bf1ef(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 120, 120], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.2657885253429413, 0.24176688492298126, 0.1687089204788208, 0.352490097284317, 0.059596914798021317, 0.22852541506290436, 0.4714093506336212, 0.21501493453979492, 0.36361047625541687, 0.23962685465812683, 0.09114010632038116, 0.19506219029426575, 0.465202271938324, 0.34914541244506836, 0.4065949618816376, 0.4701648950576782, 0.45898544788360596, 0.4252355694770813, 0.4265190660953522, 0.36471471190452576, 0.14415733516216278, 0.032774295657873154, 0.24012912809848785, 0.06465814262628555], dtype='float32').reshape([24]),
            paddle.to_tensor([0.12433996796607971, 0.1699681431055069, 0.06481463462114334, 0.49993887543678284, 0.3328625559806824, 0.4405374825000763, 0.0012278617359697819, 0.3491306006908417, 0.14596803486347198, 0.08167089521884918, 0.4263477325439453, 0.3630184829235077, 0.23803336918354034, 0.24988894164562225, 0.21053078770637512, 0.10790607333183289, 0.2307877540588379, 0.37456193566322327, 0.05541955679655075, 0.2099316567182541, 0.43722644448280334, 0.4906252920627594, 0.3197430968284607, 0.0774916484951973], dtype='float32').reshape([24]),
            paddle.to_tensor([0.13372856378555298, 0.4604690670967102, 0.26233354210853577, 0.16382940113544464, 0.30950871109962463, 0.04596935585141182, 0.24806903302669525, 0.4146198332309723, 0.3498261868953705, 0.4947211444377899, 0.06104433536529541, 0.4735141098499298, 0.2887125015258789, 0.11771734803915024, 0.20712485909461975, 0.34464389085769653, 0.37356632947921753, 0.4317685067653656, 0.284321665763855, 0.28425878286361694, 0.18967480957508087, 0.42079371213912964, 0.001963705290108919, 0.3034517765045166], dtype='float32').reshape([24]),
            paddle.to_tensor([0.48458191752433777, 0.08427365124225616, 0.38004356622695923, 0.49162471294403076, 0.13253483176231384, 0.03747623786330223, 0.20527835190296173, 0.20846007764339447, 0.4234007000923157, 0.2873794436454773, 0.15593849122524261, 0.23358440399169922, 0.20911189913749695, 0.30530795454978943, 0.08051365613937378, 0.23101675510406494, 0.3635667562484741, 0.006836315151304007, 0.1846221387386322, 0.2858435809612274, 0.45944204926490784, 0.36459648609161377, 0.1217748299241066, 0.24132435023784637], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_dc31106fae635f019469967ae108bc47(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 240, 128, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
            paddle.uniform([240], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a4325fa88056847cad207cd9ee22f2a4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 7, 7], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_56f9ca120930133a4ae24e1ba469ca43(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 72, 256, 128], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
            paddle.uniform([72], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_2f837e916878ab6693a7ef3b92ca0eae(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_b4f4da3a8d44d1b5b3a2903a097df186
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 16, 16], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_995b012684593e484bd318f7409d2597(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 100, 168], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4fa6e0f95a1fb8606ad00df10b41b4a4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 512, 4, 12], dtype='float16', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
            paddle.uniform([512], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_12d34811fb0f715838c846227b41d3fb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_fc6b6917e04b7b7cf9fdb39dde97ec40
    def get_inputs(self):
        return [
            paddle.uniform([1, 32, 160, 160], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
            paddle.uniform([32], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_7508ecbd57e112b80691f2375dec8ce2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_da4d85bc213f70819a72e68f21214cdb
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 4, 128, 128], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_44f20939ad2bb6e8611a0a98c9996fc7(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 18, 32, 64], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.097789466381073, 0.09965898096561432, 0.4905933737754822, 0.22604259848594666, 0.4490077793598175, 0.3297808766365051, 0.29276785254478455, 0.2468537837266922, 0.4150420129299164, 0.44720739126205444, 0.4254138767719269, 0.1089896559715271, 0.15332645177841187, 0.4841565489768982, 0.4388268291950226, 0.42464712262153625, 0.33383652567863464, 0.43281492590904236], dtype='float32').reshape([18]),
            paddle.to_tensor([0.2777903974056244, 0.46263790130615234, 0.12501300871372223, 0.2008386105298996, 0.12787695229053497, 0.03454957902431488, 0.4908421039581299, 0.042967941612005234, 0.45557764172554016, 0.3412184715270996, 0.3307771682739258, 0.21894344687461853, 0.05633469671010971, 0.029023922979831696, 0.2517091929912567, 0.11939169466495514, 0.49312150478363037, 0.12339624017477036], dtype='float32').reshape([18]),
            paddle.to_tensor([0.46885740756988525, 0.09406759589910507, 0.23367173969745636, 0.25081631541252136, 0.15229883790016174, 0.25876328349113464, 0.14876414835453033, 0.4607924520969391, 0.29738447070121765, 0.42670273780822754, 0.30704864859580994, 0.39566853642463684, 0.18278868496418, 0.3225157856941223, 0.03702143579721451, 0.2648472785949707, 0.21390889585018158, 0.4897204339504242], dtype='float32').reshape([18]),
            paddle.to_tensor([0.0388188436627388, 0.24968349933624268, 0.3646657466888428, 0.38302725553512573, 0.43109285831451416, 0.26209673285484314, 0.4478187561035156, 0.46559053659439087, 0.2322099208831787, 0.32695651054382324, 0.0025805241893976927, 0.4547048509120941, 0.42509329319000244, 0.3624899685382843, 0.3731491267681122, 0.06361544877290726, 0.19338352978229523, 0.18234680593013763], dtype='float32').reshape([18]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_bd96d6c1d403c56d1c1aaeb67fa8012b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 28, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.47908875346183777, 0.34928661584854126, 0.41263970732688904, 0.19154655933380127, 0.040129728615283966, 0.41412314772605896, 0.041586700826883316, 0.1847740262746811, 0.12920795381069183, 0.4260278344154358, 0.2129499912261963, 0.3144682049751282, 0.36869344115257263, 0.25504982471466064, 0.3511764109134674, 0.25630706548690796, 0.08860533684492111, 0.02601328305900097, 0.4877561628818512, 0.4676116704940796, 0.17947649955749512, 0.31390684843063354, 0.2880606949329376, 0.27600815892219543, 0.4518052339553833, 0.1083124577999115, 0.4409014880657196, 0.45223119854927063], dtype='float32').reshape([28]),
            paddle.to_tensor([0.34543824195861816, 0.15538617968559265, 0.03681402653455734, 0.3640563189983368, 0.3691689372062683, 0.38980212807655334, 0.2123551219701767, 0.08199343830347061, 0.1169830933213234, 0.2591479420661926, 0.30735689401626587, 0.47045448422431946, 0.0929877981543541, 0.04181680828332901, 0.3375890254974365, 0.28421372175216675, 0.3763800263404846, 0.4502932131290436, 0.1527351438999176, 0.13870583474636078, 0.02812015637755394, 0.45610135793685913, 0.45073896646499634, 0.36900365352630615, 0.037688858807086945, 0.47283947467803955, 0.14539523422718048, 0.053799860179424286], dtype='float32').reshape([28]),
            paddle.to_tensor([0.08060629665851593, 0.3247537612915039, 0.1515551507472992, 0.404172420501709, 0.4230325520038605, 0.4602572023868561, 0.11633465439081192, 0.12187858670949936, 0.2975747287273407, 0.07534203678369522, 0.2650165855884552, 0.08952198922634125, 0.10988056659698486, 0.2606832683086395, 0.34844186902046204, 0.030595574527978897, 0.4408576786518097, 0.39304542541503906, 0.016919532790780067, 0.38644152879714966, 0.021363817155361176, 0.4918535053730011, 0.2865881323814392, 0.20301082730293274, 0.1924000084400177, 0.007768352050334215, 0.12419793009757996, 0.07814327627420425], dtype='float32').reshape([28]),
            paddle.to_tensor([0.1366203874349594, 0.3118547797203064, 0.14078688621520996, 0.33138588070869446, 0.43109679222106934, 0.16094724833965302, 0.2777140140533447, 0.39756304025650024, 0.45056626200675964, 0.4678102433681488, 0.05457247421145439, 0.19962073862552643, 0.16327084600925446, 0.1292785406112671, 0.19765812158584595, 0.23416611552238464, 0.24587519466876984, 0.311455100774765, 0.18837255239486694, 0.2286202758550644, 0.12190523743629456, 0.31053414940834045, 0.18697090446949005, 0.2864876687526703, 0.4267125725746155, 0.4142911434173584, 0.22252297401428223, 0.2620175778865814], dtype='float32').reshape([28]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_076ea5040a268410abc6f683857b4039(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 30, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.4146807789802551, 0.022703690454363823, 0.4085629880428314, 0.2681536078453064, 0.14565154910087585, 0.22148914635181427, 0.12007579207420349, 0.3957166075706482, 0.37765568494796753, 0.47431597113609314, 0.40298354625701904, 0.3123265206813812, 0.3722339868545532, 0.11638760566711426, 0.47770434617996216, 0.010692665353417397, 0.09330771863460541, 0.21215985715389252, 0.044151660054922104, 0.30119505524635315, 0.26285678148269653, 0.0650152936577797, 0.06693074107170105, 0.3149409294128418, 0.1280990093946457, 0.4101160764694214, 0.35926392674446106, 0.2730022072792053, 0.3279467523097992, 0.4002620279788971], dtype='float32').reshape([30]),
            paddle.to_tensor([0.09512977302074432, 0.04934779182076454, 0.344463586807251, 0.45067518949508667, 0.38504213094711304, 0.383340448141098, 0.4641031324863434, 0.061447761952877045, 0.1754274070262909, 0.31747785210609436, 0.09909552335739136, 0.24849453568458557, 0.27783647179603577, 0.47761180996894836, 0.08284831792116165, 0.21810707449913025, 0.008353770710527897, 0.27279457449913025, 0.1236424595117569, 0.04784161224961281, 0.08699775487184525, 0.3088276982307434, 0.05463022366166115, 0.18174459040164948, 0.4528130292892456, 0.29586261510849, 0.02287902683019638, 0.32092976570129395, 0.3279225826263428, 0.4665844440460205], dtype='float32').reshape([30]),
            paddle.to_tensor([0.241164430975914, 0.33527493476867676, 0.21569786965847015, 0.031024184077978134, 0.4907648265361786, 0.2093818336725235, 0.45971056818962097, 0.27736330032348633, 0.2571568787097931, 0.39916712045669556, 0.34861665964126587, 0.22997400164604187, 0.09964150190353394, 0.17670685052871704, 0.2874782681465149, 0.35486745834350586, 0.15068726241588593, 0.22290678322315216, 0.478788286447525, 0.14098453521728516, 0.39089760184288025, 0.020401500165462494, 0.3321877121925354, 0.011659136041998863, 0.4738399386405945, 0.15533830225467682, 0.487179696559906, 0.09271867573261261, 0.3099006712436676, 0.20411580801010132], dtype='float32').reshape([30]),
            paddle.to_tensor([0.00846143439412117, 0.2799355983734131, 0.19044886529445648, 0.1377335488796234, 0.4909283518791199, 0.2907765209674835, 0.20594578981399536, 0.33574017882347107, 0.13802336156368256, 0.28452712297439575, 0.24571087956428528, 0.4344549775123596, 0.0201136264950037, 0.37535685300827026, 0.29429367184638977, 0.48334890604019165, 0.057625897228717804, 0.13697011768817902, 0.3126543462276459, 0.3489058315753937, 0.3229951560497284, 0.2686766982078552, 0.3301074206829071, 0.3912583887577057, 0.1470484584569931, 0.24023492634296417, 0.22354756295681, 0.4038815200328827, 0.31916847825050354, 0.15234152972698212], dtype='float32').reshape([30]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_850f7801d10bae4bcca0dc93ebd0d058(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 20, 32, 24], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.10325209051370621, 0.49153879284858704, 0.08226370066404343, 0.12427998334169388, 0.16481775045394897, 0.08603072166442871, 0.11452973634004593, 0.1544186770915985, 0.16030412912368774, 0.1414649486541748, 0.43177318572998047, 0.44626912474632263, 0.43952488899230957, 0.42199423909187317, 0.024976292625069618, 0.38023748993873596, 0.3764213025569916, 0.4851767122745514, 0.12314995378255844, 0.1076059564948082], dtype='float32').reshape([20]),
            paddle.to_tensor([0.1638372838497162, 0.09594650566577911, 0.3282797634601593, 0.21857957541942596, 0.29188600182533264, 0.13053186237812042, 0.09971695393323898, 0.15344630181789398, 0.13356773555278778, 0.15062600374221802, 0.12059474736452103, 0.4084808826446533, 0.11356690526008606, 0.29120194911956787, 0.09531418234109879, 0.08390528708696365, 0.46554991602897644, 0.008153734728693962, 0.012651071883738041, 0.4485453963279724], dtype='float32').reshape([20]),
            paddle.to_tensor([0.09572943300008774, 0.11921707540750504, 0.008758476004004478, 0.27848073840141296, 0.23050814867019653, 0.13003124296665192, 0.04707857221364975, 0.4384790360927582, 0.43490585684776306, 0.055384524166584015, 0.029226403683423996, 0.011255192570388317, 0.24686694145202637, 0.042573004961013794, 0.4151672422885895, 0.08024958521127701, 0.34758129715919495, 0.21724140644073486, 0.01903664879500866, 0.09941481053829193], dtype='float32').reshape([20]),
            paddle.to_tensor([0.04948820546269417, 0.001848040148615837, 0.19752784073352814, 0.26039236783981323, 0.23148909211158752, 0.35829490423202515, 0.4158654808998108, 0.43718844652175903, 0.467192679643631, 0.2915014326572418, 0.10251601040363312, 0.09615622460842133, 0.09282974153757095, 0.08788000792264938, 0.21830396354198456, 0.3598552346229553, 0.47935712337493896, 0.16202719509601593, 0.44113731384277344, 0.029672283679246902], dtype='float32').reshape([20]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_14250fb6dea63a62d3e88c31659def05(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 768, 28, 28], dtype='float16', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
            paddle.uniform([768], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_78d05869059c0de0a3565dd91bb748aa(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([8, 256, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_3f5896676167495b3fa2df9a372f8c30(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 112, 112], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.4433872401714325, 0.06387295573949814, 0.4215242266654968, 0.2984399199485779, 0.028552468866109848, 0.39058607816696167, 0.3932042121887207, 0.17903517186641693], dtype='float32').reshape([8]),
            paddle.to_tensor([0.11103024333715439, 0.08818923681974411, 0.09638731181621552, 0.1902414858341217, 0.48752933740615845, 0.19674155116081238, 0.20883168280124664, 0.13074852526187897], dtype='float32').reshape([8]),
            paddle.to_tensor([0.37973394989967346, 0.3139951229095459, 0.265259325504303, 0.11747702956199646, 0.2161221206188202, 0.49798598885536194, 0.2909526228904724, 0.34757286310195923], dtype='float32').reshape([8]),
            paddle.to_tensor([0.12234265357255936, 0.365144282579422, 0.33249586820602417, 0.4150637984275818, 0.20397008955478668, 0.2298785299062729, 0.12166199088096619, 0.31340786814689636], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_479cb8d89c4a619fca6fd64afb418025(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 19, 1, 1], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.15117697417736053, 0.3457278609275818, 0.3379707932472229, 0.12972261011600494, 0.15020965039730072, 0.035104408860206604, 0.3409793972969055, 0.3399008512496948, 0.17844468355178833, 0.23171305656433105, 0.15183581411838531, 0.2775949239730835, 0.45217519998550415, 0.3569883406162262, 0.16851826012134552, 0.16554699838161469, 0.474479615688324, 0.4635065495967865, 0.44872117042541504], dtype='float32').reshape([19]),
            paddle.to_tensor([0.48747771978378296, 0.10383711755275726, 0.07685679197311401, 0.3185398578643799, 0.3761698007583618, 0.25215962529182434, 0.11502020061016083, 0.20576640963554382, 0.022341875359416008, 0.3903130888938904, 0.4374162554740906, 0.30328619480133057, 0.3765583336353302, 0.15626554191112518, 0.06443014740943909, 0.22114509344100952, 0.2953651249408722, 0.13899943232536316, 0.18801076710224152], dtype='float32').reshape([19]),
            paddle.to_tensor([0.09519033133983612, 0.48493334650993347, 0.2702029347419739, 0.24218644201755524, 0.43793004751205444, 0.3784063458442688, 0.05267500877380371, 0.4847963750362396, 0.2594585120677948, 0.4372350871562958, 0.052201226353645325, 0.4738295078277588, 0.34713858366012573, 0.46442368626594543, 0.14900165796279907, 0.3106692433357239, 0.47967982292175293, 0.10939230769872665, 0.17556646466255188], dtype='float32').reshape([19]),
            paddle.to_tensor([0.28623726963996887, 0.2019701898097992, 0.0823867991566658, 0.2151053249835968, 0.2573975920677185, 0.46477454900741577, 0.06433246284723282, 0.21129073202610016, 0.07295635342597961, 0.02640903741121292, 0.3924524188041687, 0.24100235104560852, 0.4524957835674286, 0.31349804997444153, 0.13050109148025513, 0.33254358172416687, 0.20460450649261475, 0.30465129017829895, 0.2796250581741333], dtype='float32').reshape([19]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_b274ea83c927a37d520664248e3b7925(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 152, 152], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_0b7584ea6d84752ca0780906fa8237c4(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 175, 25], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_5907b03828c722d77bcaf3695ae404cf(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_b4f4da3a8d44d1b5b3a2903a097df186
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 64, 64], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d1406e8c5be2673e895036958812b2b2(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 384, 20, 20], dtype='float16', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
            paddle.uniform([384], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_41775a0ad6fce6db60f47eec578e42e9(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 432, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.uniform([432], dtype='float32', min=0, max=0.5),
            paddle.uniform([432], dtype='float32', min=0, max=0.5),
            paddle.uniform([432], dtype='float32', min=0, max=0.5),
            paddle.uniform([432], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_969df04f47281a56f03dc1a98e3f22f0(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 480, 7, 7], dtype='float16', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
            paddle.uniform([480], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_d0ee06d1d50e024c538f90647fdd85eb(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 14, 14], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_97122c3530b2753b0f51eed4f866fb76(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 4, 12], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_8c028efd3f171936765f3e188f1e9e33(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 128, 160, 160], dtype='float16', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
            paddle.uniform([128], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_4f1c0eb1d471bafeaa68fad0da51276a(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 8, 320, 320], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.20979084074497223, 0.06653925776481628, 0.12163496017456055, 0.22007523477077484, 0.07238969206809998, 0.1351539045572281, 0.06274262815713882, 0.36963972449302673], dtype='float32').reshape([8]),
            paddle.to_tensor([0.44905945658683777, 0.29706424474716187, 0.12157077342271805, 0.42103755474090576, 0.2729501724243164, 0.041085489094257355, 0.3713221251964569, 0.07234456390142441], dtype='float32').reshape([8]),
            paddle.to_tensor([0.1031985729932785, 0.4953809082508087, 0.45268845558166504, 0.19300922751426697, 0.15461836755275726, 0.08707114309072495, 0.24802687764167786, 0.09564466029405594], dtype='float32').reshape([8]),
            paddle.to_tensor([0.24614328145980835, 0.32782796025276184, 0.16755762696266174, 0.00907647144049406, 0.3441101312637329, 0.11417510360479355, 0.25310057401657104, 0.2908385992050171], dtype='float32').reshape([8]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fb26cc5cd6786a8a72008b6f489af8fd(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 10, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.16959239542484283, 0.3676764667034149, 0.46742936968803406, 0.10071887820959091, 0.16012492775917053, 0.35823962092399597, 0.23450765013694763, 0.20219765603542328, 0.029898513108491898, 0.0806112289428711], dtype='float32').reshape([10]),
            paddle.to_tensor([0.33422037959098816, 0.3220083713531494, 0.21946996450424194, 0.2238111048936844, 0.448353111743927, 0.4890853762626648, 0.2934299111366272, 0.008329694159328938, 0.16571523249149323, 0.14752726256847382], dtype='float32').reshape([10]),
            paddle.to_tensor([0.04075416550040245, 3.7162681110203266e-06, 0.3993680477142334, 0.248592808842659, 0.33523550629615784, 0.4031301736831665, 0.013315207324922085, 0.27803319692611694, 0.4968441426753998, 0.2522871494293213], dtype='float32').reshape([10]),
            paddle.to_tensor([0.42576539516448975, 0.3616030514240265, 0.26708996295928955, 0.31956568360328674, 0.277572363615036, 0.04555409401655197, 0.2138369083404541, 0.30113524198532104, 0.08023683726787567, 0.03699403256177902], dtype='float32').reshape([10]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_75b1040599ed84d9de2a5eeda5646b48(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 576, 16, 16], dtype='float32', min=0, max=0.5),
            paddle.uniform([576], dtype='float32', min=0, max=0.5),
            paddle.uniform([576], dtype='float32', min=0, max=0.5),
            paddle.uniform([576], dtype='float32', min=0, max=0.5),
            paddle.uniform([576], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_598d2b6a6b2bad8132ded145a213ba7b(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 16, 32, 24], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.1586047261953354, 0.14828357100486755, 0.13544943928718567, 0.3385903835296631, 0.33846598863601685, 0.3839902877807617, 0.09188710898160934, 0.4715004563331604, 0.13811855018138885, 0.2602512836456299, 0.38914650678634644, 0.014390637166798115, 0.133548304438591, 0.0630721002817154, 0.04033895581960678, 0.15382146835327148], dtype='float32').reshape([16]),
            paddle.to_tensor([0.25324803590774536, 0.06678261607885361, 0.09248851984739304, 0.361046701669693, 0.17048174142837524, 0.35964837670326233, 0.487727552652359, 0.09863374382257462, 0.08788490295410156, 0.3229489028453827, 0.061583250761032104, 0.026346690952777863, 0.30802685022354126, 0.25822219252586365, 0.41345834732055664, 0.46428170800209045], dtype='float32').reshape([16]),
            paddle.to_tensor([0.1092705950140953, 0.1031605526804924, 0.24416454136371613, 0.22734156250953674, 0.2058936208486557, 0.2265186458826065, 0.2813534736633301, 0.26959115266799927, 0.4741302728652954, 0.3746083378791809, 0.24446839094161987, 0.3994561433792114, 0.3387772738933563, 0.1550534963607788, 0.19426776468753815, 0.31760814785957336], dtype='float32').reshape([16]),
            paddle.to_tensor([0.2857457101345062, 0.05415813624858856, 0.26152846217155457, 0.420391708612442, 0.15394774079322815, 0.07953095436096191, 0.2150038778781891, 0.06694494187831879, 0.4070848822593689, 0.1677968055009842, 0.32700634002685547, 0.3957687020301819, 0.16552457213401794, 0.0052697123028337955, 0.2936229109764099, 0.2538420855998993], dtype='float32').reshape([16]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_419718d7255ac4d91daf74849782fa6d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 118, 28, 28], dtype='float32', min=0, max=0.5),
            paddle.uniform([118], dtype='float32', min=0, max=0.5),
            paddle.uniform([118], dtype='float32', min=0, max=0.5),
            paddle.uniform([118], dtype='float32', min=0, max=0.5),
            paddle.uniform([118], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_fee8e06f7df46e36bc40d7292b833836(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 24, 80], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9d3d9307638e54f3a3be3459544e804d(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_af91295b59c5fca17f890a802deb001f
    def get_inputs(self):
        return [
            paddle.uniform([1, 2048, 4, 8, 8], dtype='float16', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
            paddle.uniform([2048], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9a1d1cb8bcbefc02a04c913a31b69141(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_24ba03bb73cebf1d81ef643b014f655d
    def get_inputs(self):
        return [
            paddle.uniform([1, 64, 112, 112], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
            paddle.uniform([64], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_a8022d81b2c3dc5886d9c77ad5511b75(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_80fdc93e12b3de6fd4a696757456cc2d
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 56, 56], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_9ac8600c5bc12f8057b73c027133b7b3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 12, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.4861321449279785, 0.25527483224868774, 0.12078334391117096, 0.27011388540267944, 0.4432622492313385, 0.11098938435316086, 0.4964452385902405, 0.010531047359108925, 0.34406858682632446, 0.3650142550468445, 0.03355054557323456, 0.4643867611885071], dtype='float32').reshape([12]),
            paddle.to_tensor([0.018165551126003265, 0.18132910132408142, 0.38455432653427124, 0.19929742813110352, 0.28901055455207825, 0.19281086325645447, 0.12671536207199097, 0.051474135369062424, 0.34567132592201233, 0.3768673837184906, 0.30508002638816833, 0.3704338073730469], dtype='float32').reshape([12]),
            paddle.to_tensor([0.1725519597530365, 0.4141179621219635, 0.09597031027078629, 0.3786435127258301, 0.34012743830680847, 0.40472444891929626, 0.07932902127504349, 0.08708753436803818, 0.06237487867474556, 0.07029993087053299, 0.0982823297381401, 0.285396933555603], dtype='float32').reshape([12]),
            paddle.to_tensor([0.2850028872489929, 0.3135116994380951, 0.16349881887435913, 0.16641905903816223, 0.03309554606676102, 0.32166963815689087, 0.4431367814540863, 0.007847699336707592, 0.052112922072410583, 0.29935550689697266, 0.047339849174022675, 0.07585734874010086], dtype='float32').reshape([12]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_df6588e2c53d2eef256b9952ea0ca8a5(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 28, 14, 14], dtype='float16', min=0, max=0.5),
            paddle.to_tensor([0.41721871495246887, 0.44780978560447693, 0.06057889014482498, 0.3313571810722351, 0.2851734757423401, 0.019106393679976463, 0.04001544788479805, 0.02965785376727581, 0.37840837240219116, 0.2549726665019989, 0.14398112893104553, 0.10067357867956161, 0.11445340514183044, 0.2854296565055847, 0.4688684940338135, 0.07140162587165833, 0.21783117949962616, 0.38213083148002625, 0.35203826427459717, 0.3788103759288788, 0.31568795442581177, 0.37988364696502686, 0.37261563539505005, 0.07723306864500046, 0.20400965213775635, 0.23869681358337402, 0.2573055028915405, 0.005010261666029692], dtype='float32').reshape([28]),
            paddle.to_tensor([0.3596121072769165, 0.17452415823936462, 0.3065890371799469, 0.4034731090068817, 0.4989353120326996, 0.23093493282794952, 0.09402914345264435, 0.38582664728164673, 0.44134873151779175, 0.0611819252371788, 0.13199573755264282, 0.12764757871627808, 0.4402329921722412, 0.2961674928665161, 0.4256260395050049, 0.25879600644111633, 0.41215014457702637, 0.4141194522380829, 0.05118705332279205, 0.2412625551223755, 0.3338037431240082, 0.20432794094085693, 0.3931296169757843, 0.17160777747631073, 0.17558076977729797, 0.4399562478065491, 0.46026378870010376, 0.40925973653793335], dtype='float32').reshape([28]),
            paddle.to_tensor([0.13864725828170776, 0.3485846221446991, 0.2197316735982895, 0.3716394901275635, 0.009157560765743256, 0.2858976125717163, 0.29748615622520447, 0.19136762619018555, 0.33291053771972656, 0.053141284734010696, 0.1438504308462143, 0.07949217408895493, 0.21190272271633148, 0.3132832944393158, 0.36649778485298157, 0.4826502501964569, 0.3171547055244446, 0.06172315031290054, 0.2859366834163666, 0.1808442324399948, 0.11998853087425232, 0.1969136893749237, 0.24303573369979858, 0.4533267617225647, 0.04275284707546234, 0.38712212443351746, 0.22912578284740448, 0.48599809408187866], dtype='float32').reshape([28]),
            paddle.to_tensor([0.17818878591060638, 0.259179025888443, 0.4392455816268921, 0.12710684537887573, 0.12076516449451447, 0.370647668838501, 0.3527378737926483, 0.3946180045604706, 0.15883365273475647, 0.4568462371826172, 0.3059188723564148, 0.11182353645563126, 0.10884750634431839, 0.03734821453690529, 0.2162649929523468, 0.42695358395576477, 0.12120036035776138, 0.18860028684139252, 0.04133719205856323, 0.13429757952690125, 0.09625497460365295, 0.1340007781982422, 0.368003785610199, 0.2574458718299866, 0.28531283140182495, 0.4743281304836273, 0.00854403991252184, 0.4450896382331848], dtype='float32').reshape([28]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_78bb8851d109e0d8169983d703cff383(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 56, 56], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.35430020093917847, 0.0987997055053711, 0.2777608335018158, 0.262786328792572, 0.15539006888866425, 0.03583184629678726, 0.16860491037368774, 0.48779141902923584, 0.3633679747581482, 0.4883105754852295, 0.3362028896808624, 0.39826950430870056, 0.430955708026886, 0.3802274465560913, 0.20511767268180847, 0.22914771735668182, 0.054582059383392334, 0.48038268089294434, 0.36006829142570496, 0.0636010468006134, 0.07625067979097366, 0.36436450481414795, 0.420047402381897, 0.43786346912384033], dtype='float32').reshape([24]),
            paddle.to_tensor([0.17342126369476318, 0.21113689243793488, 0.26378491520881653, 0.47149911522865295, 0.28909194469451904, 0.44771096110343933, 0.006402057595551014, 0.44831717014312744, 0.09775819629430771, 0.14873933792114258, 0.11920692026615143, 0.18273983895778656, 0.09235206246376038, 0.154817134141922, 0.45716592669487, 0.4415047764778137, 0.034366022795438766, 0.10580066591501236, 0.4828447699546814, 0.29134443402290344, 0.14086079597473145, 0.13312993943691254, 0.11850897967815399, 0.21844834089279175], dtype='float32').reshape([24]),
            paddle.to_tensor([0.3752855062484741, 0.26567086577415466, 0.3691032826900482, 0.08957882225513458, 0.4847166836261749, 0.491789847612381, 0.03554714098572731, 0.4978052079677582, 0.34503114223480225, 0.4699227809906006, 0.2997759282588959, 0.1722857654094696, 0.2528334856033325, 0.05744458734989166, 0.22725990414619446, 0.062058936804533005, 0.20356786251068115, 0.0386865958571434, 0.09670083969831467, 0.16706785559654236, 0.02829437516629696, 0.22376370429992676, 0.047204889357089996, 0.27249249815940857], dtype='float32').reshape([24]),
            paddle.to_tensor([0.4579346179962158, 0.1313418745994568, 0.32889553904533386, 0.11261462420225143, 0.03774261474609375, 0.05788774788379669, 0.11037749797105789, 0.16037316620349884, 0.00042591505916789174, 0.04139398783445358, 0.46921804547309875, 0.13848455250263214, 0.2808384895324707, 0.14301438629627228, 0.35526248812675476, 0.43197935819625854, 0.02381785772740841, 0.13394637405872345, 0.03733060881495476, 0.02284623496234417, 0.2743551731109619, 0.3816293478012085, 0.4308164715766907, 0.29290705919265747], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_31ebc5dcf08ecf32b7616df8bf3eb9b3(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_1a2b114b88ba56b95370f75ce469d256
    def get_inputs(self):
        return [
            paddle.uniform([1, 24, 120, 120], dtype='float32', min=0, max=0.5),
            paddle.to_tensor([0.41226881742477417, 0.18441236019134521, 0.36982232332229614, 0.11228861659765244, 0.4992044270038605, 0.13047432899475098, 0.3148701786994934, 0.3218531012535095, 0.06626573204994202, 0.29797545075416565, 0.11759769916534424, 0.12422781437635422, 0.015163082629442215, 0.280784010887146, 0.2164435237646103, 0.4351377487182617, 0.1400296539068222, 0.2939506769180298, 0.012943047098815441, 0.36852434277534485, 0.24470260739326477, 0.06419841200113297, 0.193588525056839, 0.29076865315437317], dtype='float32').reshape([24]),
            paddle.to_tensor([0.10903831571340561, 0.32273948192596436, 0.04316405579447746, 0.019892849028110504, 0.2505752742290497, 0.012080356478691101, 0.407381534576416, 0.45480456948280334, 0.4225124716758728, 0.0885731652379036, 0.24069102108478546, 0.3596436679363251, 0.17876631021499634, 0.22132669389247894, 0.42558854818344116, 0.4376285970211029, 0.2812722325325012, 0.3924911916255951, 0.27805453538894653, 0.4640125036239624, 0.3631203770637512, 0.2075711339712143, 0.34792181849479675, 0.28111910820007324], dtype='float32').reshape([24]),
            paddle.to_tensor([0.2474014312028885, 0.12052582949399948, 0.3191300630569458, 0.112171471118927, 0.2779998183250427, 0.22159990668296814, 0.46757879853248596, 0.2415098249912262, 0.4969690442085266, 0.464304119348526, 0.3500995635986328, 0.03522602841258049, 0.40111836791038513, 0.11934471130371094, 0.006692640483379364, 0.17901794612407684, 0.2942439317703247, 0.433764785528183, 0.20792166888713837, 0.3445751368999481, 0.05666746199131012, 0.06427988409996033, 0.1360899657011032, 0.28249016404151917], dtype='float32').reshape([24]),
            paddle.to_tensor([0.129692941904068, 0.12059806287288666, 0.32641106843948364, 0.29045990109443665, 0.07849597930908203, 0.11536235362291336, 0.3046587407588959, 0.2910509705543518, 0.25949326157569885, 0.28648272156715393, 0.29116812348365784, 0.14582057297229767, 0.38304710388183594, 0.21017354726791382, 0.2941789925098419, 0.42429667711257935, 0.17993499338626862, 0.07254905998706818, 0.2765158414840698, 0.02829437330365181, 0.35349684953689575, 0.2339964359998703, 0.47590234875679016, 0.057785291224718094], dtype='float32').reshape([24]),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

@unittest.skipIf(need_skip, skip_message)
class TestPrimitiveOp_83c05042ed51ebfca06ef86feace4333(CinnTestBase, unittest.TestCase):
    
    def get_test_class(self):
        return PrimitiveOp_4497fcdfb9639144ea558daa18214543
    def get_inputs(self):
        return [
            paddle.uniform([1, 256, 8, 25], dtype='float16', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
            paddle.uniform([256], dtype='float32', min=0, max=0.5),
        ]


    def test_entry(self):
        if AthenaTryRunEnabled():
            if try_run_exit_code == 0:
                # All unittest cases passed.
                return
            if try_run_exit_code < 0:
                # program panicked.
                raise RuntimeError(f"panicked. panic stderr have been reported by the unittest `TestTryRun.test_panic`.")
        return self._test_entry()

class PrimitiveOp_390e91adf11e1069414d8e99c9dae65e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 175, 25], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2c4c3cc386cf13265b1440a9178fc404(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 8, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f830538a49880f7faa7090ed914b6a21(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7f61c38bb31decba43362a7297fa1282(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 6, 40], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_378be52d1342297d3b33c6f3d55b3bee(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1152, 6, 6], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_69d4d8d352c35b330e34d3483128f699(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b42e2b0e2baa229ef0a8f97466061c29(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f3652582614c52d5e020edd850bbf63a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 20, 32, 24], dtype='float16'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a0110df3ba1d7944208519d64614db38(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 640, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bde20d420d7966db0d5ad3bba41cdf3e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6e7ff31e0b879dcba9b6ebba828ef520(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 972, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[972], dtype='float32'),
            paddle.static.InputSpec(shape=[972], dtype='float32'),
            paddle.static.InputSpec(shape=[972], dtype='float32'),
            paddle.static.InputSpec(shape=[972], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b4b14c7ab1450b3a7d537c3eba9ce1d5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 104, 4, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f546035d8aac286bca7b3ebe029dd18e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 26, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[26], dtype='float32'),
            paddle.static.InputSpec(shape=[26], dtype='float32'),
            paddle.static.InputSpec(shape=[26], dtype='float32'),
            paddle.static.InputSpec(shape=[26], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_645d76d5c9cb991a81f41522b47a9679(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b6eae07d54d9ef5dea95b59f6544ff89(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 480, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5e07108a5fd9f5f4cdc70500da5a1d42(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 8, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4313045e50e143d0b04f4beab2d1ffe6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, 17, 17], dtype='float16'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e0667d9bad3e3f9948ecf0e9ab7356b3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d86166d29866b8690a7ca1699e996024(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 104, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6857ca0c0268dc8b0debff2e586c173e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_73abd023f1b54ec5f4d5e00139e06a6e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, 8, 6], dtype='float16'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3ddfa948ffba0daaf2bab1fc3fe10349(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 60, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9e30fae60f3b8093db147f92de4ca394(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 8, 6], dtype='float16'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9ff4635bbd75d4d9fac9dcc46fc4bae0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 64, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0c90e6f36ee62d83e7a99e0bfa0cb061(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 28, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b9857c98fc49adea29001e09fea3d68d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2cfd04874ac8e2deccd00b885bbd9725(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 36, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6f8d2811b8281ef9cc6d7e408dbcf600(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1536, 10, 10], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_acd0d714098c95881a5ad397bed31056(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6a135dd1b999265b811e0cba22b585f9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 88, 25], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0e7108aa4d5e6ec1d2977be007d1d6e5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 104, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fedc7d2ba8af28a5f2c30f5b54772f7f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 72, 61, 61], dtype='float16'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1b68c0dd3defc98dbe90fd15975e97f8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 32, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0f55a3fe994b3b3ea44afa5ed331d5a0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_43c844305a33618763210ee186f27102(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-06'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 512, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7bfe4867b2ee50af53c3d7bdc5f7f58f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 16, 256, 512], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f445cb80822c337d55dac39a368586d8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 672, 12, 12], dtype='float16'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b9e6dde078f05face473d58a826284c7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, 32, 128, 128], dtype='float16'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_90033cb242d6a196769e55d5671fa6a7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 80, 80], dtype='float16'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_df260ae3a5b7b61ca1667b85607c9a67(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 480, 12, 12], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a5874bd6cd476b15d682e91765085065(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 96, 96], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_754aecdb21f90647937ecba1c265a35c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_56388e95fe8a66c4dec6933f7243c2be(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 8, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_365d00131505c7d39b3fc12f494811a5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 320, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cfea30a48e65df8701477c50ffaabbaf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b2f70fc54e606b9ae002fdec0cfc7df6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 30, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ab1ad85a256f22d05c0ac5a2c9d383eb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1152, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b9a89d6c5d649626b0ab5c3dc05a8ee8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 120, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9fb4d2c35d0d8e55697ed2f596b3c41c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bd9785b1c71ce71addc5a0faecd777f4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 80, 80], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_14fc0a24ced80903662f15f415902531(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d50b2ed5144cbdd1ca5042fed72047fc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_62c0c2ffe276d6c810cba03d6d5dadf7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f7cfab02d7eee8ea96de3e3e634ec8cf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 672, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_83f47e87307cfd4dd8a195d9d35fd4d1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_093e50fc34ac9d700f8462a8951e7dd6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9765f38f093f55ef89226fee7f91366d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b7377c3d7f0a477016a5dc92e1a42451(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0483512e4e012119dfa48bef933ef5bf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f0eda39cb0cd71973c376984841f0139(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 128, 128, 256], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_148cd4160d3a517d70ebea43e8198ce1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e2466faae82137046237da50a7dbcf3f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 12, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3b30e6c17eef017676cfb2b3323b0c80(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9f5b52cc907400eaaf11705256ec7039(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_eb0cf3322ed5ca54d3ec6aba3fa1ed38(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 8, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b5932dbf13639be2f7d7c2f7e01d89fe(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 40, 40], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dd18401b50f195d1b69ba1a453845f02(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2560, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[2560], dtype='float32'),
            paddle.static.InputSpec(shape=[2560], dtype='float32'),
            paddle.static.InputSpec(shape=[2560], dtype='float32'),
            paddle.static.InputSpec(shape=[2560], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cf0dfdaf0c4824bd9e3f298c3ab3d32d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_95752e262f48b523693ff1b90222dec0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 104, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e04a6d914746c47bf4f361eb509d1ae5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0988a78b8dc830e8cc8cd52428e02408(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 80, 80], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8b69adeff2c71ee7b815f794bb461a6a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_275d357f63fd1ce0ac4dca3ad5d569df(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 410, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[410], dtype='float32'),
            paddle.static.InputSpec(shape=[410], dtype='float32'),
            paddle.static.InputSpec(shape=[410], dtype='float32'),
            paddle.static.InputSpec(shape=[410], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9580c5aad3dc54b081d5076a4dec374f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 20, 32, 24], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7ffe1a6ab63c287d150f682527662f65(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 20, 20], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_192bfa443e2ebf2c6bfa87b81cc89e2d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0ffdf4b0113987366d210bdd01ca249a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8947a0ba9763d29f6df3181faa00e811(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 40, 40], dtype='float16'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_758efcb799911c6f0c6d536cf461001a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_76ab480359ef928c8720cc2e4d4649b2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, 8, 6], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_73b2de52b3a25edbe06de17c3b6ab65a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 4, None], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_21c434ef269a633bfadd6adaf8a4a98d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a571297f364fbba0735c6f470addef2a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1901bf125bb1a77e4198b2c97ca6df6b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c7ec793f5fa22ce0ec1b09f6e26f303d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 320, 320], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b6c7a69c80a82f51877a679621124dc0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, 48, 48], dtype='float16'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_22e747980adc24d3e691b98fe7f97177(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3f5da1d25591a17618f32d5f4cdb2abd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, 17, 17], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_506dcb579ce2bd0471091f30183046ce(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 160, 160], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6ae3234e602e64832c0c8da733fef782(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1280, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2c9a65811391b48c16663893efad06f2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_937f0b1ae193bd9da7f88fe70cc20312(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 320, 8, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_db9f06a06e5ec263aa27a6578d0add80(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 200, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d39b6179e48b5737892563f03fbd1dd8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 64, 128, 128], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c5860002cd4776fbf03869697a41cb9a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_70e9da8c86903b09f59bb5b27e4c9293(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9bd488df21cd55f3ba5ac049a495705b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 16, 12], dtype='float16'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_acaafb69840e887faefad3ca97ae8bba(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 12, 12], dtype='float16'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9819e83c80fad20383c50c93b943ba45(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cf3bea7e81eba2f8803fd04ad3ecbdef(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 12, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_306980a35aa819da3afc5c4d630f72be(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 36, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_26198720caabf032d5962371af520b5a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 4, 3], dtype='float16'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2ae2ff6147168e6bdf1ebed988f5fba8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 40, 40], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d5a30160af175087d67221cd759bd282(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9132d1421c6776b558135871c20de887(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 120, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9f84fbdce742b74de5544e057cd28838(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0126aeef335dbb2dd53ff29c62ab0a41(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e28a464948bafd11a8012207f50f6fb3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 16, 12], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b8cad70198a579ef46dd2d7f8f14031f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f20b7d281ea20a4072d0f48c8a19c847(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_480a729c402cc1c9b50f2fa16dc802a3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_93b7eb5220c982e74801e19028ab6cea(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, 4, 3], dtype='float16'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1c27edec4a7eb3e614a35b687d2cc820(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ff9a641fa2df9e7c95e9352624577d70(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e161b0e414bcce57157ab486158e34a5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, 32, 64, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_95aba9563c2d8490e87f0ebea1f79ef9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 30, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f15ba70e4e8d2e88b323c4a8eb5175e3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_926de857b9640246d3a861bb96dc7238(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bb7bdcfc57af9f15c0c2fa0f429ffa87(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 30, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5f89151d5524d3ee789d2b4cd1a607c9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_433314088006787be77fb1fc69ad92d2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2529de372a4867bc4899bd15cd785d5d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 20, 20], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_47253853fe7e4c594fd18626e3ff76f7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 80, 80], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dadc711cf95ce5c6825bd88d9a162838(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1120, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1120], dtype='float32'),
            paddle.static.InputSpec(shape=[1120], dtype='float32'),
            paddle.static.InputSpec(shape=[1120], dtype='float32'),
            paddle.static.InputSpec(shape=[1120], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d0b1a0fda2fcb56df040381307a1519b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 32, 8, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1245b3cb3e107d262d7ecf1acbf2e1e0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c03fa0a42d9b996e79b755dcd7c02fac(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1632, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1632], dtype='float32'),
            paddle.static.InputSpec(shape=[1632], dtype='float32'),
            paddle.static.InputSpec(shape=[1632], dtype='float32'),
            paddle.static.InputSpec(shape=[1632], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_425450a29c03def74064017764b43a41(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a9f563c62a91878b553c08e2cc15b4df(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 20, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6fd75af07736e9d01ce333ebaf269a6d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8991372857f1063deb0639e3e0988529(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 18, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[18], dtype='float32'),
            paddle.static.InputSpec(shape=[18], dtype='float32'),
            paddle.static.InputSpec(shape=[18], dtype='float32'),
            paddle.static.InputSpec(shape=[18], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0f299a987c28e8264dec78dad28a831c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 28, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2d1cf063b27b9372e920c977c556bfdd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 4, 64, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_defe38e4814345693cd22be6063cb31f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 147, 147], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9b9a6a813dd98d200bcb55dbd392a4aa(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 120, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_203d377c673ee4774c67da3b7df226d8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 702, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[702], dtype='float32'),
            paddle.static.InputSpec(shape=[702], dtype='float32'),
            paddle.static.InputSpec(shape=[702], dtype='float32'),
            paddle.static.InputSpec(shape=[702], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cf4191e91099a34af466b305863c956e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_88d2ee044265eec9b2d13f287f9e5975(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_138aa3c869fc81b18cd947421698b73a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 320, 320], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b60fcd0b6f26428bc5c7770bad7ffa1d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 30, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_03d2fb3dfca39e5be82fbabc33a13c2c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d2edb31d7ed5044dc395a2ce8ae41db4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_91d3aedc21153cdc01dfdc591767df6d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_72afea2bd83e26c58d662a0ddac1de3c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_78b820bce8ae8b83dfbfd35bf290faf9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 38, 38], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_32edf953fce616597ad2b44f8310ea33(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 8, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c5edb2f18c8e47a8bf1b6d78f784956f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 150, 150], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bb759b99bdfcb9011ade436d7f22edac(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 432, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d888a268af70f20a6e8eb7f00a9b04c0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1f0ade45bd152ef82b588f2a11bdaab9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 32, 8, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8cabf75de230bd48aadcb8135ae89fee(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 12, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3c644406c3cf29f49b29643c4b71b219(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4d4fb26491e469e694653a760f51e107(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 480, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f632f6b71e76f85dde9e6a9e74d99131(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_016ed345c77e700944f86327d3e197d6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 4, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e1b19a292b33d2adecafe5164671e766(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 640, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_24e7b3aaa2ad76692a7ac50c0a6fad88(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2048, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e85b60b290717e59a466822d1e42eb3d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6a86473a8eb8714791b7ea8aceca3b47(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ad8c8da3857e6bf3898e178beaea5391(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 228, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[228], dtype='float32'),
            paddle.static.InputSpec(shape=[228], dtype='float32'),
            paddle.static.InputSpec(shape=[228], dtype='float32'),
            paddle.static.InputSpec(shape=[228], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dcda6b8c7486d0b9bc82aaf2f1b677d1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 4, 26], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_04c277ea268a9056176872ab5d1a5d4a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 19, 19], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e9100befd923b9387a58d311c9f1d4af(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 2, 2], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3a14b87cec90c13cb6254bcbcfcb2843(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2d81571c6c5da8d099f4e8488504251f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 728, 19, 19], dtype='float16'),
            paddle.static.InputSpec(shape=[728], dtype='float32'),
            paddle.static.InputSpec(shape=[728], dtype='float32'),
            paddle.static.InputSpec(shape=[728], dtype='float32'),
            paddle.static.InputSpec(shape=[728], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3336a67ab9f984ea2d3fdb4dcba03618(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 12, 40], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ce3c5438c2af00accb1b7081761e25eb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3cb3ebb4edf0758fe4ae9d28bb2db0fb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8de2a97e06d91e33f36e2fe3b70e55ea(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6672a4101d2c1bf5bf196ed91fc6113c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_60fafc0aedd1d0ef03e639ab59eed917(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 80, 80], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f8c2b56849a92c7300392b789d801624(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_474d841a3d7fd81c6525c72f0f53949e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 76, 76], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0a8f2e0b225571cbdd0378af5cd9cb3a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e1010f71f59d34d315de368d1204f260(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 480, 2, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_720e8ed98eeae62b50a77b14408c2b10(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_78ac0280d335b5d020d80c7278bc659f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 728, 19, 19], dtype='float32'),
            paddle.static.InputSpec(shape=[728], dtype='float32'),
            paddle.static.InputSpec(shape=[728], dtype='float32'),
            paddle.static.InputSpec(shape=[728], dtype='float32'),
            paddle.static.InputSpec(shape=[728], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b198c962704a1b49b5b64a507202a445(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 8, 6], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2109b8194661f3138acf2c8b007d6b84(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 480, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ba50b5cc13932f74cac90d833003cee7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 56, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8676f53cab2101556a3aa705d3f615f5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 64, 350, 25], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0657ef22db0d18caaee4c49eefa3e86c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f29f53619829634831b832b49653cd59(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dbb0395720298544a11586fc9d5563b5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 416, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[416], dtype='float32'),
            paddle.static.InputSpec(shape=[416], dtype='float32'),
            paddle.static.InputSpec(shape=[416], dtype='float32'),
            paddle.static.InputSpec(shape=[416], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1cc6aad071abf8ea64d3fe3ef4684e56(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 4, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d9d060b5e64903648e88dc3cc8cf97c0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 72, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7d292400455f3928d3db699b9b9c3a07(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 410, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[410], dtype='float32'),
            paddle.static.InputSpec(shape=[410], dtype='float32'),
            paddle.static.InputSpec(shape=[410], dtype='float32'),
            paddle.static.InputSpec(shape=[410], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c411f6f33680b03a4a7622bf3bbfefdd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e0c71bc41aeac4c359b0d5c6e94ea65c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 72, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9437e9a2ff510eb57d949f967612d7d4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6fa389afc2ca633de225c3e7427fda16(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 208, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d746ac404df2cb95d7525b307d9e10ec(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_470c513e8f7f2702c7307c0ef6a7242c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 128, 128], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bbec8392257533d466fa69c0a1ee8f14(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 104, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_72b5909584deb756eebecd96ec2248ba(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 64, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9d9a59cc1149d5482d170ab28e98dede(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-06'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 512, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3f89882aa66b7d6e0e6cf03848a21bf5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 4, 32, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_25b4477b7535e9e0765139032b8778a7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 128, 175, 25], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_53dd2771c402a2715696773cf64d4723(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fd6c62a5b14788f5279b50233d0e159f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2e6e13cef89b90578915c4c6583801aa(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_025a876c127e7008591dd621431ad86e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 17, 17], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4f79fd4b24bb5012a8cdfb4b3210f826(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7368ec1ffb57fc339f15151366d9e8e3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 4, 26], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bf9ca9a52e7714e3b3f92960025f51ac(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_96c65253665b360fd1a7b9c2f0fbf56f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d3a35937d9c5845f9df447ced66c3c0f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2048, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1c0d78493af12479591cd66912c25938(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8289b332e10e28c5a84932d651624d7d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 118, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[118], dtype='float32'),
            paddle.static.InputSpec(shape=[118], dtype='float32'),
            paddle.static.InputSpec(shape=[118], dtype='float32'),
            paddle.static.InputSpec(shape=[118], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dfaf72fe729072533518bfce8d3ee0f6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 49], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_eb989c26d3976af75d654c8ead02f5b3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, 4, 3], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1b1de1cb6185e0e58217db08ea6109f6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 32, 128, 256], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_54a5f985cd6257bfcb5e076f9e76e1e3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 256, 88, 25], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bcbd2bd27060e5444ea5c4872a8dff65(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 32, 32, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a77b7cfb6a89a98f582848027cf0d454(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 20, 20], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_02d2de9f3d7b29bc3390442d83ae46a2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1776, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1776], dtype='float32'),
            paddle.static.InputSpec(shape=[1776], dtype='float32'),
            paddle.static.InputSpec(shape=[1776], dtype='float32'),
            paddle.static.InputSpec(shape=[1776], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1ef9a37669994e790928e31cdc463b1e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 4, 26], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bce7f856811e02f9600c9a863f63cc58(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 320, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2eb2104ff4f2d8c45a218e5646e56e39(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 40, 40], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bccf32763057437a36894e2efc788dbe(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 480, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1bd086ea33b1b0f960e73d3baae21f66(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 56, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a36275c4e1ca7ee9b59d383a698ba621(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 40, 40], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a977517bd9688cb9754372ab263ef171(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2048, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ac25a9f438fbee8212f63b2d4fc03f5b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_46f1db9af75718b28443482c27151a5b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_782b1acaa87910c071667b87c8ca249c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_04d13aaf40278fc81288610b41085998(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c2dc45c893631346faf0c93366b70b07(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bec13ebeff44409d3eef899327206ea0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f924a2e0c96ef51562722729350c13a3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 8, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_04f7d080fabba449b0b04ad4c7ae09a7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 768, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_92c7cf44ca9e699763bcfc4145325e45(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 32, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b27edf12bc366327fa179e9bda6aa083(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 26, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[26], dtype='float32'),
            paddle.static.InputSpec(shape=[26], dtype='float32'),
            paddle.static.InputSpec(shape=[26], dtype='float32'),
            paddle.static.InputSpec(shape=[26], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f1c461cb5f6f22728e44806e063a738c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b37a9d0f63cb1d8171d304dd8f545e82(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_488b2e1de9d2ce6934a4a2197e547553(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 52, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[52], dtype='float32'),
            paddle.static.InputSpec(shape=[52], dtype='float32'),
            paddle.static.InputSpec(shape=[52], dtype='float32'),
            paddle.static.InputSpec(shape=[52], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_777ac0525f84696e6440d3ec6ec78be2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 640, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_644964eb153e3db1922193903e2afb1f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 10, 10], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_01f1d79d398e2cfc61a4d9da75324daf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 336, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_86bbe428ad7cdf86b46d39572e0601ab(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 30, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8e4a97bc06557d54f21f41f06e3995ea(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_424f752909aaaad1bfdd43dd820623a9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2048, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e4cab3560020f8e75167f6010b432790(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_62213fc8161303858ca4ce0a0316e3dc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_68d42fd40adb5d3b982284414f88d27f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 64, 128, 256], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d90e79439c86f700f2883ab09af3ca8b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_557d967e46ba84e246ef28143474892e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 88, 25], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1dc0990adccbbca8c2f6291e6ea35d61(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fa2b8ac4c3930469a8943e54c818190c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 128, 128], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3890f5a700857b435490454b960a454b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8ba8b0345268f0dc8f4ea2e68bcb1214(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 576, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f4cec7d657e259f4c2913c4230392b9f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_de25d736449bbdfaf418d14f121fd0ec(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2048, 10, 10], dtype='float16'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3b283aa56dc1a8591160c9e27af049ee(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cea65a08f23b566a04190ff1bc59e5e3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 16, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_307ff8dfcbd3f3dc711b97745f106617(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 64, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_237e94dd03dbb3d597731d100450d5a0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 112, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c9b3f21cd8e4297e6ac1d8630524b493(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2048, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d2856a2bb276ab11c3e63dd5174de92f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f68ee94db56350745cead1204ae65147(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d5d4acd8a1de3893d6b69b05d68ff56f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 480, 12, 12], dtype='float16'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1921d031b73340a7414904caf27a3935(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 104, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4a3780e143c6593be6a429686d661bb2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 4, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4ff6ea202881aecbdab6f3d61896bf38(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e5fdb4037aea50a357085c04275bec90(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 17, 17], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_90ba953ddfc60bc98ceb008c571446d0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_51bab3307d196b0917f4c943a4dae42e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6dfff219623c2d00fe823701d0bb2d6b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, 128], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e1a2bde34d7432d425068fe9e4eca25b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 20, 20], dtype='float16'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a89884487612d4804b14f00b0ed71e98(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 800, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[800], dtype='float32'),
            paddle.static.InputSpec(shape=[800], dtype='float32'),
            paddle.static.InputSpec(shape=[800], dtype='float32'),
            paddle.static.InputSpec(shape=[800], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_76c4233d2f19f89ed4488de2d0bad695(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8c13024078fbf85f169eb5a282e12cc1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 288, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e2c7a5f2b9b5f2b02274d479cab4c5e4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 70, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_634fbe3fe8596f1608e8d2bffa59c189(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bf9ee670235dff52a659518f1e62b055(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 12, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f7304499082d0cdfd12079a9338d57ba(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 40, 40], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9e5a434e12da2696a9f9d868b9f45477(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 53, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[53], dtype='float32'),
            paddle.static.InputSpec(shape=[53], dtype='float32'),
            paddle.static.InputSpec(shape=[53], dtype='float32'),
            paddle.static.InputSpec(shape=[53], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7c8aafac5a643847685bb567642debfc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 32, 128, 256], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cf9747313572e8321f152fb443a5b43a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 4, 64, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_eea460abdf980da44cc6e72122d0288b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 768, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_82dd2e51bf82f64805cdeb9e77f57b2e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_edf331ca5fa8f99449330bf1525d87ae(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4804a0d2e5fe69e735374909976ae0f7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1200, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fafb45dd41fc17f3a0094974515819a2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 32, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b081fb752a993ad40bd7ed7adfc6290f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 20, 20], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8908757f1e09639d6bcae97a8afe8842(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a5751d1bddadb2e498a869c2225f2f15(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 672, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9b35cc9659988100db4c2855c4d8b2ac(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 35, 35], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2aca463f726fc557a6eb6fffd721807b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a8300eb81472fa39883d842ffa8e5226(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2048, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_71b303d09dce0592b239a8d2f459e33c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 120, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7ea9f08a9b8f521acd61667dd33f31da(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 18, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[18], dtype='float32'),
            paddle.static.InputSpec(shape=[18], dtype='float32'),
            paddle.static.InputSpec(shape=[18], dtype='float32'),
            paddle.static.InputSpec(shape=[18], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_68a3a37a967d00e45a032a75e100cbbe(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 60, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5441acb8165a81688c6b7dc201fc8c0a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 20, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6cd78aa28625ecef49649dd900f1df0a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 720, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4998577f2715fb1654cdd7b9ad85a960(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 32, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4a1101b6ba9ad30c33ffb1728d76a0d3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 232, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[232], dtype='float32'),
            paddle.static.InputSpec(shape=[232], dtype='float32'),
            paddle.static.InputSpec(shape=[232], dtype='float32'),
            paddle.static.InputSpec(shape=[232], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_903c56f67d56a4bf30536590ac5a0891(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5294ebd8899a711fe2314802f5ecba45(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 20, 20], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f8079b1546954a0de68d082173b56fe1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 640, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0f091825d71d4e1ffa027da64f5d4394(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 35, 35], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1660dc9441cdb2a96059f89ab09eaa94(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_487c1fe21a345440b06740e79a052423(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 288, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c264d3b0c0b1a61a28086f770ee56d74(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 8, 6], dtype='float16'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a15e488b34fc5df2fd08f2ab54403dfb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 12, 12], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9e5b5017ef481ffe84d96a7e526c5fdc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_31c2fa12be1f4c1fc369656d85ae0b8b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 20, 20], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ad27f36a6a00300e6d2db1669fcbc05c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a65a037fa38df36bc9206503c5bc49ed(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 64, 256, 256], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_88c91ea1acff8ca87cb913f04c8fb8c2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 60, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6019f3d38c8be9d7dcead7fde144fc69(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6e3ca2d981870e25e3d4d5e6d1a4bf5e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f5b7b0fafa1d70e4588dab9864032d2a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 40, 40], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3ef1128a7f0385fe2e93b55d907ff2c7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 640], dtype='float16'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_10a96d116bdf16eac747ffae2f89d57e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1f43c1722e1112a30fd429a884406979(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 4, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d0f61ef67fec7264cd1f3036a26ed6ea(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 20, 20], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cdef1e178dcf7458f57f9c3d0f14101c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ad28ea0fc0c8e43ba920a50c234fdee9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 20, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9a6c60662dbee1ec9a8016799a0067fc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 336, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6072bbc3aa1bc3812bd0572c3fc3ee0b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_aca3f1beaff9d735bbb1a39a91103cc0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 60, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_32afe5eddedac1a313481c9673ebea20(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1864570f21287a00d91d55c94bd03f33(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 3, 3], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_40febeaac423e00552e9dbdc617b7213(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 640, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_08b46924600e40a736dc15f1f0c34c6d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 4, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_25a90f7243b0e1836ddf4f434eac60d0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_04ec187a23fb298c7d6e878a60cd785c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 40, 40], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_32f7e7e456ac4fa76328f4c94180176e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 82, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c7d2d72869661ab929394d1ec970ae31(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 30, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_08e61d2bb0735c24358435bd74a7ffb5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 300, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[300], dtype='float32'),
            paddle.static.InputSpec(shape=[300], dtype='float32'),
            paddle.static.InputSpec(shape=[300], dtype='float32'),
            paddle.static.InputSpec(shape=[300], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_950c1cc2a634fed8a379cd4dfa33615a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_354d921a3759c2556cc892a56f9f4f21(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 224, 224], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_86243190853738b96a63e9b23bce77ba(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 32, 8, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5ac548483b1eb78cd51c68228482ed73(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b22a4c2a1aad45f0f951cac899fb522d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 8, 25], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ca44f8eeb684e8da550a314b2e180d9f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ab8d8cc91b147b04c1301f363ffac4e9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 60, 20, 20], dtype='float16'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0b4b8879c39d03f646f9a91b3184e36c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_639564a6a54579140c8cf73996e45a72(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 60, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_31ca4dbd630951b82d9b5bd88e830717(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_85e67324a9e48c7e743d2bd01f5caa73(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 8, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_99be4f5ea99d94116b6830a47a351b9e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 20, 20], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_03076558df529649942a6626db697bc8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1200, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ad0d3f301065e9667c07af51436d8926(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 320, 4, 3], dtype='float16'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_795d9cc3f8e663f9f242086e329c6e31(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 20, 20], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3ce24945cf09613f0e8e068528b5b165(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 56, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_95c2a976b83a4958b1be6cfc2b6c3597(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 360, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[360], dtype='float32'),
            paddle.static.InputSpec(shape=[360], dtype='float32'),
            paddle.static.InputSpec(shape=[360], dtype='float32'),
            paddle.static.InputSpec(shape=[360], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0e05ff9dec9e57dda66ca079eab7ed32(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 512, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3b17e66f7026f48e8e8b7b11d641c130(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a9301b445d7a6ea9af96c2efe8af0364(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5b03cf60275a890056ef00c1c753d978(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 12, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bec5c67fc72cf90ddb9c224701278864(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_373728b1490bc33e9f944e7dc2e95787(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 16, 512, 1024], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ff41a1da4d2282704bf6a4f294a4e176(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 672, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_09f774ffd15045b465da35e642040316(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 62, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[62], dtype='float32'),
            paddle.static.InputSpec(shape=[62], dtype='float32'),
            paddle.static.InputSpec(shape=[62], dtype='float32'),
            paddle.static.InputSpec(shape=[62], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5b62c1eb1eea18fc53875f8c7b26385c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 8, 25], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bf44cd9e9ae104c61b72ccf5ce9bf251(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ceb994a879f8829a5efe1033d526d81f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 350, 25], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_aaf74873a9105f0af43d9af182072d3c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ee746a5623fda017a4125de9688f8805(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 10, 10], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1f3c8740f8eb41d61ae8d759be2f100d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 19, 19], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_77f52497a6de9a22ebc3d5a809e41670(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 10, 10], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0dabc235b01a7023f14e74ac18c8cdbc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 350, 25], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5a7448d0c9ed8b89a0a1ac8fbc10cf8a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 104, 4, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_432bc6d74660ffe150b7bb18a72891b2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 8, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_93cb196bfa65ee5c3f96ce05d181a31a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_42b2574f34c443a4523c2ee045cbc4df(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_970c9da2f86e92b99b203dec83780b08(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c3f0b2690804cceb3a998c4afc73fbc7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, 64, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_68d960ab8f7b2fae2e15eee62fbd6493(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1376, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1376], dtype='float32'),
            paddle.static.InputSpec(shape=[1376], dtype='float32'),
            paddle.static.InputSpec(shape=[1376], dtype='float32'),
            paddle.static.InputSpec(shape=[1376], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1b831d030b659453f9f36a2daaa0d61f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8861c9bac0c52fa7c8171476d14c50da(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_af48b53061b6c74277cd331b33b43783(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 10, 10], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a7c051871ccaee599d41bd3baf7a0dde(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 4, 3], dtype='float16'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_156d2b02725383e937bf08c7a755c7a7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 32, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_89218944f1439edc3cce4e16883d0d01(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 16, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_589998bbd81988b3e1d94e3b20397c1b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 4, 8, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d5c09a718bde4f4f9b2728061f047e4a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_75f5ed0c9a39ff313160be27327a8985(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 224, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[224], dtype='float32'),
            paddle.static.InputSpec(shape=[224], dtype='float32'),
            paddle.static.InputSpec(shape=[224], dtype='float32'),
            paddle.static.InputSpec(shape=[224], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_396e1479eb15e2e6034ce5d21e596c0c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 31, 31], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d8efe7286b43a5f44d9b4957f89df325(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 512, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c5f8394951e89737c107542c164c5fc4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 8, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b8cc8eed43baa042d1303635c404d3fb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b87e946dbb1b9643a1a946af8e4fc59e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 32, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9e98ccad392217803a8c7c0fa7e146a9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2208, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[2208], dtype='float32'),
            paddle.static.InputSpec(shape=[2208], dtype='float32'),
            paddle.static.InputSpec(shape=[2208], dtype='float32'),
            paddle.static.InputSpec(shape=[2208], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1a961731f39b2fa32ac9da1944219062(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 95, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[95], dtype='float32'),
            paddle.static.InputSpec(shape=[95], dtype='float32'),
            paddle.static.InputSpec(shape=[95], dtype='float32'),
            paddle.static.InputSpec(shape=[95], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_74aeeefd61bfad3687975db378e50ec8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 42, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[42], dtype='float32'),
            paddle.static.InputSpec(shape=[42], dtype='float32'),
            paddle.static.InputSpec(shape=[42], dtype='float32'),
            paddle.static.InputSpec(shape=[42], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8a2a6ead45870594f2d0f3f9c27bc82c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2064, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[2064], dtype='float32'),
            paddle.static.InputSpec(shape=[2064], dtype='float32'),
            paddle.static.InputSpec(shape=[2064], dtype='float32'),
            paddle.static.InputSpec(shape=[2064], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_398861261c8746d2881cedaba6979814(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_248b29acd43f66463b4bf3cd2bd304b2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 150, 150], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_88d417846c710f6bcb097ebfa8ce0714(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 8, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dea0f47511d5ea514ec4b33b6dcf178e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_04e521ec6933d773533dce64e3d45124(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 36, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e61071eff7204eee14630444e919506c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_47a6a1a509697146a8e9e71dd351270d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 40, 40], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_21cdbaf050444108fe284ba07c7eab75(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 75, 75], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c892c60e1007241eb02b17ef2d1c1ece(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_476fda35bc4beeae5de4624189dfb962(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 32, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ba5d32036b331464bee1a454f001a9f5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_beab501839800f2f2cd590f0c425f144(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 120, 4, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_75ccfb5156a59ebd3b59de71bebace12(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 4, 26], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f881908c233cff0d518f7bbddbe95ff2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 234, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[234], dtype='float32'),
            paddle.static.InputSpec(shape=[234], dtype='float32'),
            paddle.static.InputSpec(shape=[234], dtype='float32'),
            paddle.static.InputSpec(shape=[234], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1d4c603928c952a4be367cbf82cfad9d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384], dtype='float16'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_20e7dca2d21b620ebcea6442292033d5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 4, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_76819a13296acb267a71536c7d3a5599(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_be7a980d242db80da503d468c7b6bc00(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 8, 6], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_22078a374c6d5d4d8c3827de97839e41(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 88, 61, 61], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6547322fd2cf406a2d8f4d79f0d12efa(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 128, 64, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4f14c932d02b6507f9245815661bb29a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3669a9be9c0c3f4eccfeccb2035ed7b6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 8, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dc971ad76fd1055194fa611ddca13446(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 64, 256], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_73a66f5a4072a54571f8e6b41c24a31b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 102, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[102], dtype='float32'),
            paddle.static.InputSpec(shape=[102], dtype='float32'),
            paddle.static.InputSpec(shape=[102], dtype='float32'),
            paddle.static.InputSpec(shape=[102], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d0ce1bc02a3fa4134e621d76919466ca(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 320, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_93dffa82bac9460712e64f88ed24475c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 80, 80], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_358243a6c0028056dc5741d7b08c7e4c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 184, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[184], dtype='float32'),
            paddle.static.InputSpec(shape=[184], dtype='float32'),
            paddle.static.InputSpec(shape=[184], dtype='float32'),
            paddle.static.InputSpec(shape=[184], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8cc70e99c5eb0598dfe49f580581078e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 256, 32, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1624ac6e972a6b37a9132e27deaa15ee(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 768, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3cedf31f95c555c6bdb8a2150ea8087b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_614edde15238b37cab52731fd37f39b7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ab836b831e4fd09aff2ca9c06012a5f7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 6, 40], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_11d9df73c6718586e31773eefe4f9f88(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7af1867cf062e8e55bd074c997d4b02c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 4, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cc82439e4f1db73811bcc5c3210c2e04(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 200, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_680b0435e1e42580f10b70824ad2c173(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 32, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_acd1fa80420f1a6d3d273063bc1956d7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 36, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bb6c2f0038982adc38c6daa844fa9d46(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e68a28f7acc5e1fec128bfef6425f8d8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 17, 17], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e2e914dd9ce790e1723068f6c824bad3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1568, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1568], dtype='float32'),
            paddle.static.InputSpec(shape=[1568], dtype='float32'),
            paddle.static.InputSpec(shape=[1568], dtype='float32'),
            paddle.static.InputSpec(shape=[1568], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_672dd7aa42f3a021e6d00bf92e9b1d76(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9289ce5059ee500d56412e66c3fb5acc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 160, 160], dtype='float16'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ea0e8d966d9ebe90e2e6cf8e9a0f999a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, True), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_deaa4bea46982ee0218f6e86772f7fed(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 160, 160], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_58335340d600825583357bb7285fdaf2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 5, 5], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_72b552375f023e383d17aa69b279f370(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 4, 64, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ba12f00683fce2176c748a4d1787d506(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 8, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_adfbf8b3643a0b20cce605267bdb05fc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 320, 4, 3], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f0e9367ac584b9476f9ca96ce73f7233(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 4, 32, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_aed2ef1f3b09eb7417781b46d6c304a7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 32, 24], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4c20f1ab47ef6ecb4fdaaf7c4394d1e6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 6, 6], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_25903a5acdd264202599b2e994bbd6cc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_35ff2db50ad7ca1bf33f0c22454b1841(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 120, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a0a46aa0a89a0eb975d77352fd88c9ed(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 60, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_499000feeabf66e84f457239b361acaa(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 208, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_69864754ba1223a990da32a7ad8aec62(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 512, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cbf87a3922b26ccb48d13f6f2f36adbb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9fc8111224fa03c870923298a0763440(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 16, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_faca234a3b0e486e94372fd901fbd331(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 128, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_23598a8f265065965154dcdcde6d11d2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 2, None], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bc6609182c1039eafc255f808c90bb0a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 112, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4834a71b097f80307b114c152580156b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_012624d96a179bea34276af76c94f5da(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 4, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_42c7fa3df3c9ea87887339f90ff4d1e3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1056, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1056], dtype='float32'),
            paddle.static.InputSpec(shape=[1056], dtype='float32'),
            paddle.static.InputSpec(shape=[1056], dtype='float32'),
            paddle.static.InputSpec(shape=[1056], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_90e8782fd11c47935d861c75e6382c54(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3bc7ae257a778d0de8722c35efd20e3a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 640, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_58210f89e6aa313e3e1f81e847ac0802(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 28, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e790ff09b57e113e0ae53401cbebfed7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1152, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4b14494f518840e1d8943d5a28f48e86(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 416, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[416], dtype='float32'),
            paddle.static.InputSpec(shape=[416], dtype='float32'),
            paddle.static.InputSpec(shape=[416], dtype='float32'),
            paddle.static.InputSpec(shape=[416], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_895bf33b9928199543d8314581d69c80(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0cab8e769743e9b14df2ada13681fbb1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 12, 40], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3a294fe275f6cd8d1f3c173f6f1207c2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d66bd28761544b0977a883bd31ab43f5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 4, 32, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_504e7b0e17e361e8bbfbcfd1c468bd6d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9479e2f3222000ddf2ecb70587464185(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 336, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_98e11c90b7a6bbcdbacaa094234728d8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 32, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d60be19e367192a55056eb8a81936ab8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 768, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_809a14e7883394d5783c5ecea2723f94(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7e47f00b89a83b58f2b40e9c647ac925(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, 16, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fac92df0b62c9b0932a8ac0aa1ab203e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 149, 149], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0967bad01a11b325be2075c12bcf6e8d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 38, 38], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fcd0ef96fde17bfe36d316834ad81acf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_eb0b82455812b1e069ad7fe596f1bf3d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 32, 24], dtype='float16'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c9114ff816b2f3e8d535e52e7d6050e7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 16, 256, 512], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8a82e1bc607f551a13054d8cd7b874b5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 28, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a26556ca24a31378aa5d2b7a3043ef68(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 80, 80], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_59c4e0bc731681aacc4f2d01dffa1b58(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fe2cbcb53d2ab5f6bbc138deb132c192(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_70f635c2e3f70bc2b9b030d020760420(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 58, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[58], dtype='float32'),
            paddle.static.InputSpec(shape=[58], dtype='float32'),
            paddle.static.InputSpec(shape=[58], dtype='float32'),
            paddle.static.InputSpec(shape=[58], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_480b42b318e09534977c677347792d04(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 8, 25], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_892560361a7154478f5b2366eb05ea43(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_963b499957681d183a90ee6fbcf07b4e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bde2707e5c148515f5b22de093b4d6fd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9ff1465bf90f692f92fcdbb9f0609310(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 64, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5f1ffb74d8fc89de617e0bc961f4e6da(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 232, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[232], dtype='float32'),
            paddle.static.InputSpec(shape=[232], dtype='float32'),
            paddle.static.InputSpec(shape=[232], dtype='float32'),
            paddle.static.InputSpec(shape=[232], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dea9f847be393beab56b023670ca37aa(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 128, 128, 256], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7e32ab27fd7a674e8bdf0c4fc681e7e1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, True), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a674dc3c5ebc2981cf0bf4f72a31dacb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_60ec4ec8f0d673e48eaae3861d000368(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f18595d2024ae3d46e77191cf6bf85e6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 32, 128], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9e245464a02f8d5c9be937d9791fa951(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 4, 256, 512], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5b297cbdbc20fd574274afcfdf60015f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 8, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_65710063276558e901a5a7cc7b2c59de(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 576, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e14c44bb7fb8421d2f6ff9de06993154(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 480, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_42db160513b08a34ac34c58fd9f4cdfe(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_44420878aef6a461745211eca5545482(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 576, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f2b51560d87b84e91350c0e6b6f086d2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 4, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0a83b1b40261c8df6f7d4e11860b3fb7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 30, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_80e042b16153a6a13000fc69bfd64db1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 64, 48], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f1a082db6ea56cc6f834213b72feb187(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 32, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6275e405292e7be448c4e45a93e79d9b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 224, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[224], dtype='float32'),
            paddle.static.InputSpec(shape=[224], dtype='float32'),
            paddle.static.InputSpec(shape=[224], dtype='float32'),
            paddle.static.InputSpec(shape=[224], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7dc14e15b0cf7a31536b29ecaebbc806(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 6, 6], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_829e10c433363b93c63afda25e46f6be(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bc4581fc47f74a192165f23667354af1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9ea508b07f7b60324874cbe201e660d4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 185, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[185], dtype='float32'),
            paddle.static.InputSpec(shape=[185], dtype='float32'),
            paddle.static.InputSpec(shape=[185], dtype='float32'),
            paddle.static.InputSpec(shape=[185], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4ea47978706cc46255775a9741b672c9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 672, 12, 12], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7efaa2655c87330018e1ae7b3862dde2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 2, 27], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_263b59190fe88b474344cef4ce31e75c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 56, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7b12fe585d6439f409f2d665b43cdde5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_80eaa0c7bfdbd2b1e435a1a302663c21(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 10, 10], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_19bbd1b0980169ab4c479175a10906d4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1200, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1442575d9d6d8d21262f054611450fa7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 256, 256], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b82eb8b23f613db89ec89a2c78416e3c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 48, 48], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ce8fcec7c43c5e4a66ab86f6974b50ed(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5e860e3a3156f27bbc563cb769888402(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1824, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1824], dtype='float32'),
            paddle.static.InputSpec(shape=[1824], dtype='float32'),
            paddle.static.InputSpec(shape=[1824], dtype='float32'),
            paddle.static.InputSpec(shape=[1824], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_83391cfece5cd7542dbfff9afea65200(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 864, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[864], dtype='float32'),
            paddle.static.InputSpec(shape=[864], dtype='float32'),
            paddle.static.InputSpec(shape=[864], dtype='float32'),
            paddle.static.InputSpec(shape=[864], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ed693f230fbb7e9cdd184fea7edf650d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4fd88af2ada1b12474fa31f895c5a800(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_601deb1983c5cb746610281b55124c97(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0d6acac54a26f9a7b8028cc698d0c0d7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 147, 147], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_82cb6c9abe3eac1e80505b61c84166b2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8014a0a060c0e4e6a65b292adf06a740(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 672, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_edbe30540ec177cce554ce48aac27083(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1280, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f0e3c2be8a5f1a675a0f7eb63f584142(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 20, 20], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_df0a557d314c037d833d2be6f20c411f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 50, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[50], dtype='float32'),
            paddle.static.InputSpec(shape=[50], dtype='float32'),
            paddle.static.InputSpec(shape=[50], dtype='float32'),
            paddle.static.InputSpec(shape=[50], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_322b006503f083c81d7cbf2702b856ce(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 4, None], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1c44d044048a874d2e962980a95e10bb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1856, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1856], dtype='float32'),
            paddle.static.InputSpec(shape=[1856], dtype='float32'),
            paddle.static.InputSpec(shape=[1856], dtype='float32'),
            paddle.static.InputSpec(shape=[1856], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4011c567561a3bf8cf22c9e03146685a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 52, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[52], dtype='float32'),
            paddle.static.InputSpec(shape=[52], dtype='float32'),
            paddle.static.InputSpec(shape=[52], dtype='float32'),
            paddle.static.InputSpec(shape=[52], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0924953f854a529bce7b5983b306f26a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5e3f12584c8ba623b5a6686df679b43a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 95, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[95], dtype='float32'),
            paddle.static.InputSpec(shape=[95], dtype='float32'),
            paddle.static.InputSpec(shape=[95], dtype='float32'),
            paddle.static.InputSpec(shape=[95], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3caf0ad33a9071ab1f2803f58fa730cc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 149, 149], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6ee54eaec46cead9dfed8ffc36bfc004(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 30, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1e650a105736cfce972de59b79c9de2e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 162, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[162], dtype='float32'),
            paddle.static.InputSpec(shape=[162], dtype='float32'),
            paddle.static.InputSpec(shape=[162], dtype='float32'),
            paddle.static.InputSpec(shape=[162], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3fb28a18176d8047772168f049a5f096(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9f150bcd6de150d77ac8d3cac5019fbc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 20, 20], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_419833893c17c3b5d1c376f82c06ab01(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 4, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_299dae75cca55f0032f06e8de76caac3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9e6e8c696fb140f453fb8604690d225f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 480, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c4c699c445b062d30c0454b975ad4443(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 768, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dc77783f4f885beba79336191518261a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 64, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_22166f4a1b8e7b18c57ffc83278e54fa(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 40, 40], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c69868eec40a5ad5dc2d6eb4fedf8513(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, True), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_346cb3eb7a1be8bbb2058808f9fe9edf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 4, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e58ed3e90c06992381b31f7b32cf3514(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 8, None], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f6d9f98a7283b104ec057a546d2c2fe1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 19, 19], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_653a2863d504e0a421142e940bd29d6e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b3f81b916e59f28b56d7c33903e8a638(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bb603ef7e789ce3a3c734fe1a71e5586(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 16, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5f89ec4b18ab354fc7c6cf31b335ec0d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 35, 35], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_282105b8a4b9005639370dd9fb1097c9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, None], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3b849076f67f776bce6a44e2f283a361(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 16, 512, 1024], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c35518f3334794ee130ef13ab20e2243(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 960, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f8b6f0326bb25d20671547dea2ea67b8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 16, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_29cce43550466adc735962c12bec1544(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ef23543cccc6d8990591be5a7ae61303(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 19, 19], dtype='float16'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_41d527d62e185f65abb0ee3c1bfbf1f1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 4, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dcb2f2f8150f2b965cf23f091bee3fcc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 151, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[151], dtype='float32'),
            paddle.static.InputSpec(shape=[151], dtype='float32'),
            paddle.static.InputSpec(shape=[151], dtype='float32'),
            paddle.static.InputSpec(shape=[151], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_be7b23f378ba717a9d80dcf4b4dd4ee5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_22abb3e6174fe3d2c487d3c21111b5bc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 336, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_92d6fd2dc40fd765a0c08202e2f8e538(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 728, 38, 38], dtype='float16'),
            paddle.static.InputSpec(shape=[728], dtype='float32'),
            paddle.static.InputSpec(shape=[728], dtype='float32'),
            paddle.static.InputSpec(shape=[728], dtype='float32'),
            paddle.static.InputSpec(shape=[728], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_00e024924948f9530c4c5682daa6652b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_97114e27a4e86e0b0689cef0f1ac820d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 128, 128], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7b169e5997ed90c117631737166d16c2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b96bb580d591d5c3b46ed9cd4fc63c4d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 24, 80], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7eb422b6e701c3e8483c93d320397fb9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_171e444d3443a35c9e028717a8c7c8bb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 352, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[352], dtype='float32'),
            paddle.static.InputSpec(shape=[352], dtype='float32'),
            paddle.static.InputSpec(shape=[352], dtype='float32'),
            paddle.static.InputSpec(shape=[352], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_625ea443ece00e0ece297543991b2b7f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 112, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0531163f843401361762cdccecb950eb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9ea90a6b5fb2e8b263179eabec9990ff(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 32, 32, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5f9d1da8241b988564b20c5f52acfb12(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 128, 128], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4a3488f8258fad01c6ae4ec834ce7cce(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 92, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[92], dtype='float32'),
            paddle.static.InputSpec(shape=[92], dtype='float32'),
            paddle.static.InputSpec(shape=[92], dtype='float32'),
            paddle.static.InputSpec(shape=[92], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5f6cb4dd98a786871b8712340ce32611(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2688, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[2688], dtype='float32'),
            paddle.static.InputSpec(shape=[2688], dtype='float32'),
            paddle.static.InputSpec(shape=[2688], dtype='float32'),
            paddle.static.InputSpec(shape=[2688], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f6df65240ac8925a89299b808655ce54(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 16, None], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b9c38b4e2b00f747c31761d68272255f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, 32, 64, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3a84edadf5bb6023f4da6bbef1abdfe8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 16, 12], dtype='float16'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3b3f6ba1cde7cd5208a94f870259742a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 53, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[53], dtype='float32'),
            paddle.static.InputSpec(shape=[53], dtype='float32'),
            paddle.static.InputSpec(shape=[53], dtype='float32'),
            paddle.static.InputSpec(shape=[53], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cc105e3dcf83ddef145c899763feb1e9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_75364c40faf4b75602b17cd756a2fca0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_40574a8c62f9d3b897f69456cf690e39(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 35, 35], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_75b1be54a495016f7a339c8c158ba5c3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9199062195c29090c0af0cba91381c55(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 64, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_84a078b7f89b849f5e6f8da37b96b34c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 162, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[162], dtype='float32'),
            paddle.static.InputSpec(shape=[162], dtype='float32'),
            paddle.static.InputSpec(shape=[162], dtype='float32'),
            paddle.static.InputSpec(shape=[162], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f6b9111cef8062043c1cb3d4a1b73a7f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 104, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3c414d695d1891198249a313f1480f9d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_26add3720a47ff50de0328dcec26b585(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a8a7335ee76d483408684aa7a0d340cd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 16, 12], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a35dfd75f4a39aec9d44b367a36edb83(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 128, 128], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7941a58505ea31eaa83ce3690d3d3db9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_90cd01123450907824b1892589e818bb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 75, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[75], dtype='float32'),
            paddle.static.InputSpec(shape=[75], dtype='float32'),
            paddle.static.InputSpec(shape=[75], dtype='float32'),
            paddle.static.InputSpec(shape=[75], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dd120fd62325ef88316b34cd1cfc1f64(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 50, 350], dtype='float32'),
            paddle.static.InputSpec(shape=[50], dtype='float32'),
            paddle.static.InputSpec(shape=[50], dtype='float32'),
            paddle.static.InputSpec(shape=[50], dtype='float32'),
            paddle.static.InputSpec(shape=[50], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_37e9aada9535d6dfb765f3e078755a94(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1642fa8b2e044a5d3f3bf262d50d5508(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 8, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6f601b8e703e0031cc4699e666abcfeb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 8, None], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0e0a708a01ea26658a7494dbfc156383(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 960, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1f9f28d79bb8fc25e95db546ff0ad50b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 200, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_54af904919118db57b8a0801eea6f455(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b3a9c28ba2de97e4bf731fe5fcce0b52(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1104, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1104], dtype='float32'),
            paddle.static.InputSpec(shape=[1104], dtype='float32'),
            paddle.static.InputSpec(shape=[1104], dtype='float32'),
            paddle.static.InputSpec(shape=[1104], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6ec6c331c3423d959a4fcd62b6abf104(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1200, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_68999480cca03708b0919edb32d1a515(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 64, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_903d1293b7e98fd85bcbb07e234cacbf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1152, 6, 6], dtype='float16'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_20c4c08b1310eb3f64a0bced48ef615d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e2e08385fc1ffddf21545f9dd55ed8b5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dd271ad96c296143ba2bfc64860ac86a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bd66dcd297483cb71457afafd752e6f1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 10, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[10], dtype='float32'),
            paddle.static.InputSpec(shape=[10], dtype='float32'),
            paddle.static.InputSpec(shape=[10], dtype='float32'),
            paddle.static.InputSpec(shape=[10], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f8cdcba81ed75913b22b320109803f41(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 704, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_89fc545d308c9959a47926c571a761f1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6acbf0634458293ad1d55909d3372c67(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, 100], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_045547d9fd3b70e80400ddb0704629b5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1280, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4213f15def29e76c2417266d316d747c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_754f3b2eceb3c969cafb4bcbd526a9be(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 576, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4f90e3704bca0bff852ea5a203b4b63b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 32, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1805ee5f66925e2e2281b2c7beb70074(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2048, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_02825ebecc52b0450c7e5430b04e81b6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, 16, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_399ad121ba8b7abf1eedd38cf9d4f3c3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6ce9c0d4736e8154caf21aad5b0a853e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3473e493d0e249ac682f4c69cbbe0a25(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 52, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[52], dtype='float32'),
            paddle.static.InputSpec(shape=[52], dtype='float32'),
            paddle.static.InputSpec(shape=[52], dtype='float32'),
            paddle.static.InputSpec(shape=[52], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f674ecd4caee1f22c0094e96a755c269(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 112, 12, 12], dtype='float16'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6894ddba73590d8743b835ce9fdde0b8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, True), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a918669557d320cf0b2c025803eb766b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, 100], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_48202d5ecfe341ee2cc8a38321d39991(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_99af99761d0b8f2f67a9a4bce2b0398d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 40, 40], dtype='float16'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b38f647774f7456158edb7bbe2614217(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 128, 175, 25], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_15ee5177d1b96df05331b81cd2a8b8d0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_842b6ae9eb48422206b3058acc979f12(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 152, 152], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_695ade52943d861cbb70a061e1e9a339(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 512, 2, 2], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_206d8378e199fbf2586f83ad79a8b719(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 82, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5bc89a0588a01a52fe89ef44e204e770(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2048, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a3bd1bafc24393f61078257ba8e253d7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 128, 128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_99e42cd7f95916211bc300acf6ff6d97(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 56, 4, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e8261252208644d1e6de9fe1fff3934f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1568, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1568], dtype='float32'),
            paddle.static.InputSpec(shape=[1568], dtype='float32'),
            paddle.static.InputSpec(shape=[1568], dtype='float32'),
            paddle.static.InputSpec(shape=[1568], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6868ec7b78224230b1c6b5ca3874bf9b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 1, 3], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c85ba9321ecc7a325e149f98792a3fe7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 174, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[174], dtype='float32'),
            paddle.static.InputSpec(shape=[174], dtype='float32'),
            paddle.static.InputSpec(shape=[174], dtype='float32'),
            paddle.static.InputSpec(shape=[174], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4ecd3e332c82c379847f781922767736(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 31, 31], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_44202e3624021085343f21e059d11524(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 92, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[92], dtype='float32'),
            paddle.static.InputSpec(shape=[92], dtype='float32'),
            paddle.static.InputSpec(shape=[92], dtype='float32'),
            paddle.static.InputSpec(shape=[92], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4411473bb05eeca76341b499dfcf4c73(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 16, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7bd3f8b6a0fa274a9e326618eee146d4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 160, 160], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dea80a9adc2725dd7126b65cfeff8f4a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 480, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c3df70fd4a718b503a21bca5f0fa4598(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 960, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b79a03b9889171b06565fd33ddcec551(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b54f59a7929886bfe02b395062ae82a0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 8, 25], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6673197adf120ee098622608c376aa51(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 80, 80], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0003daa770852f0bc16a1add0a2c4e28(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5fc991c75e15990436c661ad4f31bc2c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 76, 76], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7a41d4c7f9022be3baa88964b959dfb3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 224, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[224], dtype='float32'),
            paddle.static.InputSpec(shape=[224], dtype='float32'),
            paddle.static.InputSpec(shape=[224], dtype='float32'),
            paddle.static.InputSpec(shape=[224], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e10ff6dc7d9b5018dce6730c897ea458(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 174, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[174], dtype='float32'),
            paddle.static.InputSpec(shape=[174], dtype='float32'),
            paddle.static.InputSpec(shape=[174], dtype='float32'),
            paddle.static.InputSpec(shape=[174], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_04553fcf88c43fb8aadb6eb006c585a8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 64, 256, 512], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ab7a00ca20552836652977003ef332fe(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_876d3325e90e8844fa413da7c1376370(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 80, 80], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3f59838f0a9cd17f718200550a2bdf3a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 102, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[102], dtype='float32'),
            paddle.static.InputSpec(shape=[102], dtype='float32'),
            paddle.static.InputSpec(shape=[102], dtype='float32'),
            paddle.static.InputSpec(shape=[102], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_68d9ca62f121598cf5fce845fe4e3830(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 224, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[224], dtype='float32'),
            paddle.static.InputSpec(shape=[224], dtype='float32'),
            paddle.static.InputSpec(shape=[224], dtype='float32'),
            paddle.static.InputSpec(shape=[224], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d8a247c0ec9ff71630a7b32ce2c9213d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 76, 76], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d685f2531a4e8e1ab9346744dc305143(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 8, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_264a1ceeb5ec3adbfe3fddce6195166a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ee81696f9229deba291d9f8d23cd674d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 512, 2, 2], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_626f3382699b88b9e7045ebd7392ddf0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 832, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3f81715057e631b27e8769c1ecebc423(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 82, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2423ebd1101a36394ab1b611cdd61dc9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 4, 64, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b9f67ea0ac53699cff422db5b35f4e7f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 64, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4e066cb3d7c6f78d55cfbada4f260ec9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_578658953af6c5334a305c1d756ab95f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 256, 88, 25], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c4d19e8d910b39d7d8899b665bb1f6a7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 702, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[702], dtype='float32'),
            paddle.static.InputSpec(shape=[702], dtype='float32'),
            paddle.static.InputSpec(shape=[702], dtype='float32'),
            paddle.static.InputSpec(shape=[702], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_968feae99ca59e88c788c7ab6dddb91d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 72, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d0af63b422c5434e9c5ad57c6b33b9d3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a9fc2bb0c0bc225642518d337cf4d8bf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0cfcadeda21a27d8481eb1381f9e54f7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 64, 256, 512], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c085bbf49f874f3d448e2e9fa624208b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 32, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b94beeb62600b02926793987f4e3ec6c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 320, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6a5770448924111a18dd88356c399321(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 120, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_801eff40f6ce09cb8d1af376fd7f30ac(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_35dbfde51f98a7f8586486c5e8c555a0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6ed2f9a7a3b3c0ddc363ed2cf9c5621f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 5, 5], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_408d245d25126df77149f7d08aa9340f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 40, 40], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1716243e54b0b8be76f839f145a0844c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 16, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_46d4a4697665df3cdd855c4e6229b962(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 8, 25], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3af3a6b6d2c5c0ae73a92daa06d7104c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 4, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b2a27438a910061f2374bc6ff571b104(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1952, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1952], dtype='float32'),
            paddle.static.InputSpec(shape=[1952], dtype='float32'),
            paddle.static.InputSpec(shape=[1952], dtype='float32'),
            paddle.static.InputSpec(shape=[1952], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a001f631eb3482fb4092b22194a111fd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1280, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e7f539abfc4c607a5f18a9fdd1e81b06(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 336, 4, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_00b7f414cd5d1873df88c7b75f6f611f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 16, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_af156719f364b1bbcc99a25272c7d11e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 336, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e30289a77f655e48c492be98ebf63d03(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, 64, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3dd51b2ec7b03b02c4aa857a43f3dffc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2f3de459085cd99a8d7aa22a10da2843(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 504, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[504], dtype='float32'),
            paddle.static.InputSpec(shape=[504], dtype='float32'),
            paddle.static.InputSpec(shape=[504], dtype='float32'),
            paddle.static.InputSpec(shape=[504], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_02e3dc0424af03931aaf0480e7113fca(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 20, 20], dtype='float16'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bec0e815d3128b0780b7221166cacd9c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 20, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
            paddle.static.InputSpec(shape=[20], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9d4af7c9a02c07253705daca1609ea05(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4662f7dbf1bfba37b6e568739b9af629(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 208, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_20ed86f8dadd07a4674aa0e32b32eae3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cc5107c08852c64c8bad5e05415fdd87(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 2, 4], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c22e309c5201e4b0852ec0fa242bab74(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 672, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6b99617946dbcbd795bd7875ad9ea3de(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 122, 122], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_224a201907eed15b1de5c8ae3bed5fc2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 20, 20], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_06188808edfa48caa40aa1fad819536e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, 31, 31], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_87d1861b61a5421593d15b19010c587b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b88bdd4822487797778819bf35905a09(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 16, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7f27863bb556262ce9ca842c1183b000(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 44, 40, 40], dtype='float16'),
            paddle.static.InputSpec(shape=[44], dtype='float32'),
            paddle.static.InputSpec(shape=[44], dtype='float32'),
            paddle.static.InputSpec(shape=[44], dtype='float32'),
            paddle.static.InputSpec(shape=[44], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c892f8255afcce5721399d8970d0c97d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1000], dtype='float32'),
            paddle.static.InputSpec(shape=[1000], dtype='float32'),
            paddle.static.InputSpec(shape=[1000], dtype='float32'),
            paddle.static.InputSpec(shape=[1000], dtype='float32'),
            paddle.static.InputSpec(shape=[1000], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bfc693cb64eafdcc84b0e80cbfb15276(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 24, 24], dtype='float16'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cd3b91bc227b5b65664dfc2688a3b672(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 32, 32, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_37ca527caca66fa6bd8b014b3f6521f5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1536, 10, 10], dtype='float16'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c879500ef43c9583e591cf88058373fe(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e8d0c135d4805cf056ec128baf6021b6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 504, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[504], dtype='float32'),
            paddle.static.InputSpec(shape=[504], dtype='float32'),
            paddle.static.InputSpec(shape=[504], dtype='float32'),
            paddle.static.InputSpec(shape=[504], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_407935324dee569608d3b6e8e126ddf7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 2, 6], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0243fce825de868f4efe321aa58a736b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f3ef53dd7ffd9dd955963e7797739157(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_954e05d384686e7f64edaf73adcd531e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 16, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_70aba72961631a21282cfb34425d9745(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 720, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e58f60cd2c596281532515892b76457b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 4, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d25e7427dd6d57504ddd98f54354eff3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 8, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cdb498724826f2d1d9074560e678eafe(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 28, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cd1ad2d50aa07808a6d27b1f1abf8ef2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 80, 80], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_831f90d1946f5d88b73a11bf9e88126f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 36, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_00c9d6d672498715a61d2ae2a6f98c7d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 88, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a40598a92049528676957c4113ab8b71(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 672, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a3e21fe52e3c2c2a118074fc71e41be9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_148a3483c0f7584668318beaeeb76022(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 270, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[270], dtype='float32'),
            paddle.static.InputSpec(shape=[270], dtype='float32'),
            paddle.static.InputSpec(shape=[270], dtype='float32'),
            paddle.static.InputSpec(shape=[270], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c23f660599def9fbcf07210411b7c578(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ac9c506333fd23e58f317f5b39f87cf8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, True), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_aa301bbd5a1e416e0b921d7e268a8e4a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, True), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e82b94524c25643933cc36c58a30e558(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e09aa6a49354526ead685eacadee9c30(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_14c847de69b40a056a624183fdd9b27f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 72, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e6281efc4237d516bf62349b005066d3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 512, 8, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1be5aad69366d175e7b219fd7a176437(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 4, 12], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_59a74a1aa3538b67792395a662b46c96(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 160, 160], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_759a950012c9a587d288dd903532d5ef(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2048, 8, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3a69be4e44c5f356df59147f61e8e82e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 4, 128, 128], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3547c3c3d5ba9688e3cca445783999de(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 768, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b242da51ea0bf8a59aa10b12507e499e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4461f75cf5c1fb8a609859991616732f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 19, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[19], dtype='float32'),
            paddle.static.InputSpec(shape=[19], dtype='float32'),
            paddle.static.InputSpec(shape=[19], dtype='float32'),
            paddle.static.InputSpec(shape=[19], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_13672e74ed748a972307260bf7e23292(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 152, 152], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7e323d9cc4d140b84a43a2e2bc7c76b5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 256, 175, 25], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_09560c7f59586714659b8574fbdbfc34(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 256, 32, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ba70d8ff83ade7d49521c28915f714e3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 20, 20], dtype='float16'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7fc0ea8eca85f35aafbd5d37c26a8733(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 432, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3ca06fdf72c37490a4acc661f47fdc1c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 480, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5f7667cbaf3d2a84b08da0bcbc1ff90d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5be7e6bac60987a818142ffaeaa572e7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 4, 12], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e14df009148bd5c0fa59a070b1ebf16e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 160, 160], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_935880d6a6dcada2471b24b5bbf2820c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 10, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[10], dtype='float32'),
            paddle.static.InputSpec(shape=[10], dtype='float32'),
            paddle.static.InputSpec(shape=[10], dtype='float32'),
            paddle.static.InputSpec(shape=[10], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a4d9198f3e3167489c2e0fb8793391b3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 576, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cfb0a2c14de50deca6e43948761b253b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 32, 24], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8cc3eb9723866708e4398e370127417a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 118, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[118], dtype='float32'),
            paddle.static.InputSpec(shape=[118], dtype='float32'),
            paddle.static.InputSpec(shape=[118], dtype='float32'),
            paddle.static.InputSpec(shape=[118], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9aaffcd1723b899daa7ac8d9d877163f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 24, 80], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ec58cf47a015169c640dba668a5ac0d4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2048, 4, 8, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9b1661a5cd671a8b70b1304b03ebf241(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b755acb08dfb84146f7673f238a48da2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 12, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9d82ba9551361ce0e94ae8e857ddafeb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 8, 25], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2eae31498e57b686bccc36b149fb23bc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 56, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4295435d2d0a847d799e7a323e990fc1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5361572d976f685ab7247dd0d524f3b5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 100, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[100], dtype='float32'),
            paddle.static.InputSpec(shape=[100], dtype='float32'),
            paddle.static.InputSpec(shape=[100], dtype='float32'),
            paddle.static.InputSpec(shape=[100], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c681fab815e93f1cbf1a2b6f35ed0f5b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2432, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[2432], dtype='float32'),
            paddle.static.InputSpec(shape=[2432], dtype='float32'),
            paddle.static.InputSpec(shape=[2432], dtype='float32'),
            paddle.static.InputSpec(shape=[2432], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_32f082526f4718640e33f8509d211cda(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 64, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_29b7631c13ce6f45783542634b6f9809(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 400, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[400], dtype='float32'),
            paddle.static.InputSpec(shape=[400], dtype='float32'),
            paddle.static.InputSpec(shape=[400], dtype='float32'),
            paddle.static.InputSpec(shape=[400], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4805c3dc30ed25876b62bc7c6d061d59(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 832, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_540325bcccbcf7dc27dcd00ff9f304dc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 35, 35], dtype='float16'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8e8f73e3bbd4fb2d7bba6e6d36e22f91(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 2, 4], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c96ef0ab05eec9f0e73cc37e470d0097(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 20, 20], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_17467ab67ef840c508175cd1d2686d33(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 624, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[624], dtype='float32'),
            paddle.static.InputSpec(shape=[624], dtype='float32'),
            paddle.static.InputSpec(shape=[624], dtype='float32'),
            paddle.static.InputSpec(shape=[624], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0b2e3d95f27029d02d70f310dffa5275(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 4, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ed4c9c36390ae7eb199866066fe8c806(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1216, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1216], dtype='float32'),
            paddle.static.InputSpec(shape=[1216], dtype='float32'),
            paddle.static.InputSpec(shape=[1216], dtype='float32'),
            paddle.static.InputSpec(shape=[1216], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4cec5627b665aa86e36d6bb0743d780c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 720, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4f5421ea7cc936dcbad01422103e791b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 960, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_659275afe5a1f942293ad5b944b482d6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 160, 160], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9bca70962b358c3795b4a1775aa1749e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 6, 6], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8caf97a8480c55f50ffb5e8a321f1a14(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 304, 304], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_eba84ae045edc580a5322d6110ee9b6c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 75, 75], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ec29c855527613d9aa5cb61230da837f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 44, 80, 80], dtype='float16'),
            paddle.static.InputSpec(shape=[44], dtype='float32'),
            paddle.static.InputSpec(shape=[44], dtype='float32'),
            paddle.static.InputSpec(shape=[44], dtype='float32'),
            paddle.static.InputSpec(shape=[44], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_90d324a1f7a05e6c93e4c5157ae1ba91(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7a26ecf10bc78b5487d0ed21b0a39662(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_faab88388cbfec387fbee6d3da415e38(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2208, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[2208], dtype='float32'),
            paddle.static.InputSpec(shape=[2208], dtype='float32'),
            paddle.static.InputSpec(shape=[2208], dtype='float32'),
            paddle.static.InputSpec(shape=[2208], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_127ed08cbf7d19b8962ef5d19cc14847(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_672672e9aaa178cbcfa2285d1b93598e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 16, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ad4300dd0dd7c1655f8157362f7a3fce(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 1, 26], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4d0b99242f00e944a18b9bafd47ea522(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 32, 24], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4a21513bcc72ad90df1926e60f25981d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6a26da77a4e4fb14b1a41ff1868ad0c2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 720, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e6bbce1dac03ffdcfed09c99e5d56cb5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1312, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1312], dtype='float32'),
            paddle.static.InputSpec(shape=[1312], dtype='float32'),
            paddle.static.InputSpec(shape=[1312], dtype='float32'),
            paddle.static.InputSpec(shape=[1312], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7aa782216c8bde1b95745d01bda60367(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 27, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[27], dtype='float32'),
            paddle.static.InputSpec(shape=[27], dtype='float32'),
            paddle.static.InputSpec(shape=[27], dtype='float32'),
            paddle.static.InputSpec(shape=[27], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f01e32f14cd76fc4e89468d4c13603c2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 16, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c48808d6c4a1774361a66dc6663beaaa(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 4, 32, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c3c82948e8ceddbaefb14f1f6e0adfe2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_441ab70336b8cf138794def7506deaa9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, 100], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_461425de9bf2a33cced22dafb4c617e0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_82af7e822b695e09d38e6ea79a3bf084(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 104, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7be5f0cb427996b075b93f0c99ba8f58(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8cff36adecaaee29f89c236933757419(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 32, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5cf4d4b147717d00b0b75a61caac84ce(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 8, 25], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ecc600a71fc16850cf7a760f447c83f3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 40, 40], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a2e2e20ea35ca3f2eb4df3088dc67f39(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4607809436989505002a1958a7354e68(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 208, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d7ef8646f6fce33d8636affb4ac762fb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 38, 38], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2518c43ed17c92ba61515166afdb2e58(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 60, 20, 20], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
            paddle.static.InputSpec(shape=[60], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7281c73301669007031a5922202d9dc6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 4, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d092b5f6fce32e2a8e87b36f36408ab1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_52fb4c7709facf72bb75957b3bc13cea(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 52, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[52], dtype='float32'),
            paddle.static.InputSpec(shape=[52], dtype='float32'),
            paddle.static.InputSpec(shape=[52], dtype='float32'),
            paddle.static.InputSpec(shape=[52], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_78c34eb38582027a3bfe5a17bdc8a49d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bd8359a9fa2df57fc823821810233f7b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1984, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1984], dtype='float32'),
            paddle.static.InputSpec(shape=[1984], dtype='float32'),
            paddle.static.InputSpec(shape=[1984], dtype='float32'),
            paddle.static.InputSpec(shape=[1984], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3039e15c3bff93f37bc753a08dedbe9e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 2, None], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_70478f5bc84b794c41b5b155e2f71fa1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c9c78c6e93a10811a79f7a80805b6d47(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 162, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[162], dtype='float32'),
            paddle.static.InputSpec(shape=[162], dtype='float32'),
            paddle.static.InputSpec(shape=[162], dtype='float32'),
            paddle.static.InputSpec(shape=[162], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_696637a7fa2dd68d58407021599fbc16(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 24, 24], dtype='float16'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4ef7505da06e846b325d100749737dc6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7b82cf83ff45043ef66c84f20c5b0beb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 26, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[26], dtype='float32'),
            paddle.static.InputSpec(shape=[26], dtype='float32'),
            paddle.static.InputSpec(shape=[26], dtype='float32'),
            paddle.static.InputSpec(shape=[26], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5548a634096bdea06c688a80376ff53e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 35, 35], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a37223a4fd84d051cac78ea33cd6f506(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0384eb41ea893946136c8462fa41d819(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 576, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d2f89acc798a3450b177fda556d9c640(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 64, 48], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_10a6e0f122747cdb1636ff08d1dba959(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 640, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ff40ca29a1d7f7c404a184f6433a19a0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1440, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1440], dtype='float32'),
            paddle.static.InputSpec(shape=[1440], dtype='float32'),
            paddle.static.InputSpec(shape=[1440], dtype='float32'),
            paddle.static.InputSpec(shape=[1440], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cab09c7c4fa56e2488ea07ea43375fe3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1760, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1760], dtype='float32'),
            paddle.static.InputSpec(shape=[1760], dtype='float32'),
            paddle.static.InputSpec(shape=[1760], dtype='float32'),
            paddle.static.InputSpec(shape=[1760], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d892ba2b1df01c09159a3b7835f549af(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9864e859e39ce967be5cf9baf8b4d3e3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 366, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[366], dtype='float32'),
            paddle.static.InputSpec(shape=[366], dtype='float32'),
            paddle.static.InputSpec(shape=[366], dtype='float32'),
            paddle.static.InputSpec(shape=[366], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c112c4673f3887b2d0c4814080b7d845(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 32, 32, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4c61f7bc0e45296f623737d3903f679f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 768, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e85aaae890c2775c3c0da038a97fc088(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1104, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1104], dtype='float32'),
            paddle.static.InputSpec(shape=[1104], dtype='float32'),
            paddle.static.InputSpec(shape=[1104], dtype='float32'),
            paddle.static.InputSpec(shape=[1104], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5f67d1a33739513a1ccf71884ffd49a9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 28, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_45f7eefe39f0cdc7e697e422c7996796(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 366, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[366], dtype='float32'),
            paddle.static.InputSpec(shape=[366], dtype='float32'),
            paddle.static.InputSpec(shape=[366], dtype='float32'),
            paddle.static.InputSpec(shape=[366], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_da28452afd87fa7ec6700e3a52e92bcc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ec88a9a1602c9efb410e4e001ac243c8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 10, 10], dtype='float16'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_05ccffff69f6576dc66e7c98f18af5b9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 26], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f010ac89e95313785804b533a6f28f66(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, 56, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2db559d88ad496e3ddf832b3c8e53914(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ea0b2107e75d7da7dece16d2818e62dd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 49], dtype='float16'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c7756dc33bcfe086f44d82256691cc50(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_21a16052f5089a16cac936325c38f956(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_31b49c768d52ebc24a46e57240ade1a2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 30, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b0a4b7c90da363fc3fd7250aa8d5641d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 64, 256, 256], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0651a63728a76e450adae5c8c420ebc3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 728, 38, 38], dtype='float32'),
            paddle.static.InputSpec(shape=[728], dtype='float32'),
            paddle.static.InputSpec(shape=[728], dtype='float32'),
            paddle.static.InputSpec(shape=[728], dtype='float32'),
            paddle.static.InputSpec(shape=[728], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c1cd91d7c1f51f7aaf5ae5a61587df98(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4e1aa685230bf4542d74141d26c0a6c8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 120, 4, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bf4172b5e8d85da0334345b3af72fd78(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 48, 160], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9ab99cbf2437e4d5fd27a05ac0ad88a9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 636, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[636], dtype='float32'),
            paddle.static.InputSpec(shape=[636], dtype='float32'),
            paddle.static.InputSpec(shape=[636], dtype='float32'),
            paddle.static.InputSpec(shape=[636], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3d0dedb55bd15e3225ad133db883e64e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, None], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ddf56b24aa755252d82b107dabd2bb60(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 10, 10], dtype='float16'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_efc5e703d69a415622d3df8531a072f6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 6, 6], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b24b502d328b553d9ef8b792d72b4b61(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 80, 80], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_46b849d24e80432a68cb996293a9a1cf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 4, 64, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5544828a7f181373d095ef45852af1a0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2112, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[2112], dtype='float32'),
            paddle.static.InputSpec(shape=[2112], dtype='float32'),
            paddle.static.InputSpec(shape=[2112], dtype='float32'),
            paddle.static.InputSpec(shape=[2112], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8d462e81eee1dce8cb3768f466d00faf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1536, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_61880af5b4bf23d7514c17811f77f734(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 512, 8, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8b71e647fe0089f035928ef765b0f2b4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 147, 147], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b2dcd37d98e3adf79f9f85ea7f694dc4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c3d447776905d934c8118eef09940f2f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 100, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[100], dtype='float32'),
            paddle.static.InputSpec(shape=[100], dtype='float32'),
            paddle.static.InputSpec(shape=[100], dtype='float32'),
            paddle.static.InputSpec(shape=[100], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2a07ea5061d7ad2a0c11974d825fb43b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 360, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[360], dtype='float32'),
            paddle.static.InputSpec(shape=[360], dtype='float32'),
            paddle.static.InputSpec(shape=[360], dtype='float32'),
            paddle.static.InputSpec(shape=[360], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_87f7d7b8718088814721bc24351b1ad7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 138, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[138], dtype='float32'),
            paddle.static.InputSpec(shape=[138], dtype='float32'),
            paddle.static.InputSpec(shape=[138], dtype='float32'),
            paddle.static.InputSpec(shape=[138], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0fc88152d11556dead22b51cf8347e8a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 82, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6884bef74e02ae00c3bff219ebcfee46(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 32, 100], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_189ca6ed72ffb5645192b7ab85ce4e09(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 40, 40], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_755455be86c40f73334f229d81cfb610(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3f42d5a9dbdc022e1c99000b5a383ec2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 436, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[436], dtype='float32'),
            paddle.static.InputSpec(shape=[436], dtype='float32'),
            paddle.static.InputSpec(shape=[436], dtype='float32'),
            paddle.static.InputSpec(shape=[436], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_31f4a4025c50f577a0d40bf10d25fb87(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_01be06e6f549cb4256fd5fbcd5391ac1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2048, 4, 8, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3ff41849c1f594b322c9c2499c6faf7a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 160, 160], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_238f535c78e73773dc86e6d27b960b9c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, 8, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f49279ac3043eda0a82805a7a8ea9779(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 128, 64, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c99838ebd7cb34868e5934a9df014868(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b0beaf0879818191f4dbac2d4c9de54e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 832, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9c791f891b65821f9042c01087883c7a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1044, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1044], dtype='float32'),
            paddle.static.InputSpec(shape=[1044], dtype='float32'),
            paddle.static.InputSpec(shape=[1044], dtype='float32'),
            paddle.static.InputSpec(shape=[1044], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_24286b574aace75bf578db8ff07e0e99(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 31, 31], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7c7e6f5bbda6509b41494959f924cae4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_11949321c9995b8a52b792790afc1e4b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 236, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[236], dtype='float32'),
            paddle.static.InputSpec(shape=[236], dtype='float32'),
            paddle.static.InputSpec(shape=[236], dtype='float32'),
            paddle.static.InputSpec(shape=[236], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7ceff3051ce8ffe6a3af6962933f1156(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 24, 24], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7f6dbe06aa383090672c873099fd6dfa(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2b078752569198765df0efc82d9b357e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 84, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[84], dtype='float32'),
            paddle.static.InputSpec(shape=[84], dtype='float32'),
            paddle.static.InputSpec(shape=[84], dtype='float32'),
            paddle.static.InputSpec(shape=[84], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_81915bd6f97fa7cc8ad5815845b097e9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 336, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_17dc6585acd08b6f30f9c456f2d4d68f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 4, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_eefcbc750dc9b0e65d40ff7907f1312b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 16, None], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2f3626e73caf1cf25b8b075ab4aeac6d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 244, 244], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_86de788e6bb94bd4959b9b0ba3ac5b8d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, 31, 31], dtype='float16'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dfc6beecf20948b2d428320a8ff22420(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 10, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[10], dtype='float32'),
            paddle.static.InputSpec(shape=[10], dtype='float32'),
            paddle.static.InputSpec(shape=[10], dtype='float32'),
            paddle.static.InputSpec(shape=[10], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fd4d8fd92633f0918a445794c425b03f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 448, 8, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[448], dtype='float32'),
            paddle.static.InputSpec(shape=[448], dtype='float32'),
            paddle.static.InputSpec(shape=[448], dtype='float32'),
            paddle.static.InputSpec(shape=[448], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_454d1a24aa37124b5dc12d58d14943d6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 128, 128], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7757654bfe9b9d46a4cd48c8a31d3306(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 36, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0bd5599424b42dedca8b043b614148ef(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2112, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[2112], dtype='float32'),
            paddle.static.InputSpec(shape=[2112], dtype='float32'),
            paddle.static.InputSpec(shape=[2112], dtype='float32'),
            paddle.static.InputSpec(shape=[2112], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2763dff4561e1c891db3c3a2068e1058(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, 8, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_57546f95726f7922fd067f54dd9b651f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 2, 27], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_45cb315e383525e33a9df82784065670(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 26, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[26], dtype='float32'),
            paddle.static.InputSpec(shape=[26], dtype='float32'),
            paddle.static.InputSpec(shape=[26], dtype='float32'),
            paddle.static.InputSpec(shape=[26], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_07c8ef96c5099df647626e5400c6e7b9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d611060c19d2b5cb27db87cc3b170ddf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 120, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d0eb36d2a7b10a53ddf6786bc2ba46c9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 31, 31], dtype='float16'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1469e5829ad6dd174a4d4301d92df512(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 38, 38], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2780c15474f5780d697bcab8c75e8283(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 184, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[184], dtype='float32'),
            paddle.static.InputSpec(shape=[184], dtype='float32'),
            paddle.static.InputSpec(shape=[184], dtype='float32'),
            paddle.static.InputSpec(shape=[184], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_75fe4f0c8a04a0ae02b307b58d9938d7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 50, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[50], dtype='float32'),
            paddle.static.InputSpec(shape=[50], dtype='float32'),
            paddle.static.InputSpec(shape=[50], dtype='float32'),
            paddle.static.InputSpec(shape=[50], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_467937c1b73b0a6445e3ed6e1e91f50b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_27a114a1ea98a13a7bb957d7d400fa15(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 12, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b33da5cfb82e732a25322a60aa9f34e6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 244, 244], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_37c9be2f2073a048ee4868212a97123c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 175, 25], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9bab6ffa8f76d6e5d8dd3c9144454c5c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 288, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_41bdd673393c791b8d2e8d2405883fa4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 336, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b5f7017c2ff4851109f25ef4646d6508(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 81, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[81], dtype='float32'),
            paddle.static.InputSpec(shape=[81], dtype='float32'),
            paddle.static.InputSpec(shape=[81], dtype='float32'),
            paddle.static.InputSpec(shape=[81], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c3b4a6d65f831f3e96d102564a9c4de6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 640, 8, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3e2e5292c99d809a2b77c71c9a22725d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 288, 31, 31], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ad8305bcdf7029aba50842d7fed98e6e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, 128], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dbd9633630d82736e0e78f1238847b14(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1488, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1488], dtype='float32'),
            paddle.static.InputSpec(shape=[1488], dtype='float32'),
            paddle.static.InputSpec(shape=[1488], dtype='float32'),
            paddle.static.InputSpec(shape=[1488], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b22610661c3aba4c91d66f9c02532a06(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d4c980e711c4e8864589d6bba9225820(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bd62a60ad014fc4d8d3bbdad0dc812e6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 62, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[62], dtype='float32'),
            paddle.static.InputSpec(shape=[62], dtype='float32'),
            paddle.static.InputSpec(shape=[62], dtype='float32'),
            paddle.static.InputSpec(shape=[62], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3254ec6234d29c6b79ccc83ef1b64d71(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2166fbdf0dc76caa9eaeb4e7f2a18035(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 164, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[164], dtype='float32'),
            paddle.static.InputSpec(shape=[164], dtype='float32'),
            paddle.static.InputSpec(shape=[164], dtype='float32'),
            paddle.static.InputSpec(shape=[164], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f70d253a8c85f173170351faade870e3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 8, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_89131f01c734536b69f798872c41bf81(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6ba1fd2d16db2400e955308f53f3c0e0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1344, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1344], dtype='float32'),
            paddle.static.InputSpec(shape=[1344], dtype='float32'),
            paddle.static.InputSpec(shape=[1344], dtype='float32'),
            paddle.static.InputSpec(shape=[1344], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dd488a3ab73b95d9e1bdd5c6be0fe6a5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 32, 100], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b6a497dbe1ffae0c9659fed8feba17d2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 960, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ec15b4f365883c06a3a1e295ec9f5f69(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b460480ca04be6d2344634c87163ebc1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 40, 40], dtype='float16'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5f5147e6dcf0a2f6457540422bd0a3b0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1824, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1824], dtype='float32'),
            paddle.static.InputSpec(shape=[1824], dtype='float32'),
            paddle.static.InputSpec(shape=[1824], dtype='float32'),
            paddle.static.InputSpec(shape=[1824], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e1fa640922dffe9ebdf7932ff113bf51(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 32, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a00bb4e257ab0a36e29ebfeeff0d842b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 200, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7f76141c7f28531e80d93ce1df1964fc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 400, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[400], dtype='float32'),
            paddle.static.InputSpec(shape=[400], dtype='float32'),
            paddle.static.InputSpec(shape=[400], dtype='float32'),
            paddle.static.InputSpec(shape=[400], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_30ab71bcbd86487bbd826b41f391c7ee(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 28, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7141b8d9126bf91af1d01faeff73cc6e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 72, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_12c379cc113a403f53a4caf467d6d562(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 336, 4, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b2696b10e83c1d3c2a92b52e01f50ed6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 64, 128, 256], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_88907b574d986afac9f3889d9e2f029f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_504102c0c322ef1243f795b3deabb28e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_92c0ba69f1d3da47805e4633b3a97d29(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 112, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9b2c2d5669307fc6f4b29b51d01f95fd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 128, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_517c329316f36622f36ec2c73e504025(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 14, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dcc0d2be3e3ead42d6e2ee26a70970cb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9578368f12d97c0d445fae3606a50612(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1280, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a43b16ef5c6d1f041d67efa1df4ef913(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 672, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_be909b13142b62385fa32c76b9f65ef9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a09f14130ee5b9fa144a8e7689b473f5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 832, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a31fbafeb6158af7b3bc553784331220(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f04b1f723198053247940073cf961599(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 32, 100], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_736bd2913629b3dcf106e7ca0229d249(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 4, 3], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0aa32758ff3449ee07ce9667f89e0a15(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 10, 10], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1b872f8572aae335a7c38cb1cc0c210f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 640, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_84021c0cf0b138b0d1fa628478787faa(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6ef1b1d00a9af60b2d7ce60ffa060a36(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1584, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1584], dtype='float32'),
            paddle.static.InputSpec(shape=[1584], dtype='float32'),
            paddle.static.InputSpec(shape=[1584], dtype='float32'),
            paddle.static.InputSpec(shape=[1584], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_21ef9c97a89964f4ef1bc3e4fd9758f8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_06d57437dde5bc9c97fe8a1ae10cbb06(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0449ac4c00d9aa171a40fc10cc3bfa5a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, 100], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5ea9bcc6c5efaff0e969963c1fbd5d7f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f19d743a92f26dc8cbc59531e4a073fd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 64, 128, 128], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a66c9ea1116f694a0225c128cd0ce6a3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 320, 320], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1ace33d8bf5c0141d2ececd56c48ff1f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 672, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0b8f44bc97a356e5c42c3042b7bd1505(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c401d1176ab51469a816b7bc7ba19212(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 48, 48], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d54dd1a553b1016b2cb9efe5deb82d10(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_db3075fe5be00cbb4c1ed9b2bee0f102(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 61, 61], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_79abae8720848df294fc716c787f83ee(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 128, 128], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_11a1f24d7a4be003f6b0979b40654476(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2048, 8, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a4cf30a4d5b9ad84515e68b21657b155(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a70b972739753d83ff305a95acc22b14(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 2, 2], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2413fec6a4581a7335100a8ec156542a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 132, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[132], dtype='float32'),
            paddle.static.InputSpec(shape=[132], dtype='float32'),
            paddle.static.InputSpec(shape=[132], dtype='float32'),
            paddle.static.InputSpec(shape=[132], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_03d990597e58999bf0a44ee1621b83d8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 112, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0dbd9fe2e1e04bf2270b23b74c3d9df4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 112, 12, 12], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_45037d8ea16b2d2e3487e5b2fae77fa2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 4, 64, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_086aa9ecd36198eb2f410fb51ec5d3c2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 38, 38], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_024d3110644a58f47c54fe0c5e6f8dff(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a0459a49b41db6a39edb87eaa20aa89c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 176, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[176], dtype='float32'),
            paddle.static.InputSpec(shape=[176], dtype='float32'),
            paddle.static.InputSpec(shape=[176], dtype='float32'),
            paddle.static.InputSpec(shape=[176], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_aadc1f2d06d73f2d1a2e0c0c53deebd9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 300, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[300], dtype='float32'),
            paddle.static.InputSpec(shape=[300], dtype='float32'),
            paddle.static.InputSpec(shape=[300], dtype='float32'),
            paddle.static.InputSpec(shape=[300], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8f9a9f3e7b40fc374b85b405c3aff53a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 71, 71], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c0bdbfe9de0855b84834472a3b76ddfc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1000], dtype='float16'),
            paddle.static.InputSpec(shape=[1000], dtype='float32'),
            paddle.static.InputSpec(shape=[1000], dtype='float32'),
            paddle.static.InputSpec(shape=[1000], dtype='float32'),
            paddle.static.InputSpec(shape=[1000], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9d0fff58868f807bae16be1c5473db29(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d99a1335c90ba60b4aafbcd7298b901a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 82, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cfd61e99ed9054a30920fda1d94f7475(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 120, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_49740e807080b999383343e1de877743(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 160, 160], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bf527dc1fcf1a6fd8a5f4eaf7405ac3e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 72, 122, 122], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0b1de604e126b1fa09ae4b5eb369b696(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 16, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3ff10796688edddd66d59da0c3e0da2e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 228, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[228], dtype='float32'),
            paddle.static.InputSpec(shape=[228], dtype='float32'),
            paddle.static.InputSpec(shape=[228], dtype='float32'),
            paddle.static.InputSpec(shape=[228], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a4da64c4daa8327aeab4500c67f4a858(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 48, 48], dtype='float16'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a90ad7972227a94e7fe7981f2ebabe70(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 160, 160], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8eb5dbf52e9523fb2b2f2fcfe9efaf89(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 10, 10], dtype='float16'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ea5dbc47a611c52732e4b437c6ca230c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[1], dtype='float32'),
            paddle.static.InputSpec(shape=[1], dtype='float32'),
            paddle.static.InputSpec(shape=[1], dtype='float32'),
            paddle.static.InputSpec(shape=[1], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bb18f953b4e05538bdade2c7611d012d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 768, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_14436615a0f1d12a9ec45300b59a5a48(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 32, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_13163815a12d509c4db92414f03c4a8e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f8dc552edc4be255e46b47d76908b345(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 8, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3ba232b3e2066a7ca07d9e9f577e151a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 528, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[528], dtype='float32'),
            paddle.static.InputSpec(shape=[528], dtype='float32'),
            paddle.static.InputSpec(shape=[528], dtype='float32'),
            paddle.static.InputSpec(shape=[528], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4f7322eae16189652b7ed5899e4a5aa0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2064, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[2064], dtype='float32'),
            paddle.static.InputSpec(shape=[2064], dtype='float32'),
            paddle.static.InputSpec(shape=[2064], dtype='float32'),
            paddle.static.InputSpec(shape=[2064], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a175dad4f9465d67843636e9bee397e5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 32, 256, 512], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e8b98632cfbae41426fc220f445e6486(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 40, 40], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_18a785bcdfcfcb78b66199742907e263(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 4, 64, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1f87c021f4db14462734097720e6b742(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 208, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_16fd19de33b48b46334b998f8ea7abd2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 8, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c316a32894e7c28555759959a17c9869(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 128, 128], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7870053f6ad183e47186933ccabb9856(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1728, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1728], dtype='float32'),
            paddle.static.InputSpec(shape=[1728], dtype='float32'),
            paddle.static.InputSpec(shape=[1728], dtype='float32'),
            paddle.static.InputSpec(shape=[1728], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_84ee48c3345df24cca6b87e894987d91(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 19, 19], dtype='float16'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3163f9128c785d514dbf57aba22c95fd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dd1b6274f2c1be924839d5d12c6206af(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2432, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[2432], dtype='float32'),
            paddle.static.InputSpec(shape=[2432], dtype='float32'),
            paddle.static.InputSpec(shape=[2432], dtype='float32'),
            paddle.static.InputSpec(shape=[2432], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_def4008dc88fc8e348ea94877e806f89(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2048, 20, 20], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ecff041ed4f2865598c13f6893c95147(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 320, 320], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5ed73f86ee2dc70f7d877855c00607d3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 4, 64, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ee7c94107106bd22819111f42d41d7dd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 64, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c4bbb9b8e17842bbd03aecaca5e7438b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1344, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1344], dtype='float32'),
            paddle.static.InputSpec(shape=[1344], dtype='float32'),
            paddle.static.InputSpec(shape=[1344], dtype='float32'),
            paddle.static.InputSpec(shape=[1344], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_344329caff1bca5104933b5553a8f640(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 38, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[38], dtype='float32'),
            paddle.static.InputSpec(shape=[38], dtype='float32'),
            paddle.static.InputSpec(shape=[38], dtype='float32'),
            paddle.static.InputSpec(shape=[38], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_127ee73f1be2c7092762a34c121d6b79(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 12, 12], dtype='float16'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f3218f271032194f87b942bbcdc87929(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 320, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_073057315402ed17b5b67313b7b57236(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 4, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2b3dcda2a21fd69aef4c26fca2eaccad(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 104, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
            paddle.static.InputSpec(shape=[104], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2e4bcdc5bafdc514906583519befe93c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 704, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_092cc30aab8f74baa2505d9006fd7209(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 4, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_723b30814165a2832e63237aa12019f6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 320, 320], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4fe064b7d072a933fc3df6cde2593246(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 160, 160], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_70d4159fe200b677a52f2592dd80cc4e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1280, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_068236b963f20ae3772d95db2836bcc6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ce24ad847c2f00a65300d4a5d0606410(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 768], dtype='float16'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c0eb1e58665b6937952e19232a64ffa1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 4, 8, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_552489f69ad39a25d3ae7d7767b1907d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 4, 256, 512], dtype='float16'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ec3b3a2a6bb81c72145d5e0bbcee4810(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1312, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1312], dtype='float32'),
            paddle.static.InputSpec(shape=[1312], dtype='float32'),
            paddle.static.InputSpec(shape=[1312], dtype='float32'),
            paddle.static.InputSpec(shape=[1312], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c3c5069945d6e6e01fc969e96a2de2bb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fb22ed800caf18e500816b81cb5a7da6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 320, 320], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e3d577d4507d8cf60b0d71c3eade4704(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 73, 73], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_67a3ffd7441e4f21b97d12c4ee68c34a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2016, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[2016], dtype='float32'),
            paddle.static.InputSpec(shape=[2016], dtype='float32'),
            paddle.static.InputSpec(shape=[2016], dtype='float32'),
            paddle.static.InputSpec(shape=[2016], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7ebeea026c6e9dc157fb83b21e461b0b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_86effc94cc6dd89c0a0bf56d23b38aab(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 44, 80, 80], dtype='float32'),
            paddle.static.InputSpec(shape=[44], dtype='float32'),
            paddle.static.InputSpec(shape=[44], dtype='float32'),
            paddle.static.InputSpec(shape=[44], dtype='float32'),
            paddle.static.InputSpec(shape=[44], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9ddf65cc5820d79f0cf44cf133b6ab42(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 528, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[528], dtype='float32'),
            paddle.static.InputSpec(shape=[528], dtype='float32'),
            paddle.static.InputSpec(shape=[528], dtype='float32'),
            paddle.static.InputSpec(shape=[528], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f8f872561986ee45e1ad7aed84c9510f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 320, 320], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5cf4bac0db298058a7de96a85d35606e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 168, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[168], dtype='float32'),
            paddle.static.InputSpec(shape=[168], dtype='float32'),
            paddle.static.InputSpec(shape=[168], dtype='float32'),
            paddle.static.InputSpec(shape=[168], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6d9296fab21040021a8756f2db62936e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, 48, 48], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_99613e5121b570118b8f970abbba7f46(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_16a6edfa5b6409b415d09ad05808ddc5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_376eb4e64c16e7dc177259bc5f5ae981(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 672, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_459f781711e3a8ec07a566bed8fc9870(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 38, 38], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_43cbbb0e3509424a4b9cc93860dc6aaf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2048, 10, 10], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_98ee279dc273eba3588a15388601fa2c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1888, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1888], dtype='float32'),
            paddle.static.InputSpec(shape=[1888], dtype='float32'),
            paddle.static.InputSpec(shape=[1888], dtype='float32'),
            paddle.static.InputSpec(shape=[1888], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b1da824bbe8be5568b51258a56eb234e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1056, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1056], dtype='float32'),
            paddle.static.InputSpec(shape=[1056], dtype='float32'),
            paddle.static.InputSpec(shape=[1056], dtype='float32'),
            paddle.static.InputSpec(shape=[1056], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_856ad6cbb55e3d24b41245c4b9560a41(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7f9778ea4534f3fc63e1557084e6046b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1120, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1120], dtype='float32'),
            paddle.static.InputSpec(shape=[1120], dtype='float32'),
            paddle.static.InputSpec(shape=[1120], dtype='float32'),
            paddle.static.InputSpec(shape=[1120], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d1427fd56e8a37dcd84075fe241cdac1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 350, 25], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5b3c8fc32ea41d5175a58db71a369fc3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5c4c45fe10373a4887651be55e63693a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1184, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1184], dtype='float32'),
            paddle.static.InputSpec(shape=[1184], dtype='float32'),
            paddle.static.InputSpec(shape=[1184], dtype='float32'),
            paddle.static.InputSpec(shape=[1184], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1006a06063d0d7187b8ed43c448efc37(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 16, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c70905879475ab161d7af4f4c9614458(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 17, 17], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cb59aa2c490dbef566260bca50a12cd0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 16, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f9d88b6dda9e692e5c8155ab0c881f8c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 32, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_98d6280b617eb479b02b2ca845c2b604(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 300, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[300], dtype='float32'),
            paddle.static.InputSpec(shape=[300], dtype='float32'),
            paddle.static.InputSpec(shape=[300], dtype='float32'),
            paddle.static.InputSpec(shape=[300], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_caa476a83ca48a09742ad8c422eaae6b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 336, 2, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5044464335106b9f394e8b42a34c2418(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f4a3dfadd1b3aaddc906e16f8f00c76d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 228, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[228], dtype='float32'),
            paddle.static.InputSpec(shape=[228], dtype='float32'),
            paddle.static.InputSpec(shape=[228], dtype='float32'),
            paddle.static.InputSpec(shape=[228], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4c58552bb28ad7d4dac823baed3c6698(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1856, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1856], dtype='float32'),
            paddle.static.InputSpec(shape=[1856], dtype='float32'),
            paddle.static.InputSpec(shape=[1856], dtype='float32'),
            paddle.static.InputSpec(shape=[1856], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ffba008e796ae2ad1f7d7fca90d15a0f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_de5634a75203069b0dbf3fbeeff796df(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 128, 128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1976f70b33b0bd4b60b0ccc3db188442(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 832, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b9bb75184127fca2f1c57ca008a66b75(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 38, 38], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c624d63392a8c8c6e4d759243edecf88(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2048, 20, 20], dtype='float16'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
            paddle.static.InputSpec(shape=[2048], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c0cfa8e556c17c27752ea924bef83b51(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 2, 2], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a65cad9da910bfc21515dc93ae466f7c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 8, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b192f3bed431296b739a7e68dd3cf5b3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 8, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2dd377f23ba6d8724508ee0503e7e7c0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1392, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1392], dtype='float32'),
            paddle.static.InputSpec(shape=[1392], dtype='float32'),
            paddle.static.InputSpec(shape=[1392], dtype='float32'),
            paddle.static.InputSpec(shape=[1392], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_458b69cc397b881816c00af070304cd6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1824, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1824], dtype='float32'),
            paddle.static.InputSpec(shape=[1824], dtype='float32'),
            paddle.static.InputSpec(shape=[1824], dtype='float32'),
            paddle.static.InputSpec(shape=[1824], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1893a07ad07eac7754dc602161ea858e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 76, 76], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f636fa30a0565d79092b2b22dd6d384f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a5e52bc3285cb8166af50a2ee45803c4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 81, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[81], dtype='float32'),
            paddle.static.InputSpec(shape=[81], dtype='float32'),
            paddle.static.InputSpec(shape=[81], dtype='float32'),
            paddle.static.InputSpec(shape=[81], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2ade1c8c123212d0b4da7fdef1129c38(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 912, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[912], dtype='float32'),
            paddle.static.InputSpec(shape=[912], dtype='float32'),
            paddle.static.InputSpec(shape=[912], dtype='float32'),
            paddle.static.InputSpec(shape=[912], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f79b3849c0ff7b562d2f8d37623473b9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 8, 25], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_635d8c6eee3d97d4ebe54fb73bcada55(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 147, 147], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d7f6d683fd92f78f719a0cf8725cd79b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 200, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1908cae05be05cf11f8e07dc0f2507d0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 56, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2a5d33304f33681fe1edc766fa8ea08d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 4, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_23134b2b217e82c7d766d0ca62787db4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 150, 150], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0aeca0fb119a33c1a5a9a8bebf74ed03(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 32, None], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b182fcdc516e36875fcde1c5e04c6870(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 4, 3], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3eba81a3bc8b921c66b1715a61894576(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f13d91383492b6e76763077e02aa2a30(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 14, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5144b12e6f28571ed44049cf5b11694a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 120, 31, 31], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c0e228c07c5e1b9d3db71607a0a58d57(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 64, 350, 25], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_140fff394b9bab4c3eb8c208a8bd719b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 840, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[840], dtype='float32'),
            paddle.static.InputSpec(shape=[840], dtype='float32'),
            paddle.static.InputSpec(shape=[840], dtype='float32'),
            paddle.static.InputSpec(shape=[840], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_658067f7e912b60287bb6ef6f7f62f00(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 432, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4eeff78ab9f44a29f6819ea991ca46eb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 128, 350, 25], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5ebf70de1703320a6f0fdaa614413bda(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 288, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d7805fdbd9ebac60aa3ec08d9e6224f9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1280, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3d76b968cb943f794da8117263870d04(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 1, 2], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ba222da548968190858df4cf68bc622e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 160, 160], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6431429537b11f1f7cafb96123d13ff8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1472, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1472], dtype='float32'),
            paddle.static.InputSpec(shape=[1472], dtype='float32'),
            paddle.static.InputSpec(shape=[1472], dtype='float32'),
            paddle.static.InputSpec(shape=[1472], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ee6a11d23b5e90b971cddd7859cb3f18(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 350, 25], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_12e434c0aba4bdedd142af1d54405487(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 6, 6], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4e1519cd585b63180b56b6e0fc574dc2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 336, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_50ffa5edbcfdf476d9ac4dce58622b7b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 80, 80], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6b466436acd8abbe30b6123b8d9f3da3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 128, 128], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3d0a848e20544900d50d701a3eb9b131(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_814f4928eb7842b35504dbba4522c3f2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 128, 128], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7ac957503cb9abd3088c2dcf6c9489dc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e84b9c88555246fdc19a330a2dc95ed7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 72, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_448c0f99cf80e2fd7cb8976990d3f7f2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_067694d2f6a1153403fd29036112d116(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 35, 35], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_467e0b54e8b3ce1fe94e1c2291ee10b1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_43f623d01e33e4dd3ada01998e069b88(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 17, 17], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bb16a312934e2197793f27fc62de7457(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 96, 96], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_75c37502a040ce81a897458d73328af1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1152, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d45ea446e0bee54a250317f48b43b7e5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 262, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[262], dtype='float32'),
            paddle.static.InputSpec(shape=[262], dtype='float32'),
            paddle.static.InputSpec(shape=[262], dtype='float32'),
            paddle.static.InputSpec(shape=[262], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5f987c0d4fff88db50576a915fbddc61(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2064, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[2064], dtype='float32'),
            paddle.static.InputSpec(shape=[2064], dtype='float32'),
            paddle.static.InputSpec(shape=[2064], dtype='float32'),
            paddle.static.InputSpec(shape=[2064], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_825e97677a5e98b9abcd3a54e281a971(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 140, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[140], dtype='float32'),
            paddle.static.InputSpec(shape=[140], dtype='float32'),
            paddle.static.InputSpec(shape=[140], dtype='float32'),
            paddle.static.InputSpec(shape=[140], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0fc98f3322a6295ddc05c1843662f7d0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 1, 26], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bfb83320a839d9c940a6fea84319cafc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 636, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[636], dtype='float32'),
            paddle.static.InputSpec(shape=[636], dtype='float32'),
            paddle.static.InputSpec(shape=[636], dtype='float32'),
            paddle.static.InputSpec(shape=[636], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b09ad1b67d807cc3d7f03b035f0d8c90(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1728, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1728], dtype='float32'),
            paddle.static.InputSpec(shape=[1728], dtype='float32'),
            paddle.static.InputSpec(shape=[1728], dtype='float32'),
            paddle.static.InputSpec(shape=[1728], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8592516a8edd37f0f9798a35009b999b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_84568216994c42dc9a78a7638acfc9f2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 4, 32, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ca3414dfe7bb7b85fd0ac3ef4a8fd707(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 72, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_09c329535e4dfc090393750161f37a90(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1824, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1824], dtype='float32'),
            paddle.static.InputSpec(shape=[1824], dtype='float32'),
            paddle.static.InputSpec(shape=[1824], dtype='float32'),
            paddle.static.InputSpec(shape=[1824], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6e3f86add704cec9080165103b6b20f7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 480, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3a06f1475253de64e3094a1f5e5fc06e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 10, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[10], dtype='float32'),
            paddle.static.InputSpec(shape=[10], dtype='float32'),
            paddle.static.InputSpec(shape=[10], dtype='float32'),
            paddle.static.InputSpec(shape=[10], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_87fdaf18cdc8c15e26667373853ec63d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1728, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1728], dtype='float32'),
            paddle.static.InputSpec(shape=[1728], dtype='float32'),
            paddle.static.InputSpec(shape=[1728], dtype='float32'),
            paddle.static.InputSpec(shape=[1728], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4139c4405ffe0867954eac812c78a849(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 30, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
            paddle.static.InputSpec(shape=[30], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_43c10356140810fa3f3a87f08f368419(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ef0fffaeee2b34a7f8447f0125ee61ab(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6cf4c086dbf6fa4fa6d2360431a57ca3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 432, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_24cb734c54a26d6072a990b24082626f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0b1ae707c8b4ab10db8aec9c3823350f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 320, 8, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7896920255674cc01e20a46599623ade(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dd5a4aad19df3645493d32f6bec951d4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 32, 8, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cc3eda264da511fb1672e23fe0d95a53(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 96, 96], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9d30283f0e3a737119e83c43c06fb560(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d610cbf6adb1ac86386c4dbe83923177(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 624, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[624], dtype='float32'),
            paddle.static.InputSpec(shape=[624], dtype='float32'),
            paddle.static.InputSpec(shape=[624], dtype='float32'),
            paddle.static.InputSpec(shape=[624], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_adbe90a6483c6d7a731fd2e1e9398487(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1488, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1488], dtype='float32'),
            paddle.static.InputSpec(shape=[1488], dtype='float32'),
            paddle.static.InputSpec(shape=[1488], dtype='float32'),
            paddle.static.InputSpec(shape=[1488], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2906dd3f3e0640042e9e677d7525ed36(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1152, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0f48c60a5de0a31df82507672ae23681(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 112, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_31eec24c3fafd54628b30060097f3435(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8bbf3293f35b054878bddb7e19320873(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 448, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[448], dtype='float32'),
            paddle.static.InputSpec(shape=[448], dtype='float32'),
            paddle.static.InputSpec(shape=[448], dtype='float32'),
            paddle.static.InputSpec(shape=[448], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_214efea8c0e6c920f8a4b8780c5b7a4b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_677631133c87b517e069a44dc621d676(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2064, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[2064], dtype='float32'),
            paddle.static.InputSpec(shape=[2064], dtype='float32'),
            paddle.static.InputSpec(shape=[2064], dtype='float32'),
            paddle.static.InputSpec(shape=[2064], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d005e9235308a277e8637c9104600f9a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 704, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_972b3a4697e46b3ca787702f45ce6c14(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 72, 61, 61], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_864143a2abfd0f3d4c024275d30b97e4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1056, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1056], dtype='float32'),
            paddle.static.InputSpec(shape=[1056], dtype='float32'),
            paddle.static.InputSpec(shape=[1056], dtype='float32'),
            paddle.static.InputSpec(shape=[1056], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4192dcd65a629e7e519714b3d3047b86(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 118, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[118], dtype='float32'),
            paddle.static.InputSpec(shape=[118], dtype='float32'),
            paddle.static.InputSpec(shape=[118], dtype='float32'),
            paddle.static.InputSpec(shape=[118], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_367efcfceb41a4eac561294e555d7a26(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 208, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
            paddle.static.InputSpec(shape=[208], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_07277bba8546a096d2aa0bb67f8b406d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 132, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[132], dtype='float32'),
            paddle.static.InputSpec(shape=[132], dtype='float32'),
            paddle.static.InputSpec(shape=[132], dtype='float32'),
            paddle.static.InputSpec(shape=[132], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f7a00d309a2128ebc111858770c69ce9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_aa2f69b8d2bb435db583b9490adcbad5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_263cd5f275bc1c540115cfb0253257d0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_801797befffbea1a2ce19d019107bf8f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 8, 25], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3a21a55bb7927674d19430ed6366df76(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 8, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1e09db09cad87ecfbedcedd6cddcd4d3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 75, 75], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_54e8bf0baf56a0e079ef4d3c0a37f1bf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e0f6ac9b4958357415fc8b72ec5ef4ad(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 1, 3], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_20ebcae51176c55d4c98ad38f10d90a9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1152, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_964f2c00730c0f617a2caab3ff97bb96(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 972, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[972], dtype='float32'),
            paddle.static.InputSpec(shape=[972], dtype='float32'),
            paddle.static.InputSpec(shape=[972], dtype='float32'),
            paddle.static.InputSpec(shape=[972], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5e1344a34fc3a134b70131e7016a7e01(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 75, 75], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fa5e018a7eef8e787a8176b3d388e876(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_84035fb2f3979b4ba0f62b4fa8c68bf3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1344, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1344], dtype='float32'),
            paddle.static.InputSpec(shape=[1344], dtype='float32'),
            paddle.static.InputSpec(shape=[1344], dtype='float32'),
            paddle.static.InputSpec(shape=[1344], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c2219a45e63d1019663f2e68962d68dc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 8, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fb39e8e283e78cd77af02a9e175f5e0f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 64, 256], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2d4a06ca563163a1911037f377fd7b54(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5881edd31c99b5b3ac47d160a72576b5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 448, 8, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[448], dtype='float32'),
            paddle.static.InputSpec(shape=[448], dtype='float32'),
            paddle.static.InputSpec(shape=[448], dtype='float32'),
            paddle.static.InputSpec(shape=[448], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_be4e7ef61b88705310d92d4ae3c098fe(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 640, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fc756a9190ee566e71ff2d981c920b58(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 672, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bafe67d17663c70b06eb0ebcf0421bd9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1968, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1968], dtype='float32'),
            paddle.static.InputSpec(shape=[1968], dtype='float32'),
            paddle.static.InputSpec(shape=[1968], dtype='float32'),
            paddle.static.InputSpec(shape=[1968], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e78aae5a583b14bbbcf1ce11fd4a7365(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4cc0a0f67bde72d35892dd309ed5aa8a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 672, 6, 6], dtype='float16'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8bd61de5b447285a1a98100e41b57a01(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 168, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[168], dtype='float32'),
            paddle.static.InputSpec(shape=[168], dtype='float32'),
            paddle.static.InputSpec(shape=[168], dtype='float32'),
            paddle.static.InputSpec(shape=[168], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_59325918902e52b53ad2792238317f38(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 112, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ea3d781e827ac74579346115b20b8002(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a78c095564d577f66b6b6211bede521c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 16, 128, 256], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_16a62df0a4b2e940b226096560e7c1c5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1296, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1296], dtype='float32'),
            paddle.static.InputSpec(shape=[1296], dtype='float32'),
            paddle.static.InputSpec(shape=[1296], dtype='float32'),
            paddle.static.InputSpec(shape=[1296], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e528e1003f083ad06760d210da39d461(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 8, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_91169ace885e4a24d300106550e4c6a1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 152, 152], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_affec5a07fd82d74e9335cf58367bd07(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 70, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ca645b29629be37054ef146b655740b8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 87, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[87], dtype='float32'),
            paddle.static.InputSpec(shape=[87], dtype='float32'),
            paddle.static.InputSpec(shape=[87], dtype='float32'),
            paddle.static.InputSpec(shape=[87], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d740db72a5878b910e548c0335883331(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 1, 2], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fcc6dde260acc7188025c4c6de7f0c74(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 24, 61, 61], dtype='float16'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
            paddle.static.InputSpec(shape=[24], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8691d18211a75347bf7b588c139f54f1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b8f3f640971c1628767991bd97777cc4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[1], dtype='float32'),
            paddle.static.InputSpec(shape=[1], dtype='float32'),
            paddle.static.InputSpec(shape=[1], dtype='float32'),
            paddle.static.InputSpec(shape=[1], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_294c3e3754c446ce8ef00acc72e1fe52(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, 32, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e54c9b446b4ef105808a049283955adf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 10, 10], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d20b18026bf4113dfbc298bef48cc387(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 40, 40], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dfd191ac4d5b9dbeaa0c4a7641e92bf5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1008, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1008], dtype='float32'),
            paddle.static.InputSpec(shape=[1008], dtype='float32'),
            paddle.static.InputSpec(shape=[1008], dtype='float32'),
            paddle.static.InputSpec(shape=[1008], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8d97cda29ed22e28b86427ac2b826eac(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 544, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[544], dtype='float32'),
            paddle.static.InputSpec(shape=[544], dtype='float32'),
            paddle.static.InputSpec(shape=[544], dtype='float32'),
            paddle.static.InputSpec(shape=[544], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6b4849f1d1bd3486ed197dbbd329b78d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 320, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e688e2824ed9df8542cecce3c58a2d73(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 32, 128], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_621e043c1b9c17d3e7dba8409765afa5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 912, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[912], dtype='float32'),
            paddle.static.InputSpec(shape=[912], dtype='float32'),
            paddle.static.InputSpec(shape=[912], dtype='float32'),
            paddle.static.InputSpec(shape=[912], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_11552d4e360ad473a97ca3997cbdb69b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 320, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_73ad91d23619060c1394e490a731523b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 26], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a9f5f909b34ff4c21dedcc1fa91a151a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 150, 150], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ad543cd8907ef994e0ba07c7d7732da5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 2, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d15a7bc616b152ced00d29a8e4488a03(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7134ccac61be0ea0fd0aefa3dabd9403(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 432, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7de4059ee7757ed73afab8a6a75785f0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1632, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1632], dtype='float32'),
            paddle.static.InputSpec(shape=[1632], dtype='float32'),
            paddle.static.InputSpec(shape=[1632], dtype='float32'),
            paddle.static.InputSpec(shape=[1632], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_87b6cbb642368169a0a6a927e0de77b2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1600, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1600], dtype='float32'),
            paddle.static.InputSpec(shape=[1600], dtype='float32'),
            paddle.static.InputSpec(shape=[1600], dtype='float32'),
            paddle.static.InputSpec(shape=[1600], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d84f6f9a89ad1fa3a5427852edccd2f5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 320, 320], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_458ff5cc93c2853052d9f3baad6cac72(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1664, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1664], dtype='float32'),
            paddle.static.InputSpec(shape=[1664], dtype='float32'),
            paddle.static.InputSpec(shape=[1664], dtype='float32'),
            paddle.static.InputSpec(shape=[1664], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2c8dbe695d486f315b97329be5e529ba(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1632, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1632], dtype='float32'),
            paddle.static.InputSpec(shape=[1632], dtype='float32'),
            paddle.static.InputSpec(shape=[1632], dtype='float32'),
            paddle.static.InputSpec(shape=[1632], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4443cb96dab2b94b7b32f581252245ca(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 8, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5c24ba61461480d4c6d3eb014c3fc86d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 8, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bdad8bf219e6ee27eb339ed3428412a6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, True), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4e20ab64f4cf542c00878428292e57de(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 8, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c198ebb05e54a668041e8d96c745ecc4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1248, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1248], dtype='float32'),
            paddle.static.InputSpec(shape=[1248], dtype='float32'),
            paddle.static.InputSpec(shape=[1248], dtype='float32'),
            paddle.static.InputSpec(shape=[1248], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_445ddf41551001cd9d5b69548dc48794(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 32, 100], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_79a5d13472e01c086f5eba705a3482a5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 80, 80], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ae5b13e0e939198ab229111bdbeea01b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 304, 304], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0b78a4c3f6f276413f0a6ddcb7b8c12f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1376, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1376], dtype='float32'),
            paddle.static.InputSpec(shape=[1376], dtype='float32'),
            paddle.static.InputSpec(shape=[1376], dtype='float32'),
            paddle.static.InputSpec(shape=[1376], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1ad5322a25dffead414186b218aab2b4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1536, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4847d91611badf5406733cc49e5f50e5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a0aff5902af7fa4414fbcb1026214c4c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 2, 4], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f8f826ad0801bba2f1aa15a16f0af589(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ad7841ec75393e1bda8df1d78b1bb062(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 31, 31], dtype='float16'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5fa00bf5118cba4b4368cd3a35cf721a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 2, 4], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d4400d0065e88fba0edb22f813447e7e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_001883c96c6f4644672acdea4d262848(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 32, 100], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a7f7776619c52925ca8dffec3fc3eba0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 128, 128], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bdfecce40297b641425a981e6aa610fb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1792, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1792], dtype='float32'),
            paddle.static.InputSpec(shape=[1792], dtype='float32'),
            paddle.static.InputSpec(shape=[1792], dtype='float32'),
            paddle.static.InputSpec(shape=[1792], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a9d9a15467cfdfd89cbb3d3fc8c28aaa(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1440, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1440], dtype='float32'),
            paddle.static.InputSpec(shape=[1440], dtype='float32'),
            paddle.static.InputSpec(shape=[1440], dtype='float32'),
            paddle.static.InputSpec(shape=[1440], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4ae532ac2d899558c963968af9e9c8cd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 304, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[304], dtype='float32'),
            paddle.static.InputSpec(shape=[304], dtype='float32'),
            paddle.static.InputSpec(shape=[304], dtype='float32'),
            paddle.static.InputSpec(shape=[304], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c2e753fdbe3567442fc1e03f5e7ae07e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1984, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1984], dtype='float32'),
            paddle.static.InputSpec(shape=[1984], dtype='float32'),
            paddle.static.InputSpec(shape=[1984], dtype='float32'),
            paddle.static.InputSpec(shape=[1984], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2efda438036f5a8927b64a011fdf01af(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f3a752b0548f4aad8400ccaaa7d67719(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1632, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1632], dtype='float32'),
            paddle.static.InputSpec(shape=[1632], dtype='float32'),
            paddle.static.InputSpec(shape=[1632], dtype='float32'),
            paddle.static.InputSpec(shape=[1632], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_897e67fbd5d9de8a56c074c35802522f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 570, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[570], dtype='float32'),
            paddle.static.InputSpec(shape=[570], dtype='float32'),
            paddle.static.InputSpec(shape=[570], dtype='float32'),
            paddle.static.InputSpec(shape=[570], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c347cb6185eb6b1a8768caffbd4c20b9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 436, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[436], dtype='float32'),
            paddle.static.InputSpec(shape=[436], dtype='float32'),
            paddle.static.InputSpec(shape=[436], dtype='float32'),
            paddle.static.InputSpec(shape=[436], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d655591997fa9244a08e06784e6c676f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 3, 3], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ba5d7d0fced3c68f4e0a7a428e5b7ba9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 8, 8], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c793602ede73f9274317587bec0f6eb4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 16, 12], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a64af683a2d6deaf7a7b974a51c6ca37(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1536, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_54fcde5cf795d819b395aaf19da10d8f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1872, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1872], dtype='float32'),
            paddle.static.InputSpec(shape=[1872], dtype='float32'),
            paddle.static.InputSpec(shape=[1872], dtype='float32'),
            paddle.static.InputSpec(shape=[1872], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3e2f62cc2e8f268471a9d1db21d2dad5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 32, 64, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_22d7cb24c63d8d8d9eb4cb0b9a2aa78d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 816, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[816], dtype='float32'),
            paddle.static.InputSpec(shape=[816], dtype='float32'),
            paddle.static.InputSpec(shape=[816], dtype='float32'),
            paddle.static.InputSpec(shape=[816], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_08c45d993c57945b8989d36d0b06f306(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1504, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1504], dtype='float32'),
            paddle.static.InputSpec(shape=[1504], dtype='float32'),
            paddle.static.InputSpec(shape=[1504], dtype='float32'),
            paddle.static.InputSpec(shape=[1504], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d03669759404c17eea5fbe0fd1605b35(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1104, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1104], dtype='float32'),
            paddle.static.InputSpec(shape=[1104], dtype='float32'),
            paddle.static.InputSpec(shape=[1104], dtype='float32'),
            paddle.static.InputSpec(shape=[1104], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dbd626ea8f6cd8bc2426556814b8ba10(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 720, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8bb03deac1cb8964dbb9828bfc08fab8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1200, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7185d170951224540264a15c38f820bd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 162, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[162], dtype='float32'),
            paddle.static.InputSpec(shape=[162], dtype='float32'),
            paddle.static.InputSpec(shape=[162], dtype='float32'),
            paddle.static.InputSpec(shape=[162], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_acfdc8595c1523b3ad35de36da9e8119(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cafb0986540bfd1a3adbb5480dabffdf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 70, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_72066883c5c8cf4c909ee2905df02c64(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 8, 25], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_117f8489baa6e501d70eb4bff85cf1d9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 4, 12], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_976b91526a5dc2f3a9e97c4a16e0b14a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 80, 80], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e878490e40f031cbdc6c269e2f5bef62(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 16, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fd5b2180e31ff9f1858fb00966299ec3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 336, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2af563fefd23155cf2ef7a456335678a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8359ec07e6727424210642ef7753057d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 84, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[84], dtype='float32'),
            paddle.static.InputSpec(shape=[84], dtype='float32'),
            paddle.static.InputSpec(shape=[84], dtype='float32'),
            paddle.static.InputSpec(shape=[84], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_379943e81a6a4c6bf2afc9dd9ce81850(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 768, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_74f7178073080c772e3857dd3fff3ea6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, True), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_75ee84f7236948cee9ff459d8bebe011(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1680, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1680], dtype='float32'),
            paddle.static.InputSpec(shape=[1680], dtype='float32'),
            paddle.static.InputSpec(shape=[1680], dtype='float32'),
            paddle.static.InputSpec(shape=[1680], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ce43855413328140ee22b283997d87ec(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 232, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[232], dtype='float32'),
            paddle.static.InputSpec(shape=[232], dtype='float32'),
            paddle.static.InputSpec(shape=[232], dtype='float32'),
            paddle.static.InputSpec(shape=[232], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_66686e954e0e07c03d3f70fb917de5e3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 88, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f0d93ff64af4d85361149e262f90dc83(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, 24], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_32887b43f5b58adac22b860128eae73c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 112, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5f8f7211854d3fd37264b6b4fa71ea8e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 71, 71], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_61a8af54b96b78bb67ab9a5783933e1e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, 24], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a245c093e969413a000f5b4cdefe6057(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 32, 100], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_316a5c1bf1092b7bd8bd6dd93656af77(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 270, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[270], dtype='float32'),
            paddle.static.InputSpec(shape=[270], dtype='float32'),
            paddle.static.InputSpec(shape=[270], dtype='float32'),
            paddle.static.InputSpec(shape=[270], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fef8fe758bca37ef2d38ea6431493539(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 118, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[118], dtype='float32'),
            paddle.static.InputSpec(shape=[118], dtype='float32'),
            paddle.static.InputSpec(shape=[118], dtype='float32'),
            paddle.static.InputSpec(shape=[118], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8de152baa28ab7e9da7e2432f6719840(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 38, 38], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f490811295f67042d6aa6ee33d9fc946(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 64, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2c4b0279f4467672e4b0396db0c2bcdc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 512, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a67d6452b2a2ceec8c222c4c76a8aaf1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1584, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1584], dtype='float32'),
            paddle.static.InputSpec(shape=[1584], dtype='float32'),
            paddle.static.InputSpec(shape=[1584], dtype='float32'),
            paddle.static.InputSpec(shape=[1584], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_eb7c00b666aecb66afa5d87f4384b4de(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 48, 160], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0a6fd276aa8102f6d3d02f53851ec6b3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 28, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
            paddle.static.InputSpec(shape=[28], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_19eafe3a5d3f9df80de4387e50945935(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2112, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[2112], dtype='float32'),
            paddle.static.InputSpec(shape=[2112], dtype='float32'),
            paddle.static.InputSpec(shape=[2112], dtype='float32'),
            paddle.static.InputSpec(shape=[2112], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_963e64b1c5f9da6455d21443ca2e14b1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 72, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_24cad3ff89579e26f4b24bc3de0dc351(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 366, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[366], dtype='float32'),
            paddle.static.InputSpec(shape=[366], dtype='float32'),
            paddle.static.InputSpec(shape=[366], dtype='float32'),
            paddle.static.InputSpec(shape=[366], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5195ebe7af6da40d319d41f61bf2ac2c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 3, 3], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_318c00cd40030242db3f705a93b2891a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1584, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1584], dtype='float32'),
            paddle.static.InputSpec(shape=[1584], dtype='float32'),
            paddle.static.InputSpec(shape=[1584], dtype='float32'),
            paddle.static.InputSpec(shape=[1584], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_df0a64ffe0bd41c2fc472b41150c17d5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 56, 4, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
            paddle.static.InputSpec(shape=[56], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a5510d82817ba9fa3db54559e25e015f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 120, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e799d33cdc34eab378e0e515e5630486(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1408, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1408], dtype='float32'),
            paddle.static.InputSpec(shape=[1408], dtype='float32'),
            paddle.static.InputSpec(shape=[1408], dtype='float32'),
            paddle.static.InputSpec(shape=[1408], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_714efb99a8ff6570f582a86d8c671fbc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 122, 122], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b4aace2371eeae83ccd23970301645e6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 72, 122, 122], dtype='float16'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_29630784f95f784bcc017bebd7e94415(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 608, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[608], dtype='float32'),
            paddle.static.InputSpec(shape=[608], dtype='float32'),
            paddle.static.InputSpec(shape=[608], dtype='float32'),
            paddle.static.InputSpec(shape=[608], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b16fefd9c298c968f9c51950d5c477df(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1248, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1248], dtype='float32'),
            paddle.static.InputSpec(shape=[1248], dtype='float32'),
            paddle.static.InputSpec(shape=[1248], dtype='float32'),
            paddle.static.InputSpec(shape=[1248], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b34827b700b57c2f272776970c550fa0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 256, 64, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_216919a91b32dc576df34acf88a4bbd2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 150, 150], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1fc9939ee612c9963c71c7c09e5c4ef8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 16, 12], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f2e4983afb6846aad56ac221ef11a43b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 16, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_acd83c9a1d85a8d1ea6e14e7c158d54e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 88, 61, 61], dtype='float16'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cf01bb348290461e0b99bb3ee855ed13(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 49], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_163c4646273b5e8bce82af6b7496fc22(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1280], dtype='float16'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3d69c22bbe12a85cc0b2687fa479e150(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 75, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[75], dtype='float32'),
            paddle.static.InputSpec(shape=[75], dtype='float32'),
            paddle.static.InputSpec(shape=[75], dtype='float32'),
            paddle.static.InputSpec(shape=[75], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e0ef5e6f2b6f232144c21a5064e862d1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 320, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c47b8a59da9ba8b283ceaee01f4f4e4f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 7, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f79d05a5e1260dd3828d3c4e9cb9c6e2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 24, 24], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_566aec332f1857bb88d57c37943246f9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1200, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
            paddle.static.InputSpec(shape=[1200], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d3b7ddd8e1f1dc602d6a5a567c3eaf5c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 416, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[416], dtype='float32'),
            paddle.static.InputSpec(shape=[416], dtype='float32'),
            paddle.static.InputSpec(shape=[416], dtype='float32'),
            paddle.static.InputSpec(shape=[416], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c4bfbc0e06310a32dd8ec11ccf8397dd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e69a017b00574952acf834813ed3760c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1600, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1600], dtype='float32'),
            paddle.static.InputSpec(shape=[1600], dtype='float32'),
            paddle.static.InputSpec(shape=[1600], dtype='float32'),
            paddle.static.InputSpec(shape=[1600], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_11b14a1ae41b0a31593836f12c423b4d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 3, 3], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b7d925347799e3b4b1f0dcf60a0ba371(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 288, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a61a6fdaddd8ab64e00f90b107930627(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1536, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3b2dcaad09e2aad7cfc7870b395b86b4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 2, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e298979dcd323edbf2ba8f2f311beba3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 480, 2, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6d6ef9509cacb40b8f843af35ad1060a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ba9b51392051e1265ee4a8d918f6e698(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1056, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1056], dtype='float32'),
            paddle.static.InputSpec(shape=[1056], dtype='float32'),
            paddle.static.InputSpec(shape=[1056], dtype='float32'),
            paddle.static.InputSpec(shape=[1056], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8756cd79d20184e2edd37a3b21596ae0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 61, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[61], dtype='float32'),
            paddle.static.InputSpec(shape=[61], dtype='float32'),
            paddle.static.InputSpec(shape=[61], dtype='float32'),
            paddle.static.InputSpec(shape=[61], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_512d64956c2da18527392987c6edf207(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1718dfd92c51788ed97747a2f0e216bb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_49c30032d4e97e0f384c253072b5a30c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 12, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
            paddle.static.InputSpec(shape=[12], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c1d8828d7848d83703110219cca6a1f7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 8, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_45e50a0de819dadc779430c6014fe33b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 288, 31, 31], dtype='float16'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ccfefb6a7a202d41a9529e931fb5938e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1408, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1408], dtype='float32'),
            paddle.static.InputSpec(shape=[1408], dtype='float32'),
            paddle.static.InputSpec(shape=[1408], dtype='float32'),
            paddle.static.InputSpec(shape=[1408], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cba8ac77abe105a6ebc0c0dc696f46ba(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 320, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8662976d923fc316f42c68b348aac645(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2160, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[2160], dtype='float32'),
            paddle.static.InputSpec(shape=[2160], dtype='float32'),
            paddle.static.InputSpec(shape=[2160], dtype='float32'),
            paddle.static.InputSpec(shape=[2160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_745afe881745275c03bbd782f2897adb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 608, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[608], dtype='float32'),
            paddle.static.InputSpec(shape=[608], dtype='float32'),
            paddle.static.InputSpec(shape=[608], dtype='float32'),
            paddle.static.InputSpec(shape=[608], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d7596fea3bbd1ad27288e9ee160c14c4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1728, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1728], dtype='float32'),
            paddle.static.InputSpec(shape=[1728], dtype='float32'),
            paddle.static.InputSpec(shape=[1728], dtype='float32'),
            paddle.static.InputSpec(shape=[1728], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6e736b36f0f4b87a9f4b5897e00b971d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1280, 6, 6], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_07b63dcebc8a727bf977076bbf0b3bc0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 200, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
            paddle.static.InputSpec(shape=[200], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1d1a1451341812987def2e844afb08fd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 336, 2, 50], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4e126e17b02f24fa0aeed65cc485aa65(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_386d24c874bfe130a74dc32f974e8743(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dbba9c6fdc045c77d11e5ce051888a27(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 32, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a6de8498df15d65b6c1e88fdee42bb3c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 4, 32, 32], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cdeac6b9a7d9d6d44b3055add64d33e3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 304, 304], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e910b95a8213a95415372c2921f6d8cb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1792, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1792], dtype='float32'),
            paddle.static.InputSpec(shape=[1792], dtype='float32'),
            paddle.static.InputSpec(shape=[1792], dtype='float32'),
            paddle.static.InputSpec(shape=[1792], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d0216cd2c002adfdd03e4fb5ed82c99f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 16, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f268700c4da013d6a83d4a923b88f716(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-06'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 128, 64, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_815a31a17cf07eefa6f7f921cee037bd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 256, 175, 25], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_441e62d2bdc69610d368e2d17dd3cadd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1760, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1760], dtype='float32'),
            paddle.static.InputSpec(shape=[1760], dtype='float32'),
            paddle.static.InputSpec(shape=[1760], dtype='float32'),
            paddle.static.InputSpec(shape=[1760], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b2209e036a8d188c0ae50a19c154061e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_01c50f499900b120da6225e338d2aa5d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1920, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1920], dtype='float32'),
            paddle.static.InputSpec(shape=[1920], dtype='float32'),
            paddle.static.InputSpec(shape=[1920], dtype='float32'),
            paddle.static.InputSpec(shape=[1920], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e60b25b6ed143be25e842d03da8550fd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 304, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[304], dtype='float32'),
            paddle.static.InputSpec(shape=[304], dtype='float32'),
            paddle.static.InputSpec(shape=[304], dtype='float32'),
            paddle.static.InputSpec(shape=[304], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_34ebfe9e21ba9c66a500d45fd7f22e41(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_37e481209dfa75f478bb7af2b6546dc0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 608, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[608], dtype='float32'),
            paddle.static.InputSpec(shape=[608], dtype='float32'),
            paddle.static.InputSpec(shape=[608], dtype='float32'),
            paddle.static.InputSpec(shape=[608], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1b08cf49c3e0c08ba7759192916c262b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 906, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[906], dtype='float32'),
            paddle.static.InputSpec(shape=[906], dtype='float32'),
            paddle.static.InputSpec(shape=[906], dtype='float32'),
            paddle.static.InputSpec(shape=[906], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f38ab147038e4d8fcfdba47ca449d86c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 480, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fd9bb3eff78417dce39f1c4c17582da6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 151, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[151], dtype='float32'),
            paddle.static.InputSpec(shape=[151], dtype='float32'),
            paddle.static.InputSpec(shape=[151], dtype='float32'),
            paddle.static.InputSpec(shape=[151], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_455292ca3cef8e477c828481e3a2100d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d558e69ba7079f1bfbd6bffa22b02a32(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 512, 4, 4], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_aa7e0b68c846dc747dd1bd9131491ec0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 80, 73, 73], dtype='float16'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
            paddle.static.InputSpec(shape=[80], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d1eff69a71e3ffa184d13ff92f3df2ab(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 4, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bafe13e1cc66d01a31631a32bf01915c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_78c4f2c3f90cca266e143cbe6eb5de63(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 840, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[840], dtype='float32'),
            paddle.static.InputSpec(shape=[840], dtype='float32'),
            paddle.static.InputSpec(shape=[840], dtype='float32'),
            paddle.static.InputSpec(shape=[840], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f6f835efd8494d188b7d808e1bf6ed16(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 106, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[106], dtype='float32'),
            paddle.static.InputSpec(shape=[106], dtype='float32'),
            paddle.static.InputSpec(shape=[106], dtype='float32'),
            paddle.static.InputSpec(shape=[106], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f9f6ec79ccff23b64aeae5231be51af2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 304, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[304], dtype='float32'),
            paddle.static.InputSpec(shape=[304], dtype='float32'),
            paddle.static.InputSpec(shape=[304], dtype='float32'),
            paddle.static.InputSpec(shape=[304], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_16756718fec4c09e92352396baa2db01(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1920, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1920], dtype='float32'),
            paddle.static.InputSpec(shape=[1920], dtype='float32'),
            paddle.static.InputSpec(shape=[1920], dtype='float32'),
            paddle.static.InputSpec(shape=[1920], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_40bdf43a585f465081df6be985c57b1c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 152, 152], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_92831198afc76b390f9884e9aaa79e85(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1920, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1920], dtype='float32'),
            paddle.static.InputSpec(shape=[1920], dtype='float32'),
            paddle.static.InputSpec(shape=[1920], dtype='float32'),
            paddle.static.InputSpec(shape=[1920], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_26aa9a3de6019caaec46f3a13bf74266(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 256, 256], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9f78f5cb4cf7428f7add9adec1b269a8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ad432a6b8fec317a29146d7760c1bc85(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2112, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[2112], dtype='float32'),
            paddle.static.InputSpec(shape=[2112], dtype='float32'),
            paddle.static.InputSpec(shape=[2112], dtype='float32'),
            paddle.static.InputSpec(shape=[2112], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e2bf442264d3c3537af1dd050f480205(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1920, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1920], dtype='float32'),
            paddle.static.InputSpec(shape=[1920], dtype='float32'),
            paddle.static.InputSpec(shape=[1920], dtype='float32'),
            paddle.static.InputSpec(shape=[1920], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_038d2626f2f1364e422ac477d1de94c8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 128, 350, 25], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2c016dd7817b554333a16cd34b1a9c4d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 88, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3cf26023911c7eda97b16db699ae0ff0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 352, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[352], dtype='float32'),
            paddle.static.InputSpec(shape=[352], dtype='float32'),
            paddle.static.InputSpec(shape=[352], dtype='float32'),
            paddle.static.InputSpec(shape=[352], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cc775f221a7a29440383ed934d6f629d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 336, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
            paddle.static.InputSpec(shape=[336], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8db2c8ea7e3b8c1d864304caad894b96(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 1, 26], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_76d92055e5e8b62b69ae9755a09f3f70(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1296, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1296], dtype='float32'),
            paddle.static.InputSpec(shape=[1296], dtype='float32'),
            paddle.static.InputSpec(shape=[1296], dtype='float32'),
            paddle.static.InputSpec(shape=[1296], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_484b9a7cf89e7e836bef6dfb32fc77f5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_af0f41bdd5b07e59f95fbca1a3f3f7c7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 70, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7e952e52418ffc8a014ade38876a7a85(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 352, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[352], dtype='float32'),
            paddle.static.InputSpec(shape=[352], dtype='float32'),
            paddle.static.InputSpec(shape=[352], dtype='float32'),
            paddle.static.InputSpec(shape=[352], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cfb69fd406c5282be428fef1cb855749(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1680, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1680], dtype='float32'),
            paddle.static.InputSpec(shape=[1680], dtype='float32'),
            paddle.static.InputSpec(shape=[1680], dtype='float32'),
            paddle.static.InputSpec(shape=[1680], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f6e78da54ca3a40268fa01c537e98ffc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f350b4c2e9ee3864e1625b6b59db562b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 64, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3046179a193fe152f8a0c8032c94fb97(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7ff7fed0f17c0351313ecfc46dc48cb3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 48, 160], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_36df1ca41ea7c15a1af97a21087edcdd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 4, 128, 128], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2bb8c3500ffce4b5f421ae1a79c4e03f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 138, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[138], dtype='float32'),
            paddle.static.InputSpec(shape=[138], dtype='float32'),
            paddle.static.InputSpec(shape=[138], dtype='float32'),
            paddle.static.InputSpec(shape=[138], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_df574d33db59afb378e0feda4f1d4f51(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 576, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_41bee5f0c71d054ae209ee5ad94286ec(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 106, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[106], dtype='float32'),
            paddle.static.InputSpec(shape=[106], dtype='float32'),
            paddle.static.InputSpec(shape=[106], dtype='float32'),
            paddle.static.InputSpec(shape=[106], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a512991055d5115589257aad5b14b81d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 2, 2], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0acfc461b9347a995634a111ab651e07(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2560, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[2560], dtype='float32'),
            paddle.static.InputSpec(shape=[2560], dtype='float32'),
            paddle.static.InputSpec(shape=[2560], dtype='float32'),
            paddle.static.InputSpec(shape=[2560], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d47d6ea8c05fac573da3b91bfaffb8ca(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 704, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_82424129111531e256cacea5e01fc53a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 300, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[300], dtype='float32'),
            paddle.static.InputSpec(shape=[300], dtype='float32'),
            paddle.static.InputSpec(shape=[300], dtype='float32'),
            paddle.static.InputSpec(shape=[300], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f235e37fe90c6c3445d82bf4fa701baa(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 256, 64, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_624cb3d959ca392f1eb717c362b9df66(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1696, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1696], dtype='float32'),
            paddle.static.InputSpec(shape=[1696], dtype='float32'),
            paddle.static.InputSpec(shape=[1696], dtype='float32'),
            paddle.static.InputSpec(shape=[1696], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_463237fde79324bae936444e62ab79a1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 70, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_db7396cc0f24295e5ab6004b01dde58a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 128, 128], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_81171a73171edbafffb791570671a2bc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 4, 4], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e4be4db5ae01ae41df990d1e11794aa8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 19, 19], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cd851a3b357e92bf5843a71272052852(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1488, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1488], dtype='float32'),
            paddle.static.InputSpec(shape=[1488], dtype='float32'),
            paddle.static.InputSpec(shape=[1488], dtype='float32'),
            paddle.static.InputSpec(shape=[1488], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f53ebda9bb5560de5f9349dfa1388398(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 17, 17], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_eb3355eb5b3a1d649b681e628ff1b3bc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 112, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ca1c2938c7fadc96b0dc8dfa9209c1c9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 82, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
            paddle.static.InputSpec(shape=[82], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fc855164fbc439186f3dc281de66f87c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.97'), float('0.001'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 160, 160], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_df6f636f40813b72b55d61f3fe6a52b4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1776, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1776], dtype='float32'),
            paddle.static.InputSpec(shape=[1776], dtype='float32'),
            paddle.static.InputSpec(shape=[1776], dtype='float32'),
            paddle.static.InputSpec(shape=[1776], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c181f9728f90d171e0887bcbb8afe44d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 720, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_08670a12885360769ac09c15fed4cd71(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 640, 8, 8], dtype='float16'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_57a31e312c2e0e2f69d658b889ce8c6e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 61, 61], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e039cbf6ffd8b4b9597ef7e012ea5b2c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 480, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_08a77721120b46e1693801de3678d379(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 117, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[117], dtype='float32'),
            paddle.static.InputSpec(shape=[117], dtype='float32'),
            paddle.static.InputSpec(shape=[117], dtype='float32'),
            paddle.static.InputSpec(shape=[117], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7bac455786c9c12769ec77051cc472e1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f1e8941418a2f8bb9b0465e9d839d2ef(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 366, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[366], dtype='float32'),
            paddle.static.InputSpec(shape=[366], dtype='float32'),
            paddle.static.InputSpec(shape=[366], dtype='float32'),
            paddle.static.InputSpec(shape=[366], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0d4ca7005d2aaa8cc3db49df521f1e39(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1968, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1968], dtype='float32'),
            paddle.static.InputSpec(shape=[1968], dtype='float32'),
            paddle.static.InputSpec(shape=[1968], dtype='float32'),
            paddle.static.InputSpec(shape=[1968], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_20e3e436c0a2a41e09dca057c88978d3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 320, 6, 6], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c7b2e018e3bc863a28446ac27d7cdf15(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 416, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[416], dtype='float32'),
            paddle.static.InputSpec(shape=[416], dtype='float32'),
            paddle.static.InputSpec(shape=[416], dtype='float32'),
            paddle.static.InputSpec(shape=[416], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_049f5fe343bd1b155b9420ba95b6c506(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 40, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
            paddle.static.InputSpec(shape=[40], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a235d6f6c02c5b2da51e817edea4be49(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 624, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[624], dtype='float32'),
            paddle.static.InputSpec(shape=[624], dtype='float32'),
            paddle.static.InputSpec(shape=[624], dtype='float32'),
            paddle.static.InputSpec(shape=[624], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b7c85a042e09774cd876c6c793c9f2f9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1392, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1392], dtype='float32'),
            paddle.static.InputSpec(shape=[1392], dtype='float32'),
            paddle.static.InputSpec(shape=[1392], dtype='float32'),
            paddle.static.InputSpec(shape=[1392], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_19f30ddf3f63d31426e6ec22a02a60f8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1584, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1584], dtype='float32'),
            paddle.static.InputSpec(shape=[1584], dtype='float32'),
            paddle.static.InputSpec(shape=[1584], dtype='float32'),
            paddle.static.InputSpec(shape=[1584], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cf58e051a021fc0201a846c5b7961f9a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1248, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1248], dtype='float32'),
            paddle.static.InputSpec(shape=[1248], dtype='float32'),
            paddle.static.InputSpec(shape=[1248], dtype='float32'),
            paddle.static.InputSpec(shape=[1248], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_99b9674fb106caee579db82db9d6fed3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 150, 150], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ff3939c6254965c12dafccc71f346fe4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1b6c71d21f96d7faf2a2db0f5a70d973(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_629b5c94a3254f00139132c6cfefec6c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b606cf513c8f64479a9b2e2cbb83813c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 160, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
            paddle.static.InputSpec(shape=[160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dcbac1292344de20ab0353af24353e0d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 6, 6], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1242730d4df1050dfbe00528d15112b9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e73dbcb3579ad48eae5a945696942094(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 112, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_28fc9786c8eb51637dc327186b33a0c2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 61, 61], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3307b0fc511040ca02006ee72fb33cd8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 720, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_16ee940437a12012167b722195d2b167(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 740, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[740], dtype='float32'),
            paddle.static.InputSpec(shape=[740], dtype='float32'),
            paddle.static.InputSpec(shape=[740], dtype='float32'),
            paddle.static.InputSpec(shape=[740], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5cb185b3b482fcf1e63a6baf58c67717(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1872, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1872], dtype='float32'),
            paddle.static.InputSpec(shape=[1872], dtype='float32'),
            paddle.static.InputSpec(shape=[1872], dtype='float32'),
            paddle.static.InputSpec(shape=[1872], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a688e009b68e15cd63b76c86ddc74bb7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 19, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[19], dtype='float32'),
            paddle.static.InputSpec(shape=[19], dtype='float32'),
            paddle.static.InputSpec(shape=[19], dtype='float32'),
            paddle.static.InputSpec(shape=[19], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_877a4855048705141aeb14c3e0cbd641(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_462e07d96819e4c7122493866f197baa(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1536, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8264cd0ed09d33c4efb6b54b04181ed2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 58, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[58], dtype='float32'),
            paddle.static.InputSpec(shape=[58], dtype='float32'),
            paddle.static.InputSpec(shape=[58], dtype='float32'),
            paddle.static.InputSpec(shape=[58], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3066684247f0145c25f963b02c805430(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 40, 40], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_eb54a6198d0d098b5df559ad16cf3d3e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1872, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1872], dtype='float32'),
            paddle.static.InputSpec(shape=[1872], dtype='float32'),
            paddle.static.InputSpec(shape=[1872], dtype='float32'),
            paddle.static.InputSpec(shape=[1872], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_827369307d64dcfbca3d462bf795cce0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1872, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1872], dtype='float32'),
            paddle.static.InputSpec(shape=[1872], dtype='float32'),
            paddle.static.InputSpec(shape=[1872], dtype='float32'),
            paddle.static.InputSpec(shape=[1872], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cb4fd531d3a3e93a949a1ba0d55bd5d1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4cefbe2f0fcd0a3c7d12d04da6294ddd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 25, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[25], dtype='float32'),
            paddle.static.InputSpec(shape=[25], dtype='float32'),
            paddle.static.InputSpec(shape=[25], dtype='float32'),
            paddle.static.InputSpec(shape=[25], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_195a3ff1f6a14a915f756aa330dc479e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 740, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[740], dtype='float32'),
            paddle.static.InputSpec(shape=[740], dtype='float32'),
            paddle.static.InputSpec(shape=[740], dtype='float32'),
            paddle.static.InputSpec(shape=[740], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_69ec786644c8aff47383c886f382f69b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 1, 2], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0c2f69248d988e152e721152baaefc6f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 320, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_92329144f48a3e23c44fb1c13743ae94(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 228, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[228], dtype='float32'),
            paddle.static.InputSpec(shape=[228], dtype='float32'),
            paddle.static.InputSpec(shape=[228], dtype='float32'),
            paddle.static.InputSpec(shape=[228], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bf10236c687704c2c9a551d17669e0f6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 160, 160], dtype='float16'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2fc9a891a92f336694b4fffc9b756281(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 736, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[736], dtype='float32'),
            paddle.static.InputSpec(shape=[736], dtype='float32'),
            paddle.static.InputSpec(shape=[736], dtype='float32'),
            paddle.static.InputSpec(shape=[736], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9a4bbba9f675abeabe36ab9163defc76(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2, 112, 224], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_11b605be9747462b91e5f789e3a7c2ab(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 624, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[624], dtype='float32'),
            paddle.static.InputSpec(shape=[624], dtype='float32'),
            paddle.static.InputSpec(shape=[624], dtype='float32'),
            paddle.static.InputSpec(shape=[624], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_019561d773e1092e6a64ee236ce8da12(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 304, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[304], dtype='float32'),
            paddle.static.InputSpec(shape=[304], dtype='float32'),
            paddle.static.InputSpec(shape=[304], dtype='float32'),
            paddle.static.InputSpec(shape=[304], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_32eaba6084a13cca4928f97bea910403(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, 24, 24], dtype='float16'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0627566346fd8d16b36302f5776ac6dc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_66848f63380fe3caf878efaf5f970000(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, 56, 112], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_dc07229b50b6f866a2bf90a74fd7609d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 27, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[27], dtype='float32'),
            paddle.static.InputSpec(shape=[27], dtype='float32'),
            paddle.static.InputSpec(shape=[27], dtype='float32'),
            paddle.static.InputSpec(shape=[27], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d733ad76e6def93a6431f82f0276bd67(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 138, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[138], dtype='float32'),
            paddle.static.InputSpec(shape=[138], dtype='float32'),
            paddle.static.InputSpec(shape=[138], dtype='float32'),
            paddle.static.InputSpec(shape=[138], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9a851f4c322cbd327bf4b5a9c7d23882(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 16, 128, 256], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9d25370aca2ae679810e076223d2c19d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 4, 512, 1024], dtype='float16'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a39b08a6d0b8c8bb33c0aae736ac2e0f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1280, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_43c0695f75d5141303c5c0768d82a893(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 4, 12], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1cf9fed88898a52836a6b9e7175f3073(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 48, 48], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_25dec4d33c0318100e2218ef5aa8a01b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 1, 2], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cac9f33a1d94791f207118c5166b9344(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1696, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1696], dtype='float32'),
            paddle.static.InputSpec(shape=[1696], dtype='float32'),
            paddle.static.InputSpec(shape=[1696], dtype='float32'),
            paddle.static.InputSpec(shape=[1696], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d115a16c90d0f6858881e4f05a9f4700(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 112, None, None], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d39e7200e9bf7d28cc6a48a96099dc48(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 832, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
            paddle.static.InputSpec(shape=[832], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cd2bb7faa7685a1a88ad514922fbdfbf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 176, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[176], dtype='float32'),
            paddle.static.InputSpec(shape=[176], dtype='float32'),
            paddle.static.InputSpec(shape=[176], dtype='float32'),
            paddle.static.InputSpec(shape=[176], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_35aac71abde1014925ad154485383613(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 640, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
            paddle.static.InputSpec(shape=[640], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4e0c57e49b7bb71ad7507c34d30b87c2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 236, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[236], dtype='float32'),
            paddle.static.InputSpec(shape=[236], dtype='float32'),
            paddle.static.InputSpec(shape=[236], dtype='float32'),
            paddle.static.InputSpec(shape=[236], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_899d17da3b944425f83c54ff28b71924(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 906, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[906], dtype='float32'),
            paddle.static.InputSpec(shape=[906], dtype='float32'),
            paddle.static.InputSpec(shape=[906], dtype='float32'),
            paddle.static.InputSpec(shape=[906], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e1c59463e9917d9924e85605f4931ef3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1216, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1216], dtype='float32'),
            paddle.static.InputSpec(shape=[1216], dtype='float32'),
            paddle.static.InputSpec(shape=[1216], dtype='float32'),
            paddle.static.InputSpec(shape=[1216], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e751baaa77fd63c21bb116e6d6b3bb51(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 70, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
            paddle.static.InputSpec(shape=[70], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_423b983ffdfc7e65b383ccfeafd45b4c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 768, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8e8f0133986ffb4135513dfd7b141efb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 44, 40, 40], dtype='float32'),
            paddle.static.InputSpec(shape=[44], dtype='float32'),
            paddle.static.InputSpec(shape=[44], dtype='float32'),
            paddle.static.InputSpec(shape=[44], dtype='float32'),
            paddle.static.InputSpec(shape=[44], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_13c08fa84333383f04cb6725995513f2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_39d2d19b7d303851549f0d5fdbc4f59a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1680, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1680], dtype='float32'),
            paddle.static.InputSpec(shape=[1680], dtype='float32'),
            paddle.static.InputSpec(shape=[1680], dtype='float32'),
            paddle.static.InputSpec(shape=[1680], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f01a7f175598123e3bf7903757624d6d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 4, 512, 1024], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_11304b7dd07aac514dcf1769ed48cdf2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 448, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[448], dtype='float32'),
            paddle.static.InputSpec(shape=[448], dtype='float32'),
            paddle.static.InputSpec(shape=[448], dtype='float32'),
            paddle.static.InputSpec(shape=[448], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_25f103fa3896da099d3cde17bf1dc78e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 224, 224], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ce70511b552491bd0868bf96927138c2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 72, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_579400ce34160557edeed1dea40b82a1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1472, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1472], dtype='float32'),
            paddle.static.InputSpec(shape=[1472], dtype='float32'),
            paddle.static.InputSpec(shape=[1472], dtype='float32'),
            paddle.static.InputSpec(shape=[1472], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_92b620aef5acafad4c0d157ef777b660(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 8, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_aed7e060052c4499918ab45fb112be96(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2160, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[2160], dtype='float32'),
            paddle.static.InputSpec(shape=[2160], dtype='float32'),
            paddle.static.InputSpec(shape=[2160], dtype='float32'),
            paddle.static.InputSpec(shape=[2160], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9ef2f6d566b5486e9623c562160de48a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1296, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1296], dtype='float32'),
            paddle.static.InputSpec(shape=[1296], dtype='float32'),
            paddle.static.InputSpec(shape=[1296], dtype='float32'),
            paddle.static.InputSpec(shape=[1296], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e418ce2e68184e8ffa573e948b40ac86(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 864, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[864], dtype='float32'),
            paddle.static.InputSpec(shape=[864], dtype='float32'),
            paddle.static.InputSpec(shape=[864], dtype='float32'),
            paddle.static.InputSpec(shape=[864], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d4c51ec0a78417bef59b68da57c0db62(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7e483b644adfab6a13faad290fd32bc8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1392, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1392], dtype='float32'),
            paddle.static.InputSpec(shape=[1392], dtype='float32'),
            paddle.static.InputSpec(shape=[1392], dtype='float32'),
            paddle.static.InputSpec(shape=[1392], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2df9b1c06f9256f8108b4a1143df35e1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 2, 6], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e68404a4033630df4d02cbe3c9aa4a92(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 480, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
            paddle.static.InputSpec(shape=[480], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_debd32127a4e41e44c2010993e6a022e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 4, 4], dtype='float16'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_26c1b854eb872ba56ebedf9793de3768(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 120, 31, 31], dtype='float16'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_02d589f4fd9bc2be1304315eb1756767(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2, 112, 224], dtype='float16'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5d9f2c3bc38fb6027110b24fa6671d9f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 175, 25], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3c866ab54cebbaed9f5d6da4974354d8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 42, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[42], dtype='float32'),
            paddle.static.InputSpec(shape=[42], dtype='float32'),
            paddle.static.InputSpec(shape=[42], dtype='float32'),
            paddle.static.InputSpec(shape=[42], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7ca719eb7eda5d71abaf08e1fe892374(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1044, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1044], dtype='float32'),
            paddle.static.InputSpec(shape=[1044], dtype='float32'),
            paddle.static.InputSpec(shape=[1044], dtype='float32'),
            paddle.static.InputSpec(shape=[1044], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_40567c7d44448ac41e165254a7ef9497(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 96, 96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c5344cdfe72e997e5159f4cf3d17f3fa(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1344, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1344], dtype='float32'),
            paddle.static.InputSpec(shape=[1344], dtype='float32'),
            paddle.static.InputSpec(shape=[1344], dtype='float32'),
            paddle.static.InputSpec(shape=[1344], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_011b6c68b397738302a4af50e26ee9a9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9e0e872a60af4fbfabce465b69e0efeb(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 61, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[61], dtype='float32'),
            paddle.static.InputSpec(shape=[61], dtype='float32'),
            paddle.static.InputSpec(shape=[61], dtype='float32'),
            paddle.static.InputSpec(shape=[61], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9a1a7d5ba2e42c33e716ce604cfade5b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 96, 96], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6872978f377bda77c1b49c5e0bf899f5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_966af676a02e7234598162cc353f5a0c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 120, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
            paddle.static.InputSpec(shape=[120], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2f0e1860449b1b7ebd00d0c0044deec2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 320, 6, 6], dtype='float16'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
            paddle.static.InputSpec(shape=[320], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e12387775509b6aa385a0c26a8955671(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1f323dba5e9c868990c2fd1d31f924e8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 32, 256, 512], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_541c5e8fce76898fc2f665d66f8d8d58(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2016, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[2016], dtype='float32'),
            paddle.static.InputSpec(shape=[2016], dtype='float32'),
            paddle.static.InputSpec(shape=[2016], dtype='float32'),
            paddle.static.InputSpec(shape=[2016], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_fff5f20a5fd67f2fd593178094d7ab30(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 2, 27], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d7b7f624e1be1fb0d2bd47d575068cac(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 4, 50], dtype='float16'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c6baed37cb404a4869b3e13de7f6e733(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 736, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[736], dtype='float32'),
            paddle.static.InputSpec(shape=[736], dtype='float32'),
            paddle.static.InputSpec(shape=[736], dtype='float32'),
            paddle.static.InputSpec(shape=[736], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8c241c5d842a81b164e8cd441263071c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1248, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1248], dtype='float32'),
            paddle.static.InputSpec(shape=[1248], dtype='float32'),
            paddle.static.InputSpec(shape=[1248], dtype='float32'),
            paddle.static.InputSpec(shape=[1248], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_152dac02533dcdd72854cac3edf3c57d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 12, 12], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6f5027e141674adef965d45913bf3e82(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 96, 96], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_07eeb3f0cd7e65e6c274d576a8cf26a8(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2016, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[2016], dtype='float32'),
            paddle.static.InputSpec(shape=[2016], dtype='float32'),
            paddle.static.InputSpec(shape=[2016], dtype='float32'),
            paddle.static.InputSpec(shape=[2016], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_47ca2c4ca14cb788357420b8389cdcbd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 25, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[25], dtype='float32'),
            paddle.static.InputSpec(shape=[25], dtype='float32'),
            paddle.static.InputSpec(shape=[25], dtype='float32'),
            paddle.static.InputSpec(shape=[25], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_011bb84273957ef1e8fcf8977296eb19(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4410ed18d17cf56c7edb48d8d9487131(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 512, 4, 4], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_85ae8fa92315e18b68b8dc16979bcd5d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
            paddle.static.InputSpec(shape=[2], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8176ec319d7b31f697db792d91833999(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 352, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[352], dtype='float32'),
            paddle.static.InputSpec(shape=[352], dtype='float32'),
            paddle.static.InputSpec(shape=[352], dtype='float32'),
            paddle.static.InputSpec(shape=[352], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6da9a98efad0ff7ae24f0aae4d7d96b4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1392, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1392], dtype='float32'),
            paddle.static.InputSpec(shape=[1392], dtype='float32'),
            paddle.static.InputSpec(shape=[1392], dtype='float32'),
            paddle.static.InputSpec(shape=[1392], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6fdd1bc8e0e47d357c9681e3afc3aa73(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2016, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[2016], dtype='float32'),
            paddle.static.InputSpec(shape=[2016], dtype='float32'),
            paddle.static.InputSpec(shape=[2016], dtype='float32'),
            paddle.static.InputSpec(shape=[2016], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e0186a6004e5dbd78c27b1d461e918a9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 87, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[87], dtype='float32'),
            paddle.static.InputSpec(shape=[87], dtype='float32'),
            paddle.static.InputSpec(shape=[87], dtype='float32'),
            paddle.static.InputSpec(shape=[87], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1e9860a676272a81feaf9cf26dc4e4db(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 112, None, None], dtype='float16'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_534bd8605a1bc9db4c21abd12de320b4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 4, 12], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c12e0a77c8c7a291cb84244dc2e185b1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1776, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1776], dtype='float32'),
            paddle.static.InputSpec(shape=[1776], dtype='float32'),
            paddle.static.InputSpec(shape=[1776], dtype='float32'),
            paddle.static.InputSpec(shape=[1776], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f0310f299a03f53c6ac5e812c44939f7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1440, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1440], dtype='float32'),
            paddle.static.InputSpec(shape=[1440], dtype='float32'),
            paddle.static.InputSpec(shape=[1440], dtype='float32'),
            paddle.static.InputSpec(shape=[1440], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6aa048fb1689e5cd251f916ed4cda099(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 17, 17], dtype='float16'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_14b082ceadee49af49a550bae9fc8c51(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 528, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[528], dtype='float32'),
            paddle.static.InputSpec(shape=[528], dtype='float32'),
            paddle.static.InputSpec(shape=[528], dtype='float32'),
            paddle.static.InputSpec(shape=[528], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1f6596b2204ca91d19f0a77cd570c1cf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1296, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1296], dtype='float32'),
            paddle.static.InputSpec(shape=[1296], dtype='float32'),
            paddle.static.InputSpec(shape=[1296], dtype='float32'),
            paddle.static.InputSpec(shape=[1296], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2f6636f0103abee5641ac26ee4dd8bcc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 544, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[544], dtype='float32'),
            paddle.static.InputSpec(shape=[544], dtype='float32'),
            paddle.static.InputSpec(shape=[544], dtype='float32'),
            paddle.static.InputSpec(shape=[544], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b5f3fe04440aef0974d3c167829a2095(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 816, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[816], dtype='float32'),
            paddle.static.InputSpec(shape=[816], dtype='float32'),
            paddle.static.InputSpec(shape=[816], dtype='float32'),
            paddle.static.InputSpec(shape=[816], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2e9afbd276ea8658a969c5b6afad74cc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 672, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_2dee71c3d502d6667acafa1bf8810326(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1488, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1488], dtype='float32'),
            paddle.static.InputSpec(shape=[1488], dtype='float32'),
            paddle.static.InputSpec(shape=[1488], dtype='float32'),
            paddle.static.InputSpec(shape=[1488], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0552be49233cc4ef96bce93ad608f02d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 47, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[47], dtype='float32'),
            paddle.static.InputSpec(shape=[47], dtype='float32'),
            paddle.static.InputSpec(shape=[47], dtype='float32'),
            paddle.static.InputSpec(shape=[47], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_1f76191763e5b0e9c94d9a20705e3d44(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 570, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[570], dtype='float32'),
            paddle.static.InputSpec(shape=[570], dtype='float32'),
            paddle.static.InputSpec(shape=[570], dtype='float32'),
            paddle.static.InputSpec(shape=[570], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_742a892484ff8f554a435063c08680e4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 7, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_056c5654ab2a0e7ab0fd6aaeeb9464ae(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 185, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[185], dtype='float32'),
            paddle.static.InputSpec(shape=[185], dtype='float32'),
            paddle.static.InputSpec(shape=[185], dtype='float32'),
            paddle.static.InputSpec(shape=[185], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a7f9e1287adef234b360f8c5ce7bf39b(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 608, 608], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_aff0a1d31bbbdf620cbc6dfa93b5124e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_867f6b2e4842a55cae6550a09fcf98b1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 232, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[232], dtype='float32'),
            paddle.static.InputSpec(shape=[232], dtype='float32'),
            paddle.static.InputSpec(shape=[232], dtype='float32'),
            paddle.static.InputSpec(shape=[232], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8ff360b784807127d8dbb229d9af68fe(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1280, 6, 6], dtype='float16'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_67484094503db28bd97a3dc60c1ca3e2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 608, 608], dtype='float16'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_ef279ca824491cb465daea4379cfef4f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 112, 56, 56], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
            paddle.static.InputSpec(shape=[112], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f93a4f741072d1bd0bddce02bc3e8f18(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 2688, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[2688], dtype='float32'),
            paddle.static.InputSpec(shape=[2688], dtype='float32'),
            paddle.static.InputSpec(shape=[2688], dtype='float32'),
            paddle.static.InputSpec(shape=[2688], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_4d152c44fc2b0626740a6b54eee6520a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 234, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[234], dtype='float32'),
            paddle.static.InputSpec(shape=[234], dtype='float32'),
            paddle.static.InputSpec(shape=[234], dtype='float32'),
            paddle.static.InputSpec(shape=[234], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8a424fee43ea8eaf70c4010d7010b5e9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 672, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_636654b5ec89cb97d90c57cd6bdfefee(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1280, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b163aa986bc09bfc647f09b83b07825e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 608, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[608], dtype='float32'),
            paddle.static.InputSpec(shape=[608], dtype='float32'),
            paddle.static.InputSpec(shape=[608], dtype='float32'),
            paddle.static.InputSpec(shape=[608], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_25ecfef120be3bf167579a0043641a4e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1536, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
            paddle.static.InputSpec(shape=[1536], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_aaee242e34764c026caea234b9e37583(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 48, 160], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_16c5998ca4ababc65fb3747fe0c754f5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f159a255d63dc15c3c3578e376ed7fba(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 36, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_26858e4bcc071dbb21f5b46f3b0410fd(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, 24, 24], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_be55e6b7bc3740624a173e767643bfa0(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 17, 17], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3cc17f3071d80ab3eb49c5addce341bf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 50, 350], dtype='float16'),
            paddle.static.InputSpec(shape=[50], dtype='float32'),
            paddle.static.InputSpec(shape=[50], dtype='float32'),
            paddle.static.InputSpec(shape=[50], dtype='float32'),
            paddle.static.InputSpec(shape=[50], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8272a9122063f5942bfd66068ee30970(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 704, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b4c0fe2fca612284865477be8e7df52a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 4, 12], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d817aba74df7b35ce1e36288368ef6b4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1440, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1440], dtype='float32'),
            paddle.static.InputSpec(shape=[1440], dtype='float32'),
            paddle.static.InputSpec(shape=[1440], dtype='float32'),
            paddle.static.InputSpec(shape=[1440], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bdd537d775a796d7f54474406ed58350(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1008, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1008], dtype='float32'),
            paddle.static.InputSpec(shape=[1008], dtype='float32'),
            paddle.static.InputSpec(shape=[1008], dtype='float32'),
            paddle.static.InputSpec(shape=[1008], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c7e92a9b76af9d3c0da8e14097506bd7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1152, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
            paddle.static.InputSpec(shape=[1152], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_33f3e60285f76e0d85b8df1938b63567(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 175, 25], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_7900aa0afd93247c2471f0ee5a44e050(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1104, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1104], dtype='float32'),
            paddle.static.InputSpec(shape=[1104], dtype='float32'),
            paddle.static.InputSpec(shape=[1104], dtype='float32'),
            paddle.static.InputSpec(shape=[1104], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bc61aa3e884868c6d7ea2b51aefc111f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 47, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[47], dtype='float32'),
            paddle.static.InputSpec(shape=[47], dtype='float32'),
            paddle.static.InputSpec(shape=[47], dtype='float32'),
            paddle.static.InputSpec(shape=[47], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_98f6af822451e6d8c0c9ff8cce770453(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 144, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
            paddle.static.InputSpec(shape=[144], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_475036204effe55e8c35258b4f43447a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1968, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[1968], dtype='float32'),
            paddle.static.InputSpec(shape=[1968], dtype='float32'),
            paddle.static.InputSpec(shape=[1968], dtype='float32'),
            paddle.static.InputSpec(shape=[1968], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0866e64952cf5cd57c2696e7aa5e52bf(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 19, 19], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5e737435984eade283e467402a97dbae(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 384, 32, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
            paddle.static.InputSpec(shape=[384], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_361174189e20d55ed399f9d9a385b3e5(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 117, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[117], dtype='float32'),
            paddle.static.InputSpec(shape=[117], dtype='float32'),
            paddle.static.InputSpec(shape=[117], dtype='float32'),
            paddle.static.InputSpec(shape=[117], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a92fd16fd9e3b064b244d6b28a1a5996(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 1, 26], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c1bd354b6b2279703032a8552648a54f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 4, 16, 16], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_75e478ce1c9870e0491a349dfe73dce1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 192, 49], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
            paddle.static.InputSpec(shape=[192], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_43a85f8db69f9254d3acc022ca1f59a2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 8, 32, 128, 128], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
            paddle.static.InputSpec(shape=[8], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a5af6cbfbe32b6178579798aa12e5117(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 768, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f966f37784637dd7f934a44ee272a76d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1776, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1776], dtype='float32'),
            paddle.static.InputSpec(shape=[1776], dtype='float32'),
            paddle.static.InputSpec(shape=[1776], dtype='float32'),
            paddle.static.InputSpec(shape=[1776], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5235f72a7f34d2f45209d273b1ca4ce3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1968, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1968], dtype='float32'),
            paddle.static.InputSpec(shape=[1968], dtype='float32'),
            paddle.static.InputSpec(shape=[1968], dtype='float32'),
            paddle.static.InputSpec(shape=[1968], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_09d7974ad6e02321106838df9e145c82(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_cb83104a20b06dc4b6ef79a0d2e04cc1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1504, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1504], dtype='float32'),
            paddle.static.InputSpec(shape=[1504], dtype='float32'),
            paddle.static.InputSpec(shape=[1504], dtype='float32'),
            paddle.static.InputSpec(shape=[1504], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d49a26835e4478a1258aea1911ea27d4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 72, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
            paddle.static.InputSpec(shape=[72], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3a77ec7dc329722caafa62532aed3870(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 304, 304], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_5b8fa27111695d91eadafafb1a520f56(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 256, 4, 12], dtype='float16'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
            paddle.static.InputSpec(shape=[256], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_842b58828ce8b0b3fea279b71a56f3f1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a57c9796087bacba8e8c4e883f980882(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 164, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[164], dtype='float32'),
            paddle.static.InputSpec(shape=[164], dtype='float32'),
            paddle.static.InputSpec(shape=[164], dtype='float32'),
            paddle.static.InputSpec(shape=[164], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_e24cbc95fb090ef9681d5e1937fd754f(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 800, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[800], dtype='float32'),
            paddle.static.InputSpec(shape=[800], dtype='float32'),
            paddle.static.InputSpec(shape=[800], dtype='float32'),
            paddle.static.InputSpec(shape=[800], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_f96500f1380943bc67b1f56886e95fb3(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 672, 6, 6], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
            paddle.static.InputSpec(shape=[672], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_31a7804c24cf00b89a64598a2aa3f789(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 720, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
            paddle.static.InputSpec(shape=[720], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_6c5ec2ea31615061027a4c86396cf6be(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 960, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
            paddle.static.InputSpec(shape=[960], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_3a411859b7a06d507d70073643073f19(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.99'), float('0.001'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1024, 19, 19], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
            paddle.static.InputSpec(shape=[1024], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_0727011728786dc7406d169be3df53e4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 528, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[528], dtype='float32'),
            paddle.static.InputSpec(shape=[528], dtype='float32'),
            paddle.static.InputSpec(shape=[528], dtype='float32'),
            paddle.static.InputSpec(shape=[528], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_bc261f501c79d13332c58d7ddd9be9c6(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.1'), float('1e-06'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[1, 128, 64, 64], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_18499de8d9e0603581052a3660719c44(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 768, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
            paddle.static.InputSpec(shape=[768], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9b2b4704b181faf4a2a3c3b95962d9fc(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 704, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
            paddle.static.InputSpec(shape=[704], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_51f2ac2cd50c5d1639b2f39112e37803(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 128, 4, 12], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
            paddle.static.InputSpec(shape=[128], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8c5167bc8aa6796e64bc2519c4f3a09c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 31, 31], dtype='float16'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_67829caec9fb9189d7dae66df909adaa(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 38, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[38], dtype='float32'),
            paddle.static.InputSpec(shape=[38], dtype='float32'),
            paddle.static.InputSpec(shape=[38], dtype='float32'),
            paddle.static.InputSpec(shape=[38], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_73d715474df6ef0f0d2778246c30dbba(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1184, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1184], dtype='float32'),
            paddle.static.InputSpec(shape=[1184], dtype='float32'),
            paddle.static.InputSpec(shape=[1184], dtype='float32'),
            paddle.static.InputSpec(shape=[1184], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9d8cb553c35dbdf8a43d27661cc9fc0c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1280, 1, 1], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
            paddle.static.InputSpec(shape=[1280], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_8f3d8a328339bec099dcaac963428d56(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1888, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1888], dtype='float32'),
            paddle.static.InputSpec(shape=[1888], dtype='float32'),
            paddle.static.InputSpec(shape=[1888], dtype='float32'),
            paddle.static.InputSpec(shape=[1888], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_609ec7bf2a07fd26a1254d491f9e4e1e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 262, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[262], dtype='float32'),
            paddle.static.InputSpec(shape=[262], dtype='float32'),
            paddle.static.InputSpec(shape=[262], dtype='float32'),
            paddle.static.InputSpec(shape=[262], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_80088b0ee55d4d921d63acab21fdb431(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1952, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[1952], dtype='float32'),
            paddle.static.InputSpec(shape=[1952], dtype='float32'),
            paddle.static.InputSpec(shape=[1952], dtype='float32'),
            paddle.static.InputSpec(shape=[1952], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_96750a0f5d1bba3bce0b9b30195dd6a9(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 31, 31], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_95f3a13b0cc6136f39a3708c17c63e9c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 288, 16, 16], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
            paddle.static.InputSpec(shape=[288], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_96a1b2b0a9e248c70d83dee2efbe4820(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 4, 112, 112], dtype='float16'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
            paddle.static.InputSpec(shape=[4], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_b260b97a7dd4463e7f763f97d23bbb0c(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 240, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
            paddle.static.InputSpec(shape=[240], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_07db224a34ba25292369d1e9d0d61db2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 64, 32, None], dtype='float16'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
            paddle.static.InputSpec(shape=[64], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_22236779444c94f02ab1a29f69c66e9a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 576, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
            paddle.static.InputSpec(shape=[576], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a22ef7c70c72a5961e472030518977ff(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 96, 31, 31], dtype='float16'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
            paddle.static.InputSpec(shape=[96], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_76d55cfaa6d0232d25592b0177013e3a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 32, 32, 32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c25a8b8f243c9cfd1cd6059f193a96d4(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 32, 35, 35], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
            paddle.static.InputSpec(shape=[32], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_56d19fb8aa02332835fbe6c0ab8ab7f2(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 432, 14, 14], dtype='float16'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
            paddle.static.InputSpec(shape=[432], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_a6a02bcf79acd87c4a2d312c69458e8d(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 88, 28, 28], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
            paddle.static.InputSpec(shape=[88], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d529bf5776f65ea06d894b0b3e3899d7(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1664, 14, 14], dtype='float32'),
            paddle.static.InputSpec(shape=[1664], dtype='float32'),
            paddle.static.InputSpec(shape=[1664], dtype='float32'),
            paddle.static.InputSpec(shape=[1664], dtype='float32'),
            paddle.static.InputSpec(shape=[1664], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_9f7e315e1209de59d7c3d8832718ef93(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 16, 32, 64, 64], dtype='float16'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
            paddle.static.InputSpec(shape=[16], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_31ac0e1776ae9a4a2dc523abd9dcaa95(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 36, 1, 1], dtype='float16'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
            paddle.static.InputSpec(shape=[36], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_c1772c848ed84edbc3c4009987e7f26a(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', False, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 1680, 7, 7], dtype='float32'),
            paddle.static.InputSpec(shape=[1680], dtype='float32'),
            paddle.static.InputSpec(shape=[1680], dtype='float32'),
            paddle.static.InputSpec(shape=[1680], dtype='float32'),
            paddle.static.InputSpec(shape=[1680], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_752486afaea0903136dae08f821d2b70(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 48, 56, 56], dtype='float16'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
            paddle.static.InputSpec(shape=[48], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_d9207fb6e42df320687e1abddc8cfd8e(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 138, 28, 28], dtype='float16'),
            paddle.static.InputSpec(shape=[138], dtype='float32'),
            paddle.static.InputSpec(shape=[138], dtype='float32'),
            paddle.static.InputSpec(shape=[138], dtype='float32'),
            paddle.static.InputSpec(shape=[138], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_efcdcd8cb72d0add1f8e32f942773658(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 140, 7, 7], dtype='float16'),
            paddle.static.InputSpec(shape=[140], dtype='float32'),
            paddle.static.InputSpec(shape=[140], dtype='float32'),
            paddle.static.InputSpec(shape=[140], dtype='float32'),
            paddle.static.InputSpec(shape=[140], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None



class PrimitiveOp_45d43e72ade43b488209dfb3f3e161b1(InstanceTrait, paddle.nn.Layer):
    
    def __init__(self):
        super().__init__()

    def forward(self, arg_0, arg_1, arg_2, arg_3, arg_4):
        input_0 = arg_0
        input_1 = arg_1
        input_2 = arg_2
        input_3 = arg_3
        input_4 = arg_4
        return (lambda x, f: f(x))(paddle._C_ops.batch_norm(input_0, input_1, input_2, input_3, input_4, True, float('0.9'), float('1e-05'), 'NCHW', True, False), lambda out: out if isinstance(out, (list, tuple)) else (out, None,None,None,None,None))

    def get_input_spec(self):
        return [
            paddle.static.InputSpec(shape=[None, 512, 2, 27], dtype='float16'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
            paddle.static.InputSpec(shape=[512], dtype='float32'),
        ]
        
    instance_ = None
    static_instance_with_cinn_ = None
    static_instance_without_cinn_ = None




if __name__ == '__main__':
    unittest.main()